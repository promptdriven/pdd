# TODO

## Features

- [ ] auto reflect
- [ ] run lint for context for better reflection
- [ ] smart includes
- [ ] auto-includes
- [ ] execute shell tools in ```<>``` <shell> </shell>
- [ ] auto split modules into another sub module
- [ ] auto check inconsistencies between modules
- [ ] routlellm
- [ ] online database of examples to reference
- [ ] auto record which model works for each prompt
- [ ] benchmark fix the prompt based on diffs to the code file
- [ ] Use perplexica to automatically find context
- [ ] increase the number of unit tests to generate
- [ ] use context caching during fixing errors
- [ ] debugger that matches prompt lines to code lines
- [ ] auto make new unit tests based on failures
- [ ] auto update the utility
- [ ] github login 
- [ ] Run on server based keys
- [ ] create a marketplace where people can earn LLM credits for their examples or people can put up bounties for examples
- [ ] ability to check in known good examples
- [ ] take a chat conversation and collapse down into a single prompt
- [ ] report a bug and generate the associated unit tests and then have it fix the code via a loop 
- [ ] warming option for temperature increase

## Bugs

- [ ] Use the response to get the token counting to speed up app and make it more accurate
- [ ] Fix change command fixing
- [ ] Continue generation has a bug with example

## Improvements

- [ ] read from a csv file the comment 
- [ ] fix make file production and prepopulate
- [ ] have fix remember what was the closest code passing and start from that to hill climb
- [ ] Analyze errors first before trying to fix
- [ ] Use existing good prompt code example to generate code for the prompt
- [ ] get the last github version of a file when trying to update instead of trying to find an old version
- [ ] prompt to generate setup.py
- [ ] update prompt for makefile

## Documentation

- [ ] Update README with installation instructions
