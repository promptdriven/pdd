<pdd-reason>Interactive model tier selection with cost transparency and strength parameter guidance.</pdd-reason>

<pdd-interface>
{
  "type": "module",
  "module": {
    "functions": [
      {"name": "interactive_selection", "signature": "(validated_providers: List[str]) -> bool", "returns": "bool"}
    ]
  }
}
</pdd-interface>

% You are an expert Python engineer. Your goal is to write the pdd/setup/model_selector.py module.

% Role & Scope
This module provides interactive model tier selection with cost transparency. It groups models by capability/cost tier, displays pricing information, and lets users select which tiers to enable. It explains how --strength controls model selection at runtime.

% Requirements
1. Function: `interactive_selection(validated_providers: List[str]) -> bool` - Guide user through tier selection for each provider
2. Tier classification: Group models into tiers (Fast/Cheap, Balanced, Most Capable) based on cost and ELO
3. Cost display: Show input/output token costs for each tier (per million tokens)
4. Provider iteration: For each validated provider, show available models grouped by tier
5. User selection: Let user choose which tiers to include (default: all) via numbered input
6. Strength explanation: Briefly explain that pdd uses --strength (0.0-1.0) to pick models by cost/quality at runtime
7. CSV filtering: After selection, update llm_model.csv to only include chosen models (or keep all if user selects all)
8. Smart defaults: If user presses Enter without input, include all models for that provider
9. Tier thresholds: Use cost as primary classifier - Cheap: <=$1 input, Balanced: >$1 and <$3 input, Capable: >=$3 input
10. Interactive display: Use rich Console to show formatted tables with model info (name, cost, ELO)

% Dependencies
<llm_model_csv_schema>
The CSV at pdd/data/llm_model.csv has columns:
provider,model,input,output,coding_arena_elo,base_url,api_key,max_reasoning_tokens,structured_output,reasoning_type,location

Example cost ranges:
- Fast/Cheap: gpt-4o-mini ($0.15/$0.6), claude-haiku-4-5 ($1/$5)
- Balanced: claude-sonnet-4-5 ($3/$15), gpt-4o ($5/$15)
- Most Capable: claude-opus-4-5 ($5/$25), gpt-4-turbo ($10/$30)
</llm_model_csv_schema>

% Instructions
- Read llm_model.csv to get all models for validated providers
- Group by provider first, then by tier within each provider
- Display clear table format: "  #  Model                          Input    Output   ELO"
- After showing tiers, prompt: "Include which models? [1,2,3] (default: all):"
- Parse user input (comma-separated numbers or "all")
- If models are filtered out, update CSV by removing those rows (atomic write with temp file)
- Show tip about --strength before starting selections: "Tip: pdd uses --strength (0.0-1.0) to pick models by cost/quality at runtime. Adding all models gives you the full range."
- Return bool indicating whether any changes were made

% Deliverables
- A Python module located at `pdd/setup/model_selector.py`.
- The module must export the following symbol:
  - `interactive_selection`: Guides the user through model tier selection for each provider.
