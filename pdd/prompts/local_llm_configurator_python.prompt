<pdd-reason>Configures local LLMs (Ollama, LM Studio, custom) with auto-detection and user CSV integration.</pdd-reason>

<pdd-interface>
{
  "type": "module",
  "module": {
    "functions": [
      {"name": "configure_local_llm", "signature": "() -> bool", "returns": "bool"}
    ]
  }
}
</pdd-interface>

% You are an expert Python engineer. Your goal is to write the pdd/setup/local_llm_configurator.py module.

% Role & Scope
Guides users through configuring local LLM tools (Ollama, LM Studio, custom endpoints). Local models need a base_url and model name, not API keys. Auto-detects installed models where possible.

% Requirements
1. Function: `configure_local_llm() -> bool` â€” interactive setup, returns True if any models were added
2. Provider menu: 1. LM Studio (default localhost:1234), 2. Ollama (default localhost:11434), 3. Other (custom base URL)
3. Ollama auto-detection: query http://localhost:11434/api/tags, show discovered models, let user select which to add (comma-separated). Fall back to manual entry if unreachable.
4. LM Studio: default base URL http://localhost:1234/v1, prompt for model name
5. Append rows to user's `~/.pdd/llm_model.csv` with LiteLLM prefix conventions (`lm_studio/`, `ollama_chat/`), empty api_key, cost=0.0
6. Validate base URL format (http/https required)
7. Atomic CSV writes; create user CSV with header if it doesn't exist
8. Handle empty input as cancel

% Dependencies
<llm_model_csv_schema>
The user's CSV at ~/.pdd/llm_model.csv has columns:
provider,model,input,output,coding_arena_elo,base_url,api_key,max_reasoning_tokens,structured_output,reasoning_type,location

Example local rows:
- lm_studio,lm_studio/openai-gpt-oss-120b-mlx-6,0,0,1000,http://localhost:1234/v1,,0,True,none,
- Ollama,ollama_chat/llama3:70b,0,0,1000,http://localhost:11434,,0,True,none,
</llm_model_csv_schema>

% Deliverables
- Module at `pdd/setup/local_llm_configurator.py` exporting `configure_local_llm`.
