% You are an expert Python Software Engineer. Your goal is to write a Python function, "change", that will change a prompt according to specified modifications.

<include>./context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        'input_prompt' - A string containing the prompt that will be modified.
        'input_code' - A string containing the code that was generated from the input_prompt.
        'change_prompt' - A string containing the instructions of how to modify the input_prompt.
        'strength' - A float between 0 and 1 that represents the strength of the LLM model to use.
        'temperature' - A float that represents the temperature parameter for the LLM model.
        'time' - A float between 0 and 1 that controls the thinking effort for the LLM model, passed to llm_invoke. Default is DEFAULT_TIME.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs:
        'modified_prompt' - A string containing the modified prompt that was changed based on the change_prompt.
        'total_cost' - A float representing the total cost of running the function.
        'model_name' - A string representing the name of the LLM model used.

% Here is how to use the internal modules:
    <internal_modules>
        % Here is an example how to preprocess the prompt from a file using an internal module: 
        <preprocess_example>
            <include>context/preprocess_example.py</include>
        </preprocess_example>

        % For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        % For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>
    </internal_modules>

% This program will do the following:
    Step 1. Load the 'change_LLM' and 'extract_prompt_change_LLM' prompt templates.
    Step 2. Preprocess the change_LLM prompt using the preprocess function from the preprocess module and set double_curly_brackets to false.
    Step 3. Run the change prompt through the model using llm_invoke, passing the 'time' input parameter.
        3a. Pass the following string parameters to the prompt during invoke:
            - 'input_prompt'
            - 'input_code'
            - 'change_prompt' (preprocess with recursive and double_curly_brackets set to false)
        3b. If verbose is True, pretty print a message letting the user know it is running and the cost information.
    Step 4. If verbose is True, pretty print the markdown formatting that is present in the result via the rich Markdown function.
    Step 5. Run the extract prompt through llm_invoke with the EXTRACTION_STRENGTH constant, passing the 'time' input parameter.
        5a. Pass the following string parameters to the prompt during invoke:
            - 'llm_output': The result from Step 3
        5b. If verbose is True, pretty print a message letting the user know it is running and the cost information.
        5c. The Pydantic output will contain the 'modified_prompt' key.
    Step 6. If verbose is True, pretty print the extracted modified prompt using the rich Markdown function.
    Step 7. Return the modified_prompt string, total_cost float, and model_name string.

% Ensure that the function handles potential errors gracefully, such as missing input parameters, issues with the LLM model responses, or missing keys in the output.