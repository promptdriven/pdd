% You are an expert Python engineer. Your goal is to write a python function called 'postprocess' that will post-process the string output of a LLM so that the code can be run.

% Here are the inputs and outputs of the function:
    Inputs:
        'llm_output' - A string contains a mix of text and sections of code separated by triple backticks generated by a LLM.
        'language' - A string that is the type (e.g. python, bash) of file that will be outputed by the LLM.
        'strength' - A string that is the strength of the LLM model to use for the post-processing. Default is 0.9.
        'temperature' - A string that is the temperature of the LLM model to use for the post-processing. Default is 0.
    Outputs:
        'extracted_code' - string that is the processed string that contains a properly commented out comments so that code can be run.
        'total_cost' - float that is the total cost of the post-processing function. This is an optional output.

% Here is an example of a Langchain LCEL program: ```<./context/langchain_lcel_example.py>```

% Here is an example how to select the Langchain llm and count tokens: ```<./context/llm_selector_example.py>```

% Here is an example how to use postprocess_0 which is a zero cost postprocessing: ```<./context/postprocess_0_example.py>```

% This function will do the following:
    Step 1. If strength is 0, use postprocess_0 generate the extracted_code. Return extracted_code and total_cost of 0.
    Step 2. Otherwise, use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/extract_code_LLM.prompt' file.
    Step 3. Create a Langchain LCEL template from extract_code_LLM prompt so that it returns a JSON output.
    Step 4. Use the llm_selector function for the LLM model.
    Step 5. Run the code through the model using Langchain LCEL.
        5a. Pass the following string parameters to the prompt during invoke:
            - 'llm_output'
            - 'language'
        5b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.
        5c. The dictionary output of the LCEL will have the key 'extracted_code' that contains the processed code string. Be sure to access this key using the get method with a default error message.
        5d. If the first and last line have triple backticks delete the entire first and last line. There will be the name of the language after the first triple backticks and that should be removed as well.
        5e. Pretty print the extracted_code using the rich Markdown function. Also, print the number of tokens in the result, the output token cost and the total_cost.
    Step 6. Return the 'extracted_code' string from the JSON and the total_cost.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.