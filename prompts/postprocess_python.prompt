% You are an expert Python Software Engineer. Your goal is to write a python function, "postprocess", that will extract code from a string output of an LLM. All output to the console will be pretty printed using the Python rich library.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        'llm_output' - A string containing a mix of text and code sections.
        'language' - A string specifying the programming language of the code to be extracted.
        'strength' - A float between 0 and 1 that represents the strength of the LLM model to use. Default is DEFAULT_STRENGTH.
        'temperature' - A float between 0 and 1 that represents the temperature parameter for the LLM model. Default is 0.
        'time' - A float between 0 and 1 that controls the thinking effort for the LLM model, passed to llm_invoke. Default is DEFAULT_TIME.
        'verbose' - A boolean that indicates whether to print detailed processing information. Default is False.
    Outputs as a tuple:
        'extracted_code' - A string containing the extracted and processed code.
        'total_cost' - A float representing the total cost of running the function.
        'model_name' - A string representing the model name used for extraction.

% Here is how to use the internal modules:
    <internal_modules>
        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>
    </internal_modules>

% This function will do the following:
    Step 1. If strength is 0, use postprocess_0 function to extract code and return (extracted_code, 0.0, "simple_extraction").
    Step 2. Load the 'extract_code_LLM.prompt' template file.
    Step 3. Process the text using llm_invoke, to which the function's input `strength`, `temperature`, and `time` parameters will be passed:
        3a. Pass the following parameters to the prompt template (via `input_json` for `llm_invoke`):
            - 'llm_output'
            - 'language'
        3b. The Pydantic output model (ExtractedCode) will contain 'focus', 'explanation', and 'extracted_code' keys to match the extract_code_LLM.prompt schema. Only the 'extracted_code' value is used in the return.
        3c. For the extracted_code, if the first and last line have triple backticks delete the entire first and last line. There will be the name of the language after the first triple backticks and that should be removed as well.
    Step 4. Return the extracted code string, total cost float and model name string.