% You are an expert Python Software Engineer. Your goal is to write a python function, "fix_code_module_errors", that will fix errors in a code module that caused a program to crash and/or have errors.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        'program' - A string containing the program code that was running the code module.
        'prompt' - A string containing the prompt that generated the code module.
        'code' - A string containing the code module that caused the crash.
        'errors' - A string that contains the errors from the program run.
        'strength' - A float between 0 and 1 that is the strength of the LLM model to use.
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs:
        'update_program': Boolean indicating whether the program needs to be updated.
        'update_code': Boolean indicating whether the code module needs to be updated.
        'fixed_program' - A string that is the fixed program.
        'fixed_code' - A string that is the fixed code module.
        'program_code_fix' - A string that is the raw output of the first LLM invoke which attempts to fix the program crash.
        'total_cost' - A float that is the total cost of the run.
        'model_name' - A string that is the name of the selected LLM model.

% Here is how to use the internal modules:
    <internal_modules>
        % For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        % For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>
    </internal_modules>

% This program will do the following:
    Step 1. Load the 'fix_code_module_errors_LLM' and 'extract_program_code_fix_LLM' prompt templates.
    Step 2. Run the code through the model using llm_invoke with the provided strength and temperature. 
        2a. Pass the following string parameters to the prompt during invoke:
            - 'program'
            - 'prompt'
            - 'code'
            - 'errors'
        2b. If verbose is True, pretty print a message letting the user know it is running and how many tokens are in the prompt and the cost.
    Step 3. If verbose is True, pretty print the markdown formatting that is present in the result via the rich Markdown function. Also pretty print the number of tokens in the result and the cost.
    Step 4. Run a second llm_invoke with the EXTRACTION_STRENGTH constant and the provided temperature to extract the fixes. 
        4a. Pass the following string parameters to the prompt during invoke:
            - 'program_code_fix': This is the result of the first llm_invoke
            - 'program'
            - 'code'
        4b. If verbose is True, pretty print a message letting the user know it is running and how many tokens are in the prompt and the cost.
    Step 5. Extract the following values from the Pydantic result:
        - 'update_program'
        - 'update_code'
        - 'fixed_program'
        - 'fixed_code'
    Step 6. If verbose is True, print the total cost of both runs.
    Step 7. Return update_program, update_code, fixed_program, fixed_code, program_code_fix, total_cost, and model_name.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.