% Here is an example prompt and associated generated example code to be used as a reference for the generation of code for the prompt below:
<example_generation>
   <example_prompt><include>context/generate/8/llm_invoke_python.prompt</include></example_prompt>
   <example_code><include>context/generate/8/llm_invoke.py</include></example_code>
</example_generation>

% You are an expert Python engineer. Your goal is to write a python function, "llm_invoke", that will run a prompt with a given input. The entire code needed for this will be in a single file called llm_invoke.py. You will use the Langchain cache.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Input:
        'prompt' - String with the prompt to be used for the LLM model.
        'input_json' - JSON object with the input to be used for the LLM model.
        'strength' - Floating point number with 0 being the cheapest model, 0.5 being the base model and 1 being the model with the highest ELO score.
        'temperature' - Floating point number indicating the temperature of the LLM.
        'verbose' - Boolean indicating whether to print extra information.
        'output_pydantic' - Optional. This specifies a pydantic_object output format from the LLM model. If not given, the output will be a string.
    Output (dictionary):
        'result' - Output from the LLM model.
        'cost' - Cost of the invoke run.
        'model_name' - Name of the selected model.
        
% Here is an example of a Langchain LCEL program: <langchain_lcel_example><include>context/langchain_lcel_example.py</include></langchain_lcel_example>
Be sure to use the correct imports as shown in the above example.

% Here is an example (but not actual values) of the llm_model.csv: <llm_model_example><shell>head -6 data/llm_model.csv</shell></llm_model_example>

% Here are the rules to follow when selecting the appropriate model:
    - If environmental variable $PDD_MODEL_DEFAULT is set, use that as the base model, otherwise it is "gpt-4o-mini". If $PDD_PATH is set load $PDD_PATH/data/llm_model.csv, otherwise, assume $PDD_PATH is current working directory.
    - When strength < 0.5 use strength to interpolate based on the average cost of input and output tokens from the base model down according to cost of the cheapest model and select the model with the closest average cost. If calling the model fails during invoke, use the next cheapest model, and repeat until a model is found that works.
    - When strength > 0.5, use strength to interpolate based on ELO up from the base model to the highest ELO model and select the model with the closest ELO score. If calling the model fails during invoke, use the next highest ELO model, and repeat until a model is found that works.
    - Use base model when strength is 0.5. If calling the model fails during invoke, use the next highest ELO model, and repeat until a model is found that works.

% To speed up model failure retries, check to see if the environment variable for the model is set before trying to invoke the model.

% CRITICAL: For models that support 'structured_output', use the '.with_structured_output()' when a desired Pydantic output is requested. Use PydanticOutputParser instead of JsonOutputParser for better Pydantic handling.

% CRITICAL: When handling model token limits:
    - For Google providers (e.g. 'google' and 'googlevertexai'), use max_output_tokens instead of max_tokens
    - When max_completion_tokens is available, set it in model_kwargs
    - Otherwise, use max_tokens if available
    - For OpenAI models with a base_url, set it during model initialization
    - For OpenAI models starting with 'o' and not containing 'mini' in their name, set model_kwargs={'reasoning_effort':'high'}

% If verbose is set to True, print the following information:
    - The selected model.
    - The per input and output token cost (in dollars per million tokens) of the selected model and the number of input and output tokens.
    - The cost of the invoke run.
    - The strength used.
    - The temperature used.
    - The input JSON (use try-except to handle rich printing failures, falling back to standard print).
    - The optional Pydantic output format.
    - The result using rprint.

% CRITICAL: In the `CompletionStatusHandler`'s `on_llm_end` method, ensure robust token usage retrieval.  Check for `usage_metadata` in `generation.message`. If it's missing or `None`, check within `generation.generation_info`. If found nowhere, default both `input_tokens` and `output_tokens` to 0.  This handles different response structures from various LLM providers.  Prioritize `generation.message.usage_metadata` if it exists.
```