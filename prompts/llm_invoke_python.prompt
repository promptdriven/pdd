% You are an expert Python engineer. Your goal is to write a python function, "llm_invoke", that will run a prompt with a given input. The entire code needed for this will be in a single file called llm_invoke.py. You will use the Langchain cache.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Input:
        'prompt' - String with the prompt to be used for the LLM model.
        'input_json' - JSON object with the input to be used for the LLM model.
        'strength' - Floating point number with 0 being the cheapest model, 0.5 being the base model and 1 being the model with the highest ELO score.
        'temperature' - Floating point number indicating the temperature of the LLM.
        'verbose' - Boolean indicating whether to print extra information.
        'output_pydantic' - Optional. This specifies a pydantic_object output format from the LLM model. If not given, the output will be a string.
        'time' - Optional. Floating point number between 0 and 1 representing the relative amount of thinking tokens to allocate (0=none, 1=max). Default is 0.25.
    Output (dictionary):
        'result' - Output from the LLM model.
        'cost' - Cost of the invoke run.
        'model_name' - Name of the selected model.
        'thinking_output' - Optional. The reasoning or thinking process output from the model, if available.
        
% The function should properly validate inputs and handle errors:
    - Validate that prompt is a non-null string and input_json is a non-null dictionary.
    - Handle prompt template errors separately from model invocation errors.
    - For prompt template errors, raise a ValueError with a clear message.
    - For model invocation errors, try each candidate model and raise Runtime errors only if all fail.
    - Provide detailed error messages in verbose mode.
    
% Here is an example of a Langchain LCEL program: <langchain_lcel_example><include>context/langchain_lcel_example.py</include></langchain_lcel_example>
Be sure to use the correct imports as shown in the above example.

% Here is an example (but not actual values) of the llm_model.csv: <llm_model_example><shell>head -6 data/llm_model.csv</shell></llm_model_example>

% The code must handle the following LLM providers using their Langchain integrations:
    - OpenAI (ChatOpenAI): Standard and Azure-hosted models
    - Anthropic (ChatAnthropic): Including Claude models
    - Google (ChatGoogleGenerativeAI): For Google's standard API
    - Google Vertex AI (ChatVertexAI): For models hosted on Google Vertex AI
    - Ollama (OllamaLLM): For local open-source models
    - Fireworks (Fireworks): For Fireworks-hosted models
    - Together (Together): For Together-hosted models
    - Groq (ChatGroq): For Groq-hosted models
    - MLX (ChatMLX): For MLX-hosted models
    - AWS (ChatBedrockConverse): For AWS-hosted models
    Each provider may have specific initialization parameters that should be set accordingly.

% Here are the rules to follow when selecting the appropriate model:
    - If environmental variable $PDD_MODEL_DEFAULT is set, use that as the base model, otherwise it is "gpt-4.1-nano". If $PDD_PATH is set load $PDD_PATH/data/llm_model.csv, otherwise, assume $PDD_PATH is current working directory.
    - When strength < 0.5 use strength to interpolate based on the average cost of input and output tokens from the base model down according to cost of the cheapest model and select the model with the closest average cost. If calling the model fails during invoke, use the next cheapest model, and repeat until a model is found that works.
    - When strength > 0.5, use strength to interpolate based on ELO up from the base model to the highest ELO model and select the model with the closest ELO score. If calling the model fails during invoke, use the next highest ELO model, and repeat until a model is found that works.
    - Use base model when strength is 0.5. If calling the model fails during invoke, use the next highest ELO model, and repeat until a model is found that works.
    - Only consider models that have valid API keys available (check environment variables based on the api_key field from the CSV).
    - Filter models first based on availability, then sort the candidates by how close they match the target cost (for strength < 0.5) or target ELO (for strength > 0.5).
    - For test environments (where api_key is "EXISTING_KEY"), use a special logic that respects the mock model setup.

% To speed up model failure retries, check to see if the environment variable for the model is set before trying to invoke the model.

% CRITICAL: For models that support 'structured_output', use the '.with_structured_output()' when a desired Pydantic output is requested. Use PydanticOutputParser instead of JsonOutputParser for better Pydantic handling.

% CRITICAL: When handling model token limits:
    - For Google providers (e.g. 'google' and 'googlevertexai'), use max_output_tokens instead of max_tokens
    - When max_completion_tokens is available, set it in model_kwargs
    - Otherwise, use max_tokens if available
    - For OpenAI models with a base_url, set it during model initialization
    - For OpenAI models starting with 'o', use the 'time' parameter to dynamically set reasoning effort. Map the 0-1 'time' range to the 'effort' setting (e.g., 0 could map to 'low', 0.25 to 'medium', 0.75 to 'high', 1 to 'extreme' - adjust mapping as needed).
    - For Anthropic 'claude-3-7-sonnet' models, use the 'time' parameter to dynamically allocate the thinking token budget. Calculate the budget based on the maximum supported (e.g., if max is 4000, time=0.5 would allocate 2000 tokens). Default (time=0.25) corresponds to a proportional allocation.
    - The default 'time' value of 0.25 should be used if the parameter is not provided.

% If verbose is set to True, print the following information:
    - The selected model.
    - The per input and output token cost (in dollars per million tokens) of the selected model and the number of input and output tokens.
    - The cost of the invoke run.
    - The strength used.
    - The temperature used.
    - The time used.
    - The input JSON (use try-except to handle rich printing failures, falling back to standard print).
    - The optional Pydantic output format.
    - The result using rprint.

% CRITICAL: In the `CompletionStatusHandler`'s `on_llm_end` method, ensure robust token usage retrieval by implementing this logic:
1. First, check if `generation.message` exists and has `usage_metadata`
2. If not, fall back to checking `generation.generation_info` for usage metadata
3. Default both `input_tokens` and `output_tokens` to 0 if no usage data is found in either location
4. The handler should also capture the finish_reason from generation_info when available