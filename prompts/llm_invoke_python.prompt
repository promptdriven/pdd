% You are an expert Python engineer. Your goal is to write a python function, "llm_invoke", that will run a prompt with a given input using the LiteLLM library. The function will also handle fetching and saving missing API keys interactively. The entire code needed for this will be in a single file called llm_invoke.py.

<include>context/python_preamble.prompt</include>

% Dependencies:
    - `python-dotenv` for managing environment variables from a `.env` file.
    - `litellm` for LLM provider abstraction.
    - `pandas` for reading the model CSV.
    - `pydantic` for structured output validation.

% Startup Behavior:
    - Load environment variables from a `.env` file located at the project root.
    - Project root detection: (1) `$PDD_PATH` if pointing to a real project (not the installed package path); (2) otherwise search upward from CWD for markers (.git, pyproject.toml, data/, .env); (3) fallback to CWD.
    - Resolve `llm_model.csv` in priority order: (a) `~/.pdd/llm_model.csv`; (b) `<PROJECT_ROOT>/.pdd/llm_model.csv` if `$PDD_PATH` points to a real project; (c) `<cwd>/.pdd/llm_model.csv`; (d) packaged `pdd/data/llm_model.csv`.

% Logging Configuration:
    - Use Python's standard logging module with a hierarchical logger named "pdd.llm_invoke".
    - Configure a separate logger for LiteLLM named "litellm".
    - Environment variables: PDD_LOG_LEVEL (default: "INFO"), LITELLM_LOG_LEVEL (default: "WARNING" in production), PDD_ENVIRONMENT ("production" reduces to WARNING), PDD_VERBOSE_LOGGING=1 enables DEBUG.
    - Provide `setup_file_logging()` for rotating file handlers (10MB, 5 backups) and `set_verbose_logging()` to toggle DEBUG level.
    - Set `litellm.drop_params` based on `LITELLM_DROP_PARAMS` env var (default: true) to ignore unsupported provider parameters.

% Function Signature:
    llm_invoke(
        prompt: Optional[str] = None,
        input_json: Optional[Union[Dict, List[Dict]]] = None,
        strength: float = 0.5,
        temperature: float = 0.1,
        verbose: bool = False,
        output_pydantic: Optional[Type[BaseModel]] = None,
        output_schema: Optional[Dict] = None,
        time: float = 0.25,
        use_batch_mode: bool = False,
        messages: Optional[Union[List[Dict], List[List[Dict]]]] = None,
        language: Optional[str] = None,
    ) -> Dict[str, Any]

% Inputs:
    - 'prompt': String prompt template with {variable} placeholders.
    - 'input_json': Dict or list of dicts for template variables. List triggers batch mode.
    - 'strength': 0=cheapest model, 0.5=base model, 1=highest ELO.
    - 'temperature': LLM temperature.
    - 'verbose': Enable detailed logging.
    - 'output_pydantic': Pydantic model class for structured output.
    - 'output_schema': Raw JSON schema dict (alternative to output_pydantic).
    - 'time': Thinking effort (0-1, default 0.25). Interpreted per model's reasoning_type in CSV.
    - 'use_batch_mode': Use litellm.batch_completion if True.
    - 'messages': Pre-formatted messages (ignores prompt/input_json if provided).
    - 'language': Language hint (e.g., "python", "javascript"). Skip Python-specific validation when not "python" or None.

% Output (dictionary):
    - 'result': String or Pydantic object (or list if batch mode).
    - 'cost': Total cost via LiteLLM callback and litellm.completion_cost, with CSV rate fallback.
    - 'model_name': Name of the selected model.
    - 'thinking_output': Reasoning output if available.

% Model Selection:
    - Base model: `$PDD_MODEL_DEFAULT` or `DEFAULT_LLM_MODEL` constant from `pdd/__init__.py`.
    - CSV columns: model, provider, input/output costs ($/M tokens), coding_arena_elo, api_key, structured_output, reasoning_type, max_reasoning_tokens, optional 'location' for Vertex AI override.
    - Filter by api_key presence. Skip interactive check if api_key is empty or "EXISTING_KEY".
    - strength < 0.5: Interpolate by cost from base down to cheapest.
    - strength > 0.5: Interpolate by ELO from base up to highest.
    - strength = 0.5: Use base model, fallback to next highest ELO on failure.
    - Soft fallback: If base model not in CSV, use first available model as surrogate and log warning.

% API Key Management:
    - Check `os.getenv(key_name)` for the required API key.
    - If missing and `PDD_FORCE` env var is set, skip model (non-interactive mode).
    - Otherwise, prompt user via `input()`, sanitize key, set env var, save to .env file.
    - Replace existing key in-place (no comment + append); remove old commented versions.
    - Log security warning about key saved to .env file.
    - Mark key as newly acquired for retry logic on auth errors.

% Reasoning Parameters:
    - CSV columns: reasoning_type ('budget', 'effort', 'none'), max_reasoning_tokens.
    - 'budget': Calculate budget_tokens = int(time * max_reasoning_tokens), pass provider-specific thinking parameters (e.g., Anthropic 'thinking' param).
    - 'effort': Map time to "low"/"medium"/"high", pass reasoning_effort or provider-specific params.
    - 'none': No reasoning parameters.

% LLM Invocation:
    - For OpenAI gpt-5* models: Call litellm.responses() API to support 'reasoning' parameter. Build text.format block for structured output.
    - For other models: Call litellm.completion() or litellm.batch_completion().
    - Pass: model, messages, temperature, num_retries=2.
    - Do NOT set max_tokens/max_completion_tokens; rely on provider defaults.
    - Anthropic with thinking enabled: Force temperature=1.

% Structured Output:
    - Check 'structured_output' CSV flag.
    - Use response_format with type='json_schema', json_schema containing name, schema, and strict=True to enforce all required fields.
    - LM Studio: Use extra_body to bypass drop_params stripping the schema.
    - Groq: Use simple json_object mode with schema instruction in system prompt.
    - If model doesn't support structured output, parse response as JSON and validate.

% Response Processing:
    - Extract result from response.choices[0].message.content.
    - Handle None content: Retry with cache bypass (modify prompt slightly).
    - Handle malformed JSON (excessive trailing newlines causing truncation): Retry with cache bypass.
    - Parse JSON strings: Try fenced ```json blocks first, then balanced JSON object extraction, then fence cleaning.
    - Repair truncated JSON by attempting various closures.
    - Unescape double-escaped newlines in code fields (_smart_unescape_code preserves \n inside string literals).
    - Repair Python syntax errors (trailing quotes, etc.) via _repair_python_syntax.
    - Skip known prose field names (reasoning, explanation, analysis, etc.) during Python validation.
    - If invalid Python code remains after repair and language is "python" or None, retry with cache bypass.
    - Extract thinking output from response._hidden_params['thinking'] or response.choices[0].message.reasoning_content.

% Retry and Fallback:
    - Define SchemaValidationError exception that triggers model fallback.
    - On auth error with newly acquired key: Re-prompt for same key and retry.
    - On schema validation failure: Raise SchemaValidationError to trigger fallback to next model.
    - Preserve provider credentials in retries: Pass vertex_credentials, vertex_project, vertex_location, api_key, base_url from original call.
    - Anthropic temperature auto-adjustment: If error mentions temperature+thinking, adjust and retry.
    - After exhausting all candidates: Raise RuntimeError.

% Vertex AI Support:
    - Detect Vertex AI models by provider or model name prefix.
    - Load credentials from VERTEX_CREDENTIALS file path, VERTEX_PROJECT, VERTEX_LOCATION env vars.
    - Per-model location override from CSV 'location' column.
    - Pass vertex_credentials (as JSON string), vertex_project, vertex_location to litellm.

% LiteLLM Caching:
    - Prefer S3-compatible cache targeting GCS using HMAC keys (GCS_HMAC_ACCESS_KEY_ID, GCS_HMAC_SECRET_ACCESS_KEY, GCS_BUCKET_NAME).
    - Fall back to disk-based SQLite cache at <PROJECT_ROOT>/litellm_cache.sqlite.
    - Respect LITELLM_CACHE_DISABLE=1 to disable caching entirely.
    - Restore cache state after retries.

% LiteLLM Callback:
    - Register a success callback with litellm.success_callback.
    - Extract token usage and finish reason.
    - Calculate cost using litellm.completion_cost(); fall back to CSV rates (_MODEL_RATE_MAP) if LiteLLM fails.
    - Store in module-level _LAST_CALLBACK_DATA.

% WSL Diagnostics:
    - Detect WSL via /proc/version or WSL_DISTRO_NAME env var.
    - Sanitize API keys by trimming whitespace and control characters.
    - On auth errors in WSL, warn about carriage returns in API keys.

% CSV Example: <llm_model_example><shell>head -6 data/llm_model.csv</shell></llm_model_example>
% LiteLLM References:
    <litellm_thinking_example><web>https://docs.litellm.ai/docs/reasoning_content</web></litellm_thinking_example>
    <litellm_json_mode_example><web>https://docs.litellm.ai/docs/completion/json_mode</web></litellm_json_mode_example>
    <litellm_batch_mode_example><web>https://docs.litellm.ai/docs/completion/batching</web></litellm_batch_mode_example>
    <litellm_exception_mapping_example><web>https://docs.litellm.ai/docs/exception_mapping</web></litellm_exception_mapping_example>
