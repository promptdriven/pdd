% You are an expert Python engineer. Your goal is to write a python function, "llm_invoke", that will run a prompt with a given input using the LiteLLM library. The function will also handle fetching and saving missing API keys interactively. The entire code needed for this will be in a single file called llm_invoke.py.

<include>context/python_preamble.prompt</include>

% Dependencies:
    - This function requires the `python-dotenv` library for managing environment variables from a `.env` file.

% Startup Behavior:
    - Upon initialization or first call, the function should attempt to load environment variables from a `.env` file located at the project root.
    - Project root detection and CSV resolution:
        * Determine project root by: (1) checking `$PDD_PATH` and using it if it points to a real project directory; (2) otherwise, search upwards from the current working directory (cwd) for project markers (like `.git`, `pyproject.toml`, `data/`, `.env`); (3) if no markers are found, default to the cwd.
        * Resolve `llm_model.csv` in this priority order: (a) `~/.pdd/llm_model.csv` if present; (b) `<PROJECT_ROOT>/.pdd/llm_model.csv` when `$PDD_PATH` points to a real project (not the installed package path); (c) `<cwd>/.pdd/llm_model.csv`; (d) otherwise use the packaged `pdd/data/llm_model.csv`.

% Logging Configuration:
    - Use Python's standard logging module with a hierarchical logger structure.
    - Create a primary logger named "pdd.llm_invoke" for the module.
    - Configure a separate logger for LiteLLM's logs named "litellm".
    - Control logging verbosity through environment variables:
        * PDD_LOG_LEVEL: Controls the module's default logging level (default: "INFO")
        * LITELLM_LOG_LEVEL: Controls the LiteLLM logger's level (default: "WARNING" in production)
        * PDD_ENVIRONMENT: When set to "production", automatically reduces logging to WARNING level
        * PDD_VERBOSE_LOGGING: When set to "1", enables DEBUG level regardless of other settings
    - Provide helper functions:
        * setup_file_logging(): Configure rotating file handlers with size limits (10MB) and backups (5)
        * set_verbose_logging(): Toggle DEBUG level based on the verbose flag or environment variable
    - Replace all print/rprint statements with appropriate logger calls (logger.debug, logger.info, logger.warning, logger.error)
    - In production environments, use WARNING level by default to reduce log output
    - For development, use more verbose INFO level by default
    - Drop unsupported provider params: set `litellm.drop_params` based on environment variable `LITELLM_DROP_PARAMS` (default: true) to gracefully ignore provider-unsupported parameters (e.g., `reasoning_effort`).

% Here are the inputs and outputs of the function:
    Input:
        'prompt' - String with the prompt template to be used for the LLM model.
        'input_json' - JSON object with the input variables OR **a list of JSON objects** if `use_batch_mode` is True.
        'strength' - Floating point number with 0 being the cheapest model, 0.5 being the base model and 1 being the model with the highest ELO score.
        'temperature' - Floating point number indicating the temperature of the LLM.
        'verbose' - Boolean indicating whether to print extra information.
        'output_pydantic' - Optional. This specifies a pydantic_object output format from the LLM model. If not given, the output will be a string.
        'time' - Optional. Floating point number between 0 and 1 representing the relative amount of thinking effort requested. Default is 0.25. How this is interpreted depends on the selected model's 'reasoning_type' specified in the CSV.
        'use_batch_mode' - Optional. Boolean indicating whether to use batch processing if supported by the model/provider via LiteLLM (default: False).
    Output (dictionary):
        'result' - Output from the LLM model (string or Pydantic object) OR **a list of outputs** if `use_batch_mode` was True.
        'cost' - Total cost of the invoke run, calculated via a LiteLLM callback and the `litellm.completion_cost` function.
        'model_name' - Name of the selected model used by LiteLLM.
        'thinking_output' - Optional. The reasoning or thinking process output from the model, if available in the LiteLLM response.
        
% Function Logic Flow:
    1. **Configure Logging:** Initialize loggers (pdd.llm_invoke and litellm) with appropriate levels based on environment variables and the verbose flag.
    2. **Load Environment:** Load `.env` file using `dotenv.load_dotenv()`.
    3. **Input Handling:** Determine if using provided `messages` or generating from `prompt` + `input_json`.
    4. **Model Selection:** Based on `strength`, filter models from CSV (retain rows where `api_key` is not NaN; do not pre-check env presence here), calculate target cost/ELO, sort candidates, and select the top candidate. Actual environment presence of the key is handled in Step 5.
    5. **API Key Check & Acquisition (CRITICAL):**
        a. Get the required API key environment variable *name* (e.g., "OPENAI_API_KEY") from the `api_key` column in the CSV for the selected model.
        b. Check if `os.getenv(key_name)` returns a value (i.e., if the key is actually set in the environment).
        c. **If the key is MISSING:**
            i. Prompt the user interactively using `input(f"Please enter the API key for {key_name}: ")`.
            ii. Basic validation: Ensure the user provided some input (not empty string).
            iii. Set the environment variable for the current process: `os.environ[key_name] = user_provided_key`.
            iv. **Update `.env` file:** 
                - Read the contents of the `.env` file (in the project root).
                - Create a new list of lines.
                - Iterate through the original lines: If a line starts with `{key_name}=` (or `{key_name} =`), prepend it with `# ` (comment it out) before adding to the new list. Otherwise, add the line as is.
                - Append the new key entry to the new list: `f'{key_name}="{user_provided_key}"\n'`.
                - Write the entire new list of lines back to the `.env` file, overwriting its contents.
            v. **Security Warning:** Print a message warning the user that the key has been saved to the `.env` file and to ensure this file is kept secure and not committed to version control.
            vi. **Mark key as newly acquired:** Set a flag indicating that the key for this `key_name` was just obtained from the user in this session.
        d. **If the key EXISTS:** Proceed silently. Ensure the "newly acquired" flag for this key is false.
    5.X. **Process `time` parameter for Reasoning:**
        a. Retrieve the `reasoning_type` (e.g., 'none', 'budget', 'effort') and `max_reasoning_tokens` (integer, nullable) for the selected model from the loaded CSV data.
        b. Initialize an empty dictionary for `model_specific_params`.
        c. If `reasoning_type` is 'budget':
            i. If `max_reasoning_tokens` is a positive integer:
                1. Calculate `budget_tokens = int(time * max_reasoning_tokens)`.
                2. If `budget_tokens > 0`, add the appropriate provider-specific parameter: for Anthropic, set `thinking = {'type': 'enabled', 'budget_tokens': budget_tokens}`; for unknown providers, omit if no known adapter mapping exists. Log this action if verbose.
        d. Else if `reasoning_type` is 'effort':
            i. Map `time` to a qualitative category: "low" (0.0-0.3), "medium" (>0.3-0.7), "high" (>0.7-1.0).
            ii. Provider mappings: for OpenAI gpt-5* (Responses API), set `reasoning = {'effort': mapped_category, 'summary': 'auto'}`; otherwise use `reasoning_effort = mapped_category` when supported by the provider adapter. Log this action if verbose.
        e. If `reasoning_type` is 'none' or conditions for 'budget'/'effort' are not met, no reasoning-specific parameters are added.
    6. **LLM Invocation:**
        - For OpenAI gpt-5* models, call the OpenAI Responses API (not LiteLLM completion) to support reasoning parameters. Responses API does not accept `temperature`; log when it is skipped.
        - Otherwise, call `litellm.completion` or `litellm.batch_completion` with the selected model, formatted messages, other parameters, and any `model_specific_params` derived from the `time` parameter.
    7. **Retry Logic:** 
        a. **On Invocation Failure:** If the call in Step 5 fails:
            i. **Check for Authentication Error:** If the exception is an `openai.AuthenticationError` (LiteLLM maps errors to OpenAI types) AND the "newly acquired" flag for the current `key_name` is true:
                - Print a message indicating the provided key for `{key_name}` was likely incorrect.
                - **Go back to Step 4.c.i** to re-prompt the user for the *same* `{key_name}`.
                - After obtaining and saving the new key, **retry Step 5** with the *same* model.
            ii. **Other Errors or Existing Key Failure:** If the error is not an `AuthenticationError`, or if the key was already present (not newly acquired), proceed to try the next model:
                - Get the next candidate model from the sorted list generated in Step 3.
                - If no more candidate models exist, raise the last encountered exception.
                - If a next model exists, **go back to Step 4.a** to check/acquire the API key for this *new* model.
        b. **Special diagnostics:** In WSL environments, detect and warn on authentication errors likely caused by stray carriage returns in API keys; suggest re-entering/sanitizing keys.
        c. **Cache-bypass retry for corrupted content:** If a successful response contains `None` content (likely corrupted cache), retry once with a slightly modified prompt to bypass cache; restore cache afterward.
    8. **Response Handling:** Process the LiteLLM response, extract results, cost, tokens, etc.
        a. **Thinking Output:** Extract reasoning/thinking output if available. **Check common locations within the LiteLLM response object, such as `response._hidden_params['thinking']` or potentially within `response.choices[0].message` attributes (like `reasoning_content`), handling potential `AttributeError` or `KeyError` exceptions gracefully.**
    9. **Return Output:** Return the standard output dictionary.

% The function should properly validate inputs and handle errors:
    - Validate inputs based on the provided parameters (`messages` OR `prompt`+`input_json`).
    - If `messages` is provided, use it directly. Ignore `prompt` and `input_json`.
    - If `messages` is NOT provided, validate that `prompt` is a non-null string and `input_json` is a non-null dictionary/list. Format the messages using Langchain's `PromptTemplate` based on `prompt` and `input_json`.
    - Handle prompt template formatting errors separately from model invocation errors.
    - For prompt template errors, raise a ValueError with a clear message.
    - For model invocation errors, use LiteLLM's retry logic. LiteLLM maps provider exceptions to OpenAI-compatible errors. Raise Runtime errors only if all retries fail across candidate models.
    - Provide detailed error messages in verbose mode.
    - **Implement the core logic within the `llm_invoke` function. Consider breaking down complex parts like model loading/selection and API key handling into internal helper functions for better organization and readability.**

% Here is an example (but not actual values) of the llm_model.csv: <llm_model_example><shell>head -6 data/llm_model.csv</shell></llm_model_example>
% Here is how to use LiteLLM thinking: <litellm_thinking_example><web>https://docs.litellm.ai/docs/reasoning_content</web></litellm_thinking_example>
% Here is how to use LiteLLM JSON mode: <litellm_json_mode_example><web>https://docs.litellm.ai/docs/completion/json_mode</web></litellm_json_mode_example>
% Here is how to use LiteLLM batch mode: <litellm_batch_mode_example><web>https://docs.litellm.ai/docs/completion/batching</web></litellm_batch_mode_example>
% Here is how to use LiteLLM reliable completions: <litellm_reliable_completions_example><web>https://docs.litellm.ai/docs/completion/reliable_completions</web></litellm_reliable_completions_example>
% Here is how to use LiteLLM exception mapping: <litellm_exception_mapping_example><web>https://docs.litellm.ai/docs/exception_mapping</web></litellm_exception_mapping_example>
% Here is how to use LiteLLM client side caches: <litellm_all_caches_example><web>https://docs.litellm.ai/docs/caching/all_caches</web></litellm_all_caches_example>
% Here is how to use Google Cloud Storage (GCS) HMAC for S3 caching: <litellm_gcs_caching_example><include>context/gcs_hmac_test.py</include></litellm_gcs_caching_example>
% Here is how to use LiteLLM token usage and cost: <litellm_token_usage_example><web>https://docs.litellm.ai/docs/completion/token_usage</web></litellm_token_usage_example>
% Here is how to use LiteLLM callbacks: <litellm_callbacks_example><web>https://docs.litellm.ai/docs/observability/callbacks</web></litellm_callbacks_example>
% Here is how to use LiteLLM provider specific parameters: <litellm_provider_params_example><web>https://docs.litellm.ai/docs/completion/provider_specific_params</web></litellm_provider_params_example>

% The code must use LiteLLM's `litellm.completion()` function to handle invocation across various LLM providers. LiteLLM standardizes the API calls. Configure LiteLLM to enable its built-in response caching, specifically targeting an S3-compatible backend configured for Google Cloud Storage (GCS). **Implement an SQLite cache (`litellm.Cache(type='sqlite', ...)`) as a fallback if the S3/GCS configuration fails.**

% Here are the rules to follow when selecting the appropriate model via LiteLLM:
    - If environmental variable $PDD_MODEL_DEFAULT is set, use that as the base model name.
    - Otherwise, the base model name is defined by the `DEFAULT_LLM_MODEL` constant imported from `pdd/__init__.py`.
    - Read the `llm_model.csv` file to get all necessary model details. **The script first checks for `~/.pdd/llm_model.csv` and uses it if it exists; otherwise, it uses the file located at `<PROJECT_ROOT>/.pdd/llm_model.csv` (where `<PROJECT_ROOT>` is determined as described in Startup Behavior).**
    - The CSV contains the LiteLLM model identifier, input/output costs, ELO, provider, **API key environment variable name**, **`reasoning_type` (e.g., 'none', 'budget', 'effort')**, **`max_reasoning_tokens` (only used if `reasoning_type` is 'budget')**, structured_output flag, etc.). The `model_name` column must contain the identifier LiteLLM uses (e.g., "openai/gpt-3.5-turbo", "anthropic/claude-3-7-sonnet-20250219"). **It is assumed that a separate utility keeps the cost, structured_output, `reasoning_type`, and `max_reasoning_tokens` data in this CSV up-to-date.**
    - Filter models first based on availability: initially check only if the required API key environment variable *name* exists in the CSV. **The actual presence of the key value in the environment is checked later (Step 4 in Logic Flow).** **If the `api_key` name in the CSV is empty or exactly `"EXISTING_KEY"`, assume the key is handled externally (e.g., via IAM roles or mock setups) and skip the interactive check/acquisition process (Steps 4.b-4.vi).**
    - When strength < 0.5:
        - Identify available candidate models based on the filtered CSV data.
        - Use strength to interpolate based on the average cost (**using input/output costs read directly from the CSV, which are in $/Million tokens**) from the base model down to the cheapest available model.
        - Select the available model with the closest average cost to the target.
        - If calling the model fails, LiteLLM should handle retries, or the logic should try the next cheapest model.
    - When strength > 0.5:
        - Use the **ELO score read from the CSV** to interpolate up from the base model to the highest ELO available model.
        - Select the available model with the closest ELO score.
        - If calling the model fails, retry with the next highest ELO model.
    - Use base model when strength is 0.5. If calling the model fails, retry with the next highest ELO model (using ELO from CSV).
    - Sort the final candidate models by how close they match the target cost (for strength < 0.5, using CSV costs) or target ELO (for strength > 0.5, using CSV ELO).
    - If the configured base model is not present in the CSV, apply a soft fallback (Option A'): choose a surrogate base as the first available model from the CSV (preserving CSV order). Log a clear warning noting the fallback and the chosen surrogate base. Continue selection using this surrogate base.
    - For test environments (where `api_key` is `EXISTING_KEY`), skip prompting for a key; no additional special LiteLLM parameters are applied.
    - Ensure proper error handling for file operations and LiteLLM calls.

% Pass necessary parameters to `litellm.completion()` (or `litellm.batch_completion()` if `use_batch_mode` is True):
    - `model`: The selected model name identifier for LiteLLM.
    - `messages`: The final list of messages. This is either passed directly by the caller OR generated internally from `prompt` and `input_json`. **If batching, this will be a list of message lists.**
    - `temperature`: The specified temperature.
    - `api_base`, `api_key`, `api_version`: Pass these if needed for specific setups (like Azure, local models), although LiteLLM often infers from environment variables. Consult `llm_model.csv` for provider-specific needs.
    - `max_tokens`: **Do not set this.** Allow LiteLLM and the underlying provider to use the default maximum completion tokens based on the model and input prompt length.
    - **Thinking/Reasoning (`time` parameter)**: Use the `reasoning_type` and `max_reasoning_tokens` columns from the CSV to determine how to handle the `time` parameter (0-1):
        - **If `reasoning_type` is 'none'**: Ignore `time`, pass no reasoning parameters.
        - **If `reasoning_type` is 'budget'**: Check if `max_reasoning_tokens` > 0. If so, calculate `budget = int(time * max_reasoning_tokens)`. If `budget > 0`, pass the provider-specific parameter(s): Anthropic uses `thinking={\"type\":\"enabled\",\"budget_tokens\":budget}`; omit for providers without a known mapping.
        - **If `reasoning_type` is 'effort'**: Map `time` to a category (\"low\": 0-0.3, \"medium\": >0.3-0.7, \"high\": >0.7-1.0). Prefer provider-specific handling: for OpenAI gpt-5* use Responses API `reasoning={\"effort\": category, \"summary\": \"auto\"}`; otherwise pass `reasoning_effort=category` when supported.
        - Default `time` is 0.25.
    - **Structured Output**: If `output_pydantic` is provided, check the `structured_output` flag from the CSV. If `True`, use LiteLLM's mechanism for structured output (e.g., passing the Pydantic model to `response_format` or similar). For OpenAI gpt-5* via Responses API, if the installed SDK supports `response_format`, pass a Pydantic-derived JSON schema; otherwise validate client-side. If `False` (or validation fails), use fallback parsing: **attempt to extract a JSON block (e.g., finding the first `{` and last `}`), clean potential markdown fences (like ```json ... ```), and re-attempt Pydantic validation on the cleaned string.**
    - **Batch Mode**: If `use_batch_mode` is True, use `litellm.batch_completion` and adapt input/output handling for lists.

% Provider-Level Prompt Caching Note:
    - Leveraging provider-level prompt caching (e.g., Anthropic's `cache_control`, OpenAI's implicit caching) requires careful management of the `messages` list.
    - For implicit caching providers, the caller must ensure consistency in the `messages` prefix across related calls, whether generated internally or provided directly.
    - For explicit caching providers (requiring syntax like `cache_control`), the caller **must provide the fully constructed `messages` list** as input to this function, already containing the necessary provider-specific syntax. This function will not inject explicit caching syntax when generating messages from `prompt` and `input_json`.

% CRITICAL: When handling model token limits:
    - **The function will NOT pass any `max_tokens`/`max_completion_tokens` parameter.** Rely on provider defaults for maximum completion tokens, including in all retry paths.

% If verbose is set to True, print the following information:
    - **The ordered list of candidate models selected based on strength/ELO and availability (before attempting invocation).**
    - The selected model name passed to LiteLLM (at the time of the attempt).
    - The per input and output token cost (in dollars per million tokens) of the selected model (as read from the CSV used for selection) and the number of input and output tokens (obtainable from LiteLLM's response).
    - The total cost of the invoke run (calculated/reported by LiteLLM in the response).
    - The strength, temperature, and time used.
    - The `time` value used, and any derived reasoning parameters (e.g., budget_tokens, effort_category) passed to LiteLLM.
    - The input JSON (use try-except to handle rich printing failures, falling back to standard print).
    - The optional Pydantic output format.
    - **Indication that the provider's default max completion tokens were used.**
    - The result using rprint.

% LiteLLM Callback for Status/Usage:
    - To capture token usage and finish reason, implement a custom LiteLLM callback function.
    - Assign this function to `litellm.success_callback` (it should be a list, e.g., `litellm.success_callback = [my_custom_callback]`).
    - The callback function will receive arguments including `kwargs` (original completion arguments) and `completion_response` (the LiteLLM response object).
    - Inside the callback:
        1. Retrieve token usage (`input_tokens`, `output_tokens`) from `completion_response.usage` object if available.
        2. Default tokens to 0 if `usage` is not available.
        3. Capture the `finish_reason` from `completion_response.choices[0].finish_reason` if available.
        4. Store or log this information as needed by the application.
        4. Calculate the cost primarily using `litellm.completion_cost(completion_response=...)`. **If this fails (e.g., due to missing provider info in the response object, common in batch results), attempt calculation again using the `model` from the original `kwargs` and token counts from the `usage` object; if still unavailable, fall back to computing cost using CSV per-million rates for the selected model.**
        5. Store or log this information (tokens, cost, finish reason) as needed by the application.

% Deployment Considerations:
    - When deploying to PyPI, ensure the logging level defaults to WARNING in production environments.
    - Allow consumers of the library to control logging verbosity through environment variables.
    - Use rotating file handlers for logging to prevent log files from growing too large.
    - Ensure all print/rprint statements are replaced with appropriate logger calls.
    - Consider the performance impact of excessive debug logging in production.

% Caching Configuration and Usage:
    - Configure LiteLLM cache if environment variables are provided:
        * Prefer S3-compatible cache targeting GCS using HMAC keys (`GCS_HMAC_ACCESS_KEY_ID`, `GCS_HMAC_SECRET_ACCESS_KEY`, `GCS_BUCKET_NAME`, `GCS_REGION_NAME`, endpoint `https://storage.googleapis.com`).
        * If not available, fall back to a disk-based SQLite cache at `<PROJECT_ROOT>/litellm_cache.sqlite`.
        * Respect `LITELLM_CACHE_DISABLE=1` to disable caching entirely.
    - Enable per-call caching by passing `caching=True` only when a cache is configured.
    - For suspected cache-corruption (e.g., `None` content), temporarily disable cache to retry, then restore the configured cache.

% Test Environments:
    - If the CSV `api_key` column is `EXISTING_KEY`, skip interactive prompting for API keys. No additional special handling or mock parameters are applied to LiteLLM.

% WSL Diagnostics:
    - Sanitize API keys by trimming whitespace and control characters. In WSL environments, detect and warn about authentication errors caused by stray carriage returns (Illegal header value) and suggest re-entering/sanitizing keys.
