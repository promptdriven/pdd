% You are an expert Python Software Engineer. Your goal is to write a Python function, "detect_change", that will analyze a list of prompt files and a change description to determine which prompts need to be changed.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        - 'prompt_files' - A list of strings, each containing the filename of a prompt that may need to be changed.
        - 'change_description' - A string that describes the changes that need to be analyzed and potentially applied to the prompts.
        - 'strength': A float value representing the strength parameter for the LLM model, used to influence the model's behavior.
        - 'temperature': A float value representing the temperature parameter for the LLM model, used to control the randomness of the model's output.
        - 'time' - A float between 0 and 1 that controls the thinking effort for the LLM model, passed to llm_invoke. Default is DEFAULT_TIME.
        - 'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs:
        - 'changes_list' - A list of JSON objects, each containing the name of a prompt that needs to be changed and detailed instructions on how to change it.
        - 'total_cost': A float value representing the total cost of running the function.
        - 'model_name': A string representing the name of the selected LLM model.

% Here is how to use the internal modules:
    <internal_modules>
        % Here is an example how to preprocess the prompt from a file: <preprocess_example><include>context/preprocess_example.py</include></preprocess_example>
        
        % For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        % For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>

        % For handling unfinished prompts:
        <unfinished_prompt_example>
            <include>context/unfinished_prompt_example.py</include>
        </unfinished_prompt_example>

        % For continuing generation:
        <continue_generation_example>
            <include>context/continue_generation_example.py</include>
        </continue_generation_example>

        % For postprocessing results:
        <postprocess_example>
            <include>context/postprocess_example.py</include>
        </postprocess_example>
    </internal_modules>

% Steps to be followed by the function:
    Step 1. Load the 'detect_change_LLM' and 'extract_detect_change_LLM' prompt templates using the `load_prompt_template` function. For 'detect_change_LLM' preprocess and set double_curly_brackets to true, excluding keys 'PROMPT_LIST' and 'CHANGE_DESCRIPTION'.
    Step 2. Run the prompt_files and change_description through the model using `llm_invoke`:
        - a. Pass the following parameters to `llm_invoke`:
            * 'prompt_template': The preprocessed 'detect_change_LLM' prompt template.
            * 'strength': The provided strength parameter.
            * 'temperature': The provided temperature parameter.
            * 'time': The provided time parameter.
            * 'verbose': The provided verbose parameter.
            * 'input_data': A dictionary containing:
                * 'PROMPT_LIST' (load the prompt files and create a list of JSON with these keys: 'PROMPT_NAME' and 'PROMPT_DESCRIPTION').
                * 'CHANGE_DESCRIPTION' (preprocess this with double_curly_brackets set to false using the `preprocess` function).
        - b. If verbose is True, pretty print the output of 2a, including the token count and estimated cost.
    Step 3. Run the output from Step 2 through the 'extract_detect_change_LLM' prompt using `llm_invoke` with a strength of .89, temperature of 0, and the provided `time` parameter.
        - a. Pass the following parameters to `llm_invoke`:
            * 'prompt_template': The loaded 'extract_detect_change_LLM' prompt template.
            * 'verbose': The provided verbose parameter.
            * 'input_data': A dictionary containing:
                * 'llm_output': The output string from Step 2.
        - b. If verbose is True, pretty print the running message with the token count and cost.
        - c. Extract 'changes_list' key values from the Pydantic output.
    Step 4. If verbose is True, pretty print the extracted changes_list using Rich Markdown function. Include token counts and costs. The list will contain a dictionary with the following keys "prompt_name" and "change_instructions".
    Step 5. Return the 'changes_list', the 'total_cost' from both `llm_invoke` calls, and 'model_name' used for the detect_change_LLM prompt.

% Ensure that the function handles potential errors gracefully, such as missing files or invalid input.