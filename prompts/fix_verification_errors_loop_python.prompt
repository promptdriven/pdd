% You are an expert Python Software Engineer. Your goal is to write a Python function, "fix_verification_errors_loop", that will attempt to fix errors in a code file based on the output of an executing program compared against the original prompt's intent, through multiple iterations. The function should include an optional verbose mode for detailed logging. Additionally, the function should track and report detailed statistics about the fixing process, including initial and final verification states, improvements made, and the best iteration.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        'program_file' - A string containing the path to the Python program file that exercises the code_file according to the prompt's intent.
        'code_file' - A string containing the path to the code file being tested/verified.
        'prompt' - A string containing the prompt that generated the code under test, defining the intended behavior.
        'verification_program' - A string containing the path to a secondary Python program that verifies if the code_file still runs correctly after modifications (e.g., checks for syntax errors or basic functionality).
        'strength' - A float between or equal to 0 and 1 that represents the strength of the LLM model to use for fixing.
        'temperature' - A float between or equal to 0 and 1 that represents the temperature parameter for the LLM model.
        'max_attempts' - An integer representing the maximum number of fix attempts before giving up.
        'budget' - A float representing the maximum cost allowed for the fixing process.
        'verification_log_file' - A string containing the path where this loop function will write detailed XML-formatted logs for each attempt (default: "verification.log").
        'verbose' - A boolean indicating whether to enable verbose logging (default: False).
        'program_args' - An optional list of strings containing command-line arguments to pass to the 'program_file' when executed.
    Outputs:
        'success' - A boolean indicating whether the code was successfully fixed to meet the prompt's intent based on program execution.
        'final_program' - A string containing the contents of the final program file (potentially modified if `fix_verification_errors` suggests changes).
        'final_code' - A string containing the contents of the final code file.
        'total_attempts' - An integer representing the number of fix attempts made.
        'total_cost' - A float representing the total cost of all fix attempts.
        'model_name' - A string representing the name of the LLM model used.

% Here are examples of how to use internal modules:
<internal_example_modules>
    % Here is an example of the fix_verification_errors function that will be used:
    <fix_verification_errors_example>
        <include>context/fix_verification_errors_example.py</include>
    </fix_verification_errors_example>
</internal_example_modules>

% This function will do the following:
    Step 1. Remove the existing verification log file specified by 'verification_log_file' if it exists.
    Step 2. Initialize variables:
        - Counter for the number of fix attempts.
        - Total cost accumulator.
        - Best iteration tracker (This stores the state of the iteration with the minimum number of verification issues found by `fix_verification_errors`. If multiple iterations have the same minimum count, the one with the lowest attempt number is preferred. Store the attempt number, program backup path, code backup path, and the issue count).
        - A statistics tracker to record the initial verification state, final state, and improvements made during the fixing process.
    Step 3. Determine the initial state:
        a. Run the `program_file` using `subprocess`, passing the optional `program_args` if provided, to get initial output (capture stdout/stderr). Store this output.
        b. Append an XML entry to the `verification_log_file` tagged `<InitialState>` containing the timestamp and the captured output. Ensure the output is properly XML-escaped (replace `<`, `>`, `&`, `'`, `"` with `&lt;`, `&gt;`, `&amp;`, `&apos;`, `&quot;`).
        c. Read the initial contents of the `program_file` and `code_file`.
        d. Call `fix_verification_errors` with the initial program/code contents, prompt, and initial output (strength/temp can be minimal or 0 if only getting the count matters initially, but respect budget). Store the result (`initial_fix_result`).
        e. Add the cost (`initial_fix_result['total_cost']`) to the total cost accumulator.
        f. Extract the initial `verification_issues_count` from `initial_fix_result`. Store this in the statistics tracker.
        g. Initialize the `best_iteration` tracker with attempt 0, initial issue count, and paths to the original files (or temporary copies if modification is possible here). Log the initial issue count.
        h. If `initial_fix_result['verification_issues_count']` is 0, set overall success flag to True, skip to Step 7 (bypassing the loop).
    Step 4. Enter a while loop that continues until max_attempts is reached or budget is exceeded:
        a. Print out to console the current attempt iteration number.
        b. Run the `program_file` using `subprocess`, passing the optional `program_args` if provided, capturing its standard output and standard error into `program_output`.
        c. Read the current contents of the `program_file` and `code_file` into `program_contents` and `code_contents` respectively.
        d. Create backup copies of the `program_file` and `code_file` in their respective directories, appending the current iteration number (e.g., `_iteration_{N}`) to the filenames. Store these backup paths.
        e. Call `fix_verification_errors` with the `program_contents`, `code_contents`, `prompt`, `program_output`, strength, temperature, and verbose flag. Store the returned dictionary (containing 'explanation', 'fixed_program', 'fixed_code', 'total_cost', 'model_name', 'verification_issues_count'). Let's call this `fix_result`.
        f. Add the returned `cost` (`fix_result['total_cost']`) to the total cost accumulator.
        g. Prepare an XML block for the current iteration, tagged `<Iteration attempt="{attempt_number}" timestamp="{current_time}">`.
           - Inside, include tags like `<ProgramOutputBeforeFix>` (containing the XML-escaped `program_output`), `<InputsToFixer>` (containing XML-escaped `program_contents`, `code_contents`, `prompt`, `program_output`), and `<FixerResult>` (containing XML-escaped `explanation`, `fixed_program`, `fixed_code`, and other fields like `total_cost`, `model_name`, `verification_issues_count` as attributes or sub-tags). Remember to escape special XML characters in all embedded text content.
        h. If the total cost exceeds the `budget`, add a `<Status>Budget Exceeded</Status>` tag to the XML block, append the block to `verification_log_file`, set overall success flag to False, and break the loop.
        i. If `fix_result['verification_issues_count']` is 0:
           - Add a `<Status>Success - 0 Issues Found</Status>` tag to the XML block.
           - Update the best iteration tracker if this attempt (attempt 0 if initial check) is better than the current best (it should be, with 0 issues).
           - Write the potentially updated `fix_result['fixed_program']` and `fix_result['fixed_code']` to their respective files.
           - Append the XML block to `verification_log_file`.
           - Set the overall success flag to True.
           - Break the loop.
        j. Check if changes were suggested by comparing `fix_result['fixed_program']` with `program_contents` and `fix_result['fixed_code']` with `code_contents`. If no changes were suggested, add `<Status>No Changes Suggested</Status>` tag to the XML block, append the block to `verification_log_file`, set overall success flag to False, and break the loop.
        k. Define boolean flags `program_updated` and `code_updated` based on the comparison above.
        l. Add a sub-tag `<FixAttempted>` to the XML block, noting whether program/code fixes were suggested.

        m. Initialize `secondary_verification_passed = True`.  # Assume pass if code not updated
        n. If `code_updated` is True:
           * Temporarily write `fix_result['fixed_code']` to a temporary file or pass it directly to the verification program if possible.
           * Run the secondary `verification_program` against the proposed code change. Capture its output (`secondary_verification_output`) and determine the success/failure outcome.
           * Set `secondary_verification_passed` based on the outcome.
           * Add a `<SecondaryVerification>` tag to the XML block containing the outcome and the XML-escaped `secondary_verification_output`.
           * Clean up the temporary file if used.

        o. If `secondary_verification_passed` is True:
           * Compare `fix_result['verification_issues_count']` with the issue count of the current best iteration. If the current count is lower, or if it's equal and the current attempt number is lower, update the best iteration tracker with the current attempt number, backup paths (from Step 4d), and issue count.
           * If `code_updated` is True:
              - Write the `fix_result['fixed_code']` back to the `code_file`.
           * If `program_updated` is True:
              - Write the `fix_result['fixed_program']` back to the `program_file`.
           * Add `<Status>Changes Applied (Secondary Verification Passed)</Status>` to the XML block.
        p. Else (`secondary_verification_passed` is False):
           * Log this event within the current XML iteration block: `<Action>Changes Discarded Due To Secondary Verification Failure</Action>`.
           * Do not update files or the best iteration tracker.

        q. Append the completed XML block for this iteration to `verification_log_file`.
        r. Increment the fix attempt counter.
    Step 5. After the loop finishes (due to success, budget, max attempts, or no changes suggested), determine the final state:
        a. Check the overall success flag. If True, the current files represent the successful state.
        b. If the success flag is False, and the best iteration tracker holds valid data (i.e., an iteration completed secondary verification successfully), restore the `program_file` and `code_file` from the backup paths stored in the best iteration tracker.
           - Print which iteration was restored.
           - Add a corresponding `<Action>Restored Best Iteration {best_attempt_number} (Issues: {best_issue_count})</Action>` entry to the log file.
        c. If the success flag is False and no best iteration was ever recorded (e.g., secondary verification always failed), the files remain in their last attempted state. Log this situation: `<Action>No successful iteration found; final state is from last attempt.</Action>`.
    Step 6. Read the final contents of `program_file` and `code_file` into `final_program` and `final_code`.
    Step 7. Calculate and print summary statistics using the tracker, including the initial issue count, final issue count (0 if successful, otherwise the best recorded count or count from the last attempt if no best), best iteration number and its issue count, and overall improvement.
    Step 8. Return the final `success` status flag, `final_program` contents, `final_code` contents, `total_attempts`, `total_cost`, and `model_name` (from the last successful `fix_verification_errors` call or the initial one).

% The function should run the `program_file` using `subprocess`, capturing both stdout and stderr. This combined output should be used for logging and as input to the `fix_verification_errors` function.

% Finally, ensure that the function tracks and reports detailed statistics about the fixing process, including:
    - Initial verification state (based on initial program execution output and initial issue count).
    - Final verification state (based on the state of the code returned, associated with an issue count of 0 if success is True, otherwise the best recorded issue count).
    - Improvements made during the process (reduction in deviation from prompt intent and reduction in issue count from initial to best/final).
    - The best iteration number, its corresponding verification results (issue count), and backup file paths.
    - Overall improvement percentage based on verification success (explicitly defined as reaching 0 issues).