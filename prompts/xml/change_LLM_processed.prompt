<context>
    You are an expert LLM Prompt Engineer. Your goal is to change the input_prompt into a modified_prompt according to the change_prompt.
</context>
<instructions>
    Here are the inputs and outputs of this prompt:
    <input/>
        'input_prompt' - A string that contains the prompt that will be modified by the change_prompt.
        'input_code' - A string that contains the code that was generated from the input_prompt.
        'change_prompt' - A string that contains the instructions of how to modify the input_prompt.
    
    <output>
        'modified_prompt' - A string that contains the modified prompt that was changed based on the change_prompt.
    </output>
</instructions>
<examples>
<example>
<example_number>4</example_number>
<example_input_prompt>```% You are an expert Python engineer. Your goal is to write a python function called 'postprocess' that will post-process the string output of a LLM so that the code can be run.

% Here are the inputs and outputs of the function:
    Inputs:
        'llm_output' - A string contains a mix of text and sections of code separated by triple backticks generated by a LLM.
        'language' - A string that is the type (e.g. python, bash) of file that will be outputed by the LLM.
        'strength' - A string that is the strength of the LLM model to use for the post-processing. Default is 0.9.
        'temperature' - A string that is the temperature of the LLM model to use for the post-processing. Default is 0.
    Outputs:
        'extracted_code' - string that is the processed string that contains a properly commented out comments so that code can be run.
        'total_cost' - float that is the total cost of the post-processing function. This is an optional output.

% Here is an example of a Langchain LCEL program: ```&lt;./context/langchain_lcel_example.py&gt;```

% Here is an example how to select the Langchain llm: ```&lt;./context/llm_selector_example.py&gt;```

% Here is an example how to use tiktoken: ```&lt;./context/tiktoken_example.py&gt;```

% Here is an example how to use postprocess_0 which is a zero cost postprocessing: ```&lt;./context/postprocess_0_example.py&gt;```

% This function will do the following:
    Step 1. If strength is 0, use postprocess_0 generate the extracted_code. Return extracted_code and total_cost of 0.
    Step 2. Otherwise, use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/extract_code_LLM.prompt' file.
    Step 3. Create a Langchain LCEL template from extract_code_LLM prompt so that it returns a JSON output.
    Step 4. Use the llm_selector function for the LLM model.
    Step 5. Run the code through the model using Langchain LCEL.
        5a. Pass the following string parameters to the prompt during invoke:
            - 'llm_output'
            - 'language'
        5b. Pretty print a message letting the user know it is running and how many tokens (using tiktoken) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.
        5c. The dictionary output of the LCEL will have the key 'extracted_code' that contains the processed code string. Be sure to access this key using the get method with a default error message.
        5d. If the first and last line have triple backticks delete both lines.
        5e. Pretty print the extracted_code using the rich Markdown function. Also, print the number of tokens in the result, the output token cost and the total_cost.
    Step 6. Return the 'extracted_code' string from the JSON and the total_cost.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.```</example_input_prompt>
<example_input_code>```import os
import json
import tiktoken
from rich import print as rprint
from rich.markdown import Markdown
from postprocess_0 import postprocess_0
from llm_selector import llm_selector
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnablePassthrough

def postprocess(llm_output, language, strength=0.9, temperature=0):
    # Step 1: Use postprocess_0 if strength is 0
    if strength == 0:
        extracted_code = postprocess_0(llm_output, language)
        return extracted_code, 0.0

    # Step 2: Load the extract_code_LLM prompt
    pdd_path = os.getenv('PDD_PATH')
    if not pdd_path:
        raise EnvironmentError("PDD_PATH environment variable is not set")
    
    prompt_path = os.path.join(pdd_path, 'prompts', 'extract_code_LLM.prompt')
    try:
        with open(prompt_path, 'r') as file:
            extract_code_prompt = file.read()
    except FileNotFoundError:
        raise FileNotFoundError(f"Prompt file not found at {prompt_path}")

    # Step 3: Create a Langchain LCEL template
    prompt_template = PromptTemplate.from_template(extract_code_prompt)
    parser = JsonOutputParser()

    # Step 4: Use llm_selector to get the LLM model
    llm, token_counter, input_cost, output_cost = llm_selector(strength, temperature)

    # Step 5: Run the code through the model using Langchain LCEL
    chain = prompt_template | llm | parser

    # Prepare the input for the model
    input_data = {
        "llm_output": llm_output,
        "language": language
    }

    # Calculate token count using tiktoken
    encoding = tiktoken.get_encoding("cl100k_base")
    token_count = len(encoding.encode(json.dumps(input_data)))

    # Pretty print the running message
    rprint(f"Running model with {token_count} tokens. Estimated cost: ${input_cost * token_count / 1_000_000:.6f}")

    # Invoke the chain
    result = chain.invoke(input_data)

    # Step 5c: Access the 'extracted_code' key
    extracted_code = result.get('extracted_code', "Error: 'extracted_code' not found in the result")

    # Calculate output token count and total cost
    output_token_count = len(encoding.encode(extracted_code))
    total_cost = (input_cost * token_count + output_cost * output_token_count) / 1_000_000

    # Step 5d: Pretty print the extracted code
    rprint(Markdown(extracted_code))
    rprint(f"Output tokens: {output_token_count}, Output cost: ${output_cost * output_token_count / 1_000_000:.6f}, Total cost: ${total_cost:.6f}")

    # Step 6: Return the extracted code and total cost
    return extracted_code, total_cost```</example_input_code>
<example_change_prompt>```Use token_counter from llm_selector instead of tiktoken to count tokens in the prompt.```</example_change_prompt>
<example_modified_prompt>```% You are an expert Python engineer. Your goal is to write a python function called 'postprocess' that will post-process the string output of a LLM so that the code can be run.

% Here are the inputs and outputs of the function:
    Inputs:
        'llm_output' - A string contains a mix of text and sections of code separated by triple backticks generated by a LLM.
        'language' - A string that is the type (e.g. python, bash) of file that will be outputed by the LLM.
        'strength' - A string that is the strength of the LLM model to use for the post-processing. Default is 0.9.
        'temperature' - A string that is the temperature of the LLM model to use for the post-processing. Default is 0.
    Outputs:
        'extracted_code' - string that is the processed string that contains a properly commented out comments so that code can be run.
        'total_cost' - float that is the total cost of the post-processing function. This is an optional output.

% Here is an example of a Langchain LCEL program: ```&lt;./context/langchain_lcel_example.py&gt;```

% Here is an example how to select the Langchain llm and count tokens: ```&lt;./context/llm_selector_example.py&gt;```

% Here is an example how to use postprocess_0 which is a zero cost postprocessing: ```&lt;./context/postprocess_0_example.py&gt;```

% This function will do the following:
    Step 1. If strength is 0, use postprocess_0 generate the extracted_code. Return extracted_code and total_cost of 0.
    Step 2. Otherwise, use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/extract_code_LLM.prompt' file.
    Step 3. Create a Langchain LCEL template from extract_code_LLM prompt so that it returns a JSON output.
    Step 4. Use the llm_selector function for the LLM model.
    Step 5. Run the code through the model using Langchain LCEL.
        5a. Pass the following string parameters to the prompt during invoke:
            - 'llm_output'
            - 'language'
        5b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.
        5c. The dictionary output of the LCEL will have the key 'extracted_code' that contains the processed code string. Be sure to access this key using the get method with a default error message.
        5d. If the first and last line have triple backticks delete both lines.
        5e. Pretty print the extracted_code using the rich Markdown function. Also, print the number of tokens in the result, the output token cost and the total_cost.
    Step 6. Return the 'extracted_code' string from the JSON and the total_cost.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.```</example_modified_prompt>
</example>
<example>
<example_number>5</example_number>
<example_input_prompt>```% You are an expert Python Software Engineer. Your goal is to write a Python function, "split", that will split a prompt into a sub_prompt and modified_prompt with no loss of functionality. All output to the console will be pretty printed using the Python Rich library.

% Here are the inputs and outputs of the function:
    Inputs:
        - 'input_prompt': A string containing the prompt that will be split into a sub_prompt and modified_prompt.
        - 'input_code': A string containing the code that was generated from the input_prompt.
        - 'example_code': A string containing the code example of how the code generated from the sub_prompt would be used by the code generated from the modified_prompt.
        - 'strength': A float value representing the strength parameter for the LLM model, used to influence the model's behavior.
        - 'temperature': A float value representing the temperature parameter for the LLM model, used to control the randomness of the model's output.
    Outputs:
        - 'sub_prompt': A string containing the sub_prompt that was split from the input_prompt.
        - 'modified_prompt': A string containing the modified prompt from input_prompt split from the sub_prompt.
        - 'total_cost': A float value representing the total cost of running the function.

% Here is an example how to preprocess the prompt from a file: ```&lt;./context/preprocess_example.py&gt;```

% Example usage of the Langchain LCEL program: ```&lt;./context/langchain_lcel_example.py&gt;```

% Example of selecting a Langchain LLM using llm_selector: ```&lt;./context/llm_selector_example.py&gt;```

% Example of using tiktoken: ```&lt;./context/tiktoken_example.py&gt;```

% Steps to be followed by the function:
    1. Load the '$PDD_PATH/prompts/split_LLM.prompt' and '$PDD_PATH/prompts/extract_prompt_split_LLM.prompt' files.
    2. Preprocess the split_LLM prompt using the preprocess function from the preprocess module.
    2. Create a Langchain LCEL template from the processed split_LLM prompt to return a string output.
    3. Use the llm_selector function for the LLM model.
    4. Run the input through the model using Langchain LCEL:
        - a. Pass the following string parameters to the prompt during invocation: 'input_prompt', 'input_code', 'example_code'.
        - b. Calculate the input and output token count using tiktoken and pretty print the running message, including the token count and estimated cost. The cost from llm_selector is in dollars per million tokens.
    5. Create a Langchain LCEL template from the extract_prompt_split_LLM prompt that outputs JSON:
        - a. Pass the following string parameters to the prompt during invocation: 'llm_output' (this string is from Step 4).
        - b. Calculate input and ouput token count using tiktoken and pretty print the running message with the token count and cost.
        - c. get 'sub_prompt' and 'modified_prompt' from the JSON output.
    6. Pretty print the extracted sub_prompt and modified_prompt using Rich Markdown function. Include token counts and costs.
    7. Return the 'sub_prompt' and 'modified_prompt' strings and the total_cost.

% Ensure the function handles edge cases, such as missing inputs or model errors, and provide clear error messages.```</example_input_prompt>
<example_input_code>```#To implement the `split` function as described, we need to follow a structured approach using the Langchain library, the `llm_selector` function, and the `tiktoken` library for token counting. Additionally, we'll use the `rich` library for pretty printing. Below is a step-by-step implementation of the function:
#
#```python
import os
import json
from rich import print as rprint
from rich.markdown import Markdown
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import ChatOpenAI
from llm_selector import llm_selector
import tiktoken

# Define the function
def split(input_prompt, input_code, example_code, strength, temperature):
    # Step 1: Load the prompt files
    pdd_path = os.getenv('PDD_PATH')
    if not pdd_path:
        raise ValueError("PDD_PATH environment variable is not set")

    split_llm_prompt_path = os.path.join(pdd_path, 'prompts/xml', 'split_xml_LLM.prompt')
    extract_prompt_split_llm_prompt_path = os.path.join(pdd_path, 'prompts', 'extract_prompt_split_LLM.prompt')

    with open(split_llm_prompt_path, 'r') as file:
        split_llm_prompt = file.read()

    with open(extract_prompt_split_llm_prompt_path, 'r') as file:
        extract_prompt_split_llm_prompt = file.read()

    # Step 2: Create Langchain LCEL template for split_LLM
    split_prompt_template = PromptTemplate.from_template(split_llm_prompt)

    # Step 3: Use llm_selector to get the LLM model
    llm, input_cost, output_cost = llm_selector(strength, temperature)

    # Step 4: Run the input through the model using Langchain LCEL
    chain = split_prompt_template | llm | StrOutputParser()
    input_data = {
        "input_prompt": input_prompt,
        "input_code": input_code,
        "example_code": example_code
    }
    llm_output = chain.invoke(input_data)

    # Calculate token count and cost
    encoding = tiktoken.get_encoding("cl100k_base")
    input_tokens = len(encoding.encode(json.dumps(input_data)))
    output_tokens = len(encoding.encode(llm_output))
    total_cost = (input_tokens * input_cost + output_tokens * output_cost) / 1_000_000

    rprint(f"[bold green]Running model...[/bold green]")
    rprint(f"Input Tokens: {input_tokens}, Output Tokens: {output_tokens}, Estimated Cost: ${total_cost:.6f}")

    # Step 5: Create Langchain LCEL template for extract_prompt_split_LLM
    extract_prompt_template = PromptTemplate.from_template(extract_prompt_split_llm_prompt)
    parser = JsonOutputParser()

    chain = extract_prompt_template | llm | parser
    result = chain.invoke({"llm_output": llm_output})

    # Calculate token count and cost for extraction
    extract_input_tokens = len(encoding.encode(llm_output))
    extract_output_tokens = len(encoding.encode(json.dumps(result)))
    extract_cost = (extract_input_tokens * input_cost + extract_output_tokens * output_cost) / 1_000_000

    rprint(f"[bold green]Extracting prompts...[/bold green]")
    rprint(f"Input Tokens: {extract_input_tokens}, Output Tokens: {extract_output_tokens}, Estimated Cost: ${extract_cost:.6f}")

    # Step 6: Pretty print the extracted sub_prompt and modified_prompt
    sub_prompt = result.get('sub_prompt', '')
    modified_prompt = result.get('modified_prompt', '')

    rprint(Markdown(f"**Sub Prompt:**\n{sub_prompt}"))
    rprint(Markdown(f"**Modified Prompt:**\n{modified_prompt}"))

    # Step 7: Return the sub_prompt, modified_prompt, and total_cost
    return sub_prompt, modified_prompt, total_cost + extract_cost

# Example usage
# sub_prompt, modified_prompt, total_cost = split("input_prompt", "input_code", "example_code", 0.5, 0.7)
# rprint(f"Sub Prompt: {sub_prompt}, Modified Prompt: {modified_prompt}, Total Cost: {total_cost}")
#```
#
#### Explanation:
#1. **Environment Variables**: The function checks for the `PDD_PATH` environment variable to locate the prompt files.
#2. **Prompt Loading**: It reads the prompt templates from the specified files.
#3. **Langchain Setup**: It sets up the Langchain LCEL templates and chains them with the selected LLM model.
#4. **Token Counting**: It uses `tiktoken` to count tokens and calculate costs based on the input and output token counts.
#5. **Rich Printing**: It uses the `rich` library to pretty print the process and results.
#6. **Error Handling**: The function raises an error if the necessary environment variables are not set.
#
#This implementation assumes that the `llm_selector` function and the `tiktoken` library are correctly set up and available in your environment. Adjust paths and configurations as necessary for your specific setup.```</example_input_code>
<example_change_prompt>```Use token_counter from llm_selector instead of tiktoken to count tokens in the prompt.```</example_change_prompt>
<example_modified_prompt>```% You are an expert Python Software Engineer. Your goal is to write a Python function, "split", that will split a prompt into a sub_prompt and modified_prompt with no loss of functionality. All output to the console will be pretty printed using the Python Rich library.

% Here are the inputs and outputs of the function:
    Inputs:
        - 'input_prompt': A string containing the prompt that will be split into a sub_prompt and modified_prompt.
        - 'input_code': A string containing the code that was generated from the input_prompt.
        - 'example_code': A string containing the code example of how the code generated from the sub_prompt would be used by the code generated from the modified_prompt.
        - 'strength': A float value representing the strength parameter for the LLM model, used to influence the model's behavior.
        - 'temperature': A float value representing the temperature parameter for the LLM model, used to control the randomness of the model's output.
    Outputs:
        - 'sub_prompt': A string containing the sub_prompt that was split from the input_prompt.
        - 'modified_prompt': A string containing the modified prompt from input_prompt split from the sub_prompt.
        - 'total_cost': A float value representing the total cost of running the function.

% Here is an example how to preprocess the prompt from a file: ```&lt;./context/preprocess_example.py&gt;```

% Example usage of the Langchain LCEL program: ```&lt;./context/langchain_lcel_example.py&gt;```

% Example of selecting a Langchain LLM and counting tokens using llm_selector: ```&lt;./context/llm_selector_example.py&gt;```

% Steps to be followed by the function:
    1. Load the '$PDD_PATH/prompts/split_LLM.prompt' and '$PDD_PATH/prompts/extract_prompt_split_LLM.prompt' files.
    2. Preprocess the split_LLM prompt using the preprocess function from the preprocess module.
    2. Create a Langchain LCEL template from the processed split_LLM prompt to return a string output.
    3. Use the llm_selector function for the LLM model and token counting.
    4. Run the input through the model using Langchain LCEL:
        - a. Pass the following string parameters to the prompt during invocation: 'input_prompt', 'input_code', 'example_code'.
        - b. Calculate the input and output token count using token_counter from llm_selector and pretty print the running message, including the token count and estimated cost. The cost from llm_selector is in dollars per million tokens.
    5. Create a Langchain LCEL template from the extract_prompt_split_LLM prompt that outputs JSON:
        - a. Pass the following string parameters to the prompt during invocation: 'llm_output' (this string is from Step 4).
        - b. Calculate input and output token count using token_counter from llm_selector and pretty print the running message with the token count and cost.
        - c. get 'sub_prompt' and 'modified_prompt' from the JSON output.
    6. Pretty print the extracted sub_prompt and modified_prompt using Rich Markdown function. Include token counts and costs.
    7. Return the 'sub_prompt' and 'modified_prompt' strings and the total_cost.

% Ensure the function handles edge cases, such as missing inputs or model errors, and provide clear error messages.```</example_modified_prompt>
</example>
<example>
<example_number>6</example_number>
<example_input_prompt>```% You are an expert Python engineer. Your goal is to write a Python function, "xml_tagger", that will enhance a given LLM prompt by adding XML tags to improve its structure and readability. All output to the console will be pretty printed using the Python rich library.

% Here are the inputs and outputs of the function:
    Input: 
        'raw_prompt' - A string containing the prompt that needs XML tagging to improve its organization and clarity.
        'strength' - A float value representing the strength parameter for the LLM model.
        'temperature' - A float value representing the temperature parameter for the LLM model.
    Output: 
        'xml_tagged' - A string containing the prompt with properly added XML tags.

% Here is an example of a Langchain LCEL program: ```&lt;./context/langchain_lcel_example.py&gt;```

% Here is an example how to select the Langchain llm: ```&lt;./context/llm_selector_example.py&gt;``` 

% Here is an example how to use tiktoken: ```&lt;./context/tiktoken_example.py&gt;``` 

% This program will use Langchain to do the following:
    Step 1. Use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/xml_convertor_LLM.prompt' and '$PDD_PATH/prompts/extract_xml_LLM.prompt' files.
    Step 2. Create a Langchain LCEL template from xml_convertor prompt so that it returns a string output.
    Step 3. Use the llm_selector function for the LLM model.
    Step 4. Run the code through the model using Langchain LCEL. 
        4a. Pass the following string parameters to the prompt during invoke:
            - 'raw_prompt'
        4b. Pretty print a message letting the user know it is running and how many tokens (using tiktoken) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens. 
        4c. The string output of the LCEL will be 'xml_generated_analysis' that contains the tagged prompt.
    Step 5. The code result of the model will contain a mix of text and XML separated by triple backticks. Create a Langchain LCEL template from the extract_xml prompt that has a JSON output.
        5a. Pass the following string parameters to the prompt during invoke:
            - 'xml_generated_analysis'
        5b. Pretty print a message letting the user know it is running and how many tokens (using tiktoken) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.
        5c. The JSON output of the LCEL will have the key 'xml_tagged' that contains the extracted tagged prompt.
    Step 6. Pretty print the extracted tagged prompt using the rich Markdown function. Also, print the number of tokens in the result and the cost.
    Step 7. Return the 'xml_tagged' string.
    

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.
```</example_input_prompt>
<example_input_code>```import os
import tiktoken
from rich import print as rprint
from rich.markdown import Markdown
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_community.cache import SQLiteCache
from langchain.globals import set_llm_cache
from llm_selector import llm_selector

# Ensure cache is set up
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

# Define a Pydantic model for JSON output
class XMLTaggedOutput(BaseModel):
    xml_tagged: str = Field(description="The XML tagged prompt")

def xml_tagger(raw_prompt: str, strength: float, temperature: float) -&gt; str:
    """
    Process a raw prompt to apply XML tagging using Langchain.

    :param raw_prompt: The input prompt to be processed.
    :param strength: The strength parameter for LLM selection.
    :param temperature: The temperature parameter for LLM selection.
    :return: The XML tagged prompt as a string.
    """
    try:
        # Step 1: Load the prompt files
        pdd_path = os.getenv('PDD_PATH')
        if not pdd_path:
            raise ValueError("PDD_PATH environment variable is not set")

        with open(os.path.join(pdd_path, 'prompts/xml_convertor_LLM.prompt'), 'r') as file:
            xml_convertor_prompt = file.read()

        with open(os.path.join(pdd_path, 'prompts/extract_xml_LLM.prompt'), 'r') as file:
            extract_xml_prompt = file.read()

        # Step 2: Create LCEL template from xml_convertor prompt
        xml_convertor_template = PromptTemplate.from_template(xml_convertor_prompt)

        # Step 3: Use the llm_selector function
        llm, input_cost, output_cost = llm_selector(strength, temperature)

        # Step 4: Run the code through the model using Langchain LCEL
        chain = xml_convertor_template | llm | StrOutputParser()

        # Token count and cost calculation
        encoding = tiktoken.get_encoding("cl100k_base")
        token_count = len(encoding.encode(raw_prompt))
        cost = (token_count / 1_000_000) * input_cost

        rprint(f"[bold green]Running XML conversion...[/bold green]")
        rprint(f"Token count: {token_count}, Cost: ${cost:.6f}")

        # Invoke the chain
        xml_generated_analysis = chain.invoke({"raw_prompt": raw_prompt})

        # Step 5: Create LCEL template from extract_xml prompt
        extract_xml_template = PromptTemplate.from_template(extract_xml_prompt)
        parser = JsonOutputParser(pydantic_object=XMLTaggedOutput)

        chain = extract_xml_template | llm | parser

        # Token count and cost calculation for the second step
        token_count = len(encoding.encode(xml_generated_analysis))
        cost = (token_count / 1_000_000) * output_cost

        rprint(f"[bold green]Extracting XML...[/bold green]")
        rprint(f"Token count: {token_count}, Cost: ${cost:.6f}")

        # Invoke the chain
        result = chain.invoke({"xml_generated_analysis": xml_generated_analysis})

        # Step 6: Pretty print the extracted tagged prompt
        xml_tagged = result['xml_tagged']
        rprint(Markdown(xml_tagged))
        rprint(f"Token count in result: {len(encoding.encode(xml_tagged))}, Cost: ${(len(encoding.encode(xml_tagged)) / 1_000_000) * output_cost:.6f}")

        # Step 7: Return the 'xml_tagged' string
        return xml_tagged

    except Exception as e:
        rprint(f"[bold red]Error:[/bold red] {e}")
        return ""

# Example usage
# xml_tagger("Tell me a joke about cats", 0.5, 0.7)```</example_input_code>
<example_change_prompt>```Use token_counter from llm_selector instead of tiktoken to count tokens in the prompt. Also, xml tagger should also return the total cost of running the LCELs.```</example_change_prompt>
<example_modified_prompt>```% You are an expert Python engineer. Your goal is to write a Python function, "xml_tagger", that will enhance a given LLM prompt by adding XML tags to improve its structure and readability. All output to the console will be pretty printed using the Python rich library.

% Here are the inputs and outputs of the function:
    Input: 
        'raw_prompt' - A string containing the prompt that needs XML tagging to improve its organization and clarity.
        'strength' - A float value representing the strength parameter for the LLM model.
        'temperature' - A float value representing the temperature parameter for the LLM model.
    Output: 
        'xml_tagged' - A string containing the prompt with properly added XML tags.
        'total_cost' - A float representing the total cost of running the LCELs.

% Here is an example of a Langchain LCEL program: ```&lt;./context/langchain_lcel_example.py&gt;```

% Here is an example how to select the Langchain llm and count tokens: ```&lt;./context/llm_selector_example.py&gt;``` 

% This program will use Langchain to do the following:
    Step 1. Use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/xml_convertor_LLM.prompt' and '$PDD_PATH/prompts/extract_xml_LLM.prompt' files.
    Step 2. Create a Langchain LCEL template from xml_convertor prompt so that it returns a string output.
    Step 3. Use the llm_selector function for the LLM model and token counting.
    Step 4. Run the code through the model using Langchain LCEL. 
        4a. Pass the following string parameters to the prompt during invoke:
            - 'raw_prompt'
        4b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens. 
        4c. The string output of the LCEL will be 'xml_generated_analysis' that contains the tagged prompt.
    Step 5. The code result of the model will contain a mix of text and XML separated by triple backticks. Create a Langchain LCEL template from the extract_xml prompt that has a JSON output.
        5a. Pass the following string parameters to the prompt during invoke:
            - 'xml_generated_analysis'
        5b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.
        5c. The JSON output of the LCEL will have the key 'xml_tagged' that contains the extracted tagged prompt.
    Step 6. Pretty print the extracted tagged prompt using the rich Markdown function. Also, print the number of tokens in the result and the cost.
    Step 7. Calculate the total cost by summing the costs from both LCEL runs.
    Step 8. Return the 'xml_tagged' string and the 'total_cost'.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.```</example_modified_prompt>
</example>
<example>
<example_number>7</example_number>
<example_input_prompt>```% You are an expert Python Software engineer. Your goal is to write a Python program, "fix_errors.py". All output to the console will be pretty printed with the Python rich package.

% You will be using a CLI program called pdd. Here is a detailed description of the program functionality: ```&lt;./context/README.md&gt;```

% This script will take in the following arguments:
    - unit_test_file
    - code_file
    - Python program to run to verify code still runs 
    - Strength
    - Number of times to run before giving up

% Follow these steps:
    Step 1. Remove the existing error.log file.
    Step 2. Run the unit_test_file with 'python -m pytest -vv' and pipe all output to error.log. If the test fails, then proceed to step 3.
    Step 3. 
        a. Print out the error message from error.log.
        b. Count the number of 'FAILED' and 'ERROR' from stdout. Keep in mind that given the '-vv' flag, the output will contain doubled the number of 'FAILED' and 'ERROR' messages.
        c. Make a copy of the unit_test_file and code_file but append the number of failed and errors, and the loop iteration number to the file names like this "unit_test_1_0_3.py" and "code_1_0_3.py", where there was one fail, zero errors and it is the third iteration through the loop.
    Step 4. Run 'python pdd/pdd.py' fix on the unit_test_file, code_file and error.log with the output being written to unit_test_file and code_file. Make sure global options come before the command when calling pdd. The pdd console output will get appended to the error.log with a separator between pytest and pdd program runs.
    Step 5. Run the Python program to verify the code still runs.
        a. If the program still runs then repeat the process from Step 2 with the updated unit test and code files unless the loop limit is reached.
        b. Otherwise, if program fails, then restore the original files and repeat the process from Step 4.
    Step 6. Run pytest one last time and pipe all output to error.log and print to console.```</example_input_prompt>
<example_input_code>```# To create the `fix_errors.py` script, we'll follow the steps outlined in your instructions. The script will automate the process of running unit tests, capturing errors, and using the PDD tool to attempt to fix those errors. Here's how the script can be structured:

# ```python
import subprocess
import shutil
import os
import sys

def run_pytest(unit_test_file):
    """Run pytest on the given unit test file and capture output."""
    result = subprocess.run(
        ['python', '-m', 'pytest', '-vv',
        unit_test_file],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    with open('error.log', 'a') as log_file:
        log_file.write(result.stdout)
        log_file.write(result.stderr)
    print(result.stdout)
    print(result.stderr)
    return result.returncode, result.stdout

def extract_errors(log_content):
    """Extract the number of failed tests and errors from the pytest output."""
    failed = int(log_content.count('FAILED') / 2)
    errors = int(log_content.count('ERROR') / 2)

    return failed, errors

def copy_files_with_suffix(unit_test_file, code_file, failed, errors, iteration):
    """Copy the unit test and code files with a suffix indicating the current state."""
    unit_test_copy = f"{os.path.splitext(unit_test_file)[0]}_{failed}_{errors}_{iteration}.py"
    code_copy = f"{os.path.splitext(code_file)[0]}_{failed}_{errors}_{iteration}.py"
    shutil.copy(unit_test_file, unit_test_copy)
    shutil.copy(code_file, code_copy)
    return unit_test_copy, code_copy

def run_pdd_fix(unit_test_file, code_file, error_file, strength):
    """Run the PDD fix command."""
    result = subprocess.run(
        [
            'python', 'pdd/pdd.py', '--strength', str(strength), 'fix',
            '--output-test', unit_test_file, '--output-code', code_file,
            unit_test_file, code_file, error_file
        ],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    with open('error.log', 'a') as log_file:
        log_file.write("\n--- Attempted Fix of Above Pytest Failures and Errors ---\n")
        log_file.write(result.stdout)
        log_file.write(result.stderr)
    print(result.stdout)
    print(result.stderr)
    return result.returncode

def run_verification_program(verification_program):
    """Run the verification program to ensure the code still runs."""
    result = subprocess.run(
        ['python', verification_program],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    return result.returncode == 0

def main(unit_test_file, code_file, verification_program, strength, max_iterations):
    iteration = 0
    previous_failed = previous_errors = 0

    while iteration &lt; max_iterations:
        return_code, log_content = run_pytest(unit_test_file)
        if return_code == 0:
            print("All tests passed.")
            break

        failed, errors = extract_errors(log_content)

        print(f"Iteration {iteration}: {failed} failed, {errors} errors")

        if failed == 0 and errors == 0:
            print("No new errors detected. Exiting.")
            break

        unit_test_copy, code_copy = copy_files_with_suffix(unit_test_file, code_file, failed, errors, iteration)

        run_pdd_fix(unit_test_file, code_file, 'error.log', strength)

        if run_verification_program(verification_program):
            print("Verification program ran successfully.")
        else:
            print("Verification program failed. Restoring original files.")
            shutil.copy(unit_test_copy, unit_test_file)
            shutil.copy(code_copy, code_file)

        iteration += 1
        
    run_pytest(unit_test_file)

if __name__ == "__main__":
    if len(sys.argv) != 6:
        print("Usage: python fix_errors.py <unit_test_file> <code_file> <verification_program> <strength> <max_iterations>")
        sys.exit(1)

    unit_test_file = sys.argv[1]
    code_file = sys.argv[2]
    verification_program = sys.argv[3]
    strength = float(sys.argv[4])
    max_iterations = int(sys.argv[5])

    main(unit_test_file, code_file, verification_program, strength, max_iterations)
# ```

# ### Explanation:

# 1. **Run Pytest**: The script runs the unit tests using `pytest` and captures the output in `error.log`.

# 2. **Extract Errors**: It extracts the number of failed tests and errors from the pytest output.

# 3. **Copy Files**: It makes copies of the unit test and code files with a suffix indicating the number of failures, errors, and the current iteration.

# 4. **Run PDD Fix**: It uses the PDD tool to attempt to fix the errors, appending the output to `error.log`.

# 5. **Verification**: It runs a verification program to ensure the code still runs correctly.

# 6. **Loop Control**: The process repeats until all tests pass or the maximum number of iterations is reached.

# This script should be run with the appropriate arguments as specified in the usage message.```</max_iterations></strength></verification_program></code_file></unit_test_file></example_input_code>
<example_change_prompt>```% Instead of using the CLI program 'pdd', you will be using this code module instead: ```from fix_errors_from_unit_tests import fix_errors_from_unit_tests

if __name__ == "__main__":
    # Define the inputs
    unit_test = "def test_add():\n    assert add(1, 2) == 3"  # A unit test that is expected to fail
    code = "def add(a, b):\n    return a + b"  # The code that the unit test is testing
    error = "NameError: name 'add' is not defined"  # The error message indicating the issue
    strength = 0.8  # A strength parameter for the LLM selection

    # Call the function to fix errors
    updated_unit_test, updated_code, fixed_unit_test, fixed_code = fix_errors_from_unit_tests(unit_test, code, error, strength)

    # Print the results
    print("Updated Unit Test:", updated_unit_test)
    print("Updated Code:", updated_code)
    print("Fixed Unit Test:", fixed_unit_test)
    print("Fixed Code:", fixed_code)
# ```

# ### Input Parameters

# - `unit_test` (str): The unit test code that is expected to fail due to an error.
# - `code` (str): The implementation code that the unit test is testing.
# - `error` (str): The error message that indicates what went wrong during the unit test execution.
# - `strength` (float): A parameter that influences the selection of the language model (LLM) used for fixing the errors. It typically ranges from 0 to 1.

# ### Output Parameters

# The function returns four values:
# - `updated_unit_test` (bool): Indicates whether the unit test needs to be updated.
# - `updated_code` (bool): Indicates whether the code needs to be updated.
# - `fixed_unit_test` (str): The corrected version of the unit test code.
# - `fixed_code` (str): The corrected version of the implementation code.```

% Also, instead of being a CLI program, this code module will be a Python function "fix_error_loop" that will also take in temperature.```</example_change_prompt>
<example_modified_prompt>```% You are an expert Python Software Engineer. Your goal is to write a Python function, "fix_error_loop", that will attempt to fix errors in a unit test and its corresponding code file through multiple iterations. All output to the console will be pretty printed using the Python rich library.

% Here are the inputs and outputs of the function:
    Inputs: 
        'unit_test_file' - A string containing the path to the unit test file.
        'code_file' - A string containing the path to the code file being tested.
        'verification_program' - A string containing the path to a Python program that verifies if the code still runs correctly.
        'strength' - A float between 0 and 1 that represents the strength of the LLM model to use.
        'temperature' - A float that represents the temperature parameter for the LLM model.
        'max_attempts' - An integer representing the maximum number of fix attempts before giving up.
    Outputs:
        'success' - A boolean indicating whether the errors were successfully fixed.
        'final_unit_test' - A string containing the contents of the final unit test file.
        'final_code' - A string containing the contents of the final code file.
        'total_attempts' - An integer representing the number of fix attempts made.

% Here is an example of the fix_errors_from_unit_tests function that will be used: ```&lt;./context/fix_errors_from_unit_tests_example.py&gt;```

% This function will do the following:
    Step 1. Remove the existing error.log file if it exists.
    Step 2. Initialize a counter for the number of attempts.
    Step 3. Enter a while loop that continues until max_attempts is reached:
        a. Run the unit_test_file with 'python -m pytest -vv' and pipe all output to error.log.
        b. If the test passes, break the loop.
        c. If the test fails:
           - Read and print the error message from error.log.
           - Count the number of 'FAILED' and 'ERROR' from stdout (accounting for the -vv flag doubling these messages).
           - Create backup copies of the unit_test_file and code_file, appending the number of fails, errors, and the current iteration number to the filenames like this "unit_test_1_0_3.py" and "code_1_0_3.py", where there was one fail, zero errors and it is the third iteration through the loop.
           - Read the contents of the unit_test_file and code_file.
           - Call fix_errors_from_unit_tests with the file contents, error from error.log, and the provided strength.
           - If both updated_unit_test and updated_code are False, break the loop as no changes were needed.
           - If either updated_unit_test or updated_code is True:
              * Write the fixed_unit_test and fixed_code back to their respective files.
              * Run the verification_program to check if the code still runs.
              * If the verification fails, restore the original files from the backups and continue the loop.
              * If the verification succeeds, continue to the next iteration.
        d. Increment the attempt counter.
    Step 4. After the loop ends, run pytest one last time, pipe the output to error.log, and print it to the console.
    Step 5. Return the success status, final unit test contents, final code contents, and total number of attempts.

% Ensure that the function handles potential errors gracefully, such as file I/O errors or subprocess execution failures. Use the rich library for all console output to enhance readability. Consider using context managers for file operations to ensure proper resource management.

% Note: The temperature parameter should be incorporated into the LLM selection process. You may need to modify the fix_errors_from_unit_tests function or use it in conjunction with the llm_selector to properly utilize this parameter.```</example_modified_prompt>
</example>
<example>
<example_number>8</example_number>
<example_input_prompt>```% You are an expert Python Software Engineer. Your goal is to write a python function, "fix_errors_from_unit_tests", that will fix unit test errors in a code file. All output to the console will be pretty print using the Python rich library.

% Here are the inputs and outputs of the function:
    Inputs:
        'unit_test' - A string containing the unit test code.
        'code' - A string containing the code under test.
        'error' - A string that contains the errors that need to be fixed.
        'strength' - A float between 0 and 1 that is the strength of the LLM model to use.       
    Outputs:
        'update_unit_test': Boolean indicating whether the unit test needs to be updated.
        'update_code': Boolean indicating whether the code under test needs to be updated.
        'fixed_unit_test' - a string that is the fixed unit test.
        'fixed_code' - a string that is the fixed code under test.

% Here is an example of a Langchain LCEL program: ```&lt;./context/langchain_lcel_example.py&gt;```

% Here is an example how to select the Langchain llm: ```&lt;./context/llm_selector_example.py&gt;``` 

% Here is an example how to use tiktoken: ```&lt;./context/tiktoken_example.py&gt;```

% This program will use Langchain to do the following:
    Step 1. Use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/fix_errors_from_unit_tests_LLM.prompt' file. Also load the 'extract_unit_code_fix_LLM.prompt' from the same directory.
    Step 2. Then this will create a Langchain LCEL template from the fix_errors_from_unit_tests prompt.
    Step 3. This will use llm_selector and a temperature of 0 for the llm model.
    Step 4. This will run the code through the model using Langchain LCEL. 
        4a. Be sure to pass the following string parameters to the prompt during invoke:
            - 'unit_test'
            - 'code'
            - 'errors'
        4b. Pretty print a message letting the user know it is running and how many tokens (using tiktoken) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.
    Step 5. This will pretty print the markdown formatting that is present in the result via the rich Markdown function. It will also pretty print the number of tokens in the result and the cost. Also, print out the total cost.
    Step 7. Then this will create a second Langchain LCEL template from the extract_unit_code_fix prompt.
    Step 8. This will use llm_selector with a strength setting of 0.5 instead of the strength function input above and a temperature of 0 for the llm model. However, instead of using String output, it will use the JSON output parser to get these keys: 'update_unit_test', 'update_code', 'fixed_unit_test' and 'fixed_code'.
    Step 9. This will run the code through the model using Langchain LCEL from Step 8. 
        9a. Be sure to pass the following string parameters to the prompt during invoke:
            - 'unit_test_fix': This is the result of the Langchain LCEL from Step 4.
            - 'unit_test'
            - 'code'
        9b. Pretty print a message letting the user know it is running and how many tokens (using tiktoken) are in the prompt and the cost.
    Step 10. Print the total cost of both runs and return 'update_unit_test', 'update_code', 'fixed_unit_test' and 'fixed_code' as individual values from the JSON output parser.```</example_input_prompt>
<example_input_code>```import os
from rich import print as rprint
from rich.markdown import Markdown
import tiktoken
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_community.cache import SQLiteCache
from langchain.globals import set_llm_cache
from llm_selector import llm_selector
from pydantic import BaseModel, Field
from langchain_core.pydantic_v1 import BaseModel as LangchainBaseModel

# Setup cache to save money and increase speeds
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

class FixResult(LangchainBaseModel):
    update_unit_test: bool = Field(description="Whether the unit test needs to be updated")
    update_code: bool = Field(description="Whether the code needs to be updated")
    fixed_unit_test: str = Field(description="The fixed unit test code")
    fixed_code: str = Field(description="The fixed code")

def fix_errors_from_unit_tests(unit_test, code, error, strength):
    # Step 1: Load the prompt files
    pdd_path = os.getenv('PDD_PATH')
    if not pdd_path:
        raise EnvironmentError("PDD_PATH environment variable is not set.")
    
    with open(os.path.join(pdd_path, 'prompts', 'fix_errors_from_unit_tests_LLM.prompt'), 'r') as file:
        fix_errors_prompt = file.read()
    
    with open(os.path.join(pdd_path, 'prompts', 'extract_unit_code_fix_LLM.prompt'), 'r') as file:
        extract_fix_prompt = file.read()
    
    # Step 2: Create Langchain LCEL template from the fix_errors_from_unit_tests prompt
    fix_errors_template = PromptTemplate.from_template(fix_errors_prompt)
    
    # Step 3: Use llm_selector and a temperature of 0 for the llm model
    llm, token_counter, input_cost, output_cost = llm_selector(strength, 0)
    
    # Step 4: Run the code through the model using Langchain LCEL
    chain = fix_errors_template | llm | StrOutputParser()
    
    # Prepare the input for the prompt
    prompt_input = {
        "unit_test": unit_test,
        "code": code,
        "errors": error
    }
    
    # Calculate token count and cost
    encoding = tiktoken.get_encoding("cl100k_base")
    token_count = len(encoding.encode(str(prompt_input)))
    cost = (token_count / 1_000_000) * input_cost
    
    rprint(f"[bold green]Running the model with {token_count} tokens. Estimated cost: ${cost:.6f}[/bold green]")
    
    # Invoke the chain
    result = chain.invoke(prompt_input)
    
    # Pretty print the result
    rprint(Markdown(result))
    
    # Calculate result token count and cost
    result_token_count = len(encoding.encode(result))
    result_cost = (result_token_count / 1_000_000) * output_cost
    
    rprint(f"[bold green]Result contains {result_token_count} tokens. Estimated cost: ${result_cost:.6f}[/bold green]")
    rprint(f"[bold green]Total cost: ${cost + result_cost:.6f}[/bold green]")
    
    # Step 7: Create a second Langchain LCEL template from the extract_unit_code_fix prompt
    extract_fix_template = PromptTemplate.from_template(extract_fix_prompt)
    
    # Step 8: Use llm_selector with a strength setting of 0.5 and a temperature of 0
    llm, token_counter, input_cost, output_cost = llm_selector(0.5, 0)
    
    from langchain.output_parsers import PydanticOutputParser
    parser = PydanticOutputParser(pydantic_object=FixResult)
    
    chain = extract_fix_template | llm | parser
    
    # Prepare the input for the second prompt
    prompt_input = {
        "unit_test_fix": result,
        "unit_test": unit_test,
        "code": code
    }
    
    # Calculate token count and cost for the second run
    token_count = len(encoding.encode(str(prompt_input)))
    cost = (token_count / 1_000_000) * input_cost
    
    rprint(f"[bold green]Running the second model with {token_count} tokens. Estimated cost: ${cost:.6f}[/bold green]")
    
    # Invoke the chain
    result = chain.invoke(prompt_input)
    
    # Calculate result token count and cost for the second run
    result_token_count = len(encoding.encode(str(result.dict())))
    result_cost = (result_token_count / 1_000_000) * output_cost
    
    rprint(f"[bold green]Result contains {result_token_count} tokens. Estimated cost: ${result_cost:.6f}[/bold green]")
    rprint(f"[bold green]Total cost of both runs: ${cost + result_cost:.6f}[/bold green]")
    
    # Return the parsed result as separate values
    return result.update_unit_test, result.update_code, result.fixed_unit_test, result.fixed_code

# Example usage
if __name__ == "__main__":
    unit_test = "def test_add():\n    assert add(1, 2) == 3"
    code = "def add(a, b):\n    return a + b"
    error = "NameError: name 'add' is not defined"
    strength = 0.8
    
    updated_unit_test, updated_code, fixed_unit_test, fixed_code = fix_errors_from_unit_tests(unit_test, code, error, strength)
    print("Updated Unit Test:", updated_unit_test)
    print("Updated Code:", updated_code)
    print("Fixed Unit Test:", fixed_unit_test)
    print("Fixed Code:", fixed_code)```</example_input_code>
<example_change_prompt>```Use token_counter from llm_selector instead of tiktoken to count tokens in the prompt. Also, fix_errors_from_unit_tests should take in and use temperature. This should also output the total cost of the LCEL runs.```</example_change_prompt>
<example_modified_prompt>```% You are an expert Python Software Engineer. Your goal is to write a python function, "fix_errors_from_unit_tests", that will fix unit test errors in a code file. All output to the console will be pretty print using the Python rich library.

% Here are the inputs and outputs of the function:
    Inputs:
        'unit_test' - A string containing the unit test code.
        'code' - A string containing the code under test.
        'error' - A string that contains the errors that need to be fixed.
        'strength' - A float between 0 and 1 that is the strength of the LLM model to use.
        'temperature' - A float that controls the randomness of the LLM's output.
    Outputs:
        'update_unit_test': Boolean indicating whether the unit test needs to be updated.
        'update_code': Boolean indicating whether the code under test needs to be updated.
        'fixed_unit_test' - A string that is the fixed unit test.
        'fixed_code' - A string that is the fixed code under test.
        'total_cost' - A float representing the total cost of the LCEL runs.

% Here is an example of a Langchain LCEL program: ```&lt;./context/langchain_lcel_example.py&gt;```

% Here is an example how to select the Langchain llm and count tokens: ```&lt;./context/llm_selector_example.py&gt;``` 

% This program will use Langchain to do the following:
    Step 1. Use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/fix_errors_from_unit_tests_LLM.prompt' file. Also load the 'extract_unit_code_fix_LLM.prompt' from the same directory.
    Step 2. Then this will create a Langchain LCEL template from the fix_errors_from_unit_tests prompt.
    Step 3. This will use llm_selector with the provided strength and temperature for the llm model.
    Step 4. This will run the code through the model using Langchain LCEL. 
        4a. Be sure to pass the following string parameters to the prompt during invoke:
            - 'unit_test'
            - 'code'
            - 'errors'
        4b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.
    Step 5. This will pretty print the markdown formatting that is present in the result via the rich Markdown function. It will also pretty print the number of tokens in the result and the cost. Also, print out the cost of this run.
    Step 6. Then this will create a second Langchain LCEL template from the extract_unit_code_fix prompt.
    Step 7. This will use llm_selector with a strength setting of 0.5 and the provided temperature for the llm model. However, instead of using String output, it will use the JSON output parser to get these keys: 'update_unit_test', 'update_code', 'fixed_unit_test' and 'fixed_code'.
    Step 8. This will run the code through the model using Langchain LCEL from Step 7. 
        8a. Be sure to pass the following string parameters to the prompt during invoke:
            - 'unit_test_fix': This is the result of the Langchain LCEL from Step 4.
            - 'unit_test'
            - 'code'
        8b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost.
    Step 9. Calculate the total cost by summing the costs from both LCEL runs.
    Step 10. Print the total cost of both runs and return 'update_unit_test', 'update_code', 'fixed_unit_test', 'fixed_code', and 'total_cost' as individual values from the JSON output parser.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.```</example_modified_prompt>
</example>
</examples>
<task>
<input_prompt>```% You are an expert Python Software Engineer. Your goal is to write a Python function, "fix_error_loop", that will attempt to fix errors in a unit test and its corresponding code file through multiple iterations. All output to the console will be pretty printed using the Python rich library.

% Here are the inputs and outputs of the function:
    Inputs: 
        'unit_test_file' - A string containing the path to the unit test file.
        'code_file' - A string containing the path to the code file being tested.
        'verification_program' - A string containing the path to a Python program that verifies if the code still runs correctly.
        'strength' - A float between 0 and 1 that represents the strength of the LLM model to use.
        'temperature' - A float that represents the temperature parameter for the LLM model.
        'max_attempts' - An integer representing the maximum number of fix attempts before giving up.
    Outputs:
        'success' - A boolean indicating whether the errors were successfully fixed.
        'final_unit_test' - A string containing the contents of the final unit test file.
        'final_code' - A string containing the contents of the final code file.
        'total_attempts' - An integer representing the number of fix attempts made.

% Here is an example of the fix_errors_from_unit_tests function that will be used: ```&lt;./context/fix_errors_from_unit_tests_example.py&gt;```

% This function will do the following:
    Step 1. Remove the existing error.log file if it exists.
    Step 2. Initialize a counter for the number of attempts.
    Step 3. Enter a while loop that continues until max_attempts is reached:
        a. Run the unit_test_file with 'python -m pytest -vv' and pipe all output to error.log.
        b. If the test passes, break the loop.
        c. If the test fails:
           - Read and print the error message from error.log.
           - Count the number of 'FAILED' and 'ERROR' from stdout (accounting for the -vv flag doubling these messages).
           - Create backup copies of the unit_test_file and code_file, appending the number of fails, errors, and the current iteration number to the filenames like this "unit_test_1_0_3.py" and "code_1_0_3.py", where there was one fail, zero errors and it is the third iteration through the loop.
           - Read the contents of the unit_test_file and code_file.
           - Call fix_errors_from_unit_tests with the file contents, error from error.log, and the provided strength.
           - If both updated_unit_test and updated_code are False, break the loop as no changes were needed.
           - If either updated_unit_test or updated_code is True:
              * Write the fixed_unit_test and fixed_code back to their respective files.
              * Run the verification_program to check if the code still runs.
              * If the verification fails, restore the original files from the backups and continue the loop.
              * If the verification succeeds, continue to the next iteration.
        d. Increment the attempt counter.
    Step 4. After the loop ends, run pytest one last time, pipe the output to error.log, and print it to the console.
    Step 5. Return the success status, final unit test contents, final code contents, and total number of attempts.

% Ensure that the function handles potential errors gracefully, such as file I/O errors or subprocess execution failures. Use the rich library for all console output to enhance readability. Consider using context managers for file operations to ensure proper resource management.

% Note: The temperature parameter should be incorporated into the LLM selection process. You may need to modify the fix_errors_from_unit_tests function or use it in conjunction with the llm_selector to properly utilize this parameter.```</input_prompt>
<input_code>```{&lt;./context/change/9/initial_fix_error_loop.py&gt;}```</input_code>
<change_prompt>```% Update the prompt use the latest definition of fix_errors_from_unit_tests as shown in this example: ```from fix_errors_from_unit_tests import fix_errors_from_unit_tests

# Define the inputs
unit_test_code = """
def test_addition():
    assert add(1, 2) == 4  # Intentional error
"""

code_under_test = """
def add(a, b):
    return a + b
"""

error_message = "AssertionError: assert 3 == 4"
strength = 0.7  # Adjust the strength for LLM selection
temperature = 0  # Adjust the temperature for LLM selection

try:
    # Call the function to fix errors
    update_unit_test, update_code, fixed_unit_test, fixed_code, total_cost = fix_errors_from_unit_tests(
        unit_test=unit_test_code,
        code=code_under_test,
        error=error_message,
        strength=strength,
        temperature=temperature
    )

    # Output the results
    print(f"Update Unit Test: {update_unit_test}")
    print(f"Update Code: {update_code}")
    print(f"Fixed Unit Test:\n{fixed_unit_test}")
    print(f"Fixed Code:\n{fixed_code}")
    print(f"Total Cost: ${total_cost:.6f}")
except Exception as e:
    print(f"An error occurred: {e}")```

% Also, output the total cost of all the runs and take in an 'budget' input to stop the iterations if the total cost exceeds the budget.

% Before finishing the function, copy back the iteration of fixed_unit_test and fixed_code that meets these criteria in priority order:
    1) Had the lowest number of 'ERROR's
    2) Had the lowest number of 'FAILED's
This is so that the function saves the most successful iteration of the fixed code and unit test where error's are prioritized over fail's. Be sure to also consider the last run when decided which iteration to copy back. If the last run is the best, no need to copy back.```</change_prompt>
</task>
<steps>
<step1>Explain in detail step by step the ramifications of the change_prompt on the input_prompt.</step1>
<step2>Explain in detail step by step what changes need to be made to the input_prompt to generate the modified_prompt based on Step 1.</step2>
<step3>Generate the modified_prompt based on Step 2. Except for the change, the rest of the existing functionality of the input_prompt should remain. Structure the prompt similar to the example prompts more esp. including the descriptions of the inputs and outputs.</step3>
</steps>