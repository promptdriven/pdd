% You are an expert Python engineer. Your goal is to write `pdd/server/routes/prompts.py`.

% Role & Scope
  This module provides REST API endpoints for prompt analysis and preprocessing.
  It enables preview and cost estimation before running expensive operations.

<context.python_preamble><include>context/python_preamble.prompt</include></context.python_preamble>

% Documentation References
  - FastAPI Routers: https://fastapi.tiangolo.com/tutorial/bigger-applications/
  - Pydantic Models: https://docs.pydantic.dev/latest/

% Requirements
  1. **POST /analyze**:
     - Body: path (required), model (optional, default "claude-sonnet-4-20250514"),
       preprocess (optional, default true), content (optional for direct analysis)
     - Returns: raw_content, processed_content (if preprocess), raw_metrics,
       processed_metrics, preprocessing_succeeded, preprocessing_error
     - Size limit: 500KB for both file reading and direct content
     - If content provided, analyze it instead of reading from file
     - Preprocess using pdd.preprocess with recursive=True, double_curly_brackets=True

  2. **GET /sync-status**:
     - Query params: basename (module name), language (e.g., "python")
     - Returns SyncStatusResponse with:
       - status: "in_sync", "prompt_changed", "code_changed", "conflict", "never_synced"
       - last_sync_timestamp, last_sync_command
       - prompt_modified, code_modified booleans
       - fingerprint_exists, prompt_exists, code_exists booleans
     - Uses read_fingerprint, get_pdd_file_paths, calculate_sha256 from sync_determine_operation

  3. **GET /models**:
     - Returns ModelsResponse with list of available LLM models
     - Each ModelInfo includes: model, provider, input_cost, output_cost, elo,
       context_limit, max_thinking_tokens, reasoning_type, structured_output
     - Loads from llm_model.csv via _load_model_data
     - Sorted by ELO descending (best models first)

  4. **POST /check-match**:
     - Body: MatchCheckRequest with prompt_content, code_content, strength
     - Uses llm_invoke to evaluate how well code matches prompt requirements
     - Returns MatchCheckResponse with:
       - result: match_score (0-100), summary, missing, extra, suggestions
       - cost, model used

  5. **POST /diff-analysis**:
     - Body: DiffAnalysisRequest with prompt_content, code_content, strength, mode
     - mode: "quick" (capped strength 0.25) or "detailed" (full analysis)
     - Uses llm_invoke for strict regeneration capability analysis
     - Returns DiffAnalysisResponse with:
       - result: DiffAnalysisResult with overallScore, canRegenerate, regenerationRisk, summary, sections, hiddenKnowledge, lineMappings, stats
       - canRegenerate: boolean indicating if prompt can safely regenerate the code
       - regenerationRisk: "low", "medium", "high", or "critical"
       - sections: list of DiffSection with id, promptRange, codeRanges, status, matchConfidence, semanticLabel, notes (required)
       - hiddenKnowledge: list of HiddenKnowledge with type, location, description, regenerationImpact, suggestedPromptAddition
       - lineMappings: list of LineMapping with promptLine, codeLines, matchType
       - stats: DiffStats with totalRequirements, matchedRequirements, missingRequirements, hiddenKnowledgeCount, criticalGaps
       - cached flag indicating if result was from cache
     - Implements in-memory caching with 10-minute TTL based on content hash
     - Cache key generated from prompt_content + code_content + mode

  6. **Pydantic Models**:
     - CostEstimateResponse: input_cost, model, tokens, cost_per_million, currency
     - TokenMetricsResponse: token_count, context_limit, context_usage_percent, cost_estimate
     - PromptAnalyzeRequest: path, model, preprocess, content
     - PromptAnalyzeResponse: raw_content, processed_content, raw_metrics,
       processed_metrics, preprocessing_succeeded, preprocessing_error
     - SyncStatusResponse: status, last_sync_timestamp, last_sync_command,
       prompt_modified, code_modified, fingerprint_exists, prompt_exists, code_exists
     - ModelInfo: model, provider, input_cost, output_cost, elo, context_limit,
       max_thinking_tokens, reasoning_type, structured_output
     - ModelsResponse: models list, default_model
     - MatchCheckRequest: prompt_content, code_content, strength
     - MatchCheckResult: match_score, summary, missing, extra, suggestions
     - MatchCheckResponse: result, cost, model
     - PromptRange: startLine, endLine, text (for diff analysis line ranges)
     - CodeRange: startLine, endLine, text (for diff analysis code ranges)
     - DiffSection: id, promptRange, codeRanges, status, matchConfidence, semanticLabel, notes (required - explains WHY the status exists)
     - LineMapping: promptLine, codeLines, matchType (exact/semantic/partial/none)
     - HiddenKnowledgeLocation: startLine, endLine
     - HiddenKnowledge: type, location, description, regenerationImpact, suggestedPromptAddition
     - DiffStats: totalRequirements, matchedRequirements, missingRequirements, totalCodeFeatures, documentedFeatures, undocumentedFeatures, promptToCodeCoverage, codeToPromptCoverage, hiddenKnowledgeCount, criticalGaps
     - DiffAnalysisResult: overallScore, canRegenerate, regenerationRisk, promptToCodeScore, codeToPromptScore, summary, sections, codeSections, hiddenKnowledge, lineMappings, stats, missing, extra, suggestions
     - DiffAnalysisRequest: prompt_content, code_content, strength, mode
     - DiffAnalysisResponse: result, cost, model, analysisMode, cached

  7. **Token metrics**:
     - Use get_token_metrics from pdd.server.token_counter
     - Pricing CSV at project_root/.pdd/llm_model.csv if exists
     - Convert returned objects to response models

  8. **Error handling**:
     - 403 for path validation failures (SecurityError)
     - 404 for non-existent files
     - 400 for directories or files too large or non-text files
     - Preprocessing errors captured in response, not thrown
     - 500 for internal errors (sync status, model loading, match checking, diff analysis)

  9. **Security**:
     - All paths validated through PathValidator dependency
     - Change to project_root during preprocessing (for relative includes)

% Dependencies
  <fastapi_example>
  <server.main><include>context/fastapi_example.py</include></server.main>
  </fastapi_example>
  <security_note>
  Use PathValidator from pdd/server/security.py via dependency injection
  <server.security><include>context/server/security_example.py</include></server.security>
  </security_note>
  Use pdd.sync_determine_operation for fingerprint and hash utilities:
    - read_fingerprint, get_pdd_file_paths, calculate_sha256
  <include>context/sync_determine_operation_example.py</include>
  Use pdd.llm_invoke for LLM-based match checking:
  <include>context/llm_invoke_example.py</include>
    - llm_invoke, _load_model_data, LLM_MODEL_CSV_PATH, DEFAULT_BASE_MODEL
  Use MODEL_CONTEXT_LIMITS from pdd.server.token_counter

% Instructions
  - Create router with prefix="/api/v1/prompts", tags=["prompts"]
  - Use Depends() for PathValidator injection via get_path_validator()
  - Provide set_path_validator() for configuration
  - Use rich.console.Console for logging (with ImportError fallback)

% Deliverables
  - Code: `pdd/server/routes/prompts.py`
  - Export: router
