% You are an expert Python engineer. Your goal is to write a Python function, 'code_generator_main', that will be the CLI wrapper for generating code from prompts. This function will read a prompt file, support parameterized prompts via a variables map, generate code using the code_generator or incremental_code_generator function, handle post-process Python hooks, support LLM toggle functionality, and handle the output location.

<include>./context/python_preamble.prompt</include>
<include>./context/ctx_obj_params.prompt</include>

% Here are the inputs and outputs of the function:
   Inputs:
      - `ctx` (`click.Context`): The Click context object (see ctx_obj_params.prompt for available keys).
      - 'prompt_file' - A string containing the path to the prompt file to generate code from.
      - 'output' - An optional string containing the path where to save the generated code. If None, uses default naming convention.
      - 'original_prompt' - An optional string containing the path to the original prompt file used for incremental generation. If None, attempts to use git to find the last committed version.
      - 'incremental' - A boolean that forces the use of incremental patching even if the diff analyzer suggests full regeneration. Default is False.
      - 'env_vars' - An optional dictionary `Dict[str, str]` containing template variables passed from the CLI via `-e/--env`.
      - 'verbose' - A boolean flag for detailed logging output.
      - 'quiet' - A boolean flag to suppress non-error output.
   Outputs:
      - Returns a tuple containing (`str`, `bool`, `float`, `str`):
            - `str`: The generated or updated code.
            - `bool`: Whether the operation was incremental (True) or a full regeneration (False).
            - `float`: The total cost of the operation.
            - `str`: The name of the model used.

<examples>
   % Here are examples of how to use internal modules:
   <internal_example_modules>
      - Here is an example of how to use the `construct_paths` function:
      <construct_paths_example>
      <include>context/construct_paths_example.py</include>
      </construct_paths_example>
      % When `construct_paths` is called with `command_options` (e.g., `{'output': output_value}`),
      % the resolved path for that option will be available in the returned `output_file_paths` dictionary
      % under the *same key* used in `command_options` (e.g., `output_file_paths.get('output')`).

      - Here is an example of how to use the `code_generator` function:
      <code_generator_example>
      ```python
      from pdd.code_generator import code_generator # Assuming pdd.code_generator is the path
      # from .code_generator import code_generator # Or relative if used within the same package context

      prompt_content = "Create a Python function that adds two numbers and returns the sum."
      language = "python"
      strength = 0.5
      temperature = 0.2
      time_budget = 60  # Example time budget in seconds, e.g., DEFAULT_TIME
      verbose = True

      generated_code, cost, model_name = code_generator(
          prompt=prompt_content,
          language=language,
          strength=strength,
          temperature=temperature,
          time=time_budget, # Pass the time budget
          verbose=verbose
      )

      print(f"Generated Code:\n{generated_code}")
      print(f"Cost: ${cost:.6f}, Model: {model_name}")
      ```
      </code_generator_example>
      
      - Here is an example of how to use the `incremental_code_generator` function:
      <incremental_code_generator_example>
      ```python
      from pdd.incremental_code_generator import incremental_code_generator # Assuming pdd.incremental_code_generator is the path
      # from .incremental_code_generator import incremental_code_generator # Or relative

      original_prompt_content = "def greet():\n    print('Hello World')"
      new_prompt_content = "def greet(name):\n    print(f'Hello, {name}!')"
      existing_code_content = "def greet():\n    print('Hello World')"
      language = "python"
      strength = 0.6
      temperature = 0.1
      time_budget = 120  # Example time budget in seconds
      force_incremental_flag = False
      verbose_flag = True
      should_preprocess_prompt = True # Typically True for incremental

      generated_code, was_incremental, cost, model_name = incremental_code_generator(
          original_prompt=original_prompt_content,
          new_prompt=new_prompt_content,
          existing_code=existing_code_content,
          language=language,
          strength=strength,
          temperature=temperature,
          time=time_budget, # Pass the time budget
          force_incremental=force_incremental_flag,
          verbose=verbose_flag,
          preprocess_prompt=should_preprocess_prompt
      )

      print(f"Generated Code:\n{generated_code}")
      print(f"Was Incremental: {was_incremental}, Cost: ${cost:.6f}, Model: {model_name}")
      ```
      </incremental_code_generator_example>

      - Here is an example of how to use the `get_jwt_token` function to get the JWT_TOKEN:
      <get_jwt_token_example>
      <include>context/get_jwt_token_example.py</include>
      </get_jwt_token_example>
      % The `get_jwt_token` function is an async function and must be run with `asyncio.run`.
      % The implementation expects environment variables `NEXT_PUBLIC_FIREBASE_API_KEY` and `GITHUB_CLIENT_ID` to be set
      % and passes them to `get_jwt_token(firebase_api_key=..., github_client_id=..., app_name="PDD Code Generator")`.

      - Here is an example of how to use the `preprocess` function:
      <preprocess_example>
      <include>context/preprocess_example.py</include>
      </preprocess_example>

      - Here is an example of how to use the `detect_host_python_executable` function:
      <python_env_detector_example>
      <include>context/python_env_detector_example.py</include>
      </python_env_detector_example>

   </internal_example_modules>

   % Here is how to call the cloud version of the code generator when not using the `--local` flag.
   % Note: The API documentation (represented by the included api-documentation.md) should reflect that
   % the JSON request payload to the cloud service can include a 'timeBudget' (or similar 'time'/'effort')
   % field if the API supports it, corresponding to the 'time' parameter from the CLI.
   <cloud_code_generator_example>
      <include>../docs/api-documentation.md</include>
   </cloud_code_generator_example>
</examples>

% Here is the README for the cli command that has details of how the 'generate' command works:
   <cli_command_readme>
      <include>./README.md</include>
   </cli_command_readme>

% LLM Toggle and Post-Process Functionality
   The function must support LLM toggle functionality to enable/disable LLM generation while still allowing post-processing:
   1. Parse YAML front matter to check for `llm` field (boolean, defaults to true)
   2. Check environment variables `llm` or `LLM` for override
   3. When `llm=false`, skip LLM generation and run post-processing only
   4. Implement force flag logic: `effective_force = force_overwrite or not llm_enabled`
   5. Support post-process Python scripts via `post_process_python` and `post_process_args` in front matter
   6. Handle placeholder substitution in post-process arguments (`{INPUT_FILE}`, `{APP_NAME}`, `{OUTPUT_HTML}`)
   7. Support both stdin and argv modes for post-process scripts
   8. Use `detect_host_python_executable()` for subprocess execution
   9. Set environment variables for post-process scripts: `PDD_LANGUAGE`, `PDD_OUTPUT_PATH`, `PDD_PROMPT_FILE`, `PDD_LLM`, `PDD_ENV_VARS`

% Cloud vs Local Execution Strategy (with parameter substitution):
   1. If the `--local` flag is explicitly provided, use local execution directly.
   2. Otherwise, attempt cloud execution first:
      - Preprocess the prompt content to expand includes only: call `preprocess(prompt_text, recursive=True, double_curly_brackets=False)`.
      - Apply variable substitution using `env_vars` to replace only `$KEY` and `${KEY}` where keys exist in `env_vars`. Leave unknown `$...` placeholders unchanged.
      - After substitution, apply curly‑brace doubling if needed by downstream generators.
      - Obtain a JWT via `get_jwt_token` using `NEXT_PUBLIC_FIREBASE_API_KEY` and `GITHUB_CLIENT_ID`.
      - Use a request timeout of 400 seconds.
      - Send JSON payload with fields: `{ "promptContent": <processed prompt>, "language": <language>, "strength": <strength>, "temperature": <temperature>, "verbose": <verbose> }`.
        % Note: A time/effort field (e.g., `timeBudget`) is not currently sent; include only if/when the API supports it.
      - Cloud endpoint: `https://us-central1-prompt-driven-development.cloudfunctions.net/generateCode`
      - Expected response fields: `generatedCode`, `totalCost`, `modelName`.
      - Post-process JSON responses: When `language` is "json" (case-insensitive), strip markdown code fences from `generatedCode` if present. The cloud API may return fenced JSON like "```json\n...\n```". Remove leading "```json" or "```" and trailing "```" to extract the raw JSON content.
   3. If cloud execution fails (auth/network/HTTP/JSON errors, timeout, or missing code):
      - Log a warning and automatically fall back to local execution.
   4. For local execution, call the local code generator with the already preprocessed and substituted prompt, and set `preprocess_prompt=False` on the generator call.
   5. For both modes, if `verbose` is true, print relevant execution information using Rich panels.

% The function handles incremental code generation:
    1. If an output location is provided and the file exists, attempt incremental generation if:
       - The original prompt is specified explicitly via the `--original-prompt` parameter; or
       - The prompt file is tracked in git. If the on-disk prompt differs from `HEAD`, use `HEAD` as the original; if it matches `HEAD`, search recent history (up to ~10 commits) for a prior different version and use it if found; otherwise fall back to `HEAD` with a warning.
    2. Before calling the incremental generator, stage with `git add` only those files (prompt/output) that are untracked or differ from `HEAD` to aid rollback.
    3. Always perform full generation via the code_generator function if:
       - No output location is provided, or
       - The output file doesn't exist, or
       - We can't get the original prompt (no --original-prompt and not in git)
       - The incremental_code_generator returns is_incremental=False indicating a full regeneration is needed
    4. Force incremental generation if the --incremental flag is set, but warn if no output file exists and do full generation
    5. Parameterized prompts and incremental generation:
       - Apply the same include expansion and variable substitution (using `env_vars`) to both the current prompt content and the original prompt content before diffing or passing to the incremental generator. This ensures like‑for‑like comparison and patching.

% The function's implementation should include these key steps:
    1. Retrieve `strength`, `temperature`, and `time` from `ctx.obj` using defaults (e.g., `time = ctx.obj.get('time', DEFAULT_TIME)`).
    2. Process the prompt_file:
       - Read content; expand includes with `preprocess(prompt, recursive=True, double_curly_brackets=False)`.
       - Apply variable substitution using `env_vars` (`$KEY` and `${KEY}` only when provided). Unknown placeholders remain unchanged.
       - After substitution, apply curly‑brace doubling if appropriate.
    3. Determine the language with clear precedence and context support:
       - If YAML front matter specifies `language`, use it.
       - Otherwise, determine the language using `construct_paths` (may infer from file extension, special cases, or filename suffix).
       - When calling `construct_paths`, pass `context_override=ctx.obj.get('context')` so a global `--context` is honored.
    4. Determine LLM state and whether to use incremental generation or full generation:
       - Parse front matter to check for `llm` field (boolean, defaults to true)
       - Check environment variables `llm` or `LLM` for override
       - If LLM is disabled, we're only doing post-processing, so skip overwrite confirmation (`effective_force = force_overwrite or not llm_enabled`)
       - Check if output is specified and file exists
       - If original_prompt is specified, use it; otherwise try to get last committed version from git
       - If both conditions met, try incremental generation
       - Note: Full generation only runs if LLM is enabled (`llm_enabled and not was_incremental_operation`)
    5. For incremental generation:
       - Read the existing code from the output file
       - Read original prompt content (from specified file or git); expand includes and apply the same variable substitution using `env_vars` as with the current prompt
       - Stage prompt_file and output file with git (if they aren't already staged/committed)
       - Call incremental_code_generator with appropriate parameters (including `strength`, `temperature`, and `time` from `ctx.obj`)
       - If incremental_code_generator returns is_incremental=False, fall back to full generation
    6. For full generation:
       - Call code_generator (local version) with the preprocessed+substituted prompt and `preprocess_prompt=False` (including `strength`, `temperature`, and `time` from `ctx.obj`). For cloud execution, ensure the `processed prompt` includes include expansion and variable substitution before sending.
        - If cloud execution fails, automatically fall back to local execution with appropriate warnings
    7. Write the final code to the output location:
       - If `--output` ends with a path separator or points to a directory, use the resolved default filename from `construct_paths`.
       - If `--output` is omitted, derive the filename and destination via `construct_paths` and write the file there. Respect any configured `PDD_GENERATE_OUTPUT_PATH` used by the path resolver.
       - Before writing, expand `$KEY`/`${KEY}` in the final `output_path` string using only keys present in `env_vars`. Leave unknown placeholders unchanged.
    8. Handle LLM toggle and post-process Python hooks:
       - Determine if LLM is enabled based on front matter `llm` field or environment variables (`llm`, `LLM`).
       - If LLM is disabled, require a `post_process_python` script to be specified in front matter.
       - Resolve post-process script from environment override, then front matter, then sensible defaults.
       - For architecture templates, default to `render_mermaid.py` if available.
       - Run post-process script with appropriate arguments and environment variables.
       - Support both stdin and argv modes for post-process scripts.
       - Handle placeholder substitution in post-process arguments (e.g., `{INPUT_FILE}`, `{APP_NAME}`).
       - Set environment variables for the post-process script: `PDD_LANGUAGE`, `PDD_OUTPUT_PATH`, `PDD_PROMPT_FILE`, `PDD_LLM`, `PDD_ENV_VARS`.
       - Handle temporary file creation for argv mode when LLM is enabled.
       - Use `detect_host_python_executable()` to find the appropriate Python interpreter.
       - Handle subprocess execution with proper error handling and timeout (300 seconds).
    9. Safety net for architecture HTML generation:
       - After writing architecture.json files, automatically run `render_mermaid.py` if HTML doesn't exist.
       - Use appropriate Python executable and pass correct arguments.
       - Only run if the HTML file doesn't already exist to avoid overwriting.
    10. Return the generated code, incremental flag, cost, and model name

% Helper function expectations:
    - Implement a small helper, e.g., `expand_vars(text: str, vars: Dict[str, str]) -> str`, that performs a single-pass substitution for `$KEY` and `${KEY}` only when `KEY in vars`. Unmatched placeholders remain unchanged; do not read from OS environment in this function.

% Template front‑matter & preprocessing requirements (Phase 2)
    - Parse YAML front matter at the very beginning of the prompt file when present (leading `---\n` … `\n---`).
      * Return a dict (or empty) and the remaining prompt body with the front matter stripped.
      * Do NOT send the YAML to the LLM; treat it as human/CLI metadata.
    - Merge variable defaults from front matter into `env_vars` (without overwriting explicit `-e` values). Validate required variables; if missing, raise a friendly `click.UsageError` listing the missing keys.
    - Honor `language` from front matter if present (overrides detection). Honor `output` from front matter when the CLI did not explicitly pass `--output`.
    - Support `llm` field in front matter to enable/disable LLM generation (boolean, defaults to true).
    - Support `post_process_python` field in front matter to specify a Python script to run after generation.
    - Support `post_process_args` field in front matter to specify arguments for the post-process script (list of strings with placeholder support).
    - Two‑pass preprocess to support variable‑driven includes:
        1) First pass: `preprocess(prompt, recursive=True, double_curly_brackets=False)` to resolve static includes while deferring unresolved ones. Strip any `<pdd>...</pdd>` blocks.
        2) Perform variable substitution with `expand_vars` on the intermediate result.
        3) Second pass: `preprocess(prompt, recursive=False, double_curly_brackets=True)` to resolve variable‑driven includes and apply brace doubling.
      Unknown `$VAR` placeholders remain unchanged in both content and output paths.
    - Support `<include-many>${VAR}</include-many>` where VAR is a comma- or newline‑separated list of file paths.
    - Maintain existing local/cloud execution and incremental generation behavior; only the prompt assembly changes as described.

    - Discover execution (optional):
        * If front matter includes a `discover` section (e.g., root, patterns, exclude, caps), execute discovery before preprocessing so its results can populate variables used in includes.
        * Merge discovered values into `env_vars` only for keys defined by the template's variables schema and not already set via `-e`. Represent multi-file results as comma-separated or newline-separated paths suitable for `<include-many>`.
        * Discovery is a CLI-side operation; ensure it runs with the appropriate caps and does not leak into the LLM prompt body.

    - Output schema validation (optional):
        * If front matter defines `output_schema` and the generated artifact is JSON (by `language` or output extension), validate the generated output against the schema.
        * On validation failure, raise a friendly `click.UsageError` summarizing the errors (and avoid writing an invalid file unless `--force` semantics dictate otherwise).
        * Note: Schema validation only runs when LLM is enabled (`if llm_enabled:`).
