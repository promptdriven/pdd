% You are an expert Python engineer. Your goal is to write a Python function, "xml_tagger", that will enhance a given LLM prompt by adding XML tags to improve its structure and readability. Additionally, the function should print the intermediate result after the XML conversion step to provide visibility into the process.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'raw_prompt' - A string containing the prompt that needs XML tagging to improve its organization and clarity.
        'strength' - A float value representing the strength parameter for the LLM model. Range is between 0 and 1.
        'temperature' - A float value representing the temperature parameter for the LLM model. Range is between 0 and 1.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs as a tuple: 
        'xml_tagged' - A string containing the prompt with properly added XML tags.
        'total_cost' - A float representing the total cost of running the LCELs.
        'model_name' - A string representing the name of the selected LLM model.

% Here is how to use the internal modules:
    <internal_modules>
        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>
    </internal_modules>

% This program will do the following:
    Step 1. Load the 'xml_convertor_LLM' and 'extract_xml_LLM' prompt templates.
    Step 2. Run the code through the model using llm_invoke with the provided strength and temperature. 
        2a. Pass the following string parameters to the prompt during invoke:
            - 'raw_prompt'
        2b. If verbose is True, pretty print a message letting the user know it is running and how many tokens are in the prompt and the cost.
        2c. The string output will be 'xml_generated_analysis' that contains the tagged prompt.
        2d. If verbose is True, print the 'xml_generated_analysis' to provide visibility into the intermediate result.
    Step 3. Run a second llm_invoke with the EXTRACTION_STRENGTH constant and the provided temperature to extract the XML.
        3a. Pass the following string parameters to the prompt during invoke:
            - 'xml_generated_analysis'
        3b. If verbose is True, pretty print a message letting the user know it is running and how many tokens are in the prompt and the cost.
        3c. The Pydantic output will have the key 'xml_tagged' that contains the extracted tagged prompt.
    Step 4. If verbose is True, pretty print the extracted tagged prompt using the rich Markdown function. Also print the number of tokens in the result and the cost.
    Step 5. Calculate the total cost by summing the costs from both runs.
    Step 6. Return the 'xml_tagged' string using 'get', the 'total_cost' and 'model_name'.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.