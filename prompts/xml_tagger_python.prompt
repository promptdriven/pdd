% You are an expert Python engineer. Your goal is to write a Python function, "xml_tagger", that will enhance a given LLM prompt by adding XML tags to improve its structure and readability. All output to the console will be pretty printed using the Python rich library.

% Here are the inputs and outputs of the function:
    Input: 
        'raw_prompt' - A string containing the prompt that needs XML tagging to improve its organization and clarity.
        'strength' - A float value representing the strength parameter for the LLM model.
        'temperature' - A float value representing the temperature parameter for the LLM model.
    Output: 
        'xml_tagged' - A string containing the prompt with properly added XML tags.
        'total_cost' - A float representing the total cost of running the LCELs.

% Here is an example of a Langchain LCEL program: ```<./context/langchain_lcel_example.py>```

% Here is an example how to select the Langchain llm and count tokens: ```<./context/llm_selector_example.py>``` 

% This program will use Langchain to do the following:
    Step 1. Use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/xml_convertor_LLM.prompt' and '$PDD_PATH/prompts/extract_xml_LLM.prompt' files.
    Step 2. Create a Langchain LCEL template from xml_convertor prompt so that it returns a string output.
    Step 3. Use the llm_selector function for the LLM model and token counting.
    Step 4. Run the code through the model using Langchain LCEL. 
        4a. Pass the following string parameters to the prompt during invoke:
            - 'raw_prompt'
        4b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens. 
        4c. The string output of the LCEL will be 'xml_generated_analysis' that contains the tagged prompt.
    Step 5. The code result of the model will contain a mix of text and XML separated by triple backticks. Create a Langchain LCEL template from the extract_xml prompt that has a JSON output.
        5a. Pass the following string parameters to the prompt during invoke:
            - 'xml_generated_analysis'
        5b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.
        5c. The JSON output of the LCEL will have the key 'xml_tagged' that contains the extracted tagged prompt.
    Step 6. Pretty print the extracted tagged prompt using the rich Markdown function. Also, print the number of tokens in the result and the cost.
    Step 7. Calculate the total cost by summing the costs from both LCEL runs.
    Step 8. Return the 'xml_tagged' string and the 'total_cost'.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.
