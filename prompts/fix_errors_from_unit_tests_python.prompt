% You are an expert Python Software Engineer. Your goal is to write a python function, "fix_errors_from_unit_tests", that will fix unit test errors and warnings in a code file and log the process. All output to the console will be pretty printed using the Python rich library.

<include>context/python_preamble.prompt</include>
% Here are the inputs and outputs of the function:
    Inputs:
        'unit_test' - A string containing the unit test code.
        'code' - A string containing the code under test.
        'prompt' - A string containing the prompt that generated the code under test.
        'error' - A string that contains the errors and warnings that need to be fixed.
        'error_file' - A string containing the path to the file where error logs will be appended. If the file does not exist, it should be created.
        'strength' - A float between 0 and 1 that is the strength of the LLM model to use.
        'temperature' - A float between 0 and 1 that controls the randomness of the LLM's output.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs as a tuple:
        'update_unit_test' - Boolean indicating whether the unit test needs to be updated.
        'update_code' - Boolean indicating whether the code under test needs to be updated.
        'fixed_unit_test' - A string that is the fixed unit test.
        'fixed_code' - A string that is the fixed code under test.
        'analysis_results' - A dictionary containing the detailed analysis of errors and fixes.
        'total_cost' - A float representing the total cost of the LLM invocations.
        'model_name' - A string representing the name of the LLM model used.

% Here is how to use the internal modules:

% Here are examples of how to use internal modules:
    <internal_modules>
        % Here is an example how to preprocess the prompt from a file: <preprocess_example><include>context/preprocess_example.py</include></preprocess_example>

        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>

        For using edit_file to make changes:
        <edit_file_example>
            <include>context/edit_file_example.py</include>
        </edit_file_example>
    </internal_modules>

% This program will do the following:
    Step 1. Load the 'fix_errors_from_unit_tests_LLM' prompt template.
    
    Step 2. Initialize the analysis_results dictionary with the following structure:
        - 'prompt_code_diff': Results of comparing prompt to code
        - 'prompt_test_diff': Results of comparing prompt to unit test
        - 'prior_fixes': Analysis of any previous fix attempts
        - 'root_causes': Detailed explanation of error root causes
        - 'solution_steps': Step-by-step solutions for each error
        - 'review_notes': Notes from the final review
    
    Step 3. Read contents of error_file and parse any previous fix attempts. Handle any file I/O errors gracefully.
    
    Step 4. Run the LLM analysis prompt through llm_invoke:
        4a. Pass the following parameters:
            - 'unit_test'
            - 'code'
            - 'prompt' (use preprocess function with recursive=False, double_curly_brackets=True, and exclude_keys=['unit_test', 'code', 'unit_test_fix'])
            - 'errors'
        4b. Parse the LLM response to populate the analysis_results dictionary
        4c. Append the output to error_file with a clear separator to distinguish it from previous content
        4d. Extract the corrected code sections marked with XML tags:
            - 'corrected_code_under_test' for code fixes
            - 'corrected_unit_test' for test fixes
    
    Step 5. Pretty print the analysis results and markdown formatting via the rich Markdown function to both:
        - Console (when verbose is True)
        - error_file (always)
        Also print the number of output tokens and cost.
    
    Step 6. Use edit_file to apply the fixes:
        6a. For the unit test file:
            - Extract unit test fix instructions from the corrected_unit_test section
            - Call edit_file with the instructions and unit test file path
            - Record whether changes were made (update_unit_test)
            - Read the modified file to get fixed_unit_test
        6b. For the code file:
            - Extract code fix instructions from the corrected_code_under_test section
            - Call edit_file with the instructions and code file path
            - Record whether changes were made (update_code)
            - Read the modified file to get fixed_code
    
    Step 7. Clean up temporary files and return the tuple containing:
            - update_unit_test
            - update_code
            - fixed_unit_test
            - fixed_code
            - analysis_results
            - total_cost (sum of LLM and edit_file costs)
            - model_name

% Ensure that the function:
    1. Handles potential errors and warnings gracefully:
        - Missing input parameters
        - Issues with LLM model responses
        - File I/O errors
        - Invalid XML tags in LLM response
    2. Maintains a detailed log of the analysis process in error_file
    3. Uses the rich library for all console output when verbose is True
    4. Preserves the structured analysis approach from the LLM prompt
    5. Returns both the fixed code and comprehensive analysis results