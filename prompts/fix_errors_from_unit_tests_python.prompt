% Here is an example generation of code from a prompt:
    <example_prompt>```<./context/generate/1/fix_errors_from_unit_tests_python.prompt>```</example_prompt>
    <example_generation>```<./context/generate/1/fix_errors_from_unit_tests.py>```</example_generation>
    
% You are an expert Python Software Engineer. Your goal is to write a python function, "fix_errors_from_unit_tests", that will fix unit test errors and warnings in a code file and log the process. All output to the console will be pretty printed using the Python rich library.

<include>context/python_preamble.prompt</include>
% Here are the inputs and outputs of the function:
    Inputs:
        'unit_test' - A string containing the unit test code.
        'code' - A string containing the code under test.
        'prompt' - A string containing the prompt that generated the code under test.
        'error' - A string that contains the errors and warnings that need to be fixed.
        'error_file' - A string containing the path to the file where error logs will be appended. If the file does not exist, it should be created.
        'strength' - A float between 0 and 1 that is the strength of the LLM model to use.
        'temperature' - A float between 0 and 1 that controls the randomness of the LLM's output.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs as a tuple:
        'update_unit_test' - Boolean indicating whether the unit test needs to be updated.
        'update_code' - Boolean indicating whether the code under test needs to be updated.
        'fixed_unit_test' - A string that is the fixed unit test.
        'fixed_code' - A string that is the fixed code under test.
        'total_cost' - A float representing the total cost of the LLM invocations.
        'model_name' - A string representing the name of the LLM model used.

% Here is how to use the internal modules:

% Here are examples of how to use internal modules:
    <internal_modules>
        % Here is an example how to preprocess the prompt from a file: <preprocess_example><include>context/preprocess_example.py</include></preprocess_example>

        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>
    </internal_modules>

% This program will do the following:
    Step 1. Load the 'fix_errors_from_unit_tests_LLM' and 'extract_unit_code_fix_LLM' prompt templates.
    Step 2. Read contents of error_file. Handle any file I/O errors gracefully.
    Step 3. Run the first prompt through llm_invoke:
        3a. Pass the following parameters:
            - 'unit_test'
            - 'code'
            - 'prompt' (use preprocess function with recursive=False, double_curly_brackets=True, and exclude_keys=['unit_test', 'code', 'unit_test_fix'])
            - 'errors'
        3b. Append the output to error_file with a clear separator to distinguish it from previous content. Ensure that existing log content is preserved.
    Step 4. This will pretty print the markdown formatting that is present in the result via the rich Markdown function to both the console and the error_file. It will also pretty print the number of output tokens in the result and the cost. Also, print out the total cost of this run.
    Step 5. Then this will preprocess the extract_unit_code_fix prompt (with recursive=False, double_curly_brackets=True, and exclude_keys=['unit_test', 'code', 'unit_test_fix'])
    Step 6. Run the second prompt through llm_invoke with strength=.97:
        6a. Pass the following parameters:
            - 'unit_test_fix': Result from Step 3
            - 'unit_test'
            - 'code'
        6b. Extract 'update_unit_test', 'update_code', 'fixed_unit_test', and 'fixed_code' from the Pydantic output.
    Step 7. Return the tuple containing the extracted values, total_cost, and model_name.

% Ensure that the function handles potential errors and warningsgracefully, such as missing input parameters, issues with the LLM model responses, or file I/O errors when reading from or writing to the error_file. Use the rich library for all console output when verbose is True.