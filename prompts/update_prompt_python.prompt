% You are an expert Python Software Engineer. Your goal is to write a Python function, "update_prompt", that will take the original code and modified code, and update the prompt that generated the original code. 

<include>./context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        - 'input_prompt' - A string that contains the prompt that generated the original code.
        - 'input_code' - A string that contains the original code that was generated from the original_prompt.
        - 'modified_code' - A string that contains the code that was modified by the user.
        - 'strength': A float value representing the strength parameter for the LLM model, used to influence the model's behavior. Range is between 0 and 1.
        - 'temperature': A float value representing the temperature parameter for the LLM model, used to control the randomness of the model's output. Range is between 0 and 1.
        - 'time' - A float between 0 and 1 that controls the thinking effort for the LLM model, passed to llm_invoke. Default is DEFAULT_TIME.
        - 'verbose': A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs as a tuple:
        - 'modified_prompt' - A string that contains the updated prompt that will generate the modified code.
        - 'total_cost': A float value representing the total cost of running the function.
        - 'model_name': A string representing the name of the selected LLM model.

% Here is how to use the internal modules:
    <internal_modules>
        % Here is an example how to preprocess the prompt from a file: <preprocess_example><include>context/preprocess_example.py</include></preprocess_example>

        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>
    </internal_modules>

% Steps to be followed by the function:
    1. Load the prompt templates:
        a. Load 'update_prompt_LLM' prompt template using load_prompt_template and preprocess it with recursive=False and double_curly_brackets=False.
        b. Load 'extract_prompt_update_LLM' prompt template using load_prompt_template and preprocess it with the same parameters.
    2. Run the first LLM invocation using llm_invoke:
        a. Use the update_prompt_LLM template.
        b. Pass the following parameters:
            * 'input_prompt'
            * 'input_code'
            * 'modified_code'
        c. Use the provided strength, temperature, and time.
        d. If verbose is True, pretty print the output, token count, and cost using rich.
    3. Run the second LLM invocation using llm_invoke:
        a. Use the extract_prompt_update_LLM template.
        b. Pass the result from step 2 as 'llm_output' parameter.
        c. Use a fixed strength value of 0.5, the provided temperature, and time.
        d. If verbose is True, pretty print the running message with token count and cost.
        e. The Pydantic output will contain the 'modified_prompt' key.
    4. If verbose is True, pretty print the extracted modified_prompt using Rich Markdown function.
    5. Calculate total cost by summing costs from both invocations.
    6. Return the modified_prompt string, total_cost, and model_name from the first invocation.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.