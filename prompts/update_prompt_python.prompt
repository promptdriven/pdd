% You are an expert Python Software Engineer. Your goal is to write a Python function, "update_prompt", that will take the original code and modified code, and update the prompt that generated the original code. 

<include>./context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        - 'input_prompt' - A string that contains the prompt that generated the original code.
        - 'input_code' - A string that contains the original code that was generated from the original_prompt.
        - 'modified_code' - A string that contains the code that was modified by the user.
        - 'strength': A float value representing the strength parameter for the LLM model, used to influence the model's behavior.
        - 'temperature': A float value representing the temperature parameter for the LLM model, used to control the randomness of the model's output.
    Outputs:
        - 'modified_prompt' - A string that contains the updated prompt that will generate the modified code.
        - 'total_cost': A float value representing the total cost of running the function.
        - 'model_name': A string representing the name of the selected LLM model.

% Here is an example of a LangChain Expression Language (LCEL) program: <lcel_example><include>context/langchain_lcel_example.py</include></lcel_example>

% Here are examples of how to use internal modules:
<internal_example_modules>
    % Here is an example how to preprocess the prompt from a file: <preprocess_example><include>context/preprocess_example.py</include></preprocess_example>

    % Example of selecting a Langchain LLM and counting tokens using llm_selector: <llm_selector_example><include>./context/llm_selector_example.py</include></llm_selector_example>
</internal_example_modules>

% Steps to be followed by the function:
    1. Load the '$PDD_PATH/prompts/update_prompt_LLM.prompt' and '$PDD_PATH/prompts/extract_prompt_update_LLM.prompt' files.
    2. Preprocess the update_prompt_LLM prompt using the preprocess function from the preprocess module and curly brackets parameter should be False.
    2. Create a Langchain LCEL template from the processed update_prompt_LLM prompt to return a string output.
    3. Use the llm_selector function for the LLM model and token counting.
    4. Run the input_prompt through the model using Langchain LCEL:
        - a. Pass the following string parameters to the prompt during invocation:             
            * 'input_prompt'
            * 'input_code'
            * 'modified_code'
        - b. Calculate the input and output token count using token_counter from llm_selector and pretty print the output of 4a, including the token count and estimated cost. The cost from llm_selector is in dollars per million tokens.
    5. Create a Langchain LCEL template from the extract_prompt_update_LLM prompt that outputs JSON:
        - a. Pass the following string parameters to the prompt during invocation: 'llm_output' (this string is from Step 4).
        - b. Calculate input and output token count using token_counter from llm_selector and pretty print the running message with the token count and cost.
        - c. Use 'get' function to extract 'modified_prompt' key values using from the dictionary output.
    6. Pretty print the extracted modified_prompt using Rich Markdown function. Include token counts and costs.
    7. Return the 'modified_prompt' string, the total_cost of both invokes and model_name use for the update_prompt_LLM prompt.