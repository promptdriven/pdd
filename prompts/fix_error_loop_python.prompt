% You are an expert Python Software Engineer. Your goal is to write a Python function, "fix_error_loop", that will attempt to fix errors in a unit test and its corresponding code file through multiple iterations. The function should include an optional verbose mode for detailed logging. Additionally, the function should track and report detailed statistics about the fixing process, including initial and final test states, improvements made, and the best iteration.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        'unit_test_file' - A string containing the path to the unit test file.
        'code_file' - A string containing the path to the code file being tested.
        'prompt' - A string containing the prompt that generated the code under test.
        'verification_program' - A string containing the path to a Python program that verifies if the code still runs correctly.
        'strength' - A float between or equal to 0 and 1 that represents the strength of the LLM model to use.
        'temperature' - A float between or equal to 0 and 1 that represents the temperature parameter for the LLM model.
        'time' - A float between 0 and 1 that controls the thinking effort for the LLM model, passed to underlying LLM calls. Default is DEFAULT_TIME.
        'max_attempts' - An integer representing the maximum number of fix attempts before giving up.
        'budget' - A float representing the maximum cost allowed for the fixing process.
        'error_log_file' - A string containing the path to the error log file (default: "error_log.txt").
        'verbose' - A boolean indicating whether to enable verbose logging (default: False).
    Outputs:
        'success' - A boolean indicating whether the errors were successfully fixed.
        'final_unit_test' - A string containing the contents of the final unit test file.
        'final_code' - A string containing the contents of the final code file.
        'total_attempts' - An integer representing the number of fix attempts made.
        'total_cost' - A float representing the total cost of all fix attempts.
        'model_name' - A string representing the name of the LLM model used.

% Here are examples of how to use internal modules:
<internal_example_modules>
    % Here is an example of the fix_errors_from_unit_tests function that will be used: <fix_errors_from_unit_tests_example><include>context/fix_errors_from_unit_tests_example.py</include></fix_errors_from_unit_tests_example>
</internal_example_modules>

% Here is an example of the TestResultCollector plugin used to capture pytest results:
<pytest_result_collector_example>
    <include>context/pytest_example.py</include>
</pytest_result_collector_example>

% This function will do the following:
    Step 1. Remove the existing error log file specified by 'error_log_file' if it exists.
    Step 2. Initialize variables:
        - Counter for the number of attempts
        - Total cost accumulator
        - Best iteration tracker (This is so that in case not all issues are solved, the iteration with the lowest errors gets restored. In case there are iterations with the same number of errors, then the iteration with the lowest fails, then the lowest warnings will get restored.)
        - A statistics tracker to record the initial state, final state, and improvements made during the fixing process.
    Step 3. Run an initial test to determine the starting state of the unit tests and store the results in the statistics tracker.
    Step 4. Enter a while loop that continues until max_attempts is reached or budget is exceeded:
        a. Print out to console and error log file the attempt iteration.
        b. Run the unit tests using pytest. First try invoking `pdd pytest-output --json-only <test_file>` via subprocess. If the command is missing, fall back to calling `run_pytest_and_capture_output` from the in-tree `pytest_output` module, and finally to `python -m pdd.pytest_output --json-only <test_file>` using the detected host Python. Strip any non-JSON text from stdout by locating the first `{` and balancing braces to extract the payload, then parse the JSON to capture:
           - Number of failures (tests that failed assertions)
           - Number of errors (tests that had errors during setup, execution, or teardown) and increment this count when `return_code` is 2
           - Number of warnings
           - Complete test output logs (Combine stdout and stderr from the test results within the JSON.)
           - Handle `json.JSONDecodeError` and other exceptions gracefully by returning placeholder failure counts and raw output in the logs field.
        c. If the test passes and has no warnings, break the loop and update the statistics tracker with the final state.
        d. If the test fails or has warnings:
           - Print the captured test output to the console and error log file, escaping square brackets for proper Rich console display.
           - Create backup copies of the unit_test_file and code_file in their respective directories, appending the current iteration number, the number of errors, fails, warnings, and a timestamp to the filenames like "unit_test_filename_3_1_0_2_20240101_120000.py" and "code_filename_3_1_0_2_20240101_120000.py" (iteration 3 with one error, zero fails, two warnings).
           - Update the best iteration tracker immediately after creating backups using the current failure, error, and warning counts so the loop can restore this snapshot later if needed (preferring lower errors, then fails, then warnings).
           - Read the contents of the unit_test_file and code_file.
           - Call fix_errors_from_unit_tests with the file contents, error and warning messages from the error log file, the error log file, and the provided strength, temperature, and time. Pass the verbose flag to enable detailed logging if specified.
           - Add the returned total_cost to the total cost accumulator.
           - If the total cost exceeds the budget, break the loop.
           - If updated_unit_test is True, write the fixed_unit_test back to the unit_test_file.
           - Increment the attempt counter.
           - If updated_code is True:
              * Write the fixed_code back to the code_file.
              * Run the verification_program to check if the code still runs.
              * If the verification fails, restore the last known working code_file from one of the backups and output the errors and results of the failed verification run into the error log file so that this can be debugged in the next iteration and indicate that a restore has happened. Then continue the loop.
           - Run the tests again in the same iteration using the pytest subprocess call to check if the fixes worked.
           - Update the statistics tracker with the results of the current iteration, including whether the fixes improved the test results.
    Step 5. If the last run isn't the best iteration, copy back the files from the best iteration and print out which iteration was restored. Update the statistics tracker with the best iteration information.
    Step 6. Calculate and print summary statistics, including the initial state, final state, best iteration identifier, reductions in failures/errors/warnings, and the overall improvement percentage across all three metrics.
    Step 7. Return the success status, final unit test contents, final code contents, total number of attempts, total cost, and model name.

% In addition, update the docstring and implementation of the `run_pytest_on_file` function. The function should:
    - Run pytest on the specified test file by first calling `pdd pytest-output --json-only <test_file>`.
    - Fall back to importing and calling `run_pytest_and_capture_output` when the CLI is unavailable, and otherwise to `python -m pdd.pytest_output --json-only <test_file>` using the detected host Python executable.
    - Return a tuple: `(failures, errors, warnings, logs)`.
    - Extract the JSON payload by locating the leading `{` and balancing braces before parsing.
    - Increment the `errors` total when the parsed `return_code` equals 2, and concatenate `standard_output` and `standard_error` for the logs.
    - Handle `json.JSONDecodeError` and other exceptions gracefully by returning placeholder failure counts and the raw combined output in the logs.

% Finally, ensure that the function tracks and reports detailed statistics about the fixing process, including:
    - Initial state of the tests (failures, errors, warnings).
    - Final state of the tests (failures, errors, warnings).
    - Improvements made during the process (reductions in failures, errors, warnings).
    - The identifier of the best iteration that can be restored.
    - The overall percentage improvement across failures, errors, and warnings combined.
