% You are an expert Python Software Engineer. Your goal is to write a robust Python module containing the `fix_error_loop` function and its associated helpers. This module implements an iterative, agentic workflow to fix errors in unit tests and their corresponding code files using an LLM.

### Role and Scope:
This module acts as the core execution engine for an automated code-repair system. It manages the lifecycle of a "fix attempt," which includes running tests, capturing failures, invoking an LLM to suggest repairs, verifying those repairs, and falling back to a more powerful agentic fix if the iterative loop fails. It is responsible for state management (backups and restoration) and detailed logging.

### Requirements:
1.  **Iterative Fixing Logic:** The `fix_error_loop` must attempt to resolve code/test mismatches over multiple iterations (up to `max_attempts`). It must track failures, errors, and warnings at each step.
2.  **Statistics & Improvement Tracking:** Maintain a `stats` dictionary to track initial/final states, improvement percentages, and identify the `best_iteration`.
3.  **Structured XML Logging:** Implement a dictionary-based log structure. This must be formatted into a string using XML tags (`<pytest_output>`, `<fix_attempt>`, `<verification_output>`) for each iteration and written to `error_log_file`.
4.  **Best State Recovery:** If the final iteration is not the most successful (prioritizing lowest errors, then fails, then warnings), the module must restore the file contents from the best recorded iteration backups.
5.  **Verification Step:** After every code update, run a verification program using `detect_host_python_executable`. If the verification returns a non-zero exit code, restore the previous code backup immediately and log the restoration.
6.  **Agentic Fallback:** If the loop finishes without total success and budget remains, invoke `run_agentic_fix`. Ensure the `error_log_file` is populated with the current state before the agent is called.
7.  **Non-Python Support:** Detect non-Python files by extension. Use `default_verify_cmd_for` to run initial verification. If it fails, skip the standard pytest loop and go directly to the agentic fallback.
8.  **Cost & Budget Management:** Accumulate `total_cost` from LLM calls and stop the loop immediately if the `budget` is exceeded.
9.  **Output Contract:** Return a 6-tuple: `(success, final_unit_test, final_code, total_attempts, total_cost, model_name)`.
    *   *Note:* If tests pass initially, return actual file contents, `0` for `total_attempts`, `0.0` for `total_cost`, and an empty string for `model_name`.
10. **Result Normalization:** Implement `_normalize_agentic_result` to handle various return shapes (2, 3, 4, or 5 elements) from the agentic fix to ensure a consistent internal state, specifically extracting `changed_files` if available.

### Public Interface:

```python
def fix_error_loop(
    unit_test_file: str,
    code_file: str,
    prompt_file: str,
    prompt: str,
    verification_program: str,
    strength: float,
    temperature: float,
    max_attempts: int,
    budget: float,
    error_log_file: str = "error_log.txt",
    verbose: bool = False,
    time: float = DEFAULT_TIME,
    agentic_fallback: bool = True
) -> tuple[bool, str, str, int, float, str]:
    """
    Returns: (success, final_unit_test, final_code, total_attempts, total_cost, model_name)
    """
```

### Internal Data Structures:

**`stats` dictionary** - Tracks improvement metrics across iterations:
```python
stats = {
    "initial_fails": int,      # Failures from initial test run
    "initial_errors": int,     # Errors from initial test run
    "initial_warnings": int,   # Warnings from initial test run
    "final_fails": int,        # Final failure count (initialized to 0)
    "final_errors": int,       # Final error count (initialized to 0)
    "final_warnings": int,     # Final warning count (initialized to 0)
    "best_iteration": int | str | None,  # Iteration number, "final", or 0 (initially passing)
    "iterations_info": list,   # Per-iteration stats
    "improvement": {           # Added at end
        "fails_reduced": int,
        "errors_reduced": int,
        "warnings_reduced": int,
        "percent_improvement": float  # 100 if initially zero issues
    }
}
```

**`_normalize_agentic_result` mapping** - Handles variable return shapes from `run_agentic_fix`:
| Input Shape | Mapping |
|-------------|---------|
| 5-tuple | `(success, msg, cost, model, changed_files)` → use directly |
| 4-tuple | `(success, msg, cost, model)` → `changed_files=[]` |
| 3-tuple | `(success, msg, cost)` → `model="agentic-cli"`, `changed_files=[]` |
| 2-tuple | `(success, msg)` → `cost=0.0`, `model="agentic-cli"`, `changed_files=[]` |

Returns normalized 5-tuple: `(bool, str, float, str, List[str])`

### Control Flow:

```mermaid
stateDiagram-v2
    [*] --> Init: validate files exist

    Init --> InitialTest: run pytest or verify cmd

    InitialTest --> NonPythonFallback: non-Python && fails
    InitialTest --> NonPythonPass: non-Python && passes
    InitialTest --> FixLoop: Python (always enters loop)

    state FixLoop {
        [*] --> CheckSuccess
        CheckSuccess --> ReadFiles_InLoop: success=true
        ReadFiles_InLoop --> [*]: break (files read HERE for initially_passing!)

        CheckSuccess --> CreateBackups: success=false
        CreateBackups --> UpdateBestIteration
        UpdateBestIteration --> CallLLMFix: fix_errors_from_unit_tests
        CallLLMFix --> IncrementAttempts: fix_attempts++
        IncrementAttempts --> WriteFiles
        WriteFiles --> RunVerification
        RunVerification --> RestoreBackup: verification fails
        RestoreBackup --> RerunPytest
        RunVerification --> RerunPytest: verification passes
        RerunPytest --> CheckSuccess: update success
    }

    FixLoop --> PostLoop: loop exits

    PostLoop --> RestoreBest: !success && best_attempt!=None
    PostLoop --> SetFinal: success OR best_attempt==None
    RestoreBest --> ReadFiles_PostLoop
    SetFinal --> ReadFiles_PostLoop: stats["best_iteration"]="final"
    ReadFiles_PostLoop --> PrintSummary: skip read if initially_passing

    PrintSummary --> AgenticFallback: !success && budget remains
    PrintSummary --> Return: success

    AgenticFallback --> Return
    NonPythonFallback --> Return
    NonPythonPass --> Return

    Return --> [*]
```

Notes:
- ALL Python files enter FixLoop (even initially-passing)
- Initially-passing: CheckSuccess → ReadFiles_InLoop → break → PostLoop → SetFinal (stats="final")
- `fix_attempts` counts LLM calls, not loop iterations
- PostLoop skips file read for initially_passing (already read in loop)

### Dependencies:
<python_preamble>
    <include>context/python_preamble.prompt</include>
</python_preamble>
<internal_modules>
    <get_language>
        <include>context/get_language_example.py</include>
    </get_language>
    <fix_errors_from_unit_tests>
        <include>context/fix_errors_from_unit_tests_example.py</include>
    </fix_errors_from_unit_tests>
    <agentic_fix>
        <include>context/agentic_fix_example.py</include>
    </agentic_fix>
    <agentic_langtest>
        <include>context/agentic_langtest_example.py</include>
    </agentic_langtest>
    <pytest_output>
        <include>context/pytest_example.py</include>
    </pytest_output>
    <python_env_detector>
        <include>context/python_env_detector.py</include>
    </python_env_detector>
</internal_modules>

### Instructions:
1.  **Implement `run_pytest_on_file(test_file)`**: Use `run_pytest_and_capture_output`. Extract failures, errors, and warnings from the first result in the JSON. Combine `standard_output` and `standard_error` for the log.
2.  **Implement `format_log_for_output(log_structure)`**: Convert the iteration list into a string. Ensure the first iteration includes the initial test output. Use XML tags with `iteration=n` attributes.
3.  **Implement `fix_error_loop`**:
    *   **Initialization**: Setup stats, best iteration tracker (using `sys.maxsize` for initial comparison), and the structured log.
    *   **Initial Check**: Run pytest (or verification for non-Python). If initially passing, exit early per the output contract.
    *   **The Loop**: 
        *   Create timestamped backups: `filename_iter_err_fail_warn_timestamp.py`.
        *   Call `fix_errors_from_unit_tests` using the `formatted_log` as context.
        *   Update files, run verification, and re-run pytest to update the state.
    *   **Finalization**: Compare final state to the best iteration and restore if necessary. Print a `rich` summary of improvements including percentage reduction.
4.  **Agentic Fallback Logic**: Implement `_safe_run_agentic_fix` which calls the agent and passes the result through `_normalize_agentic_result`. 
    * Ensure the `error_log_file` is written to disk before the agent is called. 
    * If no iterations were run, write the initial state and pytest output to the log file.
    * Log the list of `agent_changed_files` to the console if the agent modifies files.
5.  **Console Output**: Use `rich.print` (as `rprint`). Implement an `escape_brackets` helper to prevent Rich from parsing square brackets in logs.

### Deliverables:
*   A standalone Python file containing `run_pytest_on_file`, `format_log_for_output`, `fix_error_loop`, and the normalization helpers.
*   The module must include an `if __name__ == "__main__":` block demonstrating a sample execution with mock parameters.