% You are an expert Python engineer. Your goal is to write a Python function, "conflicts_in_prompts", that takes two prompts as input and finds conflicts between them and suggests how to resolve those conflicts.

<include>./context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'prompt1' - First prompt in the pair of prompts we are comparing.
        'prompt2' - Second prompt in the pair of prompts we are comparing.
        'strength' - A float that is the strength of the LLM model to use. Default is 0.5. Range is between 0 and 1.
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0. Range is between 0 and 1.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs as a tuple:
        'changes_list' - A list of JSON objects, each containing the name of a prompt that needs to be changed and detailed instructions on how to change it.
        'total_cost' - A float that is the total cost of the model run.
        'model_name' - A string that is the name of the selected LLM model.

% Here is how to use the internal modules:
    <internal_modules>
        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>
    </internal_modules>

% This function will do the following:
    Step 1. Load the 'conflict_LLM' and 'extract_conflict_LLM' prompt templates.
    Step 2. Run the prompts through the model using llm_invoke with the provided strength and temperature. 
        2a. Pass the following string parameters to the prompt during invoke:
            - 'PROMPT1'
            - 'PROMPT2'
        2b. If verbose is True, pretty print the output which will be in Markdown format.
    Step 3. Create a second llm_invoke call using a .89 strength that outputs a Pydantic object:
        3a. Pass the following string parameters to the prompt during invocation: 'llm_output' (this string is from Step 2).
        3b. If verbose is True, pretty print a message letting the user know it is running and how many tokens are in the prompt and the cost.
        3c. Extract 'changes_list' list values from the Pydantic output.
    Step 4. Return the changes_list, total_cost and model_name.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.