% You are an expert Python Programmer. Generate a Python function called 'insert_includes'. The purpose of this function is to determine needed depencencies and insert them into a prompt.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        - 'input_prompt': A string containing the prompt
        - 'directory_path': A string containing the directory path where the possible dependencies for prompt file is located.  An example would be "context/*_example.py".
        - 'csv_filename': A string containing the name of the csv file that contains the dependencies. It may not exist yet, in which case it will be created.
        - 'strength': A float value representing the strength parameter for the LLM model, used to influence the model's behavior.
        - 'temperature': A float value representing the temperature parameter for the LLM model, used to control the randomness of the model's output.
        - 'time': A float between 0 and 1 that controls the thinking effort for the LLM model, passed to underlying LLM calls. Default is DEFAULT_TIME.
        - 'verbose': A boolean that indicates whether to print out the details of the function. Default is False.
    Output:
        - 'output_prompt': A string containing the prompt with the dependencies inserted.
        - 'csv_output': A string containing the complete CSV output from the auto_include function, including full dependency information.
        - 'total_cost': A float value representing the total cost of running the function.
        - 'model_name': A string representing the name of the LLM model used.

% Here is how to use the internal modules:
    <internal_modules>
    For llm_invoke to run prompts with the LLM model:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>

    For loading prompt templates:
            <load_prompt_template_example>
                <include>context/load_prompt_template_example.py</include>
            </load_prompt_template_example>

    For determining and generating the proper prompt includes via the auto_include function:
        <auto_include_example>
            <include>context/auto_include_example.py</include>
        </auto_include_example>

    For preprocess function for preprocessing the prompt template:
        <preprocess_example>
            <include>context/preprocess_example.py</include>
        </preprocess_example>
    </internal_modules>

% Here is the format of the csv file: full_path,file_summary,content_hash

% The function should do the following:
    1. Load the prompt template 'insert_includes_LLM.prompt'.
    2. Read the CSV file from the 'csv_filename' parameter using file.read().
    3. Preprocess the prompt template using the preprocess function from the preprocess module to double curly brackets and exclude the keys 'actual_prompt_to_update' and 'actual_dependencies_to_insert' but do not recurse.
    4. Use auto_include to get the dependencies for the input prompt, passing the 'strength', 'temperature', and 'time' parameters.
    5. Run with llm_invoke (passing the 'strength', 'temperature', and 'time' parameters) with the insert includes prompt to insert the output of #4 prompt into current prompt:
        - a. Pass the following string parameters to the prompt during invocation: 'actual_prompt_to_update' and 'actual_dependencies_to_insert'.
        - b. The Pydantic output will contain the 'output_prompt' key.
    6. Return the 'output_prompt', the complete csv_output from auto_include, the total_cost and the model_name of the LLM model used.