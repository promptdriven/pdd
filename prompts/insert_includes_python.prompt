

You are writing a python code to include dependencies into a prompt.
The python code has as dependecy the file auto_include_examples.py
Each dependency may recursively include other dependencies.
In the python code generated you an use the function

<include>context/python_preamble.prompt</include>

% Here is how to use the internal modules:
    <internal_modules>
    For llm_invoke to run prompts with the LLM model:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>

    For loading prompt templates:
            <load_prompt_template_example>
                <include>context/load_prompt_template_example.py</include>
            </load_prompt_template_example>

    For auto_include function:
        <auto_include_example>
            <include>context/auto_include_example.py</include>
        </auto_include_example>

    For preprocess function:
        <preprocess_example>
            <include>context/preprocess_example.py</include>
        </preprocess_example>

    </internal_modules>

% You are an expert Python Programmer. Generate a Python function called 'insert_includes' that takes in the following arguments:
    - 'input_prompt': A string containing the prompt
    - 'directory_path': A string containing the directory path where the prompt file is located.
    - 'csv_filename': A string containing the name of the csv file that contains the dependencies.
    - 'strength': A float value representing the strength parameter for the LLM model, used to influence the model's behavior.
    - 'temperature': A float value representing the temperature parameter for the LLM model, used to control the randomness of the model's output.

% The function should do the following:
    1. Load the prompt template 'insert_includes_LLM.prompt'.
    2. Use auto_include to get the depencencies.
    3. Add the dependencies into  
    function to include the dependencies into the prompt.
    3. 
    2. Preprocess the prompt template using the preprocess function from the preprocess module.
    3. Insert_includes should use the function auto_include from auto_include_examples.c to include the dependencies into the prompt.
    4: Iterate through all files in the directory specified by the 'directory_path' input.
        4a: For each file:
            - Extract the filename using os.path.basename()
            - Check if any existing entry in the CSV has a matching filename
            - Only proceed with the depencency inclusion if either:
                1. No matching filename is found in existing entries
                2. The file is newer than the existing entry with the matching filename
            - If a match is found and the file hasn't changed, reuse the existing summary
        Step 2b: Read each file and read the contents of the file as a string.
        Step 2c: Summarize the files contents using llm_invoke with this parameter: {'file_contents': file_contents}
            - Use a Pydantic model called FileSummary with a field named 'file_summary' for consistent naming
        Step 2d: Store the relative path (not the full path) of the file, the file_summary, and the date in the current data dictionary.
    Step 3. Pretty print a message letting the user know it is running and the current progress.
    Step 4: Return the tuple containing the updated csv output string, the total_cost and the model_name of the LLM model used.



    3. Create a Langchain LCEL template from the processed prompt template to return a string output.
    4. Use the llm_selector function for the LLM model and token counting.
    5. Run the input through the model using Langchain LCEL:
        - a. Pass the following string parameters to the prompt during invocation: 'input_prompt', 'dependencies'.
        - b. Calculate the input and output token count using token_counter from llm_selector and pretty print the output of 5a, including the token count and estimated cost. The cost from llm_selector is in dollars per million tokens.
    6. Pretty print the output prompt with the dependencies inserted using Rich Markdown function. Include token counts and costs.
    7. Return the 'output_prompt' string and the total_cost of the invoke.

output:
    - 'output_prompt': A string containing the prompt with the dependencies inserted.
    - 'total_cost': A float value representing the total cost of running the function.
    - 'model_name': A string representing the name of the LLM model used.
    - 'dependencies': A string containing the dependencies extracted from the csv file.

