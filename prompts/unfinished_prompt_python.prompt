% You are an expert Python engineer. Your goal is to write a python function called 'unfinished_prompt' that will determine if a given prompt is complete or needs to continue.

<include>./context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        'prompt_text' - A string containing the prompt text to analyze.
        'strength' - A float that is the strength of the LLM model to use for the analysis. Default is DEFAULT_STRENGTH. Range is between 0 and 1.
        'temperature' - A float that is the temperature of the LLM model to use for the analysis. Default is 0. Range is between 0 and 1.
        'time' - A float between 0 and 1 that controls the thinking effort for the LLM model, passed to llm_invoke. Default is DEFAULT_TIME.
        'language' - An optional string hint for the language of the prompt (e.g., "python"). Default is None.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs as a tuple:
        'reasoning' - A string containing the structured reasoning for the completeness assessment.
        'is_finished' - A boolean indicating whether the prompt is complete (True) or incomplete (False).
        'total_cost' - A float that is the total cost of the analysis function. This is an optional output.
        'model_name' - A string that is the name of the LLM model used for the analysis. This is an optional output.

% Here is how to use the internal modules:
    <internal_modules>
        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>
    </internal_modules>

% This function will do the following:
    Step 0. Fast syntactic check: If the text appears to be Python (language="python" or heuristic detection), try ast.parse(). If it succeeds, return immediately with is_finished=True, cost=0.0, and model_name="syntactic_check".
    Step 1. Load the 'unfinished_prompt_LLM' file.
    Step 2. Run the prompt text through the model using llm_invoke, passing the 'time' and 'language' parameters.
        2a. Pass the following string parameters to the prompt during invoke:
            - 'PROMPT_TEXT'
            - 'LANGUAGE' (if provided)
        2c. The Pydantic output of the invoke will have the keys 'reasoning' and 'is_finished'.
    Step 3. Validate the result is a PromptAnalysis instance (raise TypeError if not).
    Step 4. Return the 'reasoning' string and 'is_finished' boolean from the output, and the 'total_cost' float, and 'model_name' string.

% Additional requirements:
    1. Import 'print' from rich as 'rprint' to distinguish it from regular print
    2. Use 'rprint' for all rich-formatted console output
    3. Implement a fallback mechanism for displaying input text:
        - Try using rprint first
        - If rich printing fails, fall back to regular print
