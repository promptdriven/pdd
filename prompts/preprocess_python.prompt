% You are an expert Python engineer. Your goal is to write a Python function, 'preprocess', that will preprocess the prompt from a prompt string for a LLM. This will use regular expressions to preprocess specific XML-like tags, if any, in the prompt.

<include>./context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs:
        'prompt' - A string that is the prompt to preprocess
        'recursive' - A boolean that is True if the program needs to recursively process the includes in the prompt and False if it does not need to recursively process the prompt. Default is False.
        'double_curly_brackets' - A boolean that is True if the curly brackets need to be doubled and False if they do not need to be doubled. Default is True.
        'exclude_keys' - An optional list of strings that are excluded from the curly bracket doubling.
    Outputs:
        - Returns a string that is the preprocessed prompt. To preserve significant whitespace, the function should NOT trim leading or trailing whitespace.

% Processing order and tag behavior
    Process tags in this order: 'pdd' → 'include' → 'shell' → 'web'. After tag processing completes, optionally apply curly‑brace doubling according to 'double_curly_brackets'.

% Here are the XML-like tags to preprocess, other tags will remain unmodified:
    'include' - This tag will include the content of the file indicated in the include tag. The 'include tag' will be directly replaced with the content of the file in the prompt, without wrapping it in a new tag.
    'pdd' - This tag indicates a comment and anything in this XML will be deleted from the string including the 'pdd' tags themselves.
    'shell' - This tag indicates that there are shell commands to run. Capture standard output (stdout) of the shell commands and include it in the prompt; on a non-zero exit status, replace with an error message including the exit code. Remove the shell tags themselves.
    'web' - This tag indicates that there is a web page to scrape. When scraping the web page, extract using Firecrawl the markdown version of the web page and include it in the prompt; remove the web tags themselves.

% Here is how to use the Firecrawl API to scrape the web page:
<firecrawl_example>
    <include>./context/firecrawl_example.py</include>
</firecrawl_example>

% Includes can be nested, that is there can be includes inside of the files of the includes and 'preprocess' should be called recursively on these include files if recursive is True. There are two ways of having includes in the prompt:
    1. For triple backtick includes of the form ```<path>```, replace the entire fenced segment with ```{file contents}```. Perform this replacement recursively until there are no more such fenced includes. If the file cannot be found, leave the original fenced include unchanged.
    2. The XML 'include' mentioned above.

% Include error handling specifics
    - For XML <include> tags, if the file cannot be found or read, replace the tag with a bracketed error note like: [File not found: ./path/to/file]

% If double_curly_brackets is True, the program will check to see if the file has any single curly brackets and if it does and the string in the curly brackets are not in the exclude_keys list, it will check to see if the curly brackets are already doubled before doubling the curly brackets. Apply curly-brace doubling only to the top-level prompt (not to content introduced via includes during recursive processing). The curly brackets may be on different lines and may have nested curly brackets. Special handling is needed for several cases:
    1. Handle already doubled brackets to prevent doubling them again
    2. Properly handle nested curly brackets using case-by-case pattern matching
    3. Special handling for code blocks (JSON, JavaScript, TypeScript, Python) to ensure proper formatting
    4. Apply special handling for multiline variables with curly brackets

% The program should resolve file paths using the current directory as the base path. Implement a function 'get_file_path' that takes a file name and returns the full path using './' as the base path.

% Web scraping requirements and error handling
    - Requires the 'firecrawl' package and a FIRECRAWL_API_KEY environment variable. If either is missing or scraping fails, insert a bracketed error note. If no markdown content is returned, insert a bracketed note indicating no content is available.
