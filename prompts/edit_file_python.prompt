% You are an expert Python Software Engineer. Your goal is to edit a file. You are creating an async function ('edit_file') that will edit a file using LangGraph and MCP adapters.

% Here are the inputs and outputs of the function:
<function_inputs>
    - file_path (string): The path to the file to edit.
    - edit_instructions (string): A description of the changes to make to the file.
    - mcp_config_path (string, optional): Path to MCP configuration file. If None, defaults to 'mcp_config.json' in current directory.
</function_inputs>

<function_outputs>
    - tuple[bool, Optional[str]]: A tuple containing:
        - success (boolean): Whether the file was edited successfully.
        - error_message (Optional[str]): An error message if unsuccessful, None if successful.
</function_outputs>

% Implementation Requirements:

1. Async Implementation:
    - Function should be async to handle MCP tool operations efficiently
    - Use proper async context managers and await statements

2. State Management:
    - Use LangGraph StateGraph for workflow management
    - Track file content hash for consistency between edits
    - Maintain message history for agent interactions
    - Handle tool loading and execution states

3. Error Handling:
    - Validate file existence and accessibility
    - Handle MCP tool loading failures
    - Manage tool execution errors
    - Provide detailed error messages for debugging
    - Handle file hash mismatches during edits

4. MCP Integration:
    - Use MultiServerMCPClient from langchain_mcp_adapters
    - Load MCP configuration from JSON file
    - Connect to text editor server
    - Handle tool discovery and validation
    - Manage tool execution through MCP

5. File Editing Workflow:
    - Get current file content and hash before editing
    - Make edits one at a time
    - Update hash after each edit
    - Verify changes were applied successfully
    - Support multiple edits in sequence

6. Testing Support:
    - Include example usage with test file creation
    - Provide verification of file modifications
    - Support cleanup of test files
    - Enable inspection of results

7. Caching Implementation:
    - Implement SQLiteCache from langchain_community.cache
    - Set up global LLM cache for improved performance
    - Store cache in a local file (.langchain.db)
    - Configure caching through langchain_globals

8. Subprocess Bridge:
    - Implement function to run edit_file in subprocess (run_edit_in_subprocess)
    - Bridge between async and sync worlds for non-async callers
    - Pass necessary environment variables to subprocess
    - Handle subprocess output parsing and error handling
    - Return same tuple format as edit_file function

9. Model Integration:
    - Use Claude 3.7 Sonnet specifically for planning edits
    - Set temperature to 0 for deterministic output
    - Configure with proper tool binding
    - Format tools appropriately for Claude's understanding
    - Handle Claude's message/tool call output structure

% The LangGraph Agent will use the Langchain MCP adapter to access MCPs. Here are the relevant documentation links:
    <langgraph_docs><web>https://langchain-ai.github.io/langgraph/llms-full.txt</web></langgraph_docs>
    <mcp_adapter_docs><web>https://github.com/langchain-ai/langchain-mcp-adapters</web></mcp_adapter_docs>

% This will read a JSON file to get the MCP service to use.





