% You are an expert Python Software Engineer. Your goal is to write a Python function, "generate_test", that will create a unit test from a code file.

<include>./context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'prompt' - A string containing the prompt that generated the code file to be processed.
        'code' - A string containing the code to generate a unit test from.
        'strength' - A float between 0 and 1 that is the strength of the LLM model to use.
        'temperature' - A float that is the temperature of the LLM model to use.
        'time' - A float between 0 and 1 that controls the thinking effort for the LLM model, passed to llm_invoke. Default is DEFAULT_TIME.
        'language' - A string that is the language of the unit test to be generated.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs as a tuple: 
        'unit_test'- A string that is the generated unit test code.
        'total_cost' - A float that is the total cost to generate the unit test code.
        'model_name' - A string that is the name of the selected LLM model.

% Here is how to use the internal modules:
    <internal_modules>
        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>

        For preprocessing prompts:
        <preprocess_example>
            <include>context/preprocess_example.py</include>
        </preprocess_example>

        For handling unfinished prompts:
        <unfinished_prompt_example>
            <include>context/unfinished_prompt_example.py</include>
        </unfinished_prompt_example>

        For continuing generation:
        <continue_generation_example>
            <include>context/continue_generation_example.py</include>
        </continue_generation_example>

        For postprocessing results:
        <postprocess_example>
            <include>context/postprocess_example.py</include>
        </postprocess_example>
    </internal_modules>

% This program will do the following:
    Step 1. Load the 'generate_test_LLM' prompt template.
    Step 2. Preprocess the Step 1 prompt template using the preprocess function without recursion or doubling of the curly brackets.
    Step 3. Run the inputs through the model using llm_invoke, passing the 'time' parameter to the llm_invoke function.
        3a. Pass the following string parameters to the prompt during invoke:
            - 'prompt_that_generated_code': preprocess the prompt using the preprocess function without recursion or doubling of the curly brackets.
            - 'code'
            - 'language'
        3b. If verbose is True, pretty print a message letting the user know it is running and how many tokens are in the prompt and the cost.
    Step 4. If verbose is True, pretty print the markdown formatting that is present in the result via the rich Markdown function. Also pretty print the number of tokens in the result and the cost.
    Step 4. Detect if the generation is incomplete using the unfinished_prompt function (strength .7) by passing in the last 600 characters of the output of Step 2.
        - a. If incomplete, call the continue_generation function to complete the generation.
        - b. Else, if complete, postprocess the model output result using the postprocess function with a strength of 0.7.
    Step 5. If verbose is True, print out the total_cost including the input and output tokens and functions that incur cost (e.g. postprocessing).
    Step 6. Return the unit_test, total_cost and model_name.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.