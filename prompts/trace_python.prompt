% You are an expert Python engineer. Your goal is to write a Python function, "trace", that will output the line number of the prompt_file that corresponds to the code_line in code_file properly.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'code_file' (str) - A string that contains the text of the code file.
        'code_line' (int) - An integer that represents the line number in the code file that the debugger is on.
        'prompt_file' (str) - A string that contains the text of the .prompt file.
        'strength' - A float that is the strength of the LLM model to use. Default is 0.5. Range is between 0 and 1.
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0. Range is between 0 and 1.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs:
        'prompt_line' (int) - An integer that represents the equivalent line number in the prompt file that matches with the line number in the code file.
        'total_cost' - A float that is the total cost of the function.
        'model_name' - A string that is the name of the selected LLM model

% Here is how to use the internal modules:
    <internal_modules>
        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>

        For preprocessing prompts:
        <preprocess_example>
            <include>context/preprocess_example.py</include>
        </preprocess_example>
    </internal_modules>

% This function will do the following:
    Step 1. Find the substring of the code_file that matches the code_line.
    Step 2. Load the 'trace_LLM' prompt template using load_prompt_template.
    Step 3. Preprocess the loaded trace_LLM prompt using the preprocess function with parameters recursive=False and double_curly_brackets=False.
    Step 4. Invoke the model using llm_invoke. 
        4a. Pass the following string parameters to the prompt during invoke:
            - 'CODE_FILE'
            - 'CODE_STR'
            - 'PROMPT_FILE'
        4b. If verbose is True, pretty print a message letting the user know it is running and how many tokens are in the prompt and the cost.
    Step 5. Load the 'extract_promptline_LLM' prompt template using load_prompt_template. Preprocess the loaded extract_promptline_LLM prompt using the preprocess function with parameters recursive=False and double_curly_brackets=False.
    Step 6. Invoke the model using llm_invoke with a strength of 0.5.
        6a. Pass the following string parameters to the prompt during invoke:
            - 'llm_output'
        6b. If verbose is True, pretty print a message letting the user know it is running and how many tokens are in the prompt and the cost.
        6c. Extract the prompt_line string from the Pydantic output
    Step 7. Find the prompt_line sub-string of the prompt_file that matches the prompt_line by doing a fuzzy string search to find the line number.
    Step 8. Return the prompt_line, total_cost, and model_name.

% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.