% You are an expert Python engineer. Your goal is to write a Python function, "context_generator", that will generate a concise example on how to use code_module properly.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'code_module' - A string that is the code module to generate a concise example for
        'prompt' - A string that is the prompt that was used to generate the code_module
        'language' - A string that is the language of the code module. Default is "python". 
        'strength' - A float that is the strength of the LLM model to use. Default is 0.5.
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0.
    Outputs:
        'example_code' - A string that is the concise example code generated by the function.
        'total_cost' - A float that is the total cost of the function.
        'model_name' - A string that is the name of the selected LLM model

% Here is an example of a LangChain Expression Language (LCEL) program: <lcel_example><include>context/langchain_lcel_example.py</include></lcel_example>

% Here are examples of how to use internal modules:
<internal_example_modules>
    % Here is an example how to preprocess the prompt from a file: <preprocess_example><include>context/preprocess_example.py</include></preprocess_example>

    % Example of selecting a Langchain LLM and counting tokens using llm_selector: <llm_selector_example><include>context/llm_selector_example.py</include></llm_selector_example>

    % Example usage of the unfinished_prompt function: <unfinished_prompt_example><include>context/unfinished_prompt_example.py</include></unfinished_prompt_example>

    % Here is an example how to continue the generation of a model output: <postprocess_example><include>context/continue_generation_example.py</include></postprocess_example>

    % Here is an example how to postprocess the model output result: <postprocess_example><include>context/postprocess_example.py</include></postprocess_example>
</internal_example_modules> 

% This function will use Langchain to do the following:
    Step 1. Use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/example_generator_LLM.prompt' file.
    Step 2. Preprocess the loaded example_generator prompt using the preprocess function with parameters recursive=False and double_curly_brackets=False.
    Step 3. Create a Langchain LCEL template from the preprocessed example_generator prompt.
    Step 4. Use llm_selector for the model.
    Step 5. Preprocess the prompt using the preprocess function for the processed_prompt.
    Step 6. Invoke the code through the model using Langchain LCEL. 
        6a. Pass the following string parameters to the prompt during invoke:
            - 'code_module'
            - 'processed_prompt'
            - 'language'
        6b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens. 
    Step 7. Detect if the generation is incomplete using the unfinished_prompt function (strength .5) by passing in the last 600 characters of the output of Step 6.
        - a. If incomplete, call the continue_generation function to complete the generation.
        - b. Else, if complete, postprocess the model output result using the postprocess function from the postprocess module with a strength of 0.9.
    Step 8. Print out the total_cost including the input and output tokens and functions that incur cost (e.g. postprocessing).
    Step 9. Return the example_code, total_cost, and model_name.