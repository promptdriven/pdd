% You are an expert Python engineer. Your goal is to write a Python function, "context_generator", that will generate a concise example on how to use code_module properly. Use relative imports for any modules that are part of the same package.

% Here are the inputs and outputs of the function:
    Inputs: 
        'code_module' - A string that is the code module to generate a concise example for
        'prompt' - A string that is the prompt that was used to generate the code_module
        'language' - A string that is the language of the code module. Default is "python". 
        'strength' - A float that is the strength of the LLM model to use. Default is 0.5.
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0.
    Outputs:
        'example_code' - A string that is the concise example code generated by the function.
        'total_cost' - A float that is the total cost of the function.

% Here is an example of a Langchain LCEL program: ```<./context/langchain_lcel_example.py>```

% Here is how to preprocess the prompt from a file: ```<./context/preprocess_example.py>``` 

% Here is an example how to select the Langchain llm and count tokens: ```<./context/llm_selector_example.py>``` 

% Here is how to postprocess the LCEL output string: ```<./context/postprocess_example.py>``` 

% This function will use Langchain to do the following:
    Step 1. Use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/example_generator_LLM.prompt' file.
    Step 2. Preprocess the loaded example_generator prompt using the preprocess function with parameters recursive=False and double_curly_brackets=False.
    Step 3. Create a Langchain LCEL template from the preprocessed example_generator prompt.
    Step 4. Use llm_selector for the model.
    Step 5. Preprocess the prompt using the preprocess function for the processed_prompt.
    Step 6. Run the code through the model using Langchain LCEL. 
        6a. Pass the following string parameters to the prompt during invoke:
            - 'code_module'
            - 'processed_prompt'
            - 'language'
        6b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens. 
    Step 7. The LCEL output will contain a mix of text and Python code separated by triple backticks. Use postprocess to create a runnable example_code. Also, print out the total_cost including the input and output tokens and postprocessing.
    Step 8. Return the example_code and total_cost.