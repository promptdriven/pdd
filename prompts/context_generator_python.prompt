% You are an expert Python engineer. Your goal is to write a Python function, "context_generator", that will generate a concise example on how to use code_module properly.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'code_module' - A string that is the code module to generate a concise example for.
        'prompt' - A string that is the prompt that was used to generate the code_module.
        'language' - A string that is the language of the code module. Default is "python".
        'strength' - A float that is the strength of the LLM model to use. Default is 0.5. Range is between 0 and 1.
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0. Range is between 0 and 1.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs as a tuple:
        'example_code' - A string that is the concise example code generated by the function.
        'total_cost' - A float that is the total cost of the function.
        'model_name' - A string that is the name of the selected LLM model.

% Here is how to use the internal modules:
    <internal_modules>
        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For preprocessing prompts:
        <preprocess_example>
            <include>context/preprocess_example.py</include>
        </preprocess_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>

        For handling unfinished prompts:
        <unfinished_prompt_example>
            <include>context/unfinished_prompt_example.py</include>
        </unfinished_prompt_example>

        For continuing generation:
        <continue_generation_example>
            <include>context/continue_generation_example.py</include>
        </continue_generation_example>

        For postprocessing results:
        <postprocess_example>
            <include>context/postprocess_example.py</include>
        </postprocess_example>
    </internal_modules>

% This function will do the following:
    Step 1. Load and preprocess (with parameters recursive=False and double_curly_brackets=False) the 'example_generator_LLM' prompt template.
    Step 2. Run the code through the model using llm_invoke. 
        2a. Pass the following string parameters to the prompt during invoke:
            - 'code_module'
            - 'processed_prompt' (preprocess with parameters recursive=True and double_curly_brackets=True the input prompt)
            - 'language'
    Step 3. Detect if the generation is incomplete using the unfinished_prompt function (strength .5) by passing in the last 600 characters of the output of Step 2.
        - a. If incomplete, call the continue_generation function to complete the generation.
        - b. Else, if complete, postprocess the model output result using the postprocess function with a strength of 0.9.
    Step 4. Return the example_code, total_cost, and model_name.
