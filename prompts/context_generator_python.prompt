% You are an expert Python engineer. Your goal is to write a Python function, "context_generator", that will generate a concise example on how to use code_module properly.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'code_module' - A string that is the code module to generate a concise example for.
        'prompt' - A string that is the prompt that was used to generate the code_module.
        'language' - A string that is the language of the code module. Default is "python".
        'strength' - A float that is the strength of the LLM model to use. Default is 0.5. Range is between 0 and 1.
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0. Range is between 0 and 1.
        'time' - A float between 0 and 1 that controls the thinking effort for the LLM model, passed to llm_invoke. Default is DEFAULT_TIME.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs as a tuple:
        'example_code' - A string that is the concise example code generated by the function.
        'total_cost' - A float that is the total cost of the function.
        'model_name' - A string that is the name of the selected LLM model.

% Here is how to use the internal modules:
    <internal_modules>
        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For preprocessing prompts:
        <preprocess_example>
            <include>context/preprocess_example.py</include>
        </preprocess_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>

        For handling unfinished prompts:
        <unfinished_prompt_example>
            <include>context/unfinished_prompt_example.py</include>
        </unfinished_prompt_example>

        For continuing generation:
        <continue_generation_example>
            <include>context/continue_generation_example.py</include>
        </continue_generation_example>

        For postprocessing results:
        <postprocess_example>
            <include>context/postprocess_example.py</include>
        </postprocess_example>
    </internal_modules>

% This function will do the following:
    Step 1. Load and preprocess (with parameters recursive=False and double_curly_brackets=False) the 'example_generator_LLM' prompt template.
    Step 2. Preprocess the input 'prompt' with parameters recursive=True and double_curly_brackets=True to produce 'processed_prompt'.
    Step 3. Run the processed template through the model using llm_invoke, passing:
        - input_json with keys: 'code_module', 'processed_prompt', and 'language'
        - the provided 'strength', 'temperature', 'time', and 'verbose' parameters.
    Step 4. Detect if the generation is incomplete using the unfinished_prompt function (strength 0.5) by passing the last 600 characters of the output of Step 3. Also pass 'temperature', 'time', 'language', and 'verbose'.
        - a. If incomplete, call the continue_generation function with 'formatted_input_prompt' set to the processed template from Step 1 and 'llm_output' set to the result from Step 3. Also pass 'strength', 'temperature', 'time', 'language', and 'verbose'.
        - b. Else, use the Step 3 result as the final LLM output.
    Step 5. Postprocess the final LLM output using the postprocess function with strength EXTRACTION_STRENGTH. Also pass 'temperature', 'time', 'language', and 'verbose'.
    Step 6. Return the example_code, total_cost, and model_name, where:
        - total_cost = sum of costs from llm_invoke + unfinished_prompt + (continue_generation if run) + postprocess
        - model_name = the model that produced the final LLM output before postprocess (continue_generation's model if run, else llm_invoke's).
