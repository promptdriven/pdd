% You are an expert Python engineer. Your goal is to write a python function, "code_generator", that will compile a prompt into a code file. 

<include>./context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'prompt' - A string containing the raw prompt to be processed.
        'language' - A string that is the language type (e.g. python, bash) of file that will be outputed by the LLM.
        'strength' - A float between 0 and 1 that is the strength of the LLM model to use
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0.
    Outputs:
        'runnable_code' - A string that is runnable code
        'total_cost' - A float that is the total cost of the model run
        'model_name' - A string that is the name of the selected LLM model

% Here is an example of a LangChain Expression Language (LCEL) program: <lcel_example><include>context/langchain_lcel_example.py</include></lcel_example>

% Here are examples of how to use internal modules:
<internal_example_modules>
    % Here is an example how to preprocess the prompt from a file: <preprocess_example><include>./context/preprocess_example.py</include></preprocess_example>

    % Example of selecting a Langchain LLM and counting tokens using llm_selector: <llm_selector_example><include>./context/llm_selector_example.py</include></llm_selector_example>

    % Example usage of the unfinished_prompt function: <unfinished_prompt_example><include>./context/unfinished_prompt_example.py</include></unfinished_prompt_example>

    % Here is an example how to continue the generation of a model output: <postprocess_example><include>context/continue_generation_example.py</include></postprocess_example>

    % Here is an example how to postprocess the model output result: <postprocess_example><include>context/postprocess_example.py</include></postprocess_example>
</internal_example_modules>

% This program will use Langchain to do the following:
    Step 1. Preprocess the raw prompt using the preprocess function from the preprocess module.
    Step 2. Then this will create a Langchain LCEL template from the processed prompt with recursive and double curly brackets.
    Step 3. This will use llm_selector for the model.
    Step 4. This will run the prompt through the model using Langchain LCEL. It will pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.
    Step 5. This will pretty print the markdown formatting that is present in the result via the rich Markdown function. It will also pretty print the number of tokens in the result and the cost.
    Step 6. Detect if the generation is incomplete using the unfinished_prompt function (strength .5) by passing in the last 200 characters of the output of Step 4.
        - a. If incomplete, call the continue_generation function to complete the generation.
        - b. Else, if complete, postprocess the model output result using the postprocess function from the postprocess module with a strength of 0.9.
    Step 7. Print out the total_cost including the input and output tokens and functions that incur cost (e.g. postprocessing).
    Step 8. Return the runnable_code and total_cost.