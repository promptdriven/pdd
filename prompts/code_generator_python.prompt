% You are an expert Python engineer. Your goal is to write a python function, "code_generator", that will compile a prompt into a code file. 

<include>./context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'prompt' - A string containing the raw prompt to be processed.
        'language' - A string that is the language type (e.g. python, bash) of file that will be outputed by the LLM.
        'strength' - A float between 0 and 1 that is the strength of the LLM model to use.
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
        'preprocess_prompt' - A boolean that indicates whether to preprocess the prompt. Default is True.
    Outputs:
        'runnable_code' - A string that is runnable code
        'total_cost' - A float that is the total cost of the model run
        'model_name' - A string that is the name of the selected LLM model

% Here is how to use the internal modules:
    <internal_modules>
        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>

        For preprocessing prompts:
        <preprocess_example>
            <include>context/preprocess_example.py</include>
        </preprocess_example>

        For handling unfinished prompts:
        <unfinished_prompt_example>
            <include>context/unfinished_prompt_example.py</include>
        </unfinished_prompt_example>

        For continuing generation:
        <continue_generation_example>
            <include>context/continue_generation_example.py</include>
        </continue_generation_example>

        For postprocessing results:
        <postprocess_example>
            <include>context/postprocess_example.py</include>
        </postprocess_example>
    </internal_modules>

% This program will use Langchain to do the following:
    Step 1. Conditionally preprocess the raw prompt using the preprocess function from the preprocess module based on the value of 'preprocess_prompt'. If 'preprocess_prompt' is True, preprocess the prompt; otherwise, use the raw prompt directly.

    Step 2. Run the prompt (either preprocessed or raw) through llm_invoke with an empty dictionary ('{}') and the provided strength and temperature.

    Step 3. Detect if the generation is incomplete using the unfinished_prompt function (strength .5) by passing in the last 600 characters of the output of Step 3.
        - a. If incomplete, call the continue_generation function to complete the generation.
        - b. Else, if complete, postprocess the model output result using the postprocess function from the postprocess module with the EXTRACTION_STRENGTH constant. Be sure to pass in all parameters like verbose to postprocess.
    Step 4. Return the runnable_code, total_cost and model_name.