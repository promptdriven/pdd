% You are an expert Python engineer. Your goal is to write a python function, "code_generator", that will compile a prompt into a code file. All output to the console will be pretty print using the Python rich library.

% Here are the inputs and outputs of the function:
    Input: 
        'prompt' - A string containing the filepath of the raw prompt to be processed.
        'file_type' - A string that is the type (e.g. python, bash) of file that will be outputed by the LLM
    Output: returns a string that is runnable code

% Here is an example of a Langchain LCEL program: ```<./context/langchain_lcel_example.py>```

% Here is an example how to preprocess the prompt from a file: ```<./context/preprocess_example.py>```                
                                                            
% Here is an example how to postprocess the model output result: ```<./context/postprocess_example.py>``` 

% Here is an example how to use tiktoken: ```encoding = tiktoken.get_encoding("cl100k_base")  # or another encoding name
token_count = len(encoding.encode(preprocessed_prompt))```

% This program will use Langchain to do the following:
    Step 1. Preprocess the raw prompt using the preprocess function from the preprocess module.
    Step 2. Then this will create a Langchain LCEL template from the processed prompt.
    Step 3. This will use the "gpt-4o-mini" and a temperature of 0 for the model.
    Step 4. This will run the prompt through the model using Langchain LCEL. It will pretty print a message letting the user know it is running and how many tokens (using tiktoken) are in the prompt.
    Step 5. This will pretty print the markdown formatting that is present in the result via the rich Markdown function. It will also pretty print the number of tokens in the result.
    Step 6. The code result of the model will contain a mix of text and code separated by triple backticks. Use postprocess to create a runnable code.
    Step 7. Return the runnable code
