% You are an expert Python engineer. Your goal is to write a python function, "code_generator", that will compile a prompt into a code file. 

<include>../context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'prompt' - A string containing the raw prompt to be processed.
        'language' - A string that is the language type (e.g. python, bash) of file that will be outputed by the LLM.
        'strength' - A float between 0 and 1 that is the strength of the LLM model to use.
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0.
        'time' - A float in [0,1] or None that controls the thinking effort for the LLM model, passed to llm_invoke. Default is DEFAULT_TIME. If None, treat as DEFAULT_TIME before invoking llm_invoke.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
        'preprocess_prompt' - A boolean that indicates whether to preprocess the prompt. Default is True.
    Outputs:
        'runnable_code' - A string that is runnable code
        'total_cost' - A float that is the total cost of all LLM calls within this function (initial generation, unfinished check, continuation if used, and postprocess)
        'model_name' - A string that is the name of the selected LLM model used for the main generation (or continuation). Postprocess may use a different model internally and does not change this value.

% Here is how to use the internal modules:
    <internal_modules>
        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>../context/llm_invoke_example.py</include>
        </llm_invoke_example>

        For preprocessing prompts:
        <preprocess_example>
            <include>../context/preprocess_example.py</include>
        </preprocess_example>

        For handling unfinished prompts:
        <unfinished_prompt_example>
            <include>../context/unfinished_prompt_example.py</include>
        </unfinished_prompt_example>

        For continuing generation:
        <continue_generation_example>
            <include>../context/continue_generation_example.py</include>
        </continue_generation_example>

        For postprocessing results:
        <postprocess_example>
            <include>../context/postprocess_example.py</include>
        </postprocess_example>
    </internal_modules>

% This program will do the following:
    Step 1. Conditionally preprocess the raw prompt using the preprocess function from the preprocess module based on the value of 'preprocess_prompt'. If 'preprocess_prompt' is True, preprocess the prompt; otherwise, use the raw prompt directly. When preprocessing, it is acceptable to enable options such as double_curly_brackets=True and recursive=False to preserve placeholders and avoid over-expansion.

    Step 2. Generate the initial response as follows:
        - If the prompt contains embedded data URLs (e.g., 'data:image/...;base64,...'), split the prompt into alternating text and image parts (preserving order) and call llm_invoke with messages=[{role: 'user', content: [{type: 'image_url', image_url: {url: ...}}, {type: 'text', text: ...}, ...]}] and the provided strength, temperature, time, and verbose.
        - Otherwise, call llm_invoke with the (preprocessed or raw) prompt, input_json={}, and the provided strength, temperature, time, and verbose.

    Step 3. Detect if the generation is incomplete using the unfinished_prompt function (use strength=0.5, temperature=0.0) by passing in the last 600 characters of the output of Step 2. Pass through language, time, and verbose.
        - a. If incomplete, call the continue_generation function to complete the generation and set final_output to that result.
        - b. Else, set final_output to the initial model output.

    Step 4. Postprocess the final_output using the postprocess function from the postprocess module with the EXTRACTION_STRENGTH constant. Use temperature=0.0 and pass through language, time, and verbose.

    Step 5. Return the runnable_code, total_cost and model_name.

% Validation and defaults:
    - Validate non-empty 'prompt' and 'language'.
    - Enforce 0 ≤ strength ≤ 1 and 0 ≤ temperature ≤ 2.
    - Enforce 0 ≤ time ≤ 1 when provided; if time is None, substitute DEFAULT_TIME before llm_invoke.
