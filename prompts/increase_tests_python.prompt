% You are an expert Python engineer. Your goal is to write a Python function, "increase_tests", that will generate a set of unit tests to increase the coverage of the code under test.

<include>context/python_preamble.prompt</include>

% Here are the inputs and outputs of the function:
    Inputs: 
        'existing_unit_tests' - A string that is the existing unit tests for the code under test.
        'coverage_report' - A string that is the coverage report for the code under test.
        'code' - A string that is the code under test.
        'prompt_that_generated_code' - A string that is the prompt that was used to generate the code under test.
        'language' - A string that is the language of the code module. Default is "python".
        'strength' - A float that is the strength of the LLM model to use. Default is 0.5. Range is between 0 and 1.
        'temperature' - A float that is the temperature of the LLM model to use. Default is 0. Range is between 0 and 1.
        'time' - A float between 0 and 1 that controls the thinking effort for the LLM model, passed to llm_invoke. Default is DEFAULT_TIME.
        'verbose' - A boolean that indicates whether to print out the details of the function. Default is False.
    Outputs as a tuple:
        'increase_test_function' - A string that is the unit tests that will increase code coverage.
        'total_cost' - A float that is the total cost of the function.
        'model_name' - A string that is the name of the selected LLM model.

% Here is how to use the internal modules:
    <internal_modules>
        For loading prompt templates:
        <load_prompt_template_example>
            <include>context/load_prompt_template_example.py</include>
        </load_prompt_template_example>

        For running prompts with llm_invoke:
        <llm_invoke_example>
            <include>context/llm_invoke_example.py</include>
        </llm_invoke_example>

        For postprocessing results:
        <postprocess_example>
            <include>context/postprocess_example.py</include>
        </postprocess_example>
    </internal_modules>

% This function will do the following:
    Step 1. Load the 'increase_tests_LLM' prompt template.
    Step 2. Run the prompt through the model using llm_invoke, passing the input strength and the time parameter. 
        2a. Pass the following string parameters to the prompt during invoke:
            - 'existing_unit_tests'
            - 'coverage_report'
            - 'code'
            - 'prompt_that_generated_code'
            - 'language'
            Make sure all of these arguments are included.
    Step 3. Postprocess the model output result using the postprocess function with a strength of 0.83.
    Step 4. Return the example_code, total_cost, and model_name.