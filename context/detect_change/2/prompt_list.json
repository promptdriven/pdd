[{"PROMPT_NAME": "change_python.prompt", "PROMPT_DESCRIPTION": "% You are an expert Python Software Engineer. Your goal is to write a Python function, \"change\", that will modify an input_prompt into a modified_prompt per the change_prompt. All output to the console will be pretty printed using the Python Rich library. Ensure that the module imports are done using relative imports.\n\n% Here are the inputs and outputs of the function:\n    Inputs:\n        - 'input_prompt' - A string that contains the prompt that will be modified by the change_prompt.\n        - 'input_code' - A string that contains the code that was generated from the input_prompt.\n        - 'change_prompt' - A string that contains the instructions of how to modify the input_prompt.\n        - 'strength': A float value representing the strength parameter for the LLM model, used to influence the model's behavior.\n        - 'temperature': A float value representing the temperature parameter for the LLM model, used to control the randomness of the model's output.\n    Outputs:\n        - 'modified_prompt' - A string that contains the modified prompt that was changed based on the change_prompt.\n        - 'total_cost': A float value representing the total cost of running the function.\n        - 'model_name': A string representing the name of the selected LLM model.\n\n% Here is an example how to preprocess the prompt from a file: ```<./context/preprocess_example.py>```\n\n% Example usage of the Langchain LCEL program: ```<./context/langchain_lcel_example.py>```\n\n% Example of selecting a Langchain LLM and counting tokens using llm_selector: ```<./context/llm_selector_example.py>```\n\n% Steps to be followed by the function:\n    1. Load the '$PDD_PATH/prompts/xml/change_LLM.prompt' and '$PDD_PATH/prompts/extract_prompt_change_LLM.prompt' files.\n    2. Preprocess the change_LLM prompt using the preprocess function from the preprocess module and set double_curly_brackets to false.\n    3. Create a Langchain LCEL template from the processed change_LLM prompt to return a string output.    \n    4. Use the llm_selector function for the LLM model and token counting.\n    5. Run the input_prompt through the model using Langchain LCEL:\n        - a. Pass the following string parameters to the prompt during invocation:             \n            * 'input_prompt'\n            * 'input_code'\n            * 'change_prompt' (preprocess this with double_curly_brackets set to false)\n        - b. Calculate the input and output token count using token_counter from llm_selector and pretty print the output of 4a, including the token count and estimated cost. The cost from llm_selector is in dollars per million tokens.\n    6. Create a Langchain LCEL template with strength .9 from the extract_prompt_change_LLM prompt that outputs JSON:\n        - a. Pass the following string parameters to the prompt during invocation: 'llm_output' (this string is from Step 4).\n        - b. Calculate input and output token count using token_counter from llm_selector and pretty print the running message with the token count and cost.\n        - c. Use 'get' function to extract 'modified_prompt' key values using from the dictionary output.\n    7. Pretty print the extracted modified_prompt using Rich Markdown function. Include token counts and costs.\n    8. Return the 'modified_prompt' string, the total_cost of both invokes and model_name use for the change_LLM prompt.\n\n% Ensure the function handles edge cases, such as missing inputs or model errors, and provide clear error messages."}, {"PROMPT_NAME": "preprocess_python.prompt", "PROMPT_DESCRIPTION": "% You are an expert Python engineer. Your goal is to write a Python function, 'preprocess_prompt', that will preprocess the prompt from a prompt string for a LLM. This will use regular expressions to preprocess specific XML-like tags, if any, in the prompt. All output to the console will be pretty print using the Python rich library.\n\n% Here are the inputs and outputs of the function:\n    Input: \n        'prompt' - A string that is the prompt to preprocess\n        'recursive' - A boolean that is True if the program needs to recursively process the includes in the prompt and False if it does not need to recursively process the prompt. Default is True.\n        'double_curly_brackets' - A boolean that is True if the curly brackets need to be doubled and False if they do not need to be doubled. Default is True.\n        'exclude_keys' - An optional list of strings that are excluded from the curly bracket doubling.\n    Output: returns a string that is the preprocessed prompt, with any leading or trailing whitespace removed.\n\n% Here are the XML-like tags to preprocess, other tags will remain unmodified:\n    'include' - This tag will include the content of the file indicated in the include tag. The 'include tag' will be directly replaced with the content of the file in the prompt, without wrapping it in a new tag.\n    'pdd' - This tag indicates a comment and anything in this XML will be deleted from the string including the 'pdd' tags themselves.\n    'shell' - This tag indicates that there are shell commands to run. Capture all output of the shell commands and include it in the prompt but remove the shell tags.\n\n% Includes can be nested, that is there can be includes inside of the files of the includes and 'preprocess' should be called recursively on these include files if recursive is True. There are two ways of having includes in the prompt:\n    1. Will check to see if the file has any angle brackets in triple backticks. If so, it will read the included file indicated in the angle brackets and replace the angle brackets with the content of the included file. This will be done recursively until there are no more angle brackets in triple backticks. The program will then remove the angle brackets but leave the contents in the triple backticks.\n    2. The XML 'include' mentioned above.\n\n% If double_curly_brackets is True, the program will check to see if the file has any single curly brackets and if it does and the string in the curly brackets are not in the exclude_keys list, it will check to see if the curly brackets are already doubled before doubling the curly brackets.\n\n% The program should resolve file paths using the PDD_PATH environment variable. Implement a function 'get_file_path' that takes a file name and returns the full path using this environment variable.\n\n% Keep the user informed of the progress of the program by pretty printing messages."}, {"PROMPT_NAME": "unfinished_prompt_python.prompt", "PROMPT_DESCRIPTION": "% You are an expert Python engineer. Your goal is to write a python function called 'unfinished_prompt' that will determine if a given prompt is complete or needs to continue.\n\n% Here are the inputs and outputs of the function:\n    Inputs:\n        'prompt_text' - A string containing the prompt text to analyze.\n        'strength' - A float that is the strength of the LLM model to use for the analysis. Default is 0.5.\n        'temperature' - A float that is the temperature of the LLM model to use for the analysis. Default is 0.\n    Outputs:\n        'reasoning' - A string containing the structured reasoning for the completeness assessment.\n        'is_finished' - A boolean indicating whether the prompt is complete (True) or incomplete (False).\n        'total_cost' - A float that is the total cost of the analysis function. This is an optional output.\n        'model_name' - A string that is the name of the LLM model used for the analysis. This is an optional output.\n\n% Here is an example of a Langchain LCEL program: ```<./context/langchain_lcel_example.py>```\n\n% Here is an example how to select the Langchain llm and count tokens: ```<./context/llm_selector_example.py>```\n\n% Note: Use relative import for 'llm_selector' to ensure compatibility within the package structure (i.e. 'from .llm_selector') instead of 'from pdd.llm_selector'.\n\n% This function will do the following:\n    Step 1. Use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/unfinished_prompt_LLM.prompt' file.\n    Step 2. Create a Langchain LCEL template from unfinished_prompt_LLM prompt so that it returns a JSON output.\n    Step 3. Use the llm_selector function for the LLM model.\n    Step 4. Run the prompt text through the model using Langchain LCEL.\n        4a. Pass the following string parameters to the prompt during invoke:\n            - 'PROMPT_TEXT'\n        4b. Pretty print a message letting the user know it is running and how many tokens (using token_counter function from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.\n        4c. The dictionary output of the LCEL will have the keys 'reasoning' and 'is_finished'. Be sure to access these keys using the get method with default error messages.\n        4d. Pretty print the reasoning and completion status using the rich library. Also, print the number of tokens in the result, the output token cost and the total_cost.\n    Step 5. Return the 'reasoning' string and 'is_finished' boolean from the JSON output using 'get', and the 'total_cost' float, and 'model_name' string.\n\n% Ensure that the function handles potential errors gracefully, such as missing input parameters or issues with the LLM model responses.\n\n"}, {"PROMPT_NAME": "xml_tagger_python.prompt", "PROMPT_DESCRIPTION": "% You are an expert Python engineer. Your goal is to write a Python function, \"xml_tagger\", that will enhance a given LLM prompt by adding XML tags to improve its structure and readability.\n\n<include>context/python_preamble.prompt</include>\n\n% Here are the inputs and outputs of the function:\n    Input: \n        'raw_prompt' - A string containing the prompt that needs XML tagging to improve its organization and clarity.\n        'strength' - A float value representing the strength parameter for the LLM model.\n        'temperature' - A float value representing the temperature parameter for the LLM model.\n    Output: \n        'xml_tagged' - A string containing the prompt with properly added XML tags.\n        'total_cost' - A float representing the total cost of running the LCELs.\n        'model_name' - A string representing the name of the selected LLM model.\n\n% Here is an example of a LangChain Expression Language (LCEL) program: <lcel_example><include>context/langchain_lcel_example.py</include></lcel_example>\n\n% Here are examples of how to use internal modules:\n<internal_example_modules>\n    % Example of selecting a Langchain LLM and counting tokens using llm_selector: <llm_selector_example><include>context/llm_selector_example.py</include></llm_selector_example>\n</internal_example_modules>\n\n% This program will use Langchain to do the following:\n    Step 1. Use $PDD_PATH environment variable to get the path to the project. Load the '$PDD_PATH/prompts/xml_convertor_LLM.prompt' and '$PDD_PATH/prompts/extract_xml_LLM.prompt' files.\n    Step 2. Create a Langchain LCEL template from xml_convertor prompt so that it returns a string output.\n    Step 3. Use the llm_selector function for the LLM model and token counting.\n    Step 4. Run the code through the model using Langchain LCEL. \n        4a. Pass the following string parameters to the prompt during invoke:\n            - 'raw_prompt'\n        4b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens. \n        4c. The string output of the LCEL will be 'xml_generated_analysis' that contains the tagged prompt.\n    Step 5. The code result of the model will contain a mix of text and XML separated by triple backticks. Create a Langchain LCEL template but with a llm_selector with strength .8 from the extract_xml prompt that has a JSON output.\n        5a. Pass the following string parameters to the prompt during invoke:\n            - 'xml_generated_analysis'\n        5b. Pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.\n        5c. The JSON output of the LCEL will have the key 'xml_tagged' that contains the extracted tagged prompt.\n    Step 6. Pretty print the extracted tagged prompt using the rich Markdown function. Also, print the number of tokens in the result and the cost.\n    Step 7. Calculate the total cost by summing the costs from both LCEL runs.\n    Step 8. Return the 'xml_tagged' string using 'get', the 'total_cost' and 'model_name'."}]