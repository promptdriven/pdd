[{{"PROMPT_NAME": "context/python_preamble.prompt", "PROMPT_DESCRIPTION": "% The function should be part of a Python package, using relative imports (single dot) for internal modules. All output to the console will be pretty printed using the Python Rich library. Ensure the function handles edge cases, such as missing inputs or model errors, and provide clear error messages."}}, {{"PROMPT_NAME": "prompts/change_python.prompt", "PROMPT_DESCRIPTION": "% You are an expert Python Software Engineer. Your goal is to write a Python function, \"change\", that will split an input_prompt into a modified_prompt per the change_prompt. All output to the console will be pretty printed using the Python Rich library. Ensure that the module imports are done using relative imports.\n\n% Here are the inputs and outputs of the function:\n    Inputs:\n        - 'input_prompt' - A string that contains the prompt that will be modified by the change_prompt.\n        - 'input_code' - A string that contains the code that was generated from the input_prompt.\n        - 'change_prompt' - A string that contains the instructions of how to modify the input_prompt.\n        - 'strength': A float value representing the strength parameter for the LLM model, used to influence the model's behavior.\n        - 'temperature': A float value representing the temperature parameter for the LLM model, used to control the randomness of the model's output.\n    Outputs:\n        - 'modified_prompt' - A string that contains the modified prompt that was changed based on the change_prompt.\n        - 'total_cost': A float value representing the total cost of running the function.\n        - 'model_name': A string representing the name of the selected LLM model.\n\n% Here is an example how to preprocess the prompt from a file: ```<./context/preprocess_example.py>```\n\n% Example usage of the Langchain LCEL program: ```<./context/langchain_lcel_example.py>```\n\n% Example of selecting a Langchain LLM and counting tokens using llm_selector: ```<./context/llm_selector_example.py>```\n\n% Steps to be followed by the function:\n    1. Load the '$PDD_PATH/prompts/xml/change_LLM.prompt' and '$PDD_PATH/prompts/extract_prompt_change_LLM.prompt' files.\n    2. Preprocess the change_LLM prompt using the preprocess function from the preprocess module and set double_curly_brackets to false.\n    3. Create a Langchain LCEL template from the processed change_LLM prompt to return a string output.    \n    4. Use the llm_selector function for the LLM model and token counting.\n    5. Run the input_prompt through the model using Langchain LCEL:\n        - a. Pass the following string parameters to the prompt during invocation:             \n            * 'input_prompt'\n            * 'input_code'\n            * 'change_prompt' (preprocess this with double_curly_brackets set to false)\n        - b. Calculate the input and output token count using token_counter from llm_selector and pretty print the output of 4a, including the token count and estimated cost. The cost from llm_selector is in dollars per million tokens.\n    6. Create a Langchain LCEL template from the extract_prompt_change_LLM prompt that outputs JSON:\n        - a. Pass the following string parameters to the prompt during invocation: 'llm_output' (this string is from Step 4).\n        - b. Calculate input and output token count using token_counter from llm_selector and pretty print the running message with the token count and cost.\n        - c. Use 'get' function to extract 'modified_prompt' key values using from the dictionary output.\n    7. Pretty print the extracted modified_prompt using Rich Markdown function. Include token counts and costs.\n    8. Return the 'modified_prompt' string, the total_cost of both invokes and model_name use for the change_LLM prompt.\n\n% Ensure the function handles edge cases, such as missing inputs or model errors, and provide clear error messages."}}, {{"PROMPT_NAME": "prompts/fix_error_loop_python.prompt", "PROMPT_DESCRIPTION": "% You are an expert Python Software Engineer. Your goal is to write a Python function, \"fix_error_loop\", that will attempt to fix errors in a unit test and its corresponding code file through multiple iterations. All output to the console will be pretty printed using the Python rich library.\n\n% Here are the inputs and outputs of the function:\n    Inputs: \n        'unit_test_file' - A string containing the path to the unit test file.\n        'code_file' - A string containing the path to the code file being tested.\n        'verification_program' - A string containing the path to a Python program that verifies if the code still runs correctly.\n        'strength' - A float between 0 and 1 that represents the strength of the LLM model to use.\n        'temperature' - A float that represents the temperature parameter for the LLM model.\n        'max_attempts' - An integer representing the maximum number of fix attempts before giving up.\n        'budget' - A float representing the maximum cost allowed for the fixing process.\n        'error_log_file' - A string containing the path to the error log file (default: \"error_log.txt\").\n    Outputs:\n        'success' - A boolean indicating whether the errors were successfully fixed.\n        'final_unit_test' - A string containing the contents of the final unit test file.\n        'final_code' - A string containing the contents of the final code file.\n        'total_attempts' - An integer representing the number of fix attempts made.\n        'total_cost' - A float representing the total cost of all fix attempts.\n\n% Here is an example of the fix_errors_from_unit_tests function that will be used: ```<./context/fix_errors_from_unit_tests_example.py>```\n\n% This function will do the following:\n    Step 1. Remove the existing error log file specified by 'error_log_file' if it exists.\n    Step 2. Initialize variables:\n        - Counter for the number of attempts\n        - Total cost accumulator\n        - Best iteration tracker (lowest errors, then lowest fails)\n    Step 3. Enter a while loop that continues until max_attempts is reached or budget is exceeded:\n        a. Run the unit_test_file with 'python -m pytest -vv' and pipe all output appended to the specified error log file.\n        b. If the test passes, break the loop.\n        c. If the test fails:\n           - Read and print the error message from the error log file.\n           - Count the number of 'FAILED' and 'ERROR' from stdout (accounting for the -vv flag doubling these messages).\n           - Create backup copies of the unit_test_file and code_file, appending the number of fails, errors, and the current iteration number to the filenames like this \"unit_test_1_0_3.py\" and \"code_1_0_3.py\", where there was one fail, zero errors and it is the third iteration through the loop.\n           - Read the contents of the unit_test_file and code_file.\n           - Call fix_errors_from_unit_tests with the file contents, error from the error log file, the error log file, and the provided strength and temperature. All print output of this function besides being displayed should also be appended to the error log file with a separator to indicate it is coming from this function.\n           - Add the returned total_cost to the total cost accumulator.\n           - If the total cost exceeds the budget, break the loop.\n           - If both updated_unit_test and updated_code are False, break the loop as no changes were needed.\n           - If updated_unit_test is True, write the fixed_unit_test back to the unit_test_file.           \n           - If updated_code is True:\n              * Write the fixed_code back to the code_file.\n              * Run the verification_program to check if the code still runs.\n              * If the verification fails, restore the original file from the backup and continue the loop.\n              * If the verification succeeds, update the best iteration tracker if this iteration has fewer errors or fails.\n        d. Increment the attempt counter.\n    Step 4. After the loop ends, run pytest one last time, pipe the output to the error log file, and print it to the console.\n    Step 5. If the last run isn't the best iteration, copy back the files from the best iteration.\n    Step 6. Return the success status, final unit test contents, final code contents, total number of attempts, and total cost.\n\n% Ensure that the function handles potential errors gracefully, such as file I/O errors or subprocess execution failures. Use the rich library for all console output to enhance readability. Consider using context managers for file operations to ensure proper resource management."}}, {{"PROMPT_NAME": "prompts/code_generator_python.prompt", "PROMPT_DESCRIPTION": "% You are an expert Python engineer. Your goal is to write a python function, \"code_generator\", that will compile a prompt into a code file. \n\n<include>./context/python_preamble.prompt</include>\n\n% Here are the inputs and outputs of the function:\n    Inputs: \n        'prompt' - A string containing the raw prompt to be processed.\n        'language' - A string that is the language type (e.g. python, bash) of file that will be outputed by the LLM.\n        'strength' - A float between 0 and 1 that is the strength of the LLM model to use\n        'temperature' - A float that is the temperature of the LLM model to use. Default is 0.\n    Outputs:\n        'runnable_code' - A string that is runnable code\n        'total_cost' - A float that is the total cost of the model run\n        'model_name' - A string that is the name of the selected LLM model\n\n% Here is an example of a LangChain Expression Language (LCEL) program: <lcel_example><include>context/langchain_lcel_example.py</include></lcel_example>\n\n% Here are examples of how to use internal modules:\n<internal_example_modules>\n    % Here is an example how to preprocess the prompt from a file: <preprocess_example><include>context/preprocess_example.py</include></preprocess_example>\n\n    % Example of selecting a Langchain LLM and counting tokens using llm_selector: <llm_selector_example><include>./context/llm_selector_example.py</include></llm_selector_example>\n\n    % Example usage of the unfinished_prompt function: <unfinished_prompt_example><include>context/unfinished_prompt_example.py</include></unfinished_prompt_example>\n\n    % Here is an example how to continue the generation of a model output: <postprocess_example><include>context/continue_generation_example.py</include></postprocess_example>\n\n    % Here is an example how to postprocess the model output result: <postprocess_example><include>context/postprocess_example.py</include></postprocess_example>\n</internal_example_modules>\n\n% This program will use Langchain to do the following:\n    Step 1. Preprocess the raw prompt using the preprocess function from the preprocess module.\n    Step 2. Then this will create a Langchain LCEL template from the processed prompt with recursive and double curly brackets.\n    Step 3. This will use llm_selector for the model.\n    Step 4. This will run the prompt through the model using Langchain LCEL. It will pretty print a message letting the user know it is running and how many tokens (using token_counter from llm_selector) are in the prompt and the cost. The cost from llm_selector is in dollars per million tokens.\n    Step 5. This will pretty print the markdown formatting that is present in the result via the rich Markdown function. It will also pretty print the number of tokens in the result and the cost.\n    Step 6. Detect if the generation is incomplete using the unfinished_prompt function (strength .5) by passing in the last 600 characters of the output of Step 4.\n        - a. If incomplete, call the continue_generation function to complete the generation.\n        - b. Else, if complete, postprocess the model output result using the postprocess function from the postprocess module with a strength of 0.9.\n    Step 7. Print out the total_cost including the input and output tokens and functions that incur cost (e.g. postprocessing).\n    Step 8. Return the runnable_code, total_cost and model_name."}}]