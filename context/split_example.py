#Here's a concise example of how to use the `split` function from the provided module, along with documentation for the input and output parameters.
#
#### Example Usage of `split` Function
#
#```python
# Import the split function from the module
from split_prompt import split

# Define input parameters
input_prompt = "What are the benefits of using Python?"
input_code = "def benefits_of_python(): pass"
example_code = "def example(): return 'This is an example.'"
strength = 0.5  # Model strength (0 to 1)
temperature = 0.7  # Sampling temperature (0 to 1)

# Call the split function
sub_prompt, modified_prompt, total_cost = split(input_prompt, input_code, example_code, strength, temperature)

# Print the results
print(f"Sub Prompt: {sub_prompt}")
print(f"Modified Prompt: {modified_prompt}")
print(f"Total Cost: ${total_cost:.6f}")
#```
#
#### Input Parameters
#- `input_prompt` (str): The main prompt that you want to process.
#- `input_code` (str): The code snippet related to the input prompt.
#- `example_code` (str): An example code snippet to guide the model.
#- `strength` (float): A value between 0 and 1 that indicates the strength of the model's response.
#- `temperature` (float): A value between 0 and 1 that controls the randomness of the model's output.
#
#### Output Parameters
#- `sub_prompt` (str): The extracted sub-prompt generated by the model.
#- `modified_prompt` (str): The modified version of the original prompt based on the model's output.
#- `total_cost` (float): The estimated cost of processing the input through the model, calculated based on token usage.
#
#### Notes
#- Ensure that the `PDD_PATH` environment variable is set correctly to point to the directory containing the prompt files.
#- The `llm_selector` function and the `tiktoken` library must be properly configured in your environment for the `split` function to work as intended.