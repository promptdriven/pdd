full_path,file_summary,date
"examples/backend_api_example.py","This Python script serves as an example and demonstration of how to programmatically interact with the PDD Linter API using FastAPI's `TestClient`. The script begins by setting up the Python path to ensure the backend application module (`src.backend.backend_api`) can be correctly imported from the project root. It then defines a function `run_api_example` which executes a series of simulated HTTP requests against the API without requiring a running server.

The example covers three main scenarios: first, it performs a health check by sending a GET request to the `/health` endpoint to verify the API is operational. Second, it demonstrates a standard linting workflow by sending a POST request to the `/lint` endpoint with a sample prompt (Write a python script to scrape google.) and a configuration set to use heuristic analysis only. It prints the resulting score, issue count, and summary. Finally, the script tests error handling by sending a request with an invalid configuration payload, expecting and printing the resulting validation error (status code 400 or 422). This file is useful for developers understanding how to integrate or test the linter API.",2026-01-07T19:45:13.087537+00:00
"examples/cli_example.py","This Python script serves as a programmatic usage example and test harness for the PDD Linter CLI application. It demonstrates how to invoke the CLI commands directly from Python code using Typer's `CliRunner` and `unittest.mock`. The script sets up the environment by adding the project root to the system path and importing the main CLI application object. It defines a helper function, `create_mock_report`, to generate dummy analysis results, avoiding the need for actual heavy processing or LLM API calls during the demonstration.

The `main` function executes four distinct scenarios to verify CLI functionality: basic linting, JSON output formatting, exit code behavior when using the `--fail-on` flag (simulating a failure on warnings), and configuration passing (verifying that flags like `--assume-cloud-grounding` and `--llm-provider` are correctly propagated to the internal pipeline). Throughout execution, the script creates a temporary dummy file for input, mocks the core `lint_file` function to return controlled data, prints the results of each scenario to the console, and ensures cleanup by deleting the temporary file upon completion.",2026-01-07T19:45:20.102714+00:00
"examples/fix_example.py","This Python script serves as a demonstration and test harness for a module named `PDD Prompt Fixer`. It illustrates how to programmatically apply automated fixes to a text prompt based on a structured analysis report. The script begins by setting up the system path to ensure local project modules (`src.utils.models` and `src.utils.fix`) can be imported. It defines a helper function, `get_safe_rule_category`, to robustly handle potential variations in the `RuleCategory` enum structure.

The `main` function simulates a workflow where a broken text prompt—missing root tags and lacking specificity—is repaired. It constructs a mock `Report` object containing a structural `Issue` (missing `<prompt>` tags) and an `LLMResponse` with specific suggestions for content improvement (replacing generic text with a specific persona). The core functionality is demonstrated by calling `apply_fixes`, which processes the original text against the report. Finally, the script prints the original and fixed text to the console and performs a simple verification check to confirm that the structural tags were added and the content was patched correctly.",2026-01-07T19:45:26.832612+00:00
"examples/frontend_streamlit_example.py","This Python script serves as a launcher for the PDD Prompt Linter Streamlit application. It is designed to programmatically execute a Streamlit frontend module, simulating the standard command-line behavior of running a Streamlit app. The script defines a primary function, `run_streamlit_app`, which first determines the absolute path of the target application file (`src/frontend/frontend_streamlit.py`) relative to the script's location. It includes error handling to verify the existence of the target file before proceeding. Once the path is confirmed, the script constructs a subprocess command to run the application using the current Python executable, adding flags such as `--server.headless=true` to potentially suppress automatic browser opening. The execution is wrapped in a try-except block to gracefully handle user interruptions (KeyboardInterrupt), process crashes, or missing dependencies (specifically if Streamlit is not installed). Finally, the script includes a main execution block that checks for existing Streamlit environment variables to prevent recursive execution, ensuring it acts solely as an entry point for starting the server.",2026-01-07T19:45:33.385660+00:00
"examples/helpers_example.py","This Python script serves as a demonstration and test harness for the `src.utils.helpers` module within the PDD Prompt Linter project. It begins by setting up the system path to ensure the project root is accessible, allowing the script to import the necessary internal modules. The `main` function executes three primary demonstrations of the helper utilities. First, it showcases file I/O and normalization by creating a temporary file with messy line endings and trailing whitespace, reading it back using `helpers.read_file_content`, and asserting that the content is correctly cleaned and normalized. Second, it demonstrates tag detection and extraction by parsing a sample prompt string containing `<include>` and `<web>` tags, counting their occurrences, and extracting their inner content using `helpers.count_tags` and `helpers.extract_tag_content`. Finally, the script illustrates heuristic content analysis by comparing two text blocks—one primarily natural language and one primarily Python code. It calculates a code ratio score for each using `helpers.calculate_code_ratio` to verify that the utility can distinguish between prose and programming logic. The script concludes by printing the results of these operations to the console.",2026-01-07T19:45:39.895728+00:00
"examples/llm_example.py","This Python script serves as an example demonstration for using the `src.utils.llm` utility module within the PDD Linter project. It illustrates how to integrate and utilize Large Language Model (LLM) analysis functions for prompt evaluation. The script begins by setting up the system path to ensure local modules can be imported and configuring logging to display internal warnings. The `main` function executes three primary tasks: first, it resolves and displays the active LLM provider and model (e.g., OpenAI, Anthropic, or Google) based on available environment variables. Second, it sends a sample prompt (Write a python script to scrape a website) to the `analyze_prompt` function, demonstrating how to handle the structured response object, including guide alignment summaries, prioritized fixes, and specific suggestions. Finally, it shows how to override default settings by forcing a specific model configuration (e.g., `gpt-3.5-turbo`). The code includes error handling logic to manage scenarios where API keys are missing or network requests fail, ensuring the application degrades gracefully rather than crashing.",2026-01-07T19:45:47.208738+00:00
"examples/models_example.py","This Python script serves as a demonstration and test harness for the data models defined in a project's `src.utils.models` module. It illustrates how to instantiate, validate, and serialize various Pydantic models used for linting and reporting on prompt engineering tasks. The script begins by setting up the system path to import local modules. It then proceeds through five distinct steps: creating a single linting `Issue` object with specific attributes like rule ID and severity; simulating and parsing a structured `LLMResponse` to demonstrate how the model handles valid data and ignores extra fields; aggregating these components into a full `Report` object that includes a file path, score, and analysis summary; testing validation logic by attempting to create a `Report` with an invalid score (over 100) to trigger and catch a `ValidationError`; and finally, serializing the valid `Report` object into a JSON string. The output confirms the correct behavior of the data structures, including default values, validation constraints, and serialization capabilities.",2026-01-07T19:45:54.585974+00:00
"examples/pipeline_example.py","This Python script serves as a demonstration and usage example for a text linting pipeline module, likely part of a larger project. It begins by configuring the system path to ensure imports from the project root function correctly. The script defines a helper function, `print_report_summary`, which formats and displays the results of a linting report, including scores, summaries, and specific issues categorized by severity. The `main` function showcases three distinct usage scenarios: first, a fast, heuristic-based linting of raw text without using a Large Language Model (LLM); second, a comprehensive analysis that utilizes an LLM to generate fixes and applies custom weighting to different linting categories; and third, a file-based linting operation that processes a text file from disk. The script also handles the creation and cleanup of a dummy file for the third scenario.",2026-01-07T19:46:00.935349+00:00
"examples/report_example.py","This Python script serves as a demonstration and test harness for the reporting functionality of a larger project, likely a linter or analysis tool for LLM prompts. It begins by configuring the system path to ensure imports from the parent `src` directory function correctly, specifically targeting `src.utils.models` and `src.utils.report`. The core of the script is the `create_mock_report` function, which constructs a dummy `Report` object populated with synthetic data, including various `Issue` objects (representing errors, warnings, and info logs) and a mock `LLMResponse`. The `main` function then executes four distinct examples of how to render this report using the `render_report` utility: printing a rich text version to the console, outputting a JSON string to stdout, saving a Markdown formatted report to a file, and saving a plain text version (with ANSI colors stripped) to a file. The script is designed to verify that the reporting module handles different output formats and file writing operations correctly.",2026-01-07T19:46:07.119343+00:00
"examples/report_output.md","The provided file is a PDD (Prompt Driven Development) Prompt Report for the file `prompts/customer_service_agent.txt`. The report assigns the prompt a moderate quality score of 75 out of 100. The summary highlights a critical modularity issue within the prompt structure. Specifically, the report details three distinct issues categorized by rule, severity, and line number. The most severe issue (MOD001) is an error on line 15, noting that the prompt exceeds 500 tokens without being divided into sub-prompts. A warning (SEC002) is flagged on line 42 regarding a potential PII placeholder for user email, and an informational note (STY005) suggests adding a version tag to the header. Finally, the AI analysis section comments that while the prompt is generally well-structured, it lacks specific persona definitions.",2026-01-07T22:11:34.940575+00:00
"examples/report_output.txt","The provided file is an automated analysis report for a prompt file named 'prompts/customer_service_agent.txt'. The prompt received a moderate quality score of 75 out of 100. The report highlights a critical modularity issue, specifically flagging an error (MOD001) on line 15 because the prompt exceeds 500 tokens without being divided into sub-prompts. Additionally, a security warning (SEC002) was raised on line 42 regarding a potential Personally Identifiable Information (PII) placeholder, '{{user_email}}'. A minor stylistic suggestion (STY005) recommends adding a version tag to the header. The report concludes with AI feedback noting that while the prompt is generally well-structured, it lacks specific persona definitions.",2026-01-07T22:11:34.965605+00:00
"examples/rules_example.py","This Python script serves as a demonstration and test harness for a text analysis module, specifically designed to validate prompt engineering structures against a set of rules. It begins by configuring the system path to import core modules (`src.utils.rules` and `src.utils.models`) from a parent directory. The script defines a helper function, `print_issues`, which formats and displays the results of the analysis, using icons to distinguish between errors, warnings, and informational messages. The `main` function runs four distinct scenarios: a compliant prompt that follows PDD (Prompt Driven Development) standards, a non-deterministic prompt containing forbidden tags like `<web>` and `<run>`, a repo dump prompt that simulates excessive code dumping to trigger ratio checks, and a broken prompt that violates modularity and anatomy rules. Each scenario passes a sample string to `rules.analyze_text()` and prints the resulting compliance report.",2026-01-07T19:46:26.334018+00:00
