# PDD Prompt Linter

A linting tool built using Prompt-Driven Development (PDD) to analyze and validate prompts. This project demonstrates how PDD can generate a complete, working application from specification documents.

## Features

- **Rule-Based Validation**: Checks prompts against PDD best practices
- **Multiple Interfaces**: CLI, REST API, and Streamlit UI
- **Automated Suggestions**: Provides actionable feedback for identified issues
- **100% PDD-Generated**: All code generated from prompts using PDD

## Prerequisites

```bash
pip install pdd-cli
```

Or install from source:

```bash
cd /path/to/pdd
pip install -e .
```

## Installation

After installing PDD (see Prerequisites above), install the project dependencies:

**Option 1: Using requirements.txt (recommended)**
```bash
pip install -r requirements.txt
```

**Option 2: Manual installation**
```bash
pip install typer rich pydantic fastapi uvicorn streamlit litellm httpx
```

## Usage

### CLI Interface

**Basic linting:**
```bash
python -m src.cli.cli prompts/my_prompt.prompt
```

**Enable AI analysis** (set API key first):
```bash
export OPENAI_API_KEY="sk-..."  # or ANTHROPIC_API_KEY, GOOGLE_API_KEY
python -m src.cli.cli prompts/my_prompt.prompt
```

**Common options:**
```bash
# JSON/Markdown output
python -m src.cli.cli --format json prompts/my_prompt.prompt

# Filter severity
python -m src.cli.cli --severity-threshold error prompts/my_prompt.prompt

# Auto-fix (display fixed prompt)
python -m src.cli.cli --fix prompts/my_prompt.prompt

# Auto-fix (save to specific file)
python -m src.cli.cli --fix --fix-output output/fixed_prompt.prompt prompts/my_prompt.prompt

# Auto-fix (overwrite original file)
python -m src.cli.cli --fix --in-place prompts/my_prompt.prompt

# Save report to file
python -m src.cli.cli --output report.txt prompts/my_prompt.prompt
```

**LLM options:**
```bash
# Disable AI (heuristics only)
python -m src.cli.cli --no-llm prompts/my_prompt.prompt

# Use specific model
python -m src.cli.cli --llm-model gpt-4 prompts/my_prompt.prompt
```

**CI/CD:**
```bash
python -m src.cli.cli --fail-on error prompts/my_prompt.prompt
```

### REST API

```bash
# Start server
python -m src.backend.backend_api

# Lint via API
curl -X POST http://localhost:8000/lint \
  -H "Content-Type: application/json" \
  -d '{"content": "Your prompt here", "config": {"use_llm": true}}'
```

### Streamlit UI

```bash
python -m streamlit run src/frontend/frontend_streamlit.py
# Opens at http://localhost:8501
```

## Development Workflow with PDD

> **ðŸ’¡ Quick Start with Makefile**: This project includes a `Makefile` (generated by an LLM after being given this README as context) that automates the entire workflow. Run `make help` to see all available commands. The steps below explain the workflow in detail, but you can use `make architecture`, `make prompts`, and `make sync-all` to automate the process.

### Step 1: Create Specification Documents

Create your project specification and tech stack in the `docs/` directory:

```bash
mkdir -p docs
```

**`docs/specs.md`**: Product requirements document defining what the tool should do

**`docs/tech_stack.md`**: Technology choices and framework decisions

### Step 2: Generate Architecture

```bash
pdd --force generate --template architecture/architecture_json \
  -e PRD_FILE=docs/specs.md \
  -e TECH_STACK_FILE=docs/tech_stack.md \
  --output architecture.json
```

### Step 3: Project Setup

Create `.pddrc` to configure output paths.

**Option 1: Generate from architecture** (recommended):
```bash
pdd generate --template generic/generate_pddrc \
  -e ARCHITECTURE_FILE=architecture.json \
  --output .pddrc
```

**Option 2: Create manually**:
```yaml
version: "1.0"

contexts:
  utils:
    paths: ["*models*", "*helpers*", "*rules*", "*llm*", "*fix*", "*report*", "*pipeline*"]
    defaults:
      generate_output_path: "src/utils/"
      test_output_path: "tests/"
      example_output_path: "examples/"
      default_language: "python"
      target_coverage: 90.0
      auto_deps_csv_path: "project_dependencies.csv"

  cli:
    paths: ["*cli*"]
    defaults:
      generate_output_path: "src/cli/"
      test_output_path: "tests/"
      example_output_path: "examples/"
      default_language: "python"
      auto_deps_csv_path: "project_dependencies.csv"

  backend:
    paths: ["*backend*", "*api*"]
    defaults:
      generate_output_path: "src/backend/"
      test_output_path: "tests/"
      example_output_path: "examples/"
      default_language: "python"
      auto_deps_csv_path: "project_dependencies.csv"

  frontend:
    paths: ["*frontend*", "*streamlit*"]
    defaults:
      generate_output_path: "src/frontend/"
      test_output_path: "tests/"
      example_output_path: "examples/"
      default_language: "python"
      auto_deps_csv_path: "project_dependencies.csv"

  default:
    defaults:
      generate_output_path: "src/"
      test_output_path: "tests/"
      example_output_path: "examples/"
      default_language: "python"
      auto_deps_csv_path: "project_dependencies.csv"
```

Create directory structure:

```bash
mkdir -p src/utils src/cli src/backend src/frontend prompts tests examples
```

### Step 4: Generate Prompts

**Determine the order**: The sequence for generating prompts should follow the dependency order in `architecture.json`. Modules with no dependencies come first, followed by modules that depend on them.

**Tip**: Ask an LLM to analyze your architecture.json and determine the correct sequence:
```bash
# Example prompt to an LLM:
# "Based on this architecture.json file, what is the dependency order 
# for generating modules? List them from foundation modules (no dependencies)
# to higher-level modules."
```

Generate prompts for each module in dependency order:

```bash
# Generate prompts for all modules (example order: models â†’ helpers â†’ rules â†’ llm â†’ ...)
pdd --force generate --template generic/generate_prompt \
  -e MODULE=models \
  -e LANG_OR_FRAMEWORK=python \
  -e ARCHITECTURE_FILE=architecture.json \
  -e PRD_FILE=docs/specs.md \
  -e TECH_STACK_FILE=docs/tech_stack.md \
  --output prompts/models_python.prompt

pdd --force generate --template generic/generate_prompt \
  -e MODULE=helpers \
  -e LANG_OR_FRAMEWORK=python \
  -e ARCHITECTURE_FILE=architecture.json \
  -e PRD_FILE=docs/specs.md \
  -e TECH_STACK_FILE=docs/tech_stack.md \
  --output prompts/helpers_python.prompt

pdd --force generate --template generic/generate_prompt \
  -e MODULE=rules \
  -e LANG_OR_FRAMEWORK=python \
  -e ARCHITECTURE_FILE=architecture.json \
  -e PRD_FILE=docs/specs.md \
  -e TECH_STACK_FILE=docs/tech_stack.md \
  --output prompts/rules_python.prompt

# Continue for all modules: llm, fix, report, pipeline, cli, etc.
```

### Step 5: Generate Code (Traditional Approach)

The traditional PDD workflow requires running separate commands for each step:

```bash
# Generate code
pdd --force generate --output src/utils/models.py prompts/models_python.prompt

# Generate example
pdd --force example prompts/models_python.prompt src/utils/models.py \
  --output examples/models_example.py

# Generate tests
pdd --force test prompts/models_python.prompt src/utils/models.py \
  --output tests/test_models.py

# Run tests
pytest tests/test_models.py
```

### Step 5 (Better): Use `pdd sync` Instead

**`pdd sync` does all of the above in a single command:**

```bash
pdd sync models
```

This single command:
- Generates code in `src/utils/models.py`
- Generates tests in `tests/test_models.py`
- Generates examples in `examples/models_example.py`
- Runs tests and automatically fixes errors
- Ensures everything works together

**Generate all modules in dependency order:**

The order matters! Use the same dependency sequence you determined from `architecture.json` (ask an LLM if unsure). Generate foundation modules first, then modules that depend on them.

```bash
# Example dependency order (yours may differ based on architecture.json):
# Foundation modules (no dependencies)
pdd --force sync models
pdd --force sync helpers
pdd --force sync rules

# Modules depending on foundation
pdd --force sync llm
pdd --force sync fix
pdd --force sync report
pdd --force sync pipeline

# Interface layers (depend on core modules)
pdd --force sync cli
pdd --force sync backend_api
pdd --force sync frontend_streamlit
```

Each `pdd sync` command:
- Reads the module's prompt file
- Generates implementation code
- Generates comprehensive tests
- Generates usage examples
- Runs tests and automatically fixes any errors
- Ensures ~85%+ test coverage

**Result**: A complete, working application generated entirely from specifications.

Command to check if all unit tests are passing: `pytest tests/`

### Using the Makefile (Automated Workflow)

The included `Makefile` (generated by an LLM after being given the README context) automates the entire PDD workflow:

**Complete build from scratch:**
```bash
# Install dependencies first
pip install -r requirements.txt

# One-time setup
make architecture   # Generate architecture.json
make pddrc         # Generate .pddrc
make setup         # Create directories
make prompts       # Generate all prompt files
make sync-all      # Generate all code, tests, and examples

# Run tests
make test
```

**Incremental development:**
```bash
# Sync specific module groups
make sync-foundation  # models, helpers
make sync-core       # rules, llm, fix, report, pipeline
make sync-interfaces # cli, backend_api, frontend_streamlit

# Run tests
make test
```

**Available targets:**
- `make help` - Show all available commands
- `make architecture` - Generate architecture.json from specs
- `make pddrc` - Generate .pddrc configuration
- `make setup` - Create directory structure
- `make prompts` - Generate all prompt files in dependency order
- `make sync-all` - Run pdd sync on all modules
- `make test` - Run all tests
- `make clean` - Remove generated files (keeps prompts & architecture)

## Fixing Bugs with PDD

### Approach 1: Add Test and Use `pdd sync`

The simplest approach to fix bugs is to add a failing test and run `pdd sync`:

1. **Add a test** that captures the bug:
```python
# tests/test_cli.py
def test_lint_command_handles_missing_file():
    """Test that lint command gracefully handles missing files."""
    result = runner.invoke(app, ["lint", "nonexistent.prompt"])
    assert result.exit_code == 1
    assert "not found" in result.output.lower()
```

2. **Run `pdd sync`**:
```bash
pdd sync cli
```

`pdd sync` will:
- Detect the failing test
- Analyze the code and prompt
- Regenerate the code to fix the issue
- Re-run tests to verify

If `pdd sync` doesn't fully resolve the issue, proceed to Approach 2.

### Approach 2: Use `pdd fix` for Targeted Fixes

When `pdd sync` doesn't work or you want more targeted changes, use `pdd fix`:

1. **Add or update a test** that captures the bug:
```python
# tests/test_cli.py
def test_lint_command_handles_missing_file():
    """Test that lint command gracefully handles missing files."""
    result = runner.invoke(app, ["lint", "nonexistent.prompt"])
    assert result.exit_code == 1
    assert "not found" in result.output.lower()
```

2. **Run the test to capture the failure**:
```bash
pytest tests/test_cli.py > cli_fix_results.log 2>&1
```

3. **Apply automated fix** using `pdd fix`:
```bash
pdd --force fix \
  --output-test tests/test_cli.py \
  --output-code src/cli/cli.py \
  prompts/cli_python.prompt \
  src/cli/cli.py \
  tests/test_cli.py \
  cli_fix_results.log
```

4. **Verify the fix**:
```bash
pytest tests/test_cli.py
```

## Adding New Features

To add new features or update functionality:

1. **Update the prompt** with new requirements or clarifications:
```python
# prompts/cli_python.prompt
# Add to the requirements:
# - Support linting multiple files at once
# - Add --format flag for output formatting (json, text, html)
```

2. **Run `pdd sync` to regenerate**:
```bash
pdd --force sync cli
```

3. **Verify the changes**:
```bash
pytest tests/test_cli.py
```

`pdd sync` regenerates code, tests, and examples, ensuring everything stays consistent with the updated prompt. New tests will be generated automatically for the new features.

## Making code changes and updating prompts based on that

TODO: pdd update usage

## Future work

- Improve the frontend UI
- Enhance scoring granularity - the current scoring system may be too lenient and could benefit from more nuanced deductions
- There are some bugs that show different results in the CLI and UI
