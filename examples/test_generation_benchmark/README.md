# Test Generation Benchmark: Code-based vs TDD/Example-based

This benchmark compares two approaches to generating unit tests using PDD:

1. **Code-based**: Tests generated by analyzing the actual implementation code
2. **Example-based (TDD)**: Tests generated from usage examples showing intended behavior

## Quick Start

```bash
# Run the full benchmark
make benchmark

# Or run individual steps:
make generate-example       # Generate example file using pdd example
make generate-code-based    # Generate tests from implementation
make generate-example-based # Generate tests from examples (TDD style)
make analyze                # Compare implementation detail references
make run-tests              # Run both test suites
```

## How It Works

### Code-based Approach
```bash
pdd test email_validator_python.prompt src/email_validator.py
```
- Uses `generate_test_LLM.prompt` template
- Analyzes the **actual implementation code**
- May generate tests that reference implementation details (private methods, internal patterns)

### Example-based Approach (TDD)
```bash
pdd test email_validator_python.prompt examples/email_validator_example.py
```
- Uses `generate_test_from_example_LLM.prompt` template
- Analyzes **usage examples** showing intended behavior
- Should focus on the public API contract, not implementation details
- File must end with `_example` suffix to trigger this mode

## The Email Validator Module

A simple email validator with intentional internal implementation details:

**Public API:**
- `EmailValidator.validate_email(email: str) -> ValidationResult`
- `ValidationResult` dataclass with `is_valid`, `normalized_email`, `error_message`

**Internal Implementation Details (should NOT be tested directly):**
- `_normalize()` - strips whitespace, lowercases
- `_check_local_part()` - validates part before @
- `_check_domain()` - validates part after @
- `_has_consecutive_dots()` - checks for .. pattern
- `EMAIL_PATTERN` - compiled regex (class attribute)
- `_MIN_DOMAIN_PARTS` - internal constant

## Key Hypothesis

**Example-based tests should be more implementation-agnostic** because they only see how the module is used, not how it's implemented internally.

Code-based tests may inadvertently test implementation specifics like:
- Private helper methods (`_check_domain`, `_normalize`)
- Internal regex patterns (`EMAIL_PATTERN`)
- Implementation-specific error handling

## Evaluation Criteria

After running `make benchmark`, the analysis checks:

| Metric | Description |
|--------|-------------|
| Private method refs | Count of `_normalize`, `_check_*`, etc. in generated tests |
| `EMAIL_PATTERN` refs | Count of internal regex pattern references |
| Test function count | Number of `def test_*` functions generated |
| Lines of code | Total lines in generated test file |

## Expected Results

| Aspect | Code-based | Example-based |
|--------|-----------|---------------|
| References private methods | Possibly | Should be 0 |
| Tests internal patterns | Possibly | Should be 0 |
| Focuses on public API | Mixed | Yes |
| Resilient to refactoring | Less | More |

## Directory Structure

```
test_generation_benchmark/
├── README.md                              # This file
├── Makefile                               # Automation
├── email_validator_python.prompt          # Shared prompt (describes intent)
├── src/
│   └── email_validator.py                 # Implementation (code-based input)
├── examples/
│   └── email_validator_example.py         # Usage examples (TDD input)
└── benchmark_results/
    ├── code_based/test_email_validator.py
    └── example_based/test_email_validator.py
```

## Regenerating the Example File

The example file can be regenerated using:

```bash
make generate-example
# Or directly:
pdd example email_validator_python.prompt src/email_validator.py --output examples/email_validator_example.py
```
