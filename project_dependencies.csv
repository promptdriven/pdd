full_path,file_summary,date
"context/DSPy_example.py","This Python code snippet demonstrates the initial steps for utilizing the DSPy library, aiming to eventually select optimal examples. It begins by importing necessary modules, including a presumed local setup file (`DSPy_example`) and the DSPy library itself. It configures DSPy to use OpenAI's `gpt-3.5-turbo-instruct` model as the language model (LM), setting a maximum token limit of 250.

The core of the snippet defines a custom DSPy module named `chainofthought`. This module inherits from `dspy.Module` and internally uses `dspy.ChainOfThought` with a simple question->answer signature. The module's `forward` method is defined to take a question as input and pass it to the `ChainOfThought` program to generate an answer.

Comments at the end indicate that the next intended steps, not included in this specific code block, involve compiling and optimizing the defined `chainofthought` module, which are typical procedures in a DSPy workflow to enhance the program's performance and tailor it to specific tasks or data.",2025-04-30T03:46:11.393313+00:00
"context/anthropic_counter_example.py","The provided Python code snippet demonstrates the usage of the `anthropic` library for counting tokens in a text string. It begins by importing the `anthropic` library. Although the lines are commented out, the code illustrates the standard steps: initializing the `Anthropic` client, defining a sample text string (Sample text), and then using the `client.count_tokens()` method to calculate the number of tokens in that string. A comment explicitly notes that this `count_tokens` method might not yield accurate results for Anthropic models version 3.0 and above. Finally, the code shows how to print the calculated token count, but this line is also commented out. In its current state, the script serves as a non-functional example or template, highlighting the basic process of token counting with the library while cautioning about potential inaccuracies with newer models.",2025-04-30T03:46:21.839832+00:00
"context/anthropic_tool_example.py","The provided file is a Python script that utilizes the Anthropic API to interact with an AI model, specifically claude-3-7-sonnet-20250219. The script imports the anthropic library, initializes a client, and sends a message to the AI model. The message, sent as a user role, asks for help in fixing a syntax error in a file named patent.md. The script specifies a tool, str_replace_editor, which appears to be a text editor utility, possibly for string replacement tasks. The AI model is instructed to generate a response with a maximum of 1024 tokens. Finally, the script prints the response received from the AI. Overall, the script demonstrates how to use the Anthropic API to request assistance from an AI model for editing or correcting a file, leveraging a specific tool for text manipulation.",2025-04-30T03:46:33.370975+00:00
"context/auto_deps_main_example.py","This Python script defines a command-line interface (CLI) tool using the Click library. The tool, named 'auto-deps', is designed to process a prompt file and analyze code dependencies, likely for Python projects. It provides several command-line options, such as forcing file overwrites, suppressing output, setting parameters for dependency analysis (strength and temperature), specifying paths for the prompt file, directory of example files, a CSV file for dependencies, output location, and forcing a directory rescan.

The main function collects these options, sets up the Click context, and then calls the 'auto_deps_main' function (imported from 'pdd.auto_deps_main') with the relevant parameters. The results, including the modified prompt, total cost, and model name used for the analysis, are printed to the console. If an error occurs during execution, it is caught, printed, and the process is aborted using Click's error handling. The script is intended to be run as a standalone program, as indicated by the '__main__' block at the end.",2025-04-30T03:46:44.050799+00:00
"context/auto_include_example.py","This Python script demonstrates the usage of the `auto_include` function from the `pdd.auto_include` module, likely as part of a project dependency management or automation tool. The script begins by importing necessary modules, including `os`, `pandas`, and some custom modules. The main function reads a CSV file named 'project_dependencies.csv', which presumably contains information about project dependencies. It then defines a detailed multi-line string `input_prompt` that describes the intended behavior of a function called `generate_test`, which is designed to generate unit tests for code files using a language model (LLM) via Langchain. The prompt outlines a step-by-step process for generating, formatting, and validating unit tests, including cost calculation and model selection.

The script sets parameters such as the directory path to search for Python files, the strength and temperature for the LLM, and a verbosity flag. It then calls the `auto_include` function with these parameters, which processes the dependencies and returns the results, including the dependencies found, a CSV output, the total cost of the operation, and the name of the LLM model used. Finally, the script prints these results. The script is intended to be run as a standalone program.",2025-04-30T03:46:55.799083+00:00
"context/auto_update_example.py","The provided file is a Python script demonstrating the usage of the `auto_update` function from the `pdd.auto_update` module. The script defines a `main()` function that showcases several ways to utilize `auto_update` for managing package updates. The function's docstring explains that `auto_update` works by checking the currently installed version of a package, comparing it to the latest available version (either from PyPI or a specified version), and prompting the user to upgrade if a newer version is found. If the user agrees, the upgrade is performed using pip. If the package is already up to date, the function produces no output. The script provides three example usages: checking for updates to the default 'pdd' package, checking for updates to the 'requests' package, and checking if the 'pandas' package is at least version 2.0.0. The script is designed to run the `main()` function when executed directly. Overall, the file serves as a practical guide for developers on how to implement and use the `auto_update` function to keep Python packages current.",2025-04-30T03:47:04.801528+00:00
"context/autotokenizer_example.py","The provided file is a Python script that demonstrates how to count the number of tokens in a given text using a tokenizer from the Hugging Face Transformers library. The script imports the AutoTokenizer class and defines a function called count_tokens, which takes a text string and an optional model name (defaulting to 'deepseek-ai/deepseek-coder-7b-instruct-v1.5'). The function loads the appropriate tokenizer for the specified model, tokenizes the input text, and returns the number of tokens by counting the length of the 'input_ids' in the tokenized output. An example usage is provided, where the script counts the tokens in the sentence 'Write a quick sort algorithm in Python.' and prints the result. This script is useful for developers working with language models who need to determine token counts for input texts, which is often important for managing model input limits or estimating computational costs.",2025-04-30T03:47:11.518427+00:00
"context/bug_main_example.py","This Python script demonstrates how to use the `bug_main` function from the `pdd` package to automatically generate unit tests based on observed and desired outputs of a program. The script sets up a Click context with options for overwriting files, verbosity, model strength, and temperature. It then creates an output directory and writes several example files: a prompt describing the desired function (summing even numbers in a list), a Python implementation of that function, a main program that uses the function, and files containing the current (incorrect) and desired (correct) outputs. The script then calls `bug_main`, passing in the context and paths to these files, to generate a unit test that would help identify and correct the bug (in this case, not handling empty lists). Finally, it prints the generated unit test, the cost of the operation, and the model used. The script serves as an example of automating test generation and bug detection using AI-driven tools.",2025-04-30T03:47:18.877074+00:00
"context/bug_to_unit_test_example.py","This Python script demonstrates the usage of a function called `bug_to_unit_test` from the `pdd.bug_to_unit_test` module. The script is designed to automate the process of generating unit tests for code that may contain bugs, based on a prompt and the code under test. It uses the `rich` library for enhanced console output.

The main function sets up several parameters, including sample outputs (`current_output` and `desired_output`), the module name, and reads in the prompt, code under test, and the program used to run the code from corresponding files. It also sets parameters for the language model's strength and temperature, and specifies the programming language as Python.

The core of the script is a try-except block where it calls `bug_to_unit_test` with all the prepared inputs. If successful, it prints the generated unit test, the total cost of the operation, and the model used. If an error occurs, it prints an error message. The script is intended to be run as a standalone program and serves as an example or test harness for the `bug_to_unit_test` function, facilitating the debugging and unit test generation process for Python code.",2025-04-30T03:47:28.207151+00:00
"context/bug_to_unit_test_failure_example.py","The provided file contains a short snippet of Python code. It defines a single function named `add`. This function is designed to accept two input arguments, represented by the parameters `x` and `y`. The core logic implemented within the function calculates the difference between these two arguments, specifically `x - y`. The result of this subtraction operation is then returned by the function. Notably, there is a discrepancy between the function's name, `add`, which implies addition, and its actual implementation, which performs subtraction. While syntactically correct Python, the function's behavior contradicts its name, suggesting a potential logical error or misleading naming convention.",2025-04-30T03:47:35.315656+00:00
"context/change_example.py","This Python script demonstrates the usage of a function called `change` from the `pdd.change` module, likely designed to modify code or prompts using a language model. The script begins by importing necessary modules, including `os` for environment variables, `change` for the main functionality, and `Console` from the `rich` library for formatted output. The `main` function sets up example input data: a prompt asking for a factorial function, the corresponding Python code, and a change prompt requesting the function be modified to take the square root of the factorial output. It also defines parameters for the language model, such as `strength` and `temperature`. The script then calls the `change` function with these inputs, capturing the modified prompt, the total cost of the operation, and the model name used. Results are printed to the console in a formatted manner. Error handling is included to catch and display any exceptions. The script is designed to be run as a standalone program, with the `main` function executed when the script is run directly. Overall, it serves as an example or test harness for the `change` function's capabilities.",2025-04-30T03:47:44.555335+00:00
"context/change_main_example.py","This Python script demonstrates how to use the 'change_main' function from the 'pdd.change_main' module, which is part of a command-line tool for programmatic code and prompt modification. The script is structured as an example and covers two main usage scenarios: single-change mode and CSV batch-change mode.

In single-change mode, the script creates sample input files, including a code file and a prompt file, and writes a change instruction (e.g., adding error handling for division by zero). It then calls 'change_main' with these files and displays the modified prompt, the total cost of the operation, and the model used.

In CSV batch-change mode, the script sets up a directory with multiple code and prompt files, and creates a CSV file specifying different change instructions for each file. It then calls 'change_main' in batch mode, processes all specified changes, and outputs the results, including the total cost and model used, as well as the location of the batch results.

The script uses the 'click' library for command-line context management and 'rich' for formatted output. It is intended as a practical example for users who want to automate code or prompt modifications using the 'pdd' tool, and it assumes all dependencies and environment variables are properly set up.",2025-04-30T03:47:51.302407+00:00
"context/cli_example.py","The file 'demo_run_pdd_cli.py' is a minimal, self-contained Python script that demonstrates how to programmatically invoke the PDD (Prompt-Driven Development) CLI using Python. The script is structured to perform three main tasks:

1. It creates a small prompt file in the './output/' directory. This prompt instructs the model to write a Python function `hello()` that returns the string Hello, PDD!.

2. It uses Click's `CliRunner` to invoke the PDD CLI's `generate` command with the `--local` flag, ensuring that the code generation uses a local model rather than contacting the cloud. The output is directed to './output/hello.py'. The `--quiet` flag is also used to suppress progress bars and other non-essential output.

3. After running the command, the script prints the CLI's output, the exit code, and checks if the generated Python file exists. If successful, it prints the path to the generated file and displays its first few lines; otherwise, it reports a failure.

The script serves as a concise example for developers on how to automate PDD CLI tasks within Python, especially for local, offline code generation scenarios.",2025-04-30T03:47:58.628805+00:00
"context/click_example.py","This Python script defines a command-line tool for processing images using the Pillow library, structured as a Unix-like pipeline using the Click library for command-line interfaces. The script allows users to chain multiple image processing commands, where the output of one command feeds into the next. The main commands implemented include opening images, saving processed images, displaying them, resizing, cropping, rotating, flipping, blurring, smoothening, embossing, sharpening, and pasting images together. Each command is implemented as a Click subcommand and uses decorators to handle the chaining of operations. The script supports processing multiple images at once and provides options for customizing each operation, such as specifying dimensions for resizing, the amount of blur, or the number of smoothening iterations. Error handling is included for file operations, and informative messages are printed to the console during processing. The design emphasizes modularity and extensibility, allowing users to build complex image processing pipelines directly from the command line, similar to Unix pipes. The script is well-documented with docstrings and usage examples, making it user-friendly for batch image processing tasks.",2025-04-30T03:48:06.013435+00:00
"context/cmd_test_main_example.py","The file 'test_cli_example.py' provides an example of how to build a command line interface (CLI) using the Click library to integrate the 'cmd_test_main' function, which is designed for generating or enhancing unit tests for source code files. The script defines a CLI with global options such as verbosity, AI generation strength, creativity (temperature), force overwrite, and quiet mode. The main command, 'test', requires two arguments: a prompt file (typically a text file that generated the code, such as a GPT prompt) and a code file (the source code to be tested). Optional arguments allow users to specify the output path for generated tests, override the programming language, provide a coverage report, supply existing tests for enhancement or merging, set a target coverage percentage, and enable merging of new tests into existing ones. When executed, the script calls 'cmd_test_main' with the provided parameters and outputs the generated tests, the total cost of generation, and the model used. The file includes detailed docstrings and usage examples, making it a practical template for integrating AI-powered test generation into a Python CLI workflow.",2025-04-30T03:48:14.678926+00:00
"context/code_generator_example.py","The provided file is a Python script that demonstrates how to use a function called `code_generator` from the `pdd.code_generator` module. The main purpose of the script is to generate code using a language model based on a prompt. The script reads a prompt from an external file located at `prompts/generate_test_python.prompt`. It then sets several parameters for the code generation process, including the programming language (Python), the strength and temperature of the language model, and a verbosity flag. The script attempts to call the `code_generator` function with these parameters, which returns the generated code, the total cost of the operation, and the name of the model used. These results are then printed to the console. If any exception occurs during this process, it is caught and an error message is displayed. The script is designed to be run as a standalone program, with the main logic encapsulated in a `main()` function that is executed when the script is run directly.",2025-04-30T03:48:21.076138+00:00
"context/code_generator_main_example.py","This Python script is designed to automate the process of generating code using an AI-powered code generator. It utilizes the Click library to simulate command-line interface (CLI) parameters and the Rich library for enhanced console output. The script sets up a Click context object with specific parameters such as 'strength', 'temperature', 'force', and 'quiet', which control the behavior of the AI model and the code generation process. The main functionality is provided by the 'code_generator_main' function, which is imported from the 'pdd.code_generator_main' module. The script specifies a prompt file ('prompts/get_jwt_token_python.prompt') that likely contains instructions or requirements for the code to be generated, and an output file ('pdd/get_jwt_token_python_tmp.py') where the generated code will be saved. After invoking the code generator, the script prints out the generated code, the total cost of the operation, the model used, and the location of the saved code file. Some commented-out sections suggest that the script can be adapted for different prompts and output files. Overall, the script serves as a template for programmatically generating Python code based on prompts using an AI model.",2025-04-30T03:48:27.333895+00:00
"context/comment_line_example.py","The provided file contains documentation and example usage for the `comment_line` function from the `comment_line.py` module. This function is designed to comment out a line of code using specified comment characters, making it adaptable for different programming languages and commenting styles. The function takes two parameters: `code_line`, which is the line of code to be commented, and `comment_characters`, which determines how the line is commented. If `comment_characters` is 'del', the function returns an empty string, effectively deleting the line. If it contains a space, it is interpreted as a pair of start and end comment characters (e.g., for HTML comments). Otherwise, it is treated as a single comment character (e.g., '#' for Python). The file includes example usage for Python-style comments, HTML-style comments, and the deletion case, demonstrating the function's flexibility. Additionally, the documentation clearly explains the input and output parameters, making it easy for users to understand how to use the function in various contexts.",2025-04-30T03:48:36.233490+00:00
"context/conflicts_in_prompts_example.py","This Python script demonstrates the use of a function called `conflicts_in_prompts` to detect and resolve potential conflicts between two prompt specifications for a software project. The script imports necessary modules, including a custom conflict detection utility and the `rich` library for formatted output. 

The `main` function defines two detailed prompt strings:
- The first prompt describes requirements for an `auth_helpers.py` module, focusing on authentication helper functions for Firebase Cloud Functions, including token verification, user info extraction, permission checks, error handling, and security best practices.
- The second prompt outlines the design of a `User` data model class for the PDD Cloud platform, specifying fields, methods (such as validation, serialization, Firestore interaction, credit and permission management), dependencies, error handling, and code style guidelines.

The script sets parameters for model strength and temperature, then calls `conflicts_in_prompts` with the two prompts and these parameters. It prints the results, including the model used, total cost, and any suggested changes to resolve conflicts between the prompts. If no conflicts are found, it indicates so. The script is intended to help developers ensure consistency and compatibility between different prompt-based specifications in a project.",2025-04-30T03:48:42.306428+00:00
"context/conflicts_main_example.py","The provided file is a Python script that appears to be a test or demonstration for a function called 'conflicts_main' from the 'pdd.conflicts_main' module. The script begins by importing necessary modules and then creates two example prompt files, 'prompt1_LLM.prompt' and 'prompt2_LLM.prompt', each containing a simple instruction for an AI assistant or chatbot. 

A mock context class, 'MockContext', is defined to simulate the context object typically used with the Click command-line interface library. This mock context includes an 'obj' attribute, which is a dictionary intended to hold configuration options, although it is overwritten to an empty dictionary. The class also includes an 'exit' method to mimic Click's behavior.

The script then calls the 'conflicts_main' function, passing in the mock context, the two prompt files, an output filename ('conflicts_output.csv'), and a verbosity flag. The function is expected to return a list of conflicts, a total cost, and the name of the model used. These results are printed to the console, and the output is saved to the specified CSV file. The script is likely used for testing or demonstrating how to use the 'conflicts_main' function with example data.",2025-04-30T03:48:49.595891+00:00
"context/construct_paths_example.py","The file demo_construct_paths.py is a demonstration script designed to showcase the usage of the `construct_paths` function from the `pdd.construct_paths` module. The script provides a minimal, end-to-end example that mimics how the function would be used in a command-line interface (CLI) context. 

The script performs the following steps:
1. It creates a temporary prompt file named hello_world_python.prompt in the current directory, containing a simple programming task description.
2. It constructs the arguments required by `construct_paths`, including the path to the prompt file and other options such as `force`, `quiet`, and the command to emulate (generate).
3. It calls the `construct_paths` function with these arguments, which returns three values: the contents of the input files, the paths where output will be written, and the detected programming language.
4. The script prints these returned values in a readable format, showing the structure and content of each.
5. Finally, it cleans up by deleting the temporary prompt file to avoid leaving unnecessary files in the directory.

This script serves as a practical example for developers to understand how to integrate and test the `construct_paths` function in their own workflows.",2025-04-30T03:48:58.309878+00:00
"context/context_generator_example.py","This Python script is designed to generate example code using a function called `context_generator` from the `pdd.context_generator` module. The script begins by importing necessary modules and ensuring that the `PDD_PATH` environment variable is set, raising an error if it is not. It then defines several input parameters: a simple Python function (`add`) as the code module, a prompt describing the function to be written, the programming language (Python), and two parameters (`strength` and `temperature`) that likely influence the behavior of the code generation model. The script calls the `context_generator` function with these parameters, requesting verbose output. The function returns three values: the generated example code, the total cost of the operation (possibly in terms of API usage or computation), and the name of the model used for generation. Finally, the script prints the generated code, the total cost, and the model name using the `rich` library for formatted output. The script is structured for testing or demonstration purposes, showing how to use the `context_generator` function in a controlled environment.",2025-04-30T03:49:05.316975+00:00
"context/context_generator_main_example.py","This Python script demonstrates how to use the Click library to set up a command-line context and interact with a function called `context_generator_main` from the `pdd.context_generator_main` module. The script first creates a Click context object and configures its parameters, such as 'force', 'quiet', and 'verbose', which control file overwriting, output verbosity, and message display. It also sets object attributes like 'strength' and 'temperature', which likely influence the behavior of an AI model (possibly controlling creativity and determinism). The script specifies file paths for a prompt file, a code file, and an output file. It then calls the `context_generator_main` function with these parameters, which presumably generates example code based on the provided prompt and code files, and saves the result to the output file. The function returns the generated code, the total cost (possibly in terms of API usage or computation), and the name of the AI model used. Finally, the script prints the generated code, the cost, and the model name. This script serves as an example of integrating command-line context management with AI-driven code generation workflows.",2025-04-30T03:49:12.123033+00:00
"context/continue_generation_example.py","This Python script demonstrates the use of the `continue_generation` function from the `pdd.continue_generation` module. The main purpose of the script is to continue generating text using a language model, starting from a given prompt and an initial output fragment. The script reads the input prompt from the file `context/cli_python_preprocessed.prompt` and the initial LLM output fragment from `context/llm_output_fragment.txt`. It sets specific parameters for the language model, such as `strength` (0.915) and `temperature` (0), and then calls the `continue_generation` function with these inputs. The function returns the final generated output, the total cost of the generation, and the model name used. The script prints the total cost and model name to the console, and writes the final generated output to `context/final_llm_output.py`. Error handling is included to catch and report file not found errors and other exceptions. The script is designed to be run as a standalone program, with the main logic encapsulated in the `main()` function and executed when the script is run directly.",2025-04-30T03:49:20.259323+00:00
"context/crash_main_example.py","This Python script demonstrates the use of the `crash_main` function from the `pdd.crash_main` module to automatically fix errors in a code module and its calling program. The script first ensures that an output directory exists, then creates several example files: a prompt describing the desired functionality (a factorial function that raises a ValueError for negative inputs), a buggy code module (factorial function without negative number check), a program that calls this buggy function with a negative input, and an error log capturing the resulting RecursionError. The script then sets up a mock Click context with model parameters and calls `crash_main`, passing in the paths to the created files, output locations for the fixed code, and parameters for iterative fixing (such as maximum attempts and budget). After attempting to fix the code, it prints whether the fix was successful, the number of attempts, the cost, the model used, and the contents of the fixed code and program. The script is designed to be run as a standalone demonstration of automated code repair using error logs and prompt-driven code generation.",2025-04-30T03:49:27.914328+00:00
"context/detect_change_example.py","The provided file is a Python script designed to analyze and detect changes in a set of prompt files using a function called `detect_change` from the `pdd.detect_change` module. The script utilizes the `rich` library for enhanced console output and manages file paths with the `pathlib` module. It specifies a list of prompt files located in the 'prompts' and 'context' directories, which are to be analyzed for changes based on a given description: making prompts more compact by using 'context/python_preamble.prompt'. The script sets parameters for the language model, such as 'strength' and 'temperature', which control the model's output characteristics. It then calls the `detect_change` function with these parameters and the list of prompt files. If changes are detected, the script prints out the details of each change, including the prompt name and specific instructions, as well as the total cost and the model used. If an error occurs during execution, it is caught and displayed in the console. The script is structured for easy modification and provides clear output for users analyzing prompt file changes.",2025-04-30T03:49:34.630962+00:00
"context/detect_change_main_example.py","This Python script is designed to simulate command-line interface (CLI) parameters and invoke a function called `detect_change_main` from the `pdd.detect_change_main` module. The script uses the `click` library to create a context object that mimics CLI input, specifying model parameters such as 'strength', 'temperature', 'force', and 'quiet'. It defines a list of prompt files and a change description file, writing a specific change instruction to the latter. The script ensures the output directory exists and specifies an output CSV file for results. It then calls `detect_change_main` with the context, prompt files, change file, and output path. The results, including the model name, total cost, and a list of required changes (with prompt names and instructions), are printed to the console. The script includes error handling to catch and report exceptions. Overall, this script automates the process of detecting and summarizing changes needed in prompt files, likely for use in a prompt-driven development or code generation workflow.",2025-04-30T03:49:40.844959+00:00
"context/edit_file_example.py","This Python script is designed as a test harness for the 'edit_file.py' module, specifically for the function 'run_edit_in_subprocess', which is assumed to perform file editing operations based on provided instructions. The script is intended to be run from within a 'pdd' directory, with the module located in the parent directory. It sets up the environment by adjusting the Python path and checking for the required 'PDD_PATH' environment variable, which should point to the 'pdd' directory.

The script includes helper functions to create example files and a configuration file needed for testing. The main test involves creating a dummy text file with several lines, then running the edit function with specific instructions: updating a word on line 1, replacing line 3, and adding a new line at the end. After the edit operation, the script verifies whether the changes were applied correctly by checking the file's contents and reporting the results.

The script also provides detailed error handling and verification output, making it useful for debugging and validating the integration of the file editing module. It is structured to be extensible for additional test cases, though only the main example is active in the current version. The script requires certain dependencies and environment variables to be set, as outlined in its documentation.",2025-04-30T03:49:46.855998+00:00
"context/find_section_example.py","The file provides a concise example and documentation for using the `find_section` function from the `find_section.py` module. The example demonstrates how to import the function, prepare a sample input string containing multiple code blocks in different programming languages (Python and JavaScript), and process this string by splitting it into lines. The `find_section` function is then called to extract code sections, and the results are printed, showing the language and the start and end line indices for each code block found. The documentation details the input parameters for the function: a list of lines (`lines`), an optional starting index (`start_index`), and an optional sub-section flag (`sub_section`). The output is described as a list of tuples, each containing the code language, start line, and end line indices. An example output is provided, illustrating how the function identifies and locates code blocks within the input text. Overall, the file serves as both a usage guide and reference for the `find_section` function, clarifying its purpose, inputs, outputs, and practical application.",2025-04-30T03:50:01.331727+00:00
"context/firecrawl_example.py","The file is a Python script that demonstrates how to use the 'firecrawl-py' library to scrape web content. It begins by importing the necessary modules, including 'FirecrawlApp' from the 'firecrawl' package and the 'os' module for environment variable access. The script initializes a 'FirecrawlApp' object using an API key, which it attempts to retrieve from the environment variable 'FIRECRAWL_API_KEY', defaulting to 'YOUR_API_KEY' if the variable is not set. The main functionality of the script is to scrape the contents of 'https://www.google.com' using the 'scrape_url' method of the 'FirecrawlApp' object, specifying that the output format should be Markdown. Finally, it prints the Markdown-formatted result of the scrape to the console. The script serves as a basic example of how to use the 'firecrawl-py' library for web scraping and output formatting.",2025-04-30T03:50:09.588329+00:00
"context/fix_code_loop_example.py","The provided Python script demonstrates the use of a function called `fix_code_loop` from the `pdd.fix_code_loop` module. The script is structured around a `main()` function, which performs several tasks to showcase how `fix_code_loop` can be used to automatically fix code errors. 

First, the script creates two example files in an 'output' directory: a Python module (`module_to_test.py`) containing a function `calculate_average`, and a verification script (`verify_code.py`) that imports this function and intentionally calls it with incorrect input (a string instead of a list of numbers), which is expected to cause an error. The original prompt that led to the code's creation is also defined.

The core of the script is the invocation of `fix_code_loop`, which attempts to iteratively fix the code in `module_to_test.py` by running the verification script, analyzing errors, and making corrections. The function is called with parameters controlling the model's strength, determinism, number of attempts, and budget. After running, the script prints whether the fix succeeded, the number of attempts, the cost, the model used, and the final version of the code. There is also commented-out code for cleaning up the generated files. The script is intended to be run as a standalone program.",2025-04-30T03:50:15.536132+00:00
"context/fix_code_module_errors_example.py","This Python script demonstrates the use of an automated code error-fixing module, specifically 'fix_code_module_errors' from the 'pdd.fix_code_module_errors' package. The script defines a main function that sets up a scenario where a function intended to sum a list of numbers is incorrectly called with a string input, leading to a TypeError. The script provides the original code, the prompt that generated it, the error message encountered, and then calls the 'fix_code_module_errors' function with these inputs. The function is expected to analyze the error, suggest or apply fixes, and return several outputs: whether the program or code module needs updating, the fixed versions of the program and code, the raw output from the code-fixing model, the total cost of the API call, and the model's name. The script then prints these results. This setup is useful for testing or demonstrating automated code repair tools, especially those leveraging language models to interpret error messages and suggest corrections.",2025-04-30T03:50:23.190197+00:00
"context/fix_error_loop_example.py","The provided file is a Python script that demonstrates the usage of the `fix_error_loop` function, which is imported from the `pdd.fix_error_loop` module. The script is designed to automate the process of fixing errors in code by iteratively running a loop that attempts to correct issues based on a set of parameters. The main function sets up the necessary input parameters, such as the paths to the unit test file, code file, prompt file, and verification program. It also specifies parameters for the error-fixing process, including strength, temperature, maximum attempts, and budget. The script reads a prompt from a file, then calls the `fix_error_loop` function with all these parameters. The results, including whether the process was successful, the number of attempts, total cost, the model used, and the final versions of the unit test and code, are displayed using the `rich` library for enhanced terminal output. If an exception occurs during execution, it is caught and displayed. The script is intended to be run as a standalone program and serves as an example of how to use the `fix_error_loop` function for automated code error correction.",2025-04-30T03:50:29.598551+00:00
"context/fix_errors_from_unit_tests_example.py","The provided Python script demonstrates the usage of the function `fix_errors_from_unit_tests` from the `pdd.fix_errors_from_unit_tests` module. The script sets up example inputs, including a unit test for an `add` function, the implementation of the `add` function itself, a prompt describing the function's purpose, and an error message resulting from a failed assertion and a deprecation warning. Additional parameters such as the error log file name, LLM strength, and temperature are also specified. The main function calls `fix_errors_from_unit_tests` with these inputs and receives several outputs: updated unit test and code, fixed versions of both, analysis results, total cost, and the model name used. The results are then printed using the `rich` library for enhanced formatting. The script is designed to be run as a standalone program, serving as an example or test harness for the error-fixing functionality provided by the imported module.",2025-04-30T03:50:38.634595+00:00
"context/fix_main_example.py","This Python script defines a command-line interface (CLI) tool using the Click library, designed to automate the process of fixing errors in code and unit tests. The script imports necessary modules, including 'rich' for colored output and a custom 'fix_main' function from the 'pdd.fix_main' module. The main command, 'fix_command', is decorated with several Click options that allow users to specify file paths for prompts, code, unit tests, error logs, and output files. Additional options control the fixing process, such as enabling iterative fixing, setting a maximum number of attempts, budget, and auto-submission if tests pass. When executed, the script sets up a Click context with default parameters and simulates command-line arguments for demonstration. The 'fix_command' function calls 'fix_main' with the provided parameters, and based on the result, prints a success or failure message using rich formatting. The script is intended to be run as a standalone program and serves as a wrapper to facilitate automated code and test correction workflows, likely in a development or continuous integration context.",2025-04-30T03:50:46.451160+00:00
"context/fix_verification_errors_example.py","This file is an example Python script that demonstrates how to use the `fix_verification_errors` function from the `pdd` package to automatically identify and fix errors in a code module based on verification output logs. The script assumes that the `pdd` package and its dependencies (such as prompt templates and LLM API keys) are properly installed and configured in the environment.

The script sets up an example scenario where a buggy Python module (`calculator_module.py`) is supposed to implement an `add_numbers` function, but contains a bug (it subtracts instead of adds). A separate program script is provided that uses this module, takes command-line arguments, and includes verification logic to check if the module's output matches the expected result. The output from running this program (which indicates a verification failure) is captured.

The script then calls `fix_verification_errors`, passing in the program code, the original prompt, the buggy code, the verification output, and some LLM configuration parameters (strength, temperature, verbosity). The results, including whether issues were found, if the code or program was updated, the LLM model used, cost, and explanations, are printed in a formatted way. This example illustrates an automated workflow for debugging and repairing code using LLMs and verification feedback.",2025-04-30T03:50:55.579326+00:00
"context/fix_verification_errors_loop_example.py","The provided file is a Python script that demonstrates an automated process for detecting and fixing verification errors in code using a looped approach. The script sets up a scenario where a Python module ('calculator_module.py') is intentionally written with a bug: its 'add_numbers' function subtracts instead of adds two integers. A separate program ('program.py') is created to exercise this module, taking two command-line arguments, calling the buggy function, and comparing the result to the expected sum. Additionally, a verification script ('verification_program.py') is provided, which asserts the correctness of the 'add_numbers' function.

The script then defines several parameters for the fixing process, such as model strength, randomness (temperature), maximum attempts, budget, logging options, and program arguments. It calls the 'fix_verification_errors_loop' function, which is expected to iteratively attempt to fix the buggy code until it passes verification or the budget/attempt limit is reached. Finally, the script prints out the results, including whether the fix was successful, the final versions of the program and code, the number of attempts, total cost, and the model used. This setup is likely used for testing or demonstrating automated code repair systems.",2025-04-30T03:51:07.410987+00:00
"context/fix_verification_main_example.py","This Python script demonstrates how to use the `fix_verification_main` function from the `pdd` package to perform automated code verification and correction, simulating both single-pass and iterative (loop) verification modes. The script first sets up an example scenario: it creates a prompt file describing a simple calculator module, a deliberately buggy implementation of an `add` function (which subtracts instead of adds), a program to run the code, and a verification script that asserts correct behavior. It then defines global parameters and a dummy Click context for testing.

In single-pass mode, the script runs the verification once, where the LLM (Large Language Model) checks the program's output against the prompt and may suggest fixes. In loop mode, the script repeatedly runs the verification, using the LLM to propose corrections and a verification program to confirm if the bug is fixed, stopping when success is achieved, the maximum number of attempts is reached, or the budget is exhausted. The script prints detailed results for both modes, including success status, attempts, costs, and output file locations. This example is intended for demonstration and testing of the automated code fixing and verification workflow using LLMs.",2025-04-30T03:51:15.861350+00:00
"context/generate_output_paths_example.py","This file is a Python script that demonstrates and tests the usage of a function called `generate_output_paths`, which is imported from a module located in a parent directory. The script is structured to handle various scenarios for generating output file paths based on user input, environment variables, and default behaviors. It covers the following cases:

1. Default behavior when no user input or environment variables are provided, resulting in a default filename in the current working directory.
2. User-specified output filename, which is resolved to an absolute path.
3. User-specified output directory, where the default filename is placed inside the given directory.
4. Use of environment variables to specify output paths for the 'fix' command, including both file and directory cases.
5. Mixed user input and defaults, where some outputs are specified by the user and others use defaults.
6. A command ('preprocess') with a fixed output file extension, ignoring the provided extension argument.
7. Handling of an unknown command, which results in an empty output dictionary.
8. The case where the required basename is missing, also resulting in an empty output.

The script prints the inputs, results, and expected outputs for each scenario, and includes setup and cleanup code for directories and environment variables used in the tests. It serves as a comprehensive example and test suite for the `generate_output_paths` function.",2025-04-30T03:51:28.530833+00:00
"context/generate_test_example.py","This Python script demonstrates how to use the `generate_test` function from the `pdd.generate_test` module to automatically generate unit tests for a given code snippet. The script sets up a prompt asking for a function that calculates the factorial of a number and provides a sample implementation of such a function in Python. It also specifies parameters such as `strength`, `temperature`, and `language` to control the behavior of the test generation. The script attempts to call `generate_test` with these inputs and, if successful, prints the generated unit test, the total cost of the operation, and the model name used, all with formatted output using the `rich` library for enhanced terminal display. If an error occurs during the process, it catches the exception and prints an error message in red. The script includes a commented-out line for setting the `PDD_PATH` environment variable, which may be necessary for locating project resources. Overall, the script serves as an example or template for automating unit test generation for Python code using an external tool.",2025-04-30T03:51:36.963786+00:00
"context/get_comment_example.py","The file provides a concise example and documentation for using the `get_comment` function from the `get_comment.py` module. The example demonstrates how to import and call `get_comment` with various programming language names (such as 'Python', 'Java', and 'JavaScript'), showing that the function returns the appropriate comment character(s) for each language (e.g., '#' for Python, '//' for Java). If an unknown language is provided, or if there is an error (such as a missing environment variable or CSV file), the function returns the string 'del'.

The documentation specifies that the function takes a single string parameter, `language`, which is case-insensitive, and returns the comment character(s) as a string. It also notes the importance of setting the `PDD_PATH` environment variable to the directory containing the `data/language_format.csv` file, which must have columns for `language` and `comment`. The file serves as both a usage guide and a reference for the expected behavior and requirements of the `get_comment` function.",2025-04-30T03:51:47.777797+00:00
"context/get_extension_example.py","The file demonstrates the usage of a function called 'get_extension', which is imported from the 'pdd.get_extension' module. The purpose of this function appears to be returning the appropriate file extension for a given programming language or file type. The file includes three example calls to 'get_extension': one with the argument 'Python', which returns '.py'; one with 'Makefile', which returns 'Makefile' (indicating that Makefiles do not have an extension); and one with 'JavaScript', which returns '.js'. The examples are printed to the console, showing the expected output for each input. The file serves as a simple usage demonstration or test for the 'get_extension' function, illustrating how it maps language or file names to their standard file extensions.",2025-04-30T03:51:57.519245+00:00
"context/get_jwt_token_example.py","This Python script demonstrates how to authenticate a user with Firebase using GitHub's Device Flow and obtain a JWT (JSON Web Token) for use in a CLI application. The script imports necessary modules and error classes from a local package, and retrieves configuration values (Firebase API key and GitHub client ID) from environment variables. The main asynchronous function attempts to get a Firebase ID token by calling the 'get_jwt_token' function, handling various exceptions such as authentication errors, network issues, token errors, user cancellation, and rate limiting. Upon successful authentication, the script prints the obtained token and updates the '.env' file in the current directory, replacing or adding the 'JWT_TOKEN' entry with the new token. This allows subsequent processes or applications to use the updated token for authenticated requests to a Firebase backend. The script is designed to be run as a standalone program and provides informative output messages for both successful and failed authentication attempts.",2025-04-30T03:52:06.739186+00:00
"context/get_language_example.py","The provided file is a Python script that demonstrates how to use a function called `get_language` from the `pdd.get_language` module. The script defines a `main` function, which sets an example file extension ('.py') and attempts to determine the programming language associated with this extension by calling `get_language`. If the function successfully returns a language, it prints a message indicating the programming language for the given extension. If no language is found, it prints a corresponding message. The script also includes error handling: if an exception occurs during the process, it catches the exception and prints an error message. The script is designed to be run as a standalone program, as indicated by the `if __name__ == __main__:` block, which calls the `main` function. Overall, the file serves as a simple demonstration or utility for mapping file extensions to programming languages, with basic error handling and user feedback.",2025-04-30T03:52:14.757444+00:00
"context/git_update_example.py","This Python script demonstrates the usage of a function called `git_update`, which appears to update code and prompts based on input parameters, likely leveraging a language model (LLM). The script begins by importing necessary modules and setting up parameters such as the prompt file name, the path to the code file to be modified, and LLM parameters like `strength` and `temperature`. It reads an input prompt from a file, then calls the `git_update` function with these parameters. If the update is successful, it prints the modified prompt, the total cost of the operation (suggesting usage of a paid LLM API), and the model name. The modified prompt is then saved back to the original prompt file. The script includes error handling for both value errors and general exceptions, providing informative messages in case of failure. The main function is executed when the script is run directly. Overall, the script automates the process of updating code and prompts using an LLM, with integration for file I/O and cost tracking.",2025-04-30T03:52:20.786150+00:00
"context/increase_tests_example.py","The provided file is a Python script that demonstrates how to use the `increase_tests` function from the `pdd.increase_tests` module, likely part of a test generation or code coverage improvement toolkit. The script defines an `example_usage` function, which showcases both basic and advanced usage of `increase_tests`. It sets up a simple example: a function to calculate the average of a list of numbers, a corresponding unit test, and a code coverage report indicating 60% coverage. The script then calls `increase_tests` twice: first with default parameters to generate additional tests, and second with more advanced, customized parameters (such as specifying the language, strength, temperature, and verbosity). The results, including the generated tests, total cost, and model used, are printed to the console. The script also includes error handling for input validation and unexpected exceptions. Finally, the script runs the `example_usage` function if executed as the main module. Overall, the file serves as a usage example and template for users looking to enhance their test coverage using the `increase_tests` function.",2025-04-30T03:52:27.508789+00:00
"context/insert_includes_example.py","This Python script demonstrates the usage of the `insert_includes` function from the `pdd.insert_includes` module to process project dependencies. The script is structured around an `example_usage` function, which sets up an input prompt describing a task (reading a CSV, processing data, and outputting results), specifies a directory path for relevant files, and names a CSV file containing dependency information. It then calls `insert_includes` with these parameters, including options for output strength and temperature, and enables verbose output. The results from `insert_includes`including a modified prompt, CSV output, model name, and total costare displayed using the `rich` library for enhanced console output. The script also saves the generated CSV output to a file. Error handling is implemented to catch missing files or other exceptions, providing user-friendly error messages. The script is designed to be run as a standalone program, with the main block calling `example_usage`. Overall, the file serves as an example of integrating dependency processing into a Python workflow, with clear input/output handling and robust error management.",2025-04-30T03:52:33.916523+00:00
"context/install_completion_example.py","This Python script serves as an example for installing shell completion for the PDD CLI tool using the pdd package. It demonstrates the use of two main functions: get_local_pdd_path(), which returns the absolute path to the PDD_PATH directory, and install_completion(), which sets up shell completion for the user's shell environment. The script is designed to be safe for demonstration purposes by creating dummy directories (example_home for the home directory and example_pdd_path for the PDD_PATH) within the current working directory, ensuring that no real user configuration files are affected.

The setup_example_environment() function configures environment variables (SHELL, HOME, PDD_PATH) to point to these dummy directories, creates a dummy shell RC file (like .bashrc), and a dummy completion script file (pdd_completion.sh). The main() function orchestrates the setup, displays the determined PDD_PATH, and invokes the installation of shell completion, with output provided via the Rich library for enhanced console formatting. The script is intended to be run directly and provides clear instructions and feedback throughout the process, making it a safe and informative example for users wishing to understand or test the PDD shell completion installation process.",2025-04-30T03:52:41.474044+00:00
"context/langchain_lcel_example.py","This Python script demonstrates advanced usage of the LangChain library for interacting with various large language models (LLMs) and managing their outputs. It imports a wide range of LLM providers, including OpenAI, Google, Anthropic, Groq, Fireworks, Together, DeepSeek, and AWS Bedrock, and sets up prompt templates for generating jokes and answering questions. The script defines a custom callback handler (CompletionStatusHandler) to track completion status, finish reasons, and token usage for each LLM call. It showcases how to use structured output parsing with Pydantic models (e.g., for joke setup and punchline), and demonstrates chaining prompts, models, and output parsers using the new LangChain RunnableSequence pattern. The script also illustrates the use of caching with SQLite to optimize performance and cost. Multiple examples are provided for invoking LLMs with different prompts, handling structured and unstructured outputs, and switching between models or fallback alternatives. The code is up-to-date with recent LangChain changes, such as the deprecation of LLMChain in favor of chaining with the | operator and the use of invoke instead of run. Overall, the file serves as a comprehensive reference for multi-provider LLM orchestration, prompt engineering, and output management in LangChain.",2025-04-30T03:52:53.680406+00:00
"context/llm_invoke_example.py","This Python script demonstrates the use of a language model invocation function (`llm_invoke`) to generate jokes, both in unstructured and structured formats. It uses the Pydantic library to define a structured output model (`Joke`) with 'setup' and 'punchline' fields. The main function iterates over a range of 'strength' values from 0.0 to 1.0 in increments of 0.005, invoking the language model at each step. For each strength value, it first requests an unstructured joke about programmers, then requests a structured joke about data scientists, expecting a JSON response that matches the `Joke` model. The script tracks which language model is used at each strength value and records the cost of each invocation. It also handles exceptions that may occur during structured output parsing. At the end, it summarizes the strength ranges for each model used. The script is designed to be run as a standalone program and provides detailed output for each invocation, including the joke, cost, and model name.",2025-04-30T03:53:04.191486+00:00
"context/llm_selector_example.py","The provided Python script demonstrates the usage of a function called `llm_selector` imported from the `pdd.llm_selector` module. The main function initializes two parameters, `strength` and `temperature`, which are likely used to configure the selection of a large language model (LLM). The script enters a loop, incrementing the `strength` parameter from 0.5 up to 1.1 in steps of 0.05. In each iteration, it calls `llm_selector` with the current `strength` and `temperature` values, and receives several outputs: the selected LLM object, a token counting function, input and output costs per million tokens, and the model's name. The script prints out these details for each model selection. It also demonstrates the use of the token counter by counting the tokens in a sample text. The script includes error handling for `FileNotFoundError` and `ValueError`, printing error messages if these exceptions occur. The main function is executed when the script is run directly. Overall, the script serves as a test or demonstration for selecting and evaluating different LLM models based on configurable parameters.",2025-04-30T03:53:11.010798+00:00
"context/load_prompt_template_example.py","The provided file is a Python script that demonstrates how to load and display a prompt template using a custom function and the 'rich' library for formatted output. The script imports the 'load_prompt_template' function from the 'pdd.load_prompt_template' module and the 'print' function from the 'rich' library. In the main function, it specifies a prompt template name ('generate_test_LLM'), loads the corresponding template using the imported function, and, if successful, prints a message indicating the template was loaded, followed by the template's contents. The script is designed to execute the main function when run as the main module. Overall, the script serves as a simple utility for loading and visually displaying prompt templates, likely for use in language model or prompt engineering workflows.",2025-04-30T03:53:17.709776+00:00
"context/postprocess_0_example.py","The provided file is an example script demonstrating how to use the `postprocess_0` function from the `pdd.postprocess_0` module. The script is designed to process the output from a language model (LLM) that may contain multiple code blocks in various programming languages, interspersed with text. The example shows how to extract and focus on code sections written in a specified language (e.g., Python), using the `postprocess_0` function. The function takes two parameters: `llm_output`, a string containing the LLM's output, and `language`, a string specifying the target programming language. The output of the function is a string where only the largest code section in the specified language remains uncommented, while all other code and text are commented out using the appropriate comment syntax for that language. The file also includes documentation for the input and output parameters, as well as notes indicating that helper functions like `get_comment`, `comment_line`, and `find_section` must be implemented and available for the example to work. This script serves as both a usage guide and a reference for integrating the `postprocess_0` function into other projects.",2025-04-30T03:53:23.853509+00:00
"context/postprocess_example.py","The provided Python script demonstrates the use of a 'postprocess' function, which is imported from the 'pdd.postprocess' module. The script is designed to extract code snippets from a mixed text and code output, such as those generated by large language models (LLMs). It uses the 'rich' library's Console for formatted output in the terminal.

Within the 'main' function, an example LLM output is defined as a multi-line string containing both explanatory text and Python code blocks. The script specifies parameters such as the programming language ('python'), model strength, temperature, and verbosity. It then calls the 'postprocess' function with these parameters and the example output. The function is expected to return the extracted code, the total cost of processing, and the model name used.

The results are printed to the console with formatting: the extracted code is highlighted in green, while the total cost and model name are shown in blue. The script is intended to be run as a standalone program, with the 'main' function executed when the script is run directly. Overall, the script serves as a demonstration or test harness for the 'postprocess' function, showcasing its ability to parse and extract code from LLM-generated outputs.",2025-04-30T03:53:33.657125+00:00
"context/preprocess_example.py","The provided file is a Python script that demonstrates the use of a preprocessing function, likely for handling and formatting prompts. The script imports a 'preprocess' function from a module named 'pdd.preprocess' and uses the 'rich' library's Console for styled terminal output. A multi-line string named 'prompt' is defined, containing a mix of custom XML-like tags (<shell>, <pdd>, <web>), placeholders ({test}, {test2}), and a code block. The script sets two configuration flags: 'recursive' (set to False) and 'double_curly_brackets' (set to True), and specifies an 'exclude_keys' list containing 'test2', indicating that this key should be excluded from certain processing steps (such as doubling curly brackets). Debug information about the excluded keys is printed to the console. The 'preprocess' function is then called with the prompt and configuration options, and the processed result is printed. The script appears to be for testing or demonstrating how the 'preprocess' function handles prompt formatting, placeholder substitution, and exclusion logic, with a focus on debugging and output visualization.",2025-04-30T03:53:40.903078+00:00
"context/preprocess_main_example.py","This file defines a command-line interface (CLI) tool using the Click library in Python. The tool is designed to preprocess prompt files, likely for use in natural language processing or AI applications. The CLI accepts several options: the path to the prompt file (required), an optional output path, a flag to insert XML delimiters, a recursive flag to process referenced prompt files, a flag to double curly brackets, a list of keys to exclude from doubling, and a verbose flag for detailed logging. The main function, 'cli', prepares a context object with some default parameters (strength, temperature, verbose), then calls 'preprocess_main' from the 'pdd.preprocess_main' module, passing along the user-specified options. After processing, it outputs the processed prompt, the total cost, and the model name. Error handling is included to catch and report exceptions during preprocessing. The script is intended to be run as a standalone program. Overall, the file provides a flexible and user-friendly interface for preprocessing prompt files with various customization options.",2025-04-30T03:53:47.566047+00:00
"context/process_csv_change_example.py","The provided file, `run_demo.py`, is a concise demonstration script for using the `process_csv_change` function from the `pdd.process_csv_change` module. The script walks through a complete example workflow:

1. **Setup**: It creates a small workspace directory structure, including a Python source file (`factorial.py`) that defines a factorial function, a prompt file (`factorial_python.prompt`) with an instruction to write a test for the factorial function, and a CSV file (`tasks.csv`) that specifies a change instruction (to add an example section to the prompt).

2. **Processing**: The script then calls `process_csv_change`, passing in the CSV file, code directory, and various parameters such as strength, temperature, language, file extension, and budget. This function is designed to process the change instructions in the CSV and apply them to the relevant prompt files, potentially using a language model.

3. **Output Inspection**: After processing, the script prints out the results, including whether the operation was successful, the total cost, the model name used, and a list of JSON objects detailing the changes made. The example output shows that the prompt file was successfully modified to include an example test for the factorial function.

Overall, the script serves as a practical example for users to understand how to automate prompt modifications using CSV-driven instructions and the `process_csv_change` utility.",2025-04-30T03:53:54.416917+00:00
"context/pytest_example.py","This Python script is designed to run pytest tests programmatically and collect detailed results, including failures, errors, warnings, and logs. It defines a class, TestResultCollector, which acts as a custom pytest plugin. This class tracks the number of test failures, errors, and warnings by implementing pytest hook methods such as pytest_runtest_logreport and pytest_sessionfinish. It also captures all standard output and error logs during the test run using an in-memory StringIO buffer.

The run_pytest function initializes the collector, redirects output streams, and runs pytest on a specific test file (tests/test_get_extension.py) with verbose output. After the test run, it resets the output streams and retrieves the captured logs. The function returns the counts of failures, errors, warnings, and the logs themselves.

When executed as a script, the main block prints the current working directory, runs the tests, and then prints the collected results and logs. The script includes several debug print statements to trace its execution flow. Overall, the script provides a way to programmatically run pytest, collect detailed test results, and capture all output for further analysis or reporting.",2025-04-30T03:54:02.853935+00:00
"context/pytest_output_example.py","This Python script demonstrates the usage of a module named `pytest_output` (imported as `pdd.pytest_output`) for running and capturing the results of pytest test files. The script uses several libraries, including argparse, json, pytest, and rich for pretty printing. It defines a helper function to create dummy test files and a main function (`main_example`) that showcases various use cases of the `pytest_output` module.

The main function creates several test files with different contents: a standard test file with passing, failing, error, and warning tests; a non-existent file; a non-Python file; and an empty test file. For each scenario, it demonstrates how to:
- Run pytest on the file and capture the output using `run_pytest_and_capture_output`.
- Save the output to a JSON file with `save_output_to_json`.
- Handle errors gracefully when files are missing or not Python files.
- Use the `TestResultCollector` class directly for advanced log and result collection, including capturing stdout, stderr, failures, errors, warnings, and passed tests.

The script is intended as an example or test harness for the `pytest_output` module, illustrating its features and error handling capabilities in various scenarios.",2025-04-30T03:54:11.904084+00:00
"context/split_example.py","The provided Python script demonstrates the usage of the `split` function from the `pdd.split` module. It begins by importing necessary modules, including `os` for environment management and `rich.console` for enhanced console output. The script defines a `main` function, which sets up the environment by configuring the `PDD_PATH` variable. It then prepares input parameters: a prompt asking for a Python factorial function, the corresponding code, an example usage snippet, and two float parameters (`strength` and `temperature`). The `split` function is called with these inputs, and it returns a tuple containing a sub-prompt and a modified prompt, as well as the total cost and the model name used. The results are printed to the console using Rich's formatting capabilities. The script includes error handling to catch and display any exceptions that occur during execution. The main function is executed when the script is run directly. Overall, the script serves as a demonstration or test harness for the `split` function, showcasing how to prepare inputs, call the function, and display its outputs in a user-friendly manner.",2025-04-30T03:54:21.123836+00:00
"context/split_main_example.py","The provided Python script, `example_split_usage.py`, demonstrates how to create a command-line interface (CLI) using the `click` library to interact with a function called `split_main` from the `pdd.split_main` module. The script defines a CLI command, `split-cli`, which accepts several options: paths to an input prompt file, a generated code file, an example usage code file, and optional output files for a sub-prompt and a modified prompt. Additional flags allow the user to force overwriting files and suppress console output. The CLI command sets up a context object with parameters like `strength` and `temperature` to control the behavior of `split_main`. When executed, the command calls `split_main` with the provided arguments, processes the results, and displays or saves the sub-prompt and modified prompt contents, along with information about the model used and the operation's cost. The script also includes error handling to report issues during execution. At the end, the script registers the `split-cli` command with a top-level CLI group and runs the CLI if the script is executed as the main module. This script serves as an example of integrating a prompt/code splitting function into a user-friendly CLI tool.",2025-04-30T03:54:27.265533+00:00
"context/summarize_directory_example.py","The provided file is a Python script that demonstrates how to use the 'summarize_directory' function from the 'pdd.summarize_directory' module. The script is structured around a 'main' function, which serves as an example of summarizing Python files within a specified directory. The process begins by defining a sample CSV string containing existing file summaries. The 'summarize_directory' function is then called with parameters such as the directory path (using a wildcard to match files), strength, temperature, verbosity, and the existing CSV content. The function returns a new CSV output, the total cost of the operation, and the model name used for summarization. The script prints the generated CSV content, the total cost, and the model name to the console. It also ensures that an 'output' directory exists, and writes the new CSV output to a file named 'output.csv' within this directory. Exception handling is included to catch and print any errors that occur during execution. The script is intended to be run as a standalone program, as indicated by the '__main__' guard at the end.",2025-04-30T03:54:33.511865+00:00
"context/tiktoken_example.py","The file contains a short Python script that demonstrates how to use the 'tiktoken' library to count the number of tokens in a given text. The script first imports the 'tiktoken' module, then retrieves a specific encoding scheme called 'cl100k_base' using the 'get_encoding' function. It then encodes a variable named 'preprocessed_prompt' using this encoding, and calculates the number of tokens by taking the length of the resulting encoded list. This token count is stored in the variable 'token_count'. The script is likely intended for use cases where it is important to know the number of tokens in a text, such as preparing prompts for language models that have token limits. The code assumes that 'preprocessed_prompt' is already defined elsewhere in the program.",2025-04-30T03:54:39.473311+00:00
"context/trace_example.py","The provided file is a Python script that demonstrates the use of the `trace` function from the `pdd.trace` module, likely related to program debugging or code analysis. The script imports necessary modules, including `os`, `trace` from `pdd.trace`, and `Console` from the `rich` library for formatted output. It defines a `main` function that sets up example code and prompt strings, specifies a line number to trace, and sets parameters for model strength and temperature, which are typical in language model (LLM) configurations. The script then calls the `trace` function with these parameters, aiming to map a line in the code to a corresponding line in the prompt, and retrieves the total cost and model name used. Results are printed to the console in a formatted manner. The script includes error handling for file not found, value errors, and general exceptions, providing user-friendly error messages. The main function is executed when the script is run directly. Overall, the script serves as an example or test harness for the `trace` function, showcasing its usage and output handling.",2025-04-30T03:54:46.463917+00:00
"context/trace_main_example.py","This file is a Python script that demonstrates the use of a tracing tool, likely for analyzing code with the help of a language model (LLM). The script first creates two example files: a prompt file (calculator.prompt) containing instructions for a simple calculator function, and a Python code file (calculator.py) that implements an addition function and prints the result of adding two numbers. The script then sets up a Click context with various configuration options, such as verbosity, whether to overwrite files, the strength of LLM analysis, and the randomness (temperature) of the LLM. It then calls the `trace_main` function from the `pdd.trace_main` module, passing in the context, file paths, the line number of the function to trace, and an output file for the results. The script prints out the line number in the prompt file corresponding to the traced code, the cost of the analysis, and the name of the LLM model used. If any errors occur during execution, they are caught and printed. Overall, the script serves as an example of how to set up and use a code tracing and analysis workflow with LLM support.",2025-04-30T03:54:56.972930+00:00
"context/track_cost_example.py","This Python script defines a command-line interface (CLI) for a tool called PDD, which is designed to process prompts and generate outputs with cost tracking. The script uses the Click library to handle command-line arguments and commands, and the Rich library for enhanced console output. The main CLI group accepts an optional '--output-cost' argument to enable cost tracking and output usage details to a CSV file.

The primary command implemented is 'generate', which requires a '--prompt-file' argument specifying the path to an input file containing a prompt. An optional '--output' argument allows the user to specify an output file for the generated result; otherwise, the output is printed to the console. The 'generate' function is decorated with a 'track_cost' decorator (imported from 'pdd.track_cost'), which likely handles the cost tracking functionality. The function reads the prompt from the input file, simulates processing it, and generates a placeholder output. It also simulates returning the cost of execution and the model name used (hardcoded as 0.05 dollars per million tokens and 'gpt-4', respectively). The script includes a main block that demonstrates how to invoke the CLI with example arguments.",2025-04-30T03:55:06.205668+00:00
"context/unfinished_prompt_example.py","This Python script demonstrates the use of the `unfinished_prompt` function from the `pdd.unfinished_prompt` module. The main purpose of the script is to assess whether a given prompt text is complete or unfinished using a selected large language model (LLM). The script defines a sample prompt text, Once upon a time in a land far away, there was a, which is intentionally left incomplete. It also sets parameters for the LLM, such as `strength` and `temperature`, which likely control the model's response characteristics. The script then calls the `unfinished_prompt` function with these parameters and captures its output, which includes reasoning about the prompt's completeness, a boolean indicating if the prompt is finished, the total cost of the operation, and the name of the model used. The results are printed to the console. The script includes error handling to catch and report any exceptions that occur during execution. Overall, this file serves as an example or test harness for evaluating prompt completeness using an LLM-based function.",2025-04-30T03:55:13.287209+00:00
"context/update_main_example.py","This Python script demonstrates how to create a command-line interface (CLI) using the Click library, specifically for updating a prompt based on modified code. The script defines an 'update' command that accepts several options, such as paths to the original prompt file, modified code file, and optionally the original code file. It also supports using Git history to retrieve the original code, and includes options for output file path, strength and temperature parameters for the update process, and flags for verbosity, quiet mode, and force-overwriting files.

The core functionality is provided by the 'update_main' function, which is imported from another module. The script collects all user-specified options, stores some in the Click context, and passes them to 'update_main'. After execution, it displays the results, including a snippet of the updated prompt, the total cost, and the model name, unless quiet mode is enabled. The script is structured as a CLI group, allowing for future expansion with additional commands. It serves as an example of integrating Click for CLI development, handling file and Git inputs, and managing user feedback and logging.",2025-04-30T03:55:21.769753+00:00
"context/update_prompt_example.py","The provided file is a Python script that demonstrates the usage of the `update_prompt` function from the `pdd.update_prompt` module. The script defines a `main` function, which sets up example input parameters including an input prompt, original code, modified code, and parameters for a language model (strength and temperature). It then calls the `update_prompt` function with these parameters, including a verbosity flag. The function is expected to return a modified prompt, the total cost of the operation, and the model name used. The script prints these results if the function call is successful, or prints an error message if an exception occurs. The script is designed to be run as a standalone program, as indicated by the `if __name__ == __main__:` block. Overall, the file serves as a usage example or test harness for the `update_prompt` function, demonstrating how to supply inputs and handle outputs and errors.",2025-04-30T03:55:28.422608+00:00
"context/xml_tagger_example.py","The provided file is a Python script that demonstrates how to use the 'xml_tagger' function from the 'pdd.xml_tagger' module, along with the 'rich' library for formatted console output. The script sets up a sample prompt ('Write a story about a magical forest') and specifies two parameters: 'strength' (which appears to control the quality or cost of the language model used) and 'temperature' (which likely affects the randomness of the model's output). It then attempts to process the prompt using the 'xml_tagger' function, which returns an XML-tagged version of the prompt, the total cost of the operation, and the name of the model used. The results are printed to the console in a formatted manner. If an error occurs during processing, the script catches the exception and prints an error message in red. Overall, the script serves as an example or test for using the 'xml_tagger' function with customizable parameters and provides user-friendly output.",2025-04-30T03:55:34.441519+00:00
