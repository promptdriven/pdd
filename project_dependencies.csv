full_path,file_summary,date
"context/DSPy_example.py","This code snippet demonstrates the initial setup and implementation of a DSPy-based application focused on example selection. The code begins with importing necessary DSPy components and configuring OpenAI's GPT-3.5-turbo-instruct model as the language model backend, with a maximum token limit of 250. The core of the implementation is a 'chainofthought' class that inherits from dspy.Module. This class is designed to implement a chain-of-thought reasoning process, with a basic structure that includes initialization and a forward method to process questions. The class uses DSPy's ChainOfThought feature to map questions to answers. However, the code appears to be incomplete, as indicated by empty spaces and an unfinished comment about compilation and optimization. The code represents a basic framework for implementing chain-of-thought reasoning in a DSPy environment, though it needs additional implementation details to be fully functional.",2024-12-07T01:53:08.162193+00:00
"context/anthropic_counter_example.py","This Python code snippet demonstrates how to count tokens using the Anthropic API. The code first imports the Anthropic library and initializes a client. It then defines a sample text string ('Sample text') and uses the client's count_tokens method to determine the number of tokens in this text. Finally, it prints the token count to the console. The code includes a comment noting that this token counting method is not accurate for Anthropic models version 3.0 and above. Token counting is an important function when working with large language models since API costs and context limitations are typically based on token count rather than character count. This example provides a basic implementation of the token counting feature available in the Anthropic Python SDK.",2025-04-05T08:29:21.817367+00:00
"context/anthropic_tool_example.py","The provided code snippet demonstrates the use of the Anthropics API to interact with a language model named Claude. The script begins by importing the anthropic library and initializing a client instance. It then constructs a request to the model, specifying parameters such as the model version, maximum token limit, and tools to be used. In this case, a text editor tool named 'str_replace_editor' is included. The main message sent to the model is from a user who reports a syntax error in a file named 'patent.md' and requests assistance in fixing it. Finally, the response from the model is printed to the console. This code illustrates a practical application of AI in debugging and editing text files.",2025-04-15T07:05:53.321600+00:00
"context/auto_deps_main_example.py","The provided Python script is a command-line tool implemented using the Click library. It defines a command named 'auto-deps' that facilitates dependency analysis and processing of prompt files. The script includes several command-line options, such as '--force' to overwrite files, '--quiet' to suppress output, '--strength' and '--temperature' to configure dependency analysis parameters, and paths for input prompt files, directories, and output files. The main function initializes the Click context with these options and invokes the 'auto_deps_main' function from the 'pdd.auto_deps_main' module. This function processes the specified prompt file, analyzes dependencies, and generates a modified prompt file. The script also calculates and displays the total cost of the operation and the model used. Error handling is implemented to catch exceptions and abort execution if necessary. The script is designed to be executed as a standalone program.",2024-12-13T21:07:14.439152+00:00
"context/auto_include_example.py","This Python script demonstrates the usage of the `auto_include` function from the `pdd.auto_include` module. The script loads a CSV file containing project dependencies and defines parameters for the `auto_include` function, including an extensive input prompt. The prompt describes a function called `generate_test` that creates unit tests from code files using Langchain and LLMs. The script specifies parameters such as directory path ('context/c*.py'), strength (0.97), temperature (0.5), and verbosity setting. After calling `auto_include` with these parameters, it prints the results including dependencies, CSV output, total cost of LLM usage, and the model name used. The example illustrates an automated workflow for code analysis and test generation that includes prompt processing, LLM integration, token counting, and cost tracking.",2025-03-19T23:31:14.602995+00:00
"context/auto_update_example.py","The file provides an example of how to use the 'auto_update' function from the 'pdd.auto_update' module. It includes a Python script that demonstrates various ways to utilize the function. The 'auto_update' function is designed to check the current installed version of a package, compare it with the latest version (either from PyPI or a specified version), and prompt the user to upgrade if a newer version is available. If the user confirms, the function performs the upgrade using pip. The script showcases basic usage for checking updates for the 'pdd' package, checking updates for a specific package like 'requests,' and comparing against a known version, such as 'pandas' version 2.0.0. The script is structured with a 'main' function that encapsulates these examples, and it is executed when the script is run directly. The file serves as a guide for implementing and understanding the 'auto_update' function's capabilities.",2025-01-01T07:17:26.468121+00:00
"context/autotokenizer_example.py","This Python code defines a function called 'count_tokens' that calculates the number of tokens in a given text using a specified language model's tokenizer. The function uses the Hugging Face transformers library, specifically the AutoTokenizer class. By default, it uses the 'deepseek-ai/deepseek-coder-7b-instruct-v1.5' model, but this can be changed through the model_name parameter. The function works by first loading the appropriate tokenizer, then tokenizing the input text, and finally returning the length of the resulting input_ids. The code includes a simple example that demonstrates how to use the function by counting tokens in the text 'Write a quick sort algorithm in Python.' This utility is particularly useful for working with language models where understanding token count is important for managing context windows and input limitations.",2024-12-07T01:53:08.299472+00:00
"context/bug_main_example.py","This Python script demonstrates the use of `bug_main` function from the `pdd` package to automatically generate unit tests based on the difference between observed and desired program outputs. The script begins by setting up a Click context with configuration parameters for the test generation process. It then creates sample files in an output directory, including a prompt file describing requirements for a function that sums even numbers in a list, an implementation file with a potential bug (doesn't handle empty lists correctly), a program file that uses this function, and files containing current and desired outputs. The script calls `bug_main` with these inputs to generate a unit test that would catch the identified bug. The function returns the generated test code, the cost of the operation (presumably from using an AI model), and the name of the model used. The example illustrates an automated debugging workflow where the system identifies that the sum_even_numbers function fails to return 0 for empty lists as specified in the requirements.",2025-03-19T23:31:14.628478+00:00
"context/bug_to_unit_test_example.py","This Python script demonstrates the usage of a bug_to_unit_test function for automated test generation. The main function serves as the entry point and orchestrates the process of generating unit tests from bug reports. The script reads input from three different files: a prompt file (from 'prompts' directory), a code file (from 'pdd' directory), and a context file (from 'context' directory). It processes two types of outputs (current and desired) that contain debug information and JSON-formatted explanations. The function takes several parameters including the outputs, prompt, code under test, program to run the code, strength (0.9), temperature (0), and language ('Python'). The script uses the Rich library for enhanced console output formatting, displaying the generated unit test, total cost, and model name used. Error handling is implemented to catch and display any exceptions that occur during execution. The code appears to be part of a larger system for automated testing and debugging, possibly using AI/LLM models given the presence of temperature and strength parameters.",2024-12-07T01:53:08.330225+00:00
"context/bug_to_unit_test_failure_example.py","This code snippet contains a Python function named 'add' that takes two parameters 'x' and 'y'. However, there appears to be a logical error in the implementation. Despite the function name suggesting addition, the function actually performs subtraction, returning 'x-y' instead of 'x+y'. This is likely a bug that needs to be fixed to match the intended functionality implied by the function name. The code is very simple, consisting of just two lines: the function definition and the return statement. The indentation suggests this might be part of a larger codebase, but only this single function is shown in the provided content.",2024-12-07T01:53:08.365017+00:00
"context/change_example.py","The provided Python script demonstrates the use of a `change` function from the `pdd.change` module. It includes a `main` function that sets up environment variables, defines input parameters, and calls the `change` function. The script outlines an example scenario where a function to calculate the factorial of a number is modified based on a prompt to take the square root of the factorial output. Parameters such as `strength` and `temperature` are used to control the behavior of the `change` function. The script also handles potential exceptions and prints the modified prompt, total cost, and model name using the `rich.console` module for formatted output. The `main` function is executed when the script is run directly.",2024-12-21T03:10:15.598748+00:00
"context/change_main_example.py","The provided Python script demonstrates the usage of the 'change_main' function from the 'pdd' command-line program, showcasing both single-change and CSV batch-change modes. It begins by importing necessary modules and setting up a Click context for command-line options. The script outlines prerequisites for running the 'change_main' function, including the availability of the module and required packages. In single-change mode, it creates sample input files, modifies a function to add error handling for division by zero, and displays the modified output along with the total cost and model used. The script also illustrates CSV batch-change mode by creating multiple sample code files and a CSV file that specifies changes for batch processing. It then calls 'change_main' in this mode, displaying the results and total cost. The script emphasizes the importance of setting up directories and files correctly for both modes, and it utilizes the 'rich' library for formatted output. Overall, the script serves as a practical example for users looking to implement changes in Python code using the 'pdd' tool.",2025-02-09T07:16:17.092974+00:00
"context/cli_example.py","The provided Python script demonstrates the usage of the 'generate' command from the PDD CLI module to create runnable code from a specified prompt file. It begins by disabling auto-update through an environment variable and defines a main function that outlines the process of code generation. The script assumes that necessary environment variables are set and required packages are installed. It specifies the input prompt file and the output file where the generated code will be saved. The script sets command line arguments for the generation process, including parameters for the AI model's strength and temperature, and forces generation without user confirmation. The CLI command is executed within a try-except block to handle potential errors gracefully. If successful, the generated code is saved to the designated output file, and the total cost and model name used for generation are printed to the console. The script is designed to be run as a standalone program.",2025-02-09T07:16:24.946452+00:00
"context/click_example.py","This Python script implements a command-line image processing tool using Click and Pillow (PIL) libraries. It follows a Unix-like pipe pattern where multiple image processing commands can be chained together. The script includes various image manipulation functions such as opening, saving, displaying, resizing, cropping, transposing (rotation and flipping), blurring, smoothening, embossing, sharpening, and pasting images. Each command is implemented as a Click command group with specific options and parameters. The script uses decorator patterns (@processor and @generator) to handle the command chain processing. Key features include:

1. Command chaining functionality similar to Unix pipes
2. Support for multiple input images
3. Various image transformation options
4. Error handling for image operations
5. Flexible file naming for output
6. Preservation of image filenames through transformations

The script is well-documented with docstrings and comments, making it clear how to use each command and what it does. Example usage shows commands can be combined like 'imagepipe open -i example.jpg resize -w 128 display' to process images in sequence.",2024-12-07T01:53:08.509553+00:00
"context/cloud_function_call.py","The provided code is a Python script that demonstrates how to call a Google Cloud Function using the requests library. It defines a function named `call_cloud_function` that takes a Firebase token as an argument. The function constructs a request to a specified Cloud Function URL, including the Firebase token in the Authorization header to authenticate the request. The URL is hardcoded as 'https://us-central1-prompt-driven-development.cloudfunctions.net/on_request_example'. After making the GET request, the function returns the JSON response from the Cloud Function. The script also includes a sample Firebase token, which is a long string used for authentication. Finally, the script calls the `call_cloud_function` with the provided token and prints the result. This code is useful for developers looking to integrate Firebase authentication with Google Cloud Functions.",2025-02-09T07:16:28.890615+00:00
"context/cmd_test_main_example.py","The file 'test_cli_example.py' provides an example of integrating the 'cmd_test_main' function into a Click-based command-line interface (CLI) for generating or enhancing unit tests. It includes a detailed explanation of its usage, required and optional arguments, and the expected output. The script defines a CLI with global options such as verbosity, AI generation strength, temperature, force overwrite, and quiet mode. It also includes a 'test' command that accepts arguments like 'prompt_file' (path to a text file), 'code_file' (path to the source code), and optional parameters such as output path, programming language override, coverage report, existing tests, target coverage percentage, and a merge flag. The 'test' command invokes the 'cmd_test_main' function to generate or enhance unit tests, returning the generated test code, total cost, and model name. The script demonstrates how to run the CLI from the command line with examples and provides flexibility for customizing test generation. It uses the Click library for CLI creation and ensures shared context across commands. The script is designed for developers looking to automate or improve their unit testing process using AI-driven tools.",2025-01-01T07:17:35.334047+00:00
"context/code_generator_example.py","The provided Python script demonstrates the usage of a function called 'code_generator' from the 'pdd.code_generator' module. The script's main function reads a prompt from a file ('prompts/generate_test_python.prompt') and uses it to generate Python code with the help of a language model. Key parameters for the 'code_generator' function include the programming language ('python'), model strength (0.5), temperature (0.0), and verbosity (True). The script attempts to generate runnable code, calculate the total cost, and identify the model name. If successful, it prints the generated code, cost, and model name. In case of an error, it catches and prints the exception. The script is designed to be executed as a standalone program with the main function serving as the entry point.",2024-12-21T03:10:19.903389+00:00
"context/code_generator_main_example.py","The provided code is a Python script that utilizes the Click library to create a command-line interface (CLI) for generating code based on a specified prompt. It sets up a Click context with parameters such as 'strength', 'temperature', 'force', and 'quiet' to control the behavior of the AI model used for code generation. The script defines paths for a prompt file and an output file, where the prompt file contains instructions for generating a Python function that retrieves a JSON Web Token (JWT). The code generation is executed by calling the 'code_generator_main' function, which takes the Click context, prompt file, and output file as arguments. After the code is generated, the script prints the generated code, the total cost of the operation, the model used for generation, and the location where the code is saved. The script also includes commented-out sections for creating a prompt file and output directory, indicating that it is designed for flexibility in generating various code snippets.",2025-02-09T07:16:32.177765+00:00
"context/comment_line_example.py","This file provides documentation and examples for a Python function called `comment_line` that handles code commenting across different programming languages. The function takes two parameters: `code_line` (the line to be commented) and `comment_characters` (the commenting syntax to use). The function supports three commenting styles: single-character comments (like Python's #), paired comments (like HTML's <!-- -->), and line deletion (using 'del'). The documentation includes the function's definition with detailed parameter descriptions and return value information. The file also contains practical examples demonstrating all three use cases: commenting a Python print statement, commenting an HTML tag, and deleting a line of code. Each example is accompanied by its expected output. The function is designed to be flexible and can accommodate different programming language commenting syntaxes by appropriately formatting the input line based on the specified comment characters.",2024-12-07T01:53:08.574892+00:00
"context/conflicts_in_prompts_example.py","The provided Python script demonstrates the use of a function named `conflicts_in_prompts` to detect and resolve conflicts between two example prompts. The script imports necessary modules, including `conflicts_in_prompts` from `pdd.conflicts_in_prompts` and `print` from the `rich` library for formatted output. The main function defines two detailed prompts: one for creating an `auth_helpers.py` module for Firebase authentication and another for designing a `User` class for the PDD Cloud platform. Each prompt includes specific requirements, such as functionality, dependencies, error handling, and coding standards. The script sets parameters like `strength`, `temperature`, and `verbose` for the conflict detection process. It then calls the `conflicts_in_prompts` function with the prompts and parameters, capturing the suggested changes, total cost, and model name. The results are displayed using formatted output, highlighting any detected conflicts and proposed changes. If no conflicts are found, a corresponding message is printed. The script is structured to showcase the functionality of the `conflicts_in_prompts` function while adhering to Python best practices, including type hints, docstrings, and modular design.",2024-12-21T03:10:21.724675+00:00
"context/conflicts_main_example.py","The provided code snippet is a Python script that utilizes the Click library and a custom function from the 'pdd.conflicts_main' module. It begins by defining two prompt files, 'prompt1_LLM.prompt' and 'prompt2_LLM.prompt', which contain instructions for an AI assistant and a chatbot, respectively. The script then creates these prompt files and writes the corresponding text into them. A mock context class, 'MockContext', is defined to simulate a Click context, which includes attributes like 'force', 'quiet', 'strength', and 'temperature'. The script then calls the 'conflicts_main' function with the mock context and the two prompt files, capturing the output in 'conflicts_output.csv'. It prints the number of conflicts, the total cost associated with the operation, and the model used for processing. The results are intended to be saved in a CSV file for further analysis. Overall, the script demonstrates how to set up prompts for an AI model and handle conflicts in a structured manner.",2025-02-09T07:16:35.220463+00:00
"context/construct_paths_example.py","The provided code is a Python script that demonstrates the usage of a function called `construct_paths` from the `pdd.construct_paths` module. The script defines a `main` function that orchestrates the process of setting up input parameters, invoking the `construct_paths` function, and handling its output. It begins by specifying input file paths for code and prompt files, followed by defining command options such as output settings. The script attempts to call `construct_paths` with these parameters, capturing the input strings, output file paths, and the programming language used. It includes error handling to manage any exceptions that may arise during the function call. The script repeats this process for different sets of input file paths and command options, demonstrating its flexibility in generating outputs based on various prompts. The final part of the script constructs file paths for a regression test prompt and generates corresponding outputs. Overall, the script serves as a practical example of how to utilize the `construct_paths` function for file management and output generation in a Python environment.",2025-02-09T07:16:38.839339+00:00
"context/context_generator_example.py","This Python script demonstrates the usage of a context generator tool for code generation. The script first checks for the required environment variable 'PDD_PATH' and imports necessary modules. It then sets up input parameters including a sample Python function (add), a corresponding prompt, programming language specification, and model parameters like strength and temperature. The script calls the context_generator function with these parameters to generate example code. The function returns three values: the generated example code, the cost of generation, and the name of the model used. Finally, the script prints the results using Rich library's formatted output, displaying the generated code, total cost, and the model name used for generation. This appears to be a demonstration or test script for a code generation system that likely uses AI models to generate code based on prompts.",2025-02-16T18:20:44.136786+00:00
"context/context_generator_main_example.py","The provided Python script utilizes the Click library to create a command-line interface (CLI) context for executing a function named `context_generator_main` from the `pdd.context_generator_main` module. The script initializes a Click context object with specific parameters and attributes, such as `force`, `quiet`, `verbose`, `strength`, and `temperature`, which control the behavior of the CLI and the AI model. It specifies input and output file paths, including a prompt file, a code file, and an output file where the generated code will be saved. The `context_generator_main` function is then called with the configured context and file paths, and it returns the generated code, the total cost of the operation, and the name of the AI model used. Finally, the script prints the generated code, the total cost, and the model name to the console. The script appears to be part of a larger system for generating Python code based on prompts and AI model configurations.",2025-01-01T07:17:47.378733+00:00
"context/continue_generation_example.py","The provided Python script demonstrates the usage of the `continue_generation` function, which extends text generation using a language model and calculates associated costs. The script begins by loading input data from external files: a formatted input prompt from `context/cli_python_preprocessed.prompt` and an initial LLM output fragment from `context/llm_output_fragment.txt`. It sets parameters for the language model, including `strength` and `temperature`, and then calls the `continue_generation` function with these inputs. The function returns the final generated output, the total cost of the operation, and the model name. The script prints the total cost and model name to the console and writes the final generated output to a file named `context/final_llm_output.py`. Error handling is implemented to catch and report file-related or general exceptions. The script is designed to be executed as a standalone program with the `main` function serving as the entry point.",2024-12-21T03:10:26.974173+00:00
"context/crash_main_example.py","This Python script demonstrates the usage of `crash_main` function from the `pdd` library, which is designed to automatically fix code errors that cause program crashes. The script first creates necessary example files in an output directory including: 1) a prompt requesting a factorial function implementation, 2) a factorial.py module with an intentional bug (doesn't handle negative numbers), 3) a main.py program that incorrectly calls factorial with a negative number, and 4) an error log showing the resulting recursion error. The demonstration then creates a Click context with model parameters and calls the `crash_main` function to automatically fix both the factorial implementation and the calling program. The function is configured to make up to two fix attempts, with a $5 budget limit, and can operate iteratively. After execution, the script displays whether the fix was successful, the number of attempts made, the cost incurred, the AI model used, and prints the fixed code for both files. This demonstrates an automated debugging workflow that uses AI to identify and repair programming errors across interrelated files.",2025-03-19T23:31:14.660248+00:00
"context/detect_change_example.py","The provided script is a Python program designed to analyze and detect changes in prompt files using a machine learning model. It imports necessary modules, including 'detect_change' from 'pdd.detect_change', and utilizes the 'rich.console' library for formatted console output. The script defines a list of prompt files located in 'context' and 'prompts' directories, which are analyzed for changes based on a given description. The change description specifies the goal of making prompts more compact using a specific file ('context/python_preamble.prompt').

The script sets parameters for the machine learning model, such as 'strength' (determining model intensity) and 'temperature' (controlling randomness in output). It then attempts to detect changes in the prompt files by calling the 'detect_change' function with the specified parameters. If successful, it prints the detected changes, including the prompt name and change instructions, along with the total cost of the operation and the model used. In case of an error, it catches the exception and displays an error message.

Overall, the script automates the process of analyzing and updating prompt files to improve their compactness and efficiency, while providing detailed feedback and cost analysis.",2025-01-01T07:17:53.244881+00:00
"context/detect_change_main_example.py","The provided code is a Python script that utilizes the Click library to simulate command-line interface (CLI) parameters and invoke a function named `detect_change_main` from the `pdd.detect_change_main` module. The script defines a main function that sets up a Click context with specific model parameters, including strength, temperature, force, and quiet options. It specifies a list of prompt files and a change description file, which is written to a specified output file. The script ensures that the output directory exists before calling the `detect_change_main` function, which processes the prompts and generates a list of changes needed, along with the total cost and model name used. The results, including the model name, total cost, and detailed change instructions for each prompt, are printed to the console. The script is designed to handle exceptions gracefully, providing error messages if any issues arise during execution. Overall, this script serves as a utility for detecting changes in prompts based on specified criteria.",2025-02-09T07:16:47.203735+00:00
"context/edit_file_example.py","The provided script is a Python program designed to facilitate the editing of text files using a specified editing module, `edit_file.py`. It begins by configuring the environment, ensuring that the necessary paths are set up correctly, particularly the `PDD_PATH` environment variable, which points to the directory where output files will be created. The script defines a function to create a dummy text file with initial content and a set of instructions for editing this file. The main functionality is encapsulated in the `run_edit_file_test` function, which executes the editing process based on the provided instructions and verifies the results. The script includes error handling to manage issues such as missing modules or incorrect paths. The `run_example` function orchestrates the creation of example files and the execution of the editing tests. It emphasizes the need for certain prerequisites, including the installation of required packages and the availability of API keys for language model services. The script concludes by running the example function if executed as the main program, ensuring that all necessary conditions are met before proceeding.",2025-04-15T07:05:56.343481+00:00
"context/execute_bug_to_unit_test_failure.py","The provided code snippet imports a function named 'add' from a module called 'context.bug_to_unit_test_failure_example'. It then calls this 'add' function with two arguments, 2 and 3, and prints the result. The purpose of this code is to demonstrate a simple addition operation, where the expected output would be the sum of the two numbers, which is 5. This example may be part of a larger context where the function 'add' is being tested or utilized, possibly in relation to debugging or unit testing scenarios.",2025-02-09T07:16:50.487380+00:00
"context/final_llm_output.py","The provided code is a command-line interface (CLI) for a tool called PDD (Prompt-Driven Development), which facilitates the generation, modification, and testing of code based on user prompts. It utilizes the Click library for command handling and Rich for console output. The CLI supports various commands such as generating runnable code from prompts, creating example files, generating unit tests, preprocessing prompts, fixing errors in code and tests, and splitting large prompts into smaller ones. Each command is designed to handle specific tasks, with options for output file paths, verbosity, and cost tracking. The code includes error handling to manage exceptions during execution and provides feedback to the user through console messages. Additionally, it features a cost tracking mechanism to log the expenses associated with each operation in a CSV file. The CLI also supports shell completion installation for user convenience and displays the version of the tool. Overall, the code is structured to enhance the development workflow by automating code generation and testing processes.",2025-02-09T07:16:53.927541+00:00
"context/find_section_example.py","This file provides documentation and examples for using the `find_section` function from the `find_section.py` module. The function is designed to identify and extract code blocks from text content. The documentation includes a complete example that demonstrates how to use the function with a sample input containing multiple code blocks in different programming languages (Python and JavaScript). The file details both the input parameters (`lines`, `start_index`, and `sub_section`) and the output format (a list of tuples containing language, start line, and end line information). The example shows how to process a string containing code blocks by first splitting it into lines and then using the find_section function to locate the code sections. It also includes a practical example of reading from a file ('unrunnable_raw_llm_output.py') and processing its contents. The documentation concludes with sample output showing how the function identifies code blocks and their locations within the text. The function appears to be particularly useful for parsing and processing text that contains embedded code blocks in various programming languages.",2024-12-07T01:53:08.778520+00:00
"context/firecrawl_example.py","This code demonstrates the usage of the Firecrawl Python library for web scraping. The example begins with installation instructions via pip and imports the necessary FirecrawlApp class along with the os module. It initializes a FirecrawlApp instance using an API key either from environment variables or a placeholder value. The code then makes a request to scrape 'google.com' specifically requesting the content in markdown format. The script prints the response, which contains both the scraped content and metadata about the operation. The response shows Google's homepage content converted to markdown, including navigation links (About, Store, Gmail, Images), sign-in options, the Google logo, search features, various 'I'm Feeling...' options, and footer links (Advertising, Business, Privacy, Terms). The metadata section includes information such as the page title, language, scrape ID, source URL, final URL, and HTTP status code. This serves as a practical example of how to extract and format web content using the Firecrawl library.",2025-03-19T23:31:14.677508+00:00
"context/fix_code_loop_example.py","The provided Python script demonstrates the use of a function called `fix_code_loop` to iteratively debug and fix a code file. The script begins by creating example files: a Python module (`module_to_test.py`) containing a function `calculate_average` that calculates the average of a list of numbers, and a verification script (`verify_code.py`) that tests this function with an erroneous input (a string instead of a list). The script also defines a prompt describing the intended functionality of the code. The `fix_code_loop` function is then invoked with parameters such as the code file, verification program, prompt, and constraints like model strength, temperature, maximum attempts, and budget. The loop attempts to fix the code iteratively, logging errors and outputs, until the code passes the verification or the constraints are exhausted. The results, including success status, number of attempts, cost, and the final code, are printed. The script also includes commented-out code for cleaning up the generated files. This script serves as an example of automated debugging and code correction using a predefined loop mechanism.",2025-01-01T07:17:58.918831+00:00
"context/fix_code_module_errors_example.py","The provided Python script demonstrates a process for identifying and fixing errors in a code module using a function called `fix_code_module_errors`. The script begins with a sample program that contains an error, where a function `calculate_sum` attempts to sum a string instead of a list of numbers, resulting in a `TypeError`. The script also includes the original prompt that generated the code, the erroneous code module, and the error message encountered during execution. The `fix_code_module_errors` function is then called with these inputs, along with parameters such as model strength and temperature, to generate a corrected version of the program and code module. The function returns several outputs, including whether updates are needed, the fixed program and code, the total cost of the API calls, and the name of the model used. The script concludes by printing these results, providing a clear demonstration of how the error was identified and resolved. This script serves as an example of automated debugging and code correction using external tools or APIs.",2025-01-01T07:18:02.055770+00:00
"context/fix_error_loop_example.py","This Python script demonstrates the usage of the 'fix_error_loop' function from the pdd module, which appears to be designed for automated code error correction. The main function establishes various parameters for the error-fixing process, including paths to a unit test file, a code file, a prompt file containing instructions, and a verification program. It also defines operational parameters such as strength (0.85), temperature (1), maximum attempts (5), and a budget limit (100.0). The script loads a prompt from a file, then calls the fix_error_loop function with all the configured parameters. Upon completion, it displays the results using Rich library's formatting capabilities, showing whether the error correction was successful, the number of attempts made, the total cost incurred, the model name used, and the final versions of both the unit test and code files. The implementation includes error handling to catch and display any exceptions that might occur during execution. The script is designed to run as a standalone demonstration when executed directly.",2025-04-05T08:29:37.224693+00:00
"context/fix_errors_from_unit_tests_example.py","This Python file demonstrates the usage of the 'fix_errors_from_unit_tests' function from the pdd module. The script sets up example inputs including a unit test for an 'add' function, a code implementation with potential issues, a prompt describing the function's purpose, and an error message from a failed test. The example unit test contains an incorrect assertion (expecting 0+0=1), while the code implementation includes a deprecated NumPy function. The script calls the fix_errors_from_unit_tests function with these inputs along with parameters like error logging file path, LLM strength (0.85), and temperature (1.0). After processing, it prints the results including whether the unit test and code should be updated, the fixed versions of both, analysis results, the cost of the operation, and the model name used. This appears to be a demonstration of an AI-powered tool that can automatically diagnose and fix code issues based on failing unit tests, using language models to generate corrections.",2025-04-05T08:29:37.236985+00:00
"context/fix_main_example.py","This Python script defines a command-line interface for a code fixing tool using the Click library. The main function `fix_command` acts as an entry point that processes various command-line arguments and passes them to a `fix_main` function from the pdd module. The tool is designed to automatically fix errors in both code files and unit tests.

The command requires several inputs including paths to prompt files, code files, unit test files, and error logs. It also accepts optional parameters like output paths, maximum fix attempts (default 3), budget constraints (default $5.0), and whether to use iterative fixing.

Upon completion, the script reports success or failure with colored output using the rich library, and provides metrics including the number of attempts, cost, and model used. The script includes a hardcoded example in the main block that demonstrates how to use the tool for a specific file called 'get_extension.py', complete with test files and verification program. This appears to be part of a larger debugging or development toolkit.",2025-03-19T23:31:14.693282+00:00
"context/generate_output_paths_example.py","The file is a Python script that demonstrates the usage of a function called `generate_output_paths` from the `pdd.generate_output_paths` module. The script sets up example inputs, including a command, output locations, a basename, a programming language, and a file extension. It then calls the `generate_output_paths` function to generate output paths based on these inputs. The script includes examples for different scenarios: a basic 'generate' command, a case where an environment variable (`PDD_GENERATE_OUTPUT_PATH`) is used to specify the output path, and a 'fix' command that generates multiple output paths for test and code files. The results of these function calls are printed to the console, showcasing the generated output paths for each scenario. The script is structured to demonstrate the flexibility and functionality of the `generate_output_paths` function in handling various input configurations and commands.",2024-12-13T21:07:19.225256+00:00
"context/generate_test_example.py","The file contains a Python script that demonstrates the generation of a unit test for a given function using the `generate_test` function from the `pdd.generate_test` module. The script begins by importing necessary modules, including `os` and `rich` for environment variable handling and formatted output, respectively. It defines a prompt for creating a factorial function, along with the corresponding Python code implementation. Additional parameters such as `strength`, `temperature`, and `language` are specified to guide the test generation process. The script attempts to call the `generate_test` function with these inputs and captures the resulting unit test, total cost, and model name. If successful, it prints the generated unit test, cost, and model details using formatted output. In case of an error, it catches the exception and displays an error message. The script also includes commented-out code for setting an environment variable (`PDD_PATH`) if needed.",2024-12-21T03:10:34.209936+00:00
"context/get_comment_example.py","The provided document outlines the usage and documentation for the `get_comment` function from the `get_comment.py` module. It begins with an example of how to import and utilize the function, demonstrating its application for various programming languages such as Python, Java, and JavaScript. The function is designed to return the appropriate comment character(s) based on the specified programming language, with case insensitivity for the input. The documentation details the input parameter, which is a string representing the programming language, and the output, which is a string of the comment character(s). In cases where the environment variable `PDD_PATH` is not set, or if the language is not found in the associated CSV file, the function will return 'del'. Additionally, the document emphasizes the importance of setting the `PDD_PATH` environment variable correctly and mentions the structure of the required CSV file, which should include columns for `language` and `comment`.",2025-02-09T07:17:07.729500+00:00
"context/get_extension_example.py","The provided code snippet demonstrates the usage of a function named 'get_extension' imported from the 'pdd' module. This function is designed to return the file extension associated with various programming languages. Three example calls to the function are included, showcasing its functionality. The first call, with the argument 'Python', returns the output '.py', indicating the standard file extension for Python scripts. The second call uses 'Makefile' as an argument, which returns 'Makefile', reflecting that Makefiles do not have a specific extension. The third example, with 'JavaScript', returns '.js', the common file extension for JavaScript files. Overall, the snippet serves as a simple demonstration of how to retrieve file extensions for different programming languages using the 'get_extension' function.",2025-02-09T07:17:10.534224+00:00
"context/get_jwt_token_example.py","The provided code is an asynchronous Python script that facilitates authentication with Firebase using GitHub's Device Flow. It imports necessary modules and defines constants for the Firebase API key, GitHub client ID, and application name. The main function initiates the authentication process, attempting to retrieve a Firebase ID token through the `get_jwt_token` function. It handles various exceptions, including authentication errors, network issues, token errors, and rate limits, providing user-friendly messages for each case. Upon successful authentication, the script updates a `.env` file with the new JWT token, ensuring that the token is either replaced if it already exists or added if it does not. The script is designed to be run as a standalone application, utilizing the asyncio library to manage asynchronous operations.",2025-02-09T07:17:13.397627+00:00
"context/get_language_example.py","This Python script demonstrates the usage of a 'get_language' function that determines the programming language associated with a file extension. The code consists of a main function that serves as an example implementation. Inside the main function, it defines a sample file extension '.py' and attempts to get the corresponding programming language using the imported get_language function. The code includes error handling through a try-except block to catch any potential exceptions during execution. If a language is successfully identified, it prints the file extension and its associated programming language. If no language is found, it displays a message indicating that no programming language was found for the given extension. The script uses type hints for better code readability and follows Python's standard if __name__ == '__main__' pattern for script execution. The code is well-structured and includes proper documentation through docstrings.",2024-12-07T01:53:09.050133+00:00
"context/git_update_example.py","This Python script demonstrates the usage of a 'git_update' function from the 'pdd.git_update' module. The script loads an input prompt from a file named 'prompts/fix_error_loop_python.prompt', then passes it along with a target code file path ('pdd/fix_error_loop.py') to the git_update function. Additional parameters include strength (0.97) and temperature (0) for the language model. Upon successful execution, the script prints the modified prompt, the cost of the operation, and the model name used. It then saves the updated prompt back to the original file. The script includes error handling for both specific ValueError exceptions and generic errors. The function appears to be part of a system that uses language models to automatically update code based on text prompts, likely for automated code modification or generation purposes.",2025-04-05T08:29:48.283959+00:00
"context/increase_tests_example.py","This file demonstrates the usage of the 'increase_tests' function from the 'pdd.increase_tests' module. The code showcases how to generate additional unit tests to improve test coverage for existing code. The file contains an 'example_usage' function that illustrates both basic and advanced usage patterns. It sets up sample inputs including a simple function that calculates the average of a list of numbers, a basic unit test for this function, a coverage report showing 60% coverage, and the original prompt that generated the code. The example then calls 'increase_tests' twice: first with minimal required parameters and then with additional customization options including language specification, strength (0.97), temperature (0.0), and verbose output. The function captures and displays the generated tests, the cost of generating them, and the model used. The code includes error handling for both validation errors and unexpected exceptions. The file is designed to run the demonstration when executed directly as a script, serving as a practical example of how to leverage the test generation functionality.",2025-04-05T08:29:48.295833+00:00
"context/insert_includes_example.py","This Python file demonstrates the usage of an insert_includes module for processing dependencies. The main function, example_usage(), showcases how to set up and execute dependency processing with error handling. The code initializes a Rich console for formatted output and defines input parameters including a prompt for a Python function that handles CSV files, a directory path for example files, and a CSV filename for dependency information. The insert_includes function is called with specific parameters like strength and temperature to control the output. The code then displays the results including the original prompt, modified prompt with dependencies, CSV output, model name, and total cost. The processed CSV output is saved to a file. Error handling is implemented for common issues like missing files or processing errors. The code uses the Rich library for enhanced console output formatting and the pathlib library for file operations. This appears to be part of a larger system for managing and processing project dependencies with AI model integration.",2024-12-07T22:11:27.202537+00:00
"context/install_completion_example.py","This Python script serves as an example for installing shell completion for the PDD command-line interface (CLI). It demonstrates the use of functions from the pdd package, specifically `get_local_pdd_path()` to retrieve the absolute path to the PDD_PATH directory and `install_completion()` to set up shell completion. The script creates a controlled environment by setting dummy environment variables and generating necessary files without affecting the user's actual configuration. It forces the shell to bash, creates a dummy home directory, and a PDD_PATH directory, where it generates a sample completion script and a bash RC file. The main function orchestrates the setup and execution, providing feedback through the Rich library for better console output. The example concludes with instructions on how to run the script, ensuring users can replicate the setup safely.",2025-02-09T07:17:19.675230+00:00
"context/langchain_lcel_example.py","This Python file demonstrates comprehensive LangChain integration with various LLM providers and features. It showcases how to use multiple AI models including OpenAI, Google's Gemini, Claude, Azure OpenAI, Fireworks, Groq, Together.ai, Deepseek, and Ollama. The code illustrates modern LangChain Expression Language (LCEL) patterns using pipe operators for chain construction, structured output parsing with JSON and Pydantic models, custom callback handlers for tracking completion status and token usage, and caching mechanisms to improve performance. The file contains numerous examples of prompt templates, model configuration, fallback strategies, and specialized features like Claude's thinking tokens. Throughout the examples, a joke-generation use case demonstrates how different models handle the same prompt with various output formats and configurations.",2025-03-09T19:49:10.864511+00:00
"context/llm_invoke_example.py","This Python script demonstrates the usage of the `llm_invoke` function for generating content using language models with different parameters. The code showcases two primary use cases: generating unstructured text output and structured output using Pydantic models. It defines a `Joke` class with setup and punchline fields for structured output validation. The main function iteratively tests different 'strength' values (ranging from 0.0 to 1.0 in 0.005 increments) to show how this parameter affects model selection, output quality, and cost. For each strength value, the script generates jokes about programmers (unstructured) and data scientists (structured), tracking which model is used at different strength thresholds. The code captures and displays the result, cost, and model name for each invocation. When using structured output, it properly parses the returned JSON into the Pydantic model. Finally, the script summarizes the strength ranges associated with each model that was used during execution, providing insights into the relationship between the strength parameter and model selection.",2025-03-09T19:49:27.119243+00:00
"context/llm_selector_example.py","The provided code is a Python script that demonstrates the usage of a function called `llm_selector` from the `pdd.llm_selector` module. The main function initializes parameters for a language model, specifically `strength` and `temperature`, which influence the model's behavior. A while loop iterates as long as the `strength` is less than or equal to 1.1, incrementing it by 0.05 in each iteration. Within the loop, the `llm_selector` function is called with the current `strength` and `temperature`, returning details about the selected language model, including its name, input and output costs per million tokens. The script also includes an example of counting tokens in a sample text using a `token_counter` function. Error handling is implemented to catch potential `FileNotFoundError` and `ValueError` exceptions. Overall, the script serves as a practical example of how to select and utilize different language models based on specified parameters.",2025-02-09T21:05:38.634023+00:00
"context/load_prompt_template_example.py","This Python script demonstrates a simple program that loads and displays a prompt template. The code imports two functions: 'load_prompt_template' from a custom module 'pdd.load_prompt_template' and 'print' from the 'rich' library, which provides rich text formatting capabilities. The main function defines a variable 'prompt_name' set to 'generate_test_LLM' (presumably the name of a prompt template file without its extension), loads the template using the imported function, and then displays it with blue-colored formatting if the template is successfully loaded. The script uses a standard Python idiom with the '__name__ == __main__' check to ensure the main function only runs when the script is executed directly. This appears to be part of a larger system dealing with prompt templates, possibly for working with Large Language Models (LLMs).",2024-12-07T01:53:09.302428+00:00
"context/postprocess_0_example.py","This file provides a comprehensive example of how to use the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The example demonstrates processing output from a language model (LLM) that contains multiple code sections in different programming languages. The main example shows how to extract and process Python code specifically, though the function can work with other languages. The code includes a sample LLM output containing Python and Java code blocks, along with regular text. The example demonstrates how to use the function to process this mixed content, focusing on extracting Python code while commenting out other content. The documentation section clearly outlines the input parameters (`llm_output` and `language`) and explains that the function returns a string with only the largest section of the specified language's code uncommented, while other content is commented out. The file concludes with a note about ensuring proper implementation of supporting functions (`get_comment`, `comment_line`, and `find_section`) for the example to work correctly.",2024-12-07T01:53:09.330350+00:00
"context/postprocess_example.py","This Python script demonstrates the usage of a postprocessing function designed to extract code from mixed text and code output typically generated by Large Language Models (LLMs). The main function contains an example LLM output that includes a factorial function implementation along with usage examples, all embedded within markdown-style code blocks. The script uses the 'postprocess' function from a 'pdd.postprocess' module to extract the pure code from this mixed output. The script also utilizes the 'rich' library for enhanced console output formatting. The postprocessing function takes several parameters including the target programming language (python), model strength (0.7), and temperature (0.2). The script outputs the extracted code, the total cost of the operation, and the model name used. The output is formatted with color coding using the rich console library's formatting capabilities. This appears to be a demonstration or utility script that might be part of a larger toolkit for handling LLM-generated code outputs.",2024-12-07T01:53:09.359769+00:00
"context/postprocessed_runnable_llm_output.py","The provided code outlines the implementation of a function named `context_generator`, which is designed to read a specified Python file, preprocess its content, generate a prompt for a language model (GPT-4), and write the output to a designated file. The function takes three parameters: the name of the Python file to read, the name of the output file, and an optional boolean `force` parameter. The function begins by attempting to open and read the specified Python file. If the file is not found, it prints an error message and returns `False`. After reading the file, it is expected to preprocess the content (though the actual preprocessing function is not included in the snippet). A prompt is then generated, instructing the model to create a concise usage example based on the provided Python code. The function concludes with a usage example, demonstrating how to call `context_generator` with appropriate arguments. Overall, the code serves as a template for generating usage examples from Python code using a language model.",2025-02-09T07:17:29.125824+00:00
"context/preprocess_example.py","This Python script demonstrates the usage of a preprocessing utility for structured text. It imports a 'preprocess' function from 'pdd.preprocess' and initializes a Rich console for formatted output. The core of the script defines a multi-line prompt string containing various custom XML-like tags (<shell>, <pdd>, <web>), placeholder variables ({test}, {test2}), and references to external files. The script then configures processing parameters including 'recursive' (set to False), 'double_curly_brackets' (set to True), and 'exclude_keys' (a list containing 'test2'). After displaying debug information about the excluded keys, the script calls the preprocess function with the prompt and configuration parameters, then prints the processed result. The preprocessing appears to handle special tags and transform placeholders, likely converting them to double curly bracket format except for explicitly excluded keys. This example demonstrates how to prepare structured text with embedded commands, comments, and variables for further processing in a larger system.",2025-04-05T08:29:48.312674+00:00
"context/preprocess_main_example.py","The provided Python script defines a command-line interface (CLI) using the Click library for preprocessing prompt files. The main function, `cli`, includes several options for customization, such as specifying the input prompt file (`--prompt-file`), output file path (`--output`), enabling XML tagging (`--xml`), recursive processing (`--recursive`), doubling curly brackets (`--double`), excluding specific keys from doubling (`--exclude`), and enabling verbose logging (`--verbose`). The script utilizes the `preprocess_main` function from the `pdd.preprocess_main` module to handle the preprocessing logic. It processes the input prompt file based on the provided options, applies transformations like XML tagging or bracket doubling, and outputs the results. The script also includes error handling to catch and display exceptions during execution. Additionally, it provides feedback to the user, such as the processed prompt, total cost, and model name. The CLI is designed to be flexible and user-friendly, allowing users to customize the preprocessing workflow for prompt files efficiently. The script is executed as a standalone program when run directly.",2025-01-01T07:18:16.216462+00:00
"context/process_csv_change_example.py","This Python code demonstrates the usage of a function called 'process_csv_change' from a module named 'pdd.process_csv_change'. The script is designed to process changes in code files based on CSV input. It initializes several parameters including the CSV file path, strength (0.8), temperature (0), code directory path, programming language (Python), file extension (.py), and a maximum budget of $10.00. The main function call is wrapped in a try-except block for error handling. If successful, it prints the total cost of changes, the AI model used, and displays modified prompts for each affected file. The code includes error handling for file not found scenarios and unexpected errors. The function appears to be part of a larger system that automates code modifications, possibly using AI models, while maintaining budget constraints and tracking costs.",2024-12-07T01:53:09.415833+00:00
"context/pytest_example.py","This Python script implements a test result collection and reporting system using pytest. The main component is the TestResultCollector class, which tracks test failures, errors, and warnings during test execution. The class includes methods to capture test outcomes through pytest hooks (pytest_runtest_logreport and pytest_sessionfinish) and manages log capturing by redirecting stdout and stderr. The run_pytest() function serves as the main execution point, creating a TestResultCollector instance and running pytest on a specific test file ('tests/test_get_extension.py'). The function returns the count of failures, errors, warnings, and captured logs. The script includes detailed debug logging throughout its execution, particularly in the main block where it prints the current working directory and various execution stages. The implementation demonstrates a comprehensive approach to test result collection and logging, making it useful for automated testing scenarios where detailed test execution information needs to be captured and analyzed.",2025-02-21T17:29:36.156758+00:00
"context/pytest_output_example.py","This Python script demonstrates various ways to use the pytest_output module through different examples. The code creates and manipulates test files to showcase different testing scenarios. The main_example() function contains six distinct examples:

1. Demonstrates using the main function with argparse by simulating command-line arguments
2. Shows how to use run_pytest_and_capture_output and save_output_to_json functions
3. Tests handling of non-existent test files
4. Demonstrates handling non-Python test files
5. Shows behavior with empty test files containing no tests
6. Provides an advanced example using TestResultCollector directly

The script includes helper functions like create_dummy_test_file() to create test files with specified content. It uses various Python modules including argparse, json, pytest, and rich for console output formatting. The example test cases include passing tests, failing tests, error cases, and tests with warnings. The script also demonstrates file handling, JSON output generation, and log capturing functionality.",2025-02-21T17:29:40.430019+00:00
"context/split_example.py","The provided Python script demonstrates the use of the 'split' function from the 'pdd.split' module. It begins by importing necessary libraries, including 'os' for environment variable management and 'rich.console' for enhanced console output. The main function sets the 'PDD_PATH' environment variable to the parent directory of the script's location. It defines an input prompt asking for a Python function to calculate the factorial of a number, along with a sample implementation of the factorial function and an example usage. The script then calls the 'split' function with these inputs, along with parameters for 'strength' and 'temperature', which likely influence the output's creativity and variability. The results, including the sub prompt, modified prompt, model name, and total cost of the operation, are printed to the console in a formatted manner. The script is wrapped in a try-except block to handle potential errors gracefully, providing feedback in case of exceptions. Overall, the script serves as a practical example of how to utilize the 'split' function for generating and modifying code prompts.",2025-02-09T08:10:57.880306+00:00
"context/split_main_example.py","The provided Python script, `example_split_usage.py`, is a command-line interface (CLI) tool that utilizes the Click library to facilitate the splitting of input prompts and code files. It imports the `split_main` function from a module named `pdd.split_main`, which is presumably responsible for the core functionality of splitting prompts. The script defines a command `split_cli` that requires three input files: an input prompt file, a generated code file, and an example code file. It also includes optional parameters for output file paths, a force overwrite flag, and a quiet mode to suppress console output. The command allows users to customize settings such as the strength of the split and randomness through temperature. Upon execution, it attempts to call the `split_main` function with the provided parameters and handles any exceptions that may arise, displaying relevant output if not in quiet mode. The script concludes by defining a top-level CLI group and adding the `split_cli` command to it, making it executable as a standalone program.",2025-02-09T17:23:23.548186+00:00
"context/summarize_directory_example.py","The provided code is a Python script that demonstrates the usage of a module called `summarize_directory`. The script's main function performs several tasks: it creates a sample CSV string containing existing summaries of Python files, then it calls the `summarize_directory` function to summarize Python files located in a specified directory (matching the pattern 'context/c*.py'). The function parameters include options for model strength, temperature, and verbosity, as well as the existing CSV content. After summarizing, the script prints the generated CSV content, the total cost of the summarization process, and the model used. It also ensures that an output directory exists and saves the generated CSV output to a file named 'output.csv' within that directory. The script is wrapped in a try-except block to handle any potential errors gracefully. Overall, this script serves as an example of how to summarize Python files in a directory and manage the output effectively.",2025-04-15T07:06:00.297436+00:00
"context/tiktoken_example.py","This code snippet demonstrates the use of the tiktoken library for token counting in text processing. The code first imports the tiktoken library, then creates an encoding object using the 'cl100k_base' encoding scheme (which is commonly used with newer GPT models). Finally, it counts the number of tokens in a variable called 'preprocessed_prompt' by encoding the text and measuring the length of the resulting encoded sequence. The code is particularly useful for applications that need to track token usage when working with language models, as it provides a way to count tokens before sending text to an API. The 'cl100k_base' encoding is specifically mentioned, though the comment indicates other encoding options are available. This is a common pattern when working with OpenAI's GPT models, where understanding token count is important for both cost management and staying within model context limits.",2024-12-07T01:53:09.500027+00:00
"context/trace_example.py","This Python script demonstrates the usage of a 'trace' function from the 'pdd.trace' module. The main function serves as an example implementation that shows how to use the trace functionality. The script includes imports for required modules, including 'os' and 'rich.console' for enhanced console output. Within the main function, it sets up example inputs including a sample code file containing a 'hello_world' function, a specific code line number to trace, a prompt file with instructions, and parameters for LLM (Language Learning Model) settings like strength and temperature. The script then calls the trace function with these inputs and handles the output, displaying the corresponding prompt line, total cost, and model name used. Error handling is implemented through try-except blocks to catch potential FileNotFoundError, ValueError, and other unexpected exceptions. The output is formatted using the rich console library for better readability with colored text. The script is structured to run the main function when executed directly rather than being imported as a module.",2025-02-16T18:21:12.368020+00:00
"context/trace_main_example.py","The provided code snippet demonstrates how to create a simple calculator using Python. It includes a prompt that outlines the requirements for a function that adds two numbers together. The function, named `add_numbers`, takes two float inputs, adds them, and returns the result. The code also includes a test case that calls this function with the numbers 5.0 and 3.0, printing the sum to the console. Additionally, the script utilizes the Click library to set up a command-line context for tracing the execution of the code. It configures parameters such as verbosity, file overwrite options, analysis strength, and randomness for a language model (LLM) analysis. The `trace_main` function is called to analyze the code, with the results being saved to a specified output file. The script handles exceptions gracefully, providing error messages if any issues arise during execution. Overall, this code serves as a practical example of integrating user prompts, function definitions, and command-line interfaces in Python.",2025-02-09T07:17:34.415338+00:00
"context/track_cost_example.py","This Python file implements a Command-Line Interface (CLI) tool called PDD using the Click library. The tool is designed for processing prompts and generating outputs with cost tracking functionality. The code consists of two main components: a base CLI group and a 'generate' command. The CLI accepts an optional '--output-cost' parameter to track costs and save usage details to a CSV file. The 'generate' command requires a prompt file input and has an optional output file parameter. It's decorated with a 'track_cost' decorator for cost monitoring. The command reads a prompt from the input file, processes it (currently using placeholder logic), and either writes the output to a specified file or prints it to the console. The function returns a tuple containing the generated output, the cost of execution (simulated at $0.05 per million tokens), and the model name (set to 'gpt-4'). The file includes proper type hints, documentation, and error handling through Click's built-in functionality. The main execution block demonstrates example usage of the CLI with specific command-line arguments.",2024-12-07T01:53:09.597187+00:00
"context/unfinished_prompt_example.py","This Python file demonstrates the usage of an 'unfinished_prompt' function from a 'pdd' module. The code consists of a main function that analyzes the completeness of a given text prompt using a Language Learning Model (LLM). The example uses a simple fairy tale beginning ('Once upon a time...') as the test prompt. The function takes several parameters including the prompt text, model strength (0.5), and temperature (0.0). The unfinished_prompt function returns four values: reasoning, whether the prompt is finished, the total cost of the analysis, and the model name used. The code includes error handling through a try-except block and is structured to run the main function only when the file is executed directly. This appears to be a demonstration or testing script for a larger prompt analysis system, likely used to determine if a given text prompt is complete or needs additional content.",2024-12-07T01:53:09.626009+00:00
"context/unrunnable_raw_llm_output.py","The provided content outlines the implementation of a Python function named `context_generator`. This function is designed to read a specified Python file, preprocess its content, generate a prompt for a model (specifically GPT-4), and write the output to a designated file. The implementation includes error handling for cases where the specified file does not exist, returning `False` in such instances. The function constructs a prompt that asks for a concise example of how to use the module based on the provided Python code. The usage of the function is demonstrated with an example call, indicating how to specify the input and output file names. Overall, the content serves as a guide for creating a utility that aids in generating usage examples for Python modules.",2025-02-09T07:17:41.579824+00:00
"context/update_main_example.py","The provided Python script demonstrates the usage of a CLI (Command Line Interface) tool built using the Click library. The script defines a command named 'update' that facilitates updating a prompt based on modified code. It accepts various options such as paths to input prompt files, modified code files, and optionally original code files. Users can also specify parameters like 'strength' (to control how strongly the model incorporates changes), 'temperature' (for randomness in text generation), verbosity, and whether to use Git history instead of an input code file. The script integrates with a function named 'update_main' from the 'pdd.update_main' module, which performs the core update operation. The results, including the updated prompt snippet, total cost, and model name, are displayed unless suppressed by the 'quiet' flag. Additional options include overwriting files ('force') and enabling verbose logging. The script also supports grouping commands using Click's 'group' functionality, allowing for future extensibility. Overall, the script serves as an example of how to build a flexible and user-friendly CLI for managing and updating text prompts based on code changes.",2025-01-01T07:18:31.507257+00:00
"context/update_prompt_example.py","The provided Python script demonstrates the usage of the `update_prompt` function from the `pdd.update_prompt` module. The script defines a `main` function that sets up input parameters, including an input prompt, input code, modified code, and parameters like strength and temperature for a language model. It then calls the `update_prompt` function with these parameters. The function is expected to return a modified prompt, the total cost of the operation, and the model name. If successful, the script prints these results; otherwise, it handles exceptions and prints an error message. The script is designed to be executed as a standalone program, with the `main` function being called when the script is run directly.",2024-12-21T03:10:40.913389+00:00
"context/xml_tagger_example.py","This Python script demonstrates the usage of an XML tagging functionality using a custom module 'pdd.xml_tagger'. The code imports the 'xml_tagger' function and the 'rich' library for enhanced console printing. The script takes a raw text prompt ('Write a story about a magical forest') and processes it with two parameters: 'strength' (set to 0.5, representing base model performance) and 'temperature' (set to 0.7, controlling output randomness). The code is wrapped in a try-except block for error handling. When successful, it outputs the XML-tagged version of the prompt, the total cost of processing, and the name of the model used. The output is formatted using rich's printing capabilities with color coding (green for success, red for errors) and bold text formatting. This appears to be a demonstration or example implementation of an AI-powered text processing tool that adds XML markup to input text.",2024-12-07T01:53:09.681130+00:00
