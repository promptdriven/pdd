full_path,file_summary,date
"context/DSPy_example.py","This code snippet demonstrates the initial setup and implementation of a DSPy-based application focused on example selection. The code begins with importing necessary DSPy components and configuring OpenAI's GPT-3.5-turbo-instruct model as the language model backend, with a maximum token limit of 250. The core of the implementation is a 'chainofthought' class that inherits from dspy.Module. This class is designed to implement a chain-of-thought reasoning process, with a basic structure that includes initialization and a forward method to process questions. The class uses DSPy's ChainOfThought feature to map questions to answers. However, the code appears to be incomplete, as indicated by empty spaces and an unfinished comment about compilation and optimization. The code represents a basic framework for implementing chain-of-thought reasoning in a DSPy environment, though it needs additional implementation details to be fully functional.",2024-12-07T01:53:08.162193+00:00
"context/anthropic_counter_example.py","This Python code demonstrates how to count tokens in a text string using the Anthropic API. The code begins by importing the Anthropic library and initializing an Anthropic client. It then defines a sample text string and uses the client's count_tokens method to calculate the number of tokens in that text. The result is printed to the console. However, there's an important note in the comments indicating that this token counting method is not accurate for Anthropic models version 3.0 and above. The code is straightforward and serves as a basic example of token counting functionality, which is often useful when working with language models to stay within token limits or estimate API costs.",2024-12-07T01:53:08.238576+00:00
"context/auto_deps_main_example.py","The provided Python script is a command-line tool implemented using the Click library. It defines a command named 'auto-deps' that facilitates dependency analysis and processing of prompt files. The script includes several command-line options, such as '--force' to overwrite files, '--quiet' to suppress output, '--strength' and '--temperature' to configure dependency analysis parameters, and paths for input prompt files, directories, and output files. The main function initializes the Click context with these options and invokes the 'auto_deps_main' function from the 'pdd.auto_deps_main' module. This function processes the specified prompt file, analyzes dependencies, and generates a modified prompt file. The script also calculates and displays the total cost of the operation and the model used. Error handling is implemented to catch exceptions and abort execution if necessary. The script is designed to be executed as a standalone program.",2024-12-13T21:07:14.439152+00:00
"context/auto_include_example.py","The provided Python script demonstrates the usage of the auto_include function from the pdd.auto_include module. The script is designed to process project dependencies and generate unit tests using a language model (LLM) via Langchain. It begins by loading a CSV file named project_dependencies.csv and defining an input prompt that specifies the requirements for generating unit tests. The input prompt includes details about the inputs, outputs, and steps involved in the process, such as preprocessing the prompt, creating a Langchain LCEL template, selecting an LLM model, and running the inputs through the model. The script also includes steps for handling incomplete generations, postprocessing the output, and calculating the total cost of the operation. Key parameters like strength, temperature, and verbosity are defined for the auto_include function. The function is then called with the specified parameters, and the results, including dependencies, CSV output, total cost, and model name, are printed. The script is structured with a main function and is intended to be executed as a standalone program.",2025-01-01T07:17:23.527000+00:00
"context/auto_update_example.py","The file provides an example of how to use the 'auto_update' function from the 'pdd.auto_update' module. It includes a Python script that demonstrates various ways to utilize the function. The 'auto_update' function is designed to check the current installed version of a package, compare it with the latest version (either from PyPI or a specified version), and prompt the user to upgrade if a newer version is available. If the user confirms, the function performs the upgrade using pip. The script showcases basic usage for checking updates for the 'pdd' package, checking updates for a specific package like 'requests,' and comparing against a known version, such as 'pandas' version 2.0.0. The script is structured with a 'main' function that encapsulates these examples, and it is executed when the script is run directly. The file serves as a guide for implementing and understanding the 'auto_update' function's capabilities.",2025-01-01T07:17:26.468121+00:00
"context/autotokenizer_example.py","This Python code defines a function called 'count_tokens' that calculates the number of tokens in a given text using a specified language model's tokenizer. The function uses the Hugging Face transformers library, specifically the AutoTokenizer class. By default, it uses the 'deepseek-ai/deepseek-coder-7b-instruct-v1.5' model, but this can be changed through the model_name parameter. The function works by first loading the appropriate tokenizer, then tokenizing the input text, and finally returning the length of the resulting input_ids. The code includes a simple example that demonstrates how to use the function by counting tokens in the text 'Write a quick sort algorithm in Python.' This utility is particularly useful for working with language models where understanding token count is important for managing context windows and input limitations.",2024-12-07T01:53:08.299472+00:00
"context/bug_main_example.py","The provided Python script demonstrates how to use the 'bug_main' function to generate unit tests based on the observed and desired outputs of a program. It utilizes the Click library for command-line interface management and the Rich library for enhanced output formatting. The script begins by setting up a Click context with options for file handling and model parameters. It then creates an output directory and writes example prompt, code, and program files that define a function to sum even numbers from a list. The script also includes current and desired outputs, highlighting a bug in the function that fails to handle empty lists correctly. The 'bug_main' function is called to generate a unit test that addresses this issue, and the results, including the generated unit test, total cost, and model name, are printed to the console. This example serves as a practical guide for developers looking to automate unit test generation and improve code reliability.",2025-02-09T07:16:12.520706+00:00
"context/bug_to_unit_test_example.py","This Python script demonstrates the usage of a bug_to_unit_test function for automated test generation. The main function serves as the entry point and orchestrates the process of generating unit tests from bug reports. The script reads input from three different files: a prompt file (from 'prompts' directory), a code file (from 'pdd' directory), and a context file (from 'context' directory). It processes two types of outputs (current and desired) that contain debug information and JSON-formatted explanations. The function takes several parameters including the outputs, prompt, code under test, program to run the code, strength (0.9), temperature (0), and language ('Python'). The script uses the Rich library for enhanced console output formatting, displaying the generated unit test, total cost, and model name used. Error handling is implemented to catch and display any exceptions that occur during execution. The code appears to be part of a larger system for automated testing and debugging, possibly using AI/LLM models given the presence of temperature and strength parameters.",2024-12-07T01:53:08.330225+00:00
"context/bug_to_unit_test_failure_example.py","This code snippet contains a Python function named 'add' that takes two parameters 'x' and 'y'. However, there appears to be a logical error in the implementation. Despite the function name suggesting addition, the function actually performs subtraction, returning 'x-y' instead of 'x+y'. This is likely a bug that needs to be fixed to match the intended functionality implied by the function name. The code is very simple, consisting of just two lines: the function definition and the return statement. The indentation suggests this might be part of a larger codebase, but only this single function is shown in the provided content.",2024-12-07T01:53:08.365017+00:00
"context/change_example.py","The provided Python script demonstrates the use of a `change` function from the `pdd.change` module. It includes a `main` function that sets up environment variables, defines input parameters, and calls the `change` function. The script outlines an example scenario where a function to calculate the factorial of a number is modified based on a prompt to take the square root of the factorial output. Parameters such as `strength` and `temperature` are used to control the behavior of the `change` function. The script also handles potential exceptions and prints the modified prompt, total cost, and model name using the `rich.console` module for formatted output. The `main` function is executed when the script is run directly.",2024-12-21T03:10:15.598748+00:00
"context/change_main_example.py","The provided Python script demonstrates the usage of the 'change_main' function from the 'pdd' command-line program, showcasing both single-change and CSV batch-change modes. It begins by importing necessary modules and setting up a Click context for command-line options. The script outlines prerequisites for running the 'change_main' function, including the availability of the module and required packages. In single-change mode, it creates sample input files, modifies a function to add error handling for division by zero, and displays the modified output along with the total cost and model used. The script also illustrates CSV batch-change mode by creating multiple sample code files and a CSV file that specifies changes for batch processing. It then calls 'change_main' in this mode, displaying the results and total cost. The script emphasizes the importance of setting up directories and files correctly for both modes, and it utilizes the 'rich' library for formatted output. Overall, the script serves as a practical example for users looking to implement changes in Python code using the 'pdd' tool.",2025-02-09T07:16:17.092974+00:00
"context/cli_example.py","The provided Python script demonstrates the usage of the 'generate' command from the PDD CLI module to create runnable code from a specified prompt file. It begins by disabling auto-update through an environment variable and defines a main function that outlines the process of code generation. The script assumes that necessary environment variables are set and required packages are installed. It specifies the input prompt file and the output file where the generated code will be saved. The script sets command line arguments for the generation process, including parameters for the AI model's strength and temperature, and forces generation without user confirmation. The CLI command is executed within a try-except block to handle potential errors gracefully. If successful, the generated code is saved to the designated output file, and the total cost and model name used for generation are printed to the console. The script is designed to be run as a standalone program.",2025-02-09T07:16:24.946452+00:00
"context/click_example.py","This Python script implements a command-line image processing tool using Click and Pillow (PIL) libraries. It follows a Unix-like pipe pattern where multiple image processing commands can be chained together. The script includes various image manipulation functions such as opening, saving, displaying, resizing, cropping, transposing (rotation and flipping), blurring, smoothening, embossing, sharpening, and pasting images. Each command is implemented as a Click command group with specific options and parameters. The script uses decorator patterns (@processor and @generator) to handle the command chain processing. Key features include:

1. Command chaining functionality similar to Unix pipes
2. Support for multiple input images
3. Various image transformation options
4. Error handling for image operations
5. Flexible file naming for output
6. Preservation of image filenames through transformations

The script is well-documented with docstrings and comments, making it clear how to use each command and what it does. Example usage shows commands can be combined like 'imagepipe open -i example.jpg resize -w 128 display' to process images in sequence.",2024-12-07T01:53:08.509553+00:00
"context/cloud_function_call.py","The provided code is a Python script that demonstrates how to call a Google Cloud Function using the requests library. It defines a function named `call_cloud_function` that takes a Firebase token as an argument. The function constructs a request to a specified Cloud Function URL, including the Firebase token in the Authorization header to authenticate the request. The URL is hardcoded as 'https://us-central1-prompt-driven-development.cloudfunctions.net/on_request_example'. After making the GET request, the function returns the JSON response from the Cloud Function. The script also includes a sample Firebase token, which is a long string used for authentication. Finally, the script calls the `call_cloud_function` with the provided token and prints the result. This code is useful for developers looking to integrate Firebase authentication with Google Cloud Functions.",2025-02-09T07:16:28.890615+00:00
"context/cmd_test_main_example.py","The file 'test_cli_example.py' provides an example of integrating the 'cmd_test_main' function into a Click-based command-line interface (CLI) for generating or enhancing unit tests. It includes a detailed explanation of its usage, required and optional arguments, and the expected output. The script defines a CLI with global options such as verbosity, AI generation strength, temperature, force overwrite, and quiet mode. It also includes a 'test' command that accepts arguments like 'prompt_file' (path to a text file), 'code_file' (path to the source code), and optional parameters such as output path, programming language override, coverage report, existing tests, target coverage percentage, and a merge flag. The 'test' command invokes the 'cmd_test_main' function to generate or enhance unit tests, returning the generated test code, total cost, and model name. The script demonstrates how to run the CLI from the command line with examples and provides flexibility for customizing test generation. It uses the Click library for CLI creation and ensures shared context across commands. The script is designed for developers looking to automate or improve their unit testing process using AI-driven tools.",2025-01-01T07:17:35.334047+00:00
"context/code_generator_example.py","The provided Python script demonstrates the usage of a function called 'code_generator' from the 'pdd.code_generator' module. The script's main function reads a prompt from a file ('prompts/generate_test_python.prompt') and uses it to generate Python code with the help of a language model. Key parameters for the 'code_generator' function include the programming language ('python'), model strength (0.5), temperature (0.0), and verbosity (True). The script attempts to generate runnable code, calculate the total cost, and identify the model name. If successful, it prints the generated code, cost, and model name. In case of an error, it catches and prints the exception. The script is designed to be executed as a standalone program with the main function serving as the entry point.",2024-12-21T03:10:19.903389+00:00
"context/code_generator_main_example.py","The provided code is a Python script that utilizes the Click library to create a command-line interface (CLI) for generating code based on a specified prompt. It sets up a Click context with parameters such as 'strength', 'temperature', 'force', and 'quiet' to control the behavior of the AI model used for code generation. The script defines paths for a prompt file and an output file, where the prompt file contains instructions for generating a Python function that retrieves a JSON Web Token (JWT). The code generation is executed by calling the 'code_generator_main' function, which takes the Click context, prompt file, and output file as arguments. After the code is generated, the script prints the generated code, the total cost of the operation, the model used for generation, and the location where the code is saved. The script also includes commented-out sections for creating a prompt file and output directory, indicating that it is designed for flexibility in generating various code snippets.",2025-02-09T07:16:32.177765+00:00
"context/comment_line_example.py","This file provides documentation and examples for a Python function called `comment_line` that handles code commenting across different programming languages. The function takes two parameters: `code_line` (the line to be commented) and `comment_characters` (the commenting syntax to use). The function supports three commenting styles: single-character comments (like Python's #), paired comments (like HTML's <!-- -->), and line deletion (using 'del'). The documentation includes the function's definition with detailed parameter descriptions and return value information. The file also contains practical examples demonstrating all three use cases: commenting a Python print statement, commenting an HTML tag, and deleting a line of code. Each example is accompanied by its expected output. The function is designed to be flexible and can accommodate different programming language commenting syntaxes by appropriately formatting the input line based on the specified comment characters.",2024-12-07T01:53:08.574892+00:00
"context/conflicts_in_prompts_example.py","The provided Python script demonstrates the use of a function named `conflicts_in_prompts` to detect and resolve conflicts between two example prompts. The script imports necessary modules, including `conflicts_in_prompts` from `pdd.conflicts_in_prompts` and `print` from the `rich` library for formatted output. The main function defines two detailed prompts: one for creating an `auth_helpers.py` module for Firebase authentication and another for designing a `User` class for the PDD Cloud platform. Each prompt includes specific requirements, such as functionality, dependencies, error handling, and coding standards. The script sets parameters like `strength`, `temperature`, and `verbose` for the conflict detection process. It then calls the `conflicts_in_prompts` function with the prompts and parameters, capturing the suggested changes, total cost, and model name. The results are displayed using formatted output, highlighting any detected conflicts and proposed changes. If no conflicts are found, a corresponding message is printed. The script is structured to showcase the functionality of the `conflicts_in_prompts` function while adhering to Python best practices, including type hints, docstrings, and modular design.",2024-12-21T03:10:21.724675+00:00
"context/conflicts_main_example.py","The provided code snippet is a Python script that utilizes the Click library and a custom function from the 'pdd.conflicts_main' module. It begins by defining two prompt files, 'prompt1_LLM.prompt' and 'prompt2_LLM.prompt', which contain instructions for an AI assistant and a chatbot, respectively. The script then creates these prompt files and writes the corresponding text into them. A mock context class, 'MockContext', is defined to simulate a Click context, which includes attributes like 'force', 'quiet', 'strength', and 'temperature'. The script then calls the 'conflicts_main' function with the mock context and the two prompt files, capturing the output in 'conflicts_output.csv'. It prints the number of conflicts, the total cost associated with the operation, and the model used for processing. The results are intended to be saved in a CSV file for further analysis. Overall, the script demonstrates how to set up prompts for an AI model and handle conflicts in a structured manner.",2025-02-09T07:16:35.220463+00:00
"context/construct_paths_example.py","The provided code is a Python script that demonstrates the usage of a function called `construct_paths` from the `pdd.construct_paths` module. The script defines a `main` function that orchestrates the process of setting up input parameters, invoking the `construct_paths` function, and handling its output. It begins by specifying input file paths for code and prompt files, followed by defining command options such as output settings. The script attempts to call `construct_paths` with these parameters, capturing the input strings, output file paths, and the programming language used. It includes error handling to manage any exceptions that may arise during the function call. The script repeats this process for different sets of input file paths and command options, demonstrating its flexibility in generating outputs based on various prompts. The final part of the script constructs file paths for a regression test prompt and generates corresponding outputs. Overall, the script serves as a practical example of how to utilize the `construct_paths` function for file management and output generation in a Python environment.",2025-02-09T07:16:38.839339+00:00
"context/context_generator_example.py","The file is a Python script that utilizes a context generation function from the 'pdd.context_generator' module to generate example code based on specified parameters. It begins by checking if the 'PDD_PATH' environment variable is set, raising an error if it is not. The script defines input parameters, including a code module (a simple 'add' function), a prompt describing the task, the programming language ('python'), and additional parameters like 'strength' and 'temperature' for the context generation process. The 'context_generator' function is then called with these inputs, and it returns the generated example code, the total cost of the operation, and the model name used. The script concludes by printing the generated code, the cost, and the model name using the 'rich' library for formatted output.",2025-01-01T07:17:44.759820+00:00
"context/context_generator_main_example.py","The provided Python script utilizes the Click library to create a command-line interface (CLI) context for executing a function named `context_generator_main` from the `pdd.context_generator_main` module. The script initializes a Click context object with specific parameters and attributes, such as `force`, `quiet`, `verbose`, `strength`, and `temperature`, which control the behavior of the CLI and the AI model. It specifies input and output file paths, including a prompt file, a code file, and an output file where the generated code will be saved. The `context_generator_main` function is then called with the configured context and file paths, and it returns the generated code, the total cost of the operation, and the name of the AI model used. Finally, the script prints the generated code, the total cost, and the model name to the console. The script appears to be part of a larger system for generating Python code based on prompts and AI model configurations.",2025-01-01T07:17:47.378733+00:00
"context/continue_generation_example.py","The provided Python script demonstrates the usage of the `continue_generation` function, which extends text generation using a language model and calculates associated costs. The script begins by loading input data from external files: a formatted input prompt from `context/cli_python_preprocessed.prompt` and an initial LLM output fragment from `context/llm_output_fragment.txt`. It sets parameters for the language model, including `strength` and `temperature`, and then calls the `continue_generation` function with these inputs. The function returns the final generated output, the total cost of the operation, and the model name. The script prints the total cost and model name to the console and writes the final generated output to a file named `context/final_llm_output.py`. Error handling is implemented to catch and report file-related or general exceptions. The script is designed to be executed as a standalone program with the `main` function serving as the entry point.",2024-12-21T03:10:26.974173+00:00
"context/crash_main_example.py","The provided Python script demonstrates the use of a function called `crash_main` to fix errors in a code module and its associated program that caused a crash. The script begins by creating an output directory and defining a function `demonstrate_crash_main`. Within this function, it generates several files: a prompt file that describes the task of writing a factorial function, a faulty code module that does not handle negative inputs, and a main program that attempts to calculate the factorial of a negative number, leading to a recursion error. An error log is also created to capture the details of the crash. The script then sets up a mock Click context with parameters for the `crash_main` function, which is called to attempt to fix the errors iteratively. The function is configured to allow a maximum of three attempts and a budget of five dollars for the fixing process. Finally, the script prints the success status, number of attempts, total cost, model used, and the fixed code and program. This example illustrates error handling and automated debugging in Python.",2025-02-09T07:16:43.026749+00:00
"context/detect_change_example.py","The provided script is a Python program designed to analyze and detect changes in prompt files using a machine learning model. It imports necessary modules, including 'detect_change' from 'pdd.detect_change', and utilizes the 'rich.console' library for formatted console output. The script defines a list of prompt files located in 'context' and 'prompts' directories, which are analyzed for changes based on a given description. The change description specifies the goal of making prompts more compact using a specific file ('context/python_preamble.prompt').

The script sets parameters for the machine learning model, such as 'strength' (determining model intensity) and 'temperature' (controlling randomness in output). It then attempts to detect changes in the prompt files by calling the 'detect_change' function with the specified parameters. If successful, it prints the detected changes, including the prompt name and change instructions, along with the total cost of the operation and the model used. In case of an error, it catches the exception and displays an error message.

Overall, the script automates the process of analyzing and updating prompt files to improve their compactness and efficiency, while providing detailed feedback and cost analysis.",2025-01-01T07:17:53.244881+00:00
"context/detect_change_main_example.py","The provided code is a Python script that utilizes the Click library to simulate command-line interface (CLI) parameters and invoke a function named `detect_change_main` from the `pdd.detect_change_main` module. The script defines a main function that sets up a Click context with specific model parameters, including strength, temperature, force, and quiet options. It specifies a list of prompt files and a change description file, which is written to a specified output file. The script ensures that the output directory exists before calling the `detect_change_main` function, which processes the prompts and generates a list of changes needed, along with the total cost and model name used. The results, including the model name, total cost, and detailed change instructions for each prompt, are printed to the console. The script is designed to handle exceptions gracefully, providing error messages if any issues arise during execution. Overall, this script serves as a utility for detecting changes in prompts based on specified criteria.",2025-02-09T07:16:47.203735+00:00
"context/execute_bug_to_unit_test_failure.py","The provided code snippet imports a function named 'add' from a module called 'context.bug_to_unit_test_failure_example'. It then calls this 'add' function with two arguments, 2 and 3, and prints the result. The purpose of this code is to demonstrate a simple addition operation, where the expected output would be the sum of the two numbers, which is 5. This example may be part of a larger context where the function 'add' is being tested or utilized, possibly in relation to debugging or unit testing scenarios.",2025-02-09T07:16:50.487380+00:00
"context/final_llm_output.py","The provided code is a command-line interface (CLI) for a tool called PDD (Prompt-Driven Development), which facilitates the generation, modification, and testing of code based on user prompts. It utilizes the Click library for command handling and Rich for console output. The CLI supports various commands such as generating runnable code from prompts, creating example files, generating unit tests, preprocessing prompts, fixing errors in code and tests, and splitting large prompts into smaller ones. Each command is designed to handle specific tasks, with options for output file paths, verbosity, and cost tracking. The code includes error handling to manage exceptions during execution and provides feedback to the user through console messages. Additionally, it features a cost tracking mechanism to log the expenses associated with each operation in a CSV file. The CLI also supports shell completion installation for user convenience and displays the version of the tool. Overall, the code is structured to enhance the development workflow by automating code generation and testing processes.",2025-02-09T07:16:53.927541+00:00
"context/find_section_example.py","This file provides documentation and examples for using the `find_section` function from the `find_section.py` module. The function is designed to identify and extract code blocks from text content. The documentation includes a complete example that demonstrates how to use the function with a sample input containing multiple code blocks in different programming languages (Python and JavaScript). The file details both the input parameters (`lines`, `start_index`, and `sub_section`) and the output format (a list of tuples containing language, start line, and end line information). The example shows how to process a string containing code blocks by first splitting it into lines and then using the find_section function to locate the code sections. It also includes a practical example of reading from a file ('unrunnable_raw_llm_output.py') and processing its contents. The documentation concludes with sample output showing how the function identifies code blocks and their locations within the text. The function appears to be particularly useful for parsing and processing text that contains embedded code blocks in various programming languages.",2024-12-07T01:53:08.778520+00:00
"context/fix_code_loop_example.py","The provided Python script demonstrates the use of a function called `fix_code_loop` to iteratively debug and fix a code file. The script begins by creating example files: a Python module (`module_to_test.py`) containing a function `calculate_average` that calculates the average of a list of numbers, and a verification script (`verify_code.py`) that tests this function with an erroneous input (a string instead of a list). The script also defines a prompt describing the intended functionality of the code. The `fix_code_loop` function is then invoked with parameters such as the code file, verification program, prompt, and constraints like model strength, temperature, maximum attempts, and budget. The loop attempts to fix the code iteratively, logging errors and outputs, until the code passes the verification or the constraints are exhausted. The results, including success status, number of attempts, cost, and the final code, are printed. The script also includes commented-out code for cleaning up the generated files. This script serves as an example of automated debugging and code correction using a predefined loop mechanism.",2025-01-01T07:17:58.918831+00:00
"context/fix_code_module_errors_example.py","The provided Python script demonstrates a process for identifying and fixing errors in a code module using a function called `fix_code_module_errors`. The script begins with a sample program that contains an error, where a function `calculate_sum` attempts to sum a string instead of a list of numbers, resulting in a `TypeError`. The script also includes the original prompt that generated the code, the erroneous code module, and the error message encountered during execution. The `fix_code_module_errors` function is then called with these inputs, along with parameters such as model strength and temperature, to generate a corrected version of the program and code module. The function returns several outputs, including whether updates are needed, the fixed program and code, the total cost of the API calls, and the name of the model used. The script concludes by printing these results, providing a clear demonstration of how the error was identified and resolved. This script serves as an example of automated debugging and code correction using external tools or APIs.",2025-01-01T07:18:02.055770+00:00
"context/fix_error_loop_example.py","The provided code is a Python script that demonstrates the usage of a function called `fix_error_loop`, which is imported from the `pdd` module. The main function sets up various parameters required for error fixing, including paths to unit test files, code files, and a prompt file. It also defines parameters such as strength, temperature, maximum attempts, and budget for the error fixing process. The script attempts to call the `fix_error_loop` function with these parameters and captures the results, including whether the operation was successful, the total number of attempts made, the total cost incurred, and the model used for fixing errors. The results are printed in a formatted manner using the `rich` library, which enhances the output display. If an error occurs during the execution, it is caught and displayed. The script is designed to be run as a standalone program, as indicated by the `if __name__ == '__main__':` block.",2025-02-09T17:23:12.762714+00:00
"context/fix_errors_from_unit_tests_example.py","The provided Python script demonstrates the use of a function called `fix_errors_from_unit_tests`, which is designed to correct errors in unit tests. The script begins by importing necessary modules and defining a `main` function. Within `main`, example unit tests and a simple addition function are defined. An incorrect assertion in the unit test is highlighted, which is expected to trigger an error when the test is run. The script sets parameters for the error correction process, including a prompt, an error message, and configuration settings for the language model's strength and temperature. It then calls the `fix_errors_from_unit_tests` function with these inputs, which returns updated unit tests and code, along with the total cost of the operation and the model used for the correction. Finally, the results are printed in a formatted manner using the `rich` library. This script serves as a practical example of how to automate the debugging of unit tests in Python.",2025-02-09T07:17:00.907366+00:00
"context/fix_main_example.py","The provided code is a Python script that utilizes the Click library to create a command-line interface for fixing errors in code and unit tests. It defines a command called 'fix_command' which accepts several options, including paths to prompt files, code files, unit test files, and error log files. The command also allows for configuration of output paths for fixed test results and code, as well as parameters for iterative fixing, maximum attempts, budget constraints, and auto-submission of tests if they pass. The core functionality is handled by the 'fix_main' function, which attempts to fix the provided code and tests based on the specified parameters. Upon completion, the script provides feedback on whether the fixing process was successful, along with details on the number of attempts made, the cost incurred, and the model used for fixing. The script is designed to be run from the command line, with default values set for the Click context, and it includes error handling to inform the user of the success or failure of the operation.",2025-02-09T07:17:04.152337+00:00
"context/generate_output_paths_example.py","The file is a Python script that demonstrates the usage of a function called `generate_output_paths` from the `pdd.generate_output_paths` module. The script sets up example inputs, including a command, output locations, a basename, a programming language, and a file extension. It then calls the `generate_output_paths` function to generate output paths based on these inputs. The script includes examples for different scenarios: a basic 'generate' command, a case where an environment variable (`PDD_GENERATE_OUTPUT_PATH`) is used to specify the output path, and a 'fix' command that generates multiple output paths for test and code files. The results of these function calls are printed to the console, showcasing the generated output paths for each scenario. The script is structured to demonstrate the flexibility and functionality of the `generate_output_paths` function in handling various input configurations and commands.",2024-12-13T21:07:19.225256+00:00
"context/generate_test_example.py","The file contains a Python script that demonstrates the generation of a unit test for a given function using the `generate_test` function from the `pdd.generate_test` module. The script begins by importing necessary modules, including `os` and `rich` for environment variable handling and formatted output, respectively. It defines a prompt for creating a factorial function, along with the corresponding Python code implementation. Additional parameters such as `strength`, `temperature`, and `language` are specified to guide the test generation process. The script attempts to call the `generate_test` function with these inputs and captures the resulting unit test, total cost, and model name. If successful, it prints the generated unit test, cost, and model details using formatted output. In case of an error, it catches the exception and displays an error message. The script also includes commented-out code for setting an environment variable (`PDD_PATH`) if needed.",2024-12-21T03:10:34.209936+00:00
"context/get_comment_example.py","The provided document outlines the usage and documentation for the `get_comment` function from the `get_comment.py` module. It begins with an example of how to import and utilize the function, demonstrating its application for various programming languages such as Python, Java, and JavaScript. The function is designed to return the appropriate comment character(s) based on the specified programming language, with case insensitivity for the input. The documentation details the input parameter, which is a string representing the programming language, and the output, which is a string of the comment character(s). In cases where the environment variable `PDD_PATH` is not set, or if the language is not found in the associated CSV file, the function will return 'del'. Additionally, the document emphasizes the importance of setting the `PDD_PATH` environment variable correctly and mentions the structure of the required CSV file, which should include columns for `language` and `comment`.",2025-02-09T07:17:07.729500+00:00
"context/get_extension_example.py","The provided code snippet demonstrates the usage of a function named 'get_extension' imported from the 'pdd' module. This function is designed to return the file extension associated with various programming languages. Three example calls to the function are included, showcasing its functionality. The first call, with the argument 'Python', returns the output '.py', indicating the standard file extension for Python scripts. The second call uses 'Makefile' as an argument, which returns 'Makefile', reflecting that Makefiles do not have a specific extension. The third example, with 'JavaScript', returns '.js', the common file extension for JavaScript files. Overall, the snippet serves as a simple demonstration of how to retrieve file extensions for different programming languages using the 'get_extension' function.",2025-02-09T07:17:10.534224+00:00
"context/get_jwt_token_example.py","The provided code is an asynchronous Python script that facilitates authentication with Firebase using GitHub's Device Flow. It imports necessary modules and defines constants for the Firebase API key, GitHub client ID, and application name. The main function initiates the authentication process, attempting to retrieve a Firebase ID token through the `get_jwt_token` function. It handles various exceptions, including authentication errors, network issues, token errors, and rate limits, providing user-friendly messages for each case. Upon successful authentication, the script updates a `.env` file with the new JWT token, ensuring that the token is either replaced if it already exists or added if it does not. The script is designed to be run as a standalone application, utilizing the asyncio library to manage asynchronous operations.",2025-02-09T07:17:13.397627+00:00
"context/get_language_example.py","This Python script demonstrates the usage of a 'get_language' function that determines the programming language associated with a file extension. The code consists of a main function that serves as an example implementation. Inside the main function, it defines a sample file extension '.py' and attempts to get the corresponding programming language using the imported get_language function. The code includes error handling through a try-except block to catch any potential exceptions during execution. If a language is successfully identified, it prints the file extension and its associated programming language. If no language is found, it displays a message indicating that no programming language was found for the given extension. The script uses type hints for better code readability and follows Python's standard if __name__ == '__main__' pattern for script execution. The code is well-structured and includes proper documentation through docstrings.",2024-12-07T01:53:09.050133+00:00
"context/git_update_example.py","The provided Python script demonstrates the usage of a function called `git_update_prompt` from the `pdd.git_update` module. The main function reads an input prompt from a file located in the 'prompts' directory, specifically named 'llm_invoke_python.prompt'. It sets parameters for the function call, including a strength and temperature for the language model, which influence the output's creativity and variability. The script attempts to call the `git_update` function with the input prompt and other parameters, capturing the result, which includes a modified prompt, total cost, and model name. If successful, it prints these results; otherwise, it handles errors gracefully, providing feedback on any issues encountered. Finally, the modified prompt is saved back to the same file. The script is structured to be executed as a standalone program, with a clear entry point defined by the `if __name__ == '__main__':` block.",2025-02-09T19:40:23.810997+00:00
"context/increase_tests_example.py","This Python file demonstrates the usage of an 'increase_tests' function, which is designed to generate additional unit tests for existing code. The file contains an 'example_usage' function that showcases both basic and advanced implementations of the test generation process. The example uses a simple 'calculate_average' function as the target code, along with an existing basic unit test and a coverage report showing 60% code coverage. The function demonstrates two ways to call 'increase_tests': a basic usage with default parameters and an advanced usage with custom parameters including language specification, strength, temperature, and verbosity settings. The code includes proper error handling using try-except blocks to catch both ValueError for input validation errors and general exceptions. The example is structured to run when the script is executed directly. The code serves as a practical demonstration of how to use the test generation functionality while handling potential errors and displaying relevant output information such as the generated tests, associated costs, and the model used for generation.",2024-12-07T01:53:09.111951+00:00
"context/insert_includes_example.py","This Python file demonstrates the usage of an insert_includes module for processing dependencies. The main function, example_usage(), showcases how to set up and execute dependency processing with error handling. The code initializes a Rich console for formatted output and defines input parameters including a prompt for a Python function that handles CSV files, a directory path for example files, and a CSV filename for dependency information. The insert_includes function is called with specific parameters like strength and temperature to control the output. The code then displays the results including the original prompt, modified prompt with dependencies, CSV output, model name, and total cost. The processed CSV output is saved to a file. Error handling is implemented for common issues like missing files or processing errors. The code uses the Rich library for enhanced console output formatting and the pathlib library for file operations. This appears to be part of a larger system for managing and processing project dependencies with AI model integration.",2024-12-07T22:11:27.202537+00:00
"context/install_completion_example.py","This Python script serves as an example for installing shell completion for the PDD command-line interface (CLI). It demonstrates the use of functions from the pdd package, specifically `get_local_pdd_path()` to retrieve the absolute path to the PDD_PATH directory and `install_completion()` to set up shell completion. The script creates a controlled environment by setting dummy environment variables and generating necessary files without affecting the user's actual configuration. It forces the shell to bash, creates a dummy home directory, and a PDD_PATH directory, where it generates a sample completion script and a bash RC file. The main function orchestrates the setup and execution, providing feedback through the Rich library for better console output. The example concludes with instructions on how to run the script, ensuring users can replicate the setup safely.",2025-02-09T07:17:19.675230+00:00
"context/langchain_lcel_example.py","The provided code is a comprehensive implementation of various language models and prompt templates using the LangChain framework. It imports multiple libraries, including those for OpenAI, Google, and Anthropic models, and sets up a caching mechanism to optimize performance. The code defines a custom callback handler, `CompletionStatusHandler`, to track the completion status and token usage of language model responses. It demonstrates the creation of prompt templates for generating jokes based on user-defined topics, utilizing different models like ChatOpenAI, ChatGoogleGenerativeAI, and others. The code also showcases structured output parsing using Pydantic and JSON formats, allowing for organized responses. Additionally, it includes examples of invoking chains with specific queries, handling environmental variables for API keys, and configuring fallback models for enhanced reliability. Overall, the script illustrates the versatility of LangChain in integrating various LLMs for conversational and creative tasks.",2025-02-09T19:40:27.627989+00:00
"context/llm_invoke_example.py","The provided code demonstrates the use of the Pydantic library to create a structured model for generating jokes. It defines a `Joke` class with fields for the setup and punchline of a joke. The main function showcases how to invoke a language model (using `llm_invoke`) to generate jokes based on a given topic, specifically targeting programmers and data scientists. The code tracks the strength of the model's responses, adjusting from 0.0 to 1.0, and records the model used for each strength level. It includes two examples: one for unstructured output and another for structured output formatted as JSON. The structured output is validated against the `Joke` Pydantic model, ensuring the response adheres to the expected format. The program also prints the cost of each invocation and the strength ranges for each model used. Overall, the code serves as a practical demonstration of integrating Pydantic with a language model to generate and structure humorous content.",2025-02-09T17:23:20.061632+00:00
"context/llm_selector_example.py","The provided code is a Python script that demonstrates the usage of a function called `llm_selector` from the `pdd.llm_selector` module. The main function initializes parameters for a language model, specifically `strength` and `temperature`, which influence the model's behavior. A while loop iterates as long as the `strength` is less than or equal to 1.1, incrementing it by 0.05 in each iteration. Within the loop, the `llm_selector` function is called with the current `strength` and `temperature`, returning details about the selected language model, including its name, input and output costs per million tokens. The script also includes an example of counting tokens in a sample text using a `token_counter` function. Error handling is implemented to catch potential `FileNotFoundError` and `ValueError` exceptions. Overall, the script serves as a practical example of how to select and utilize different language models based on specified parameters.",2025-02-09T21:05:38.634023+00:00
"context/load_prompt_template_example.py","This Python script demonstrates a simple program that loads and displays a prompt template. The code imports two functions: 'load_prompt_template' from a custom module 'pdd.load_prompt_template' and 'print' from the 'rich' library, which provides rich text formatting capabilities. The main function defines a variable 'prompt_name' set to 'generate_test_LLM' (presumably the name of a prompt template file without its extension), loads the template using the imported function, and then displays it with blue-colored formatting if the template is successfully loaded. The script uses a standard Python idiom with the '__name__ == __main__' check to ensure the main function only runs when the script is executed directly. This appears to be part of a larger system dealing with prompt templates, possibly for working with Large Language Models (LLMs).",2024-12-07T01:53:09.302428+00:00
"context/postprocess_0_example.py","This file provides a comprehensive example of how to use the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The example demonstrates processing output from a language model (LLM) that contains multiple code sections in different programming languages. The main example shows how to extract and process Python code specifically, though the function can work with other languages. The code includes a sample LLM output containing Python and Java code blocks, along with regular text. The example demonstrates how to use the function to process this mixed content, focusing on extracting Python code while commenting out other content. The documentation section clearly outlines the input parameters (`llm_output` and `language`) and explains that the function returns a string with only the largest section of the specified language's code uncommented, while other content is commented out. The file concludes with a note about ensuring proper implementation of supporting functions (`get_comment`, `comment_line`, and `find_section`) for the example to work correctly.",2024-12-07T01:53:09.330350+00:00
"context/postprocess_example.py","This Python script demonstrates the usage of a postprocessing function designed to extract code from mixed text and code output typically generated by Large Language Models (LLMs). The main function contains an example LLM output that includes a factorial function implementation along with usage examples, all embedded within markdown-style code blocks. The script uses the 'postprocess' function from a 'pdd.postprocess' module to extract the pure code from this mixed output. The script also utilizes the 'rich' library for enhanced console output formatting. The postprocessing function takes several parameters including the target programming language (python), model strength (0.7), and temperature (0.2). The script outputs the extracted code, the total cost of the operation, and the model name used. The output is formatted with color coding using the rich console library's formatting capabilities. This appears to be a demonstration or utility script that might be part of a larger toolkit for handling LLM-generated code outputs.",2024-12-07T01:53:09.359769+00:00
"context/postprocessed_runnable_llm_output.py","The provided code outlines the implementation of a function named `context_generator`, which is designed to read a specified Python file, preprocess its content, generate a prompt for a language model (GPT-4), and write the output to a designated file. The function takes three parameters: the name of the Python file to read, the name of the output file, and an optional boolean `force` parameter. The function begins by attempting to open and read the specified Python file. If the file is not found, it prints an error message and returns `False`. After reading the file, it is expected to preprocess the content (though the actual preprocessing function is not included in the snippet). A prompt is then generated, instructing the model to create a concise usage example based on the provided Python code. The function concludes with a usage example, demonstrating how to call `context_generator` with appropriate arguments. Overall, the code serves as a template for generating usage examples from Python code using a language model.",2025-02-09T07:17:29.125824+00:00
"context/preprocess_example.py","The file contains Python code that utilizes a preprocessing module to process various prompts and write the processed outputs to files. It imports the 'preprocess' function from the 'pdd.preprocess' module and uses the 'rich.console.Console' for formatted console output. The script processes multiple prompts, including a multiline string prompt, a file named 'change_LLM.prompt', and another file named 'example_generator_LLM.prompt'. The preprocessing involves options like recursion, handling double curly brackets, and excluding specific keys (e.g., 'test2'). The processed outputs are displayed in the console and written to corresponding files in the 'tests' directory. Additionally, the script includes a mock database example as a prompt, which is also processed and displayed. The code demonstrates the use of file reading, writing, and string manipulation for prompt processing tasks.",2025-01-01T07:18:13.611338+00:00
"context/preprocess_main_example.py","The provided Python script defines a command-line interface (CLI) using the Click library for preprocessing prompt files. The main function, `cli`, includes several options for customization, such as specifying the input prompt file (`--prompt-file`), output file path (`--output`), enabling XML tagging (`--xml`), recursive processing (`--recursive`), doubling curly brackets (`--double`), excluding specific keys from doubling (`--exclude`), and enabling verbose logging (`--verbose`). The script utilizes the `preprocess_main` function from the `pdd.preprocess_main` module to handle the preprocessing logic. It processes the input prompt file based on the provided options, applies transformations like XML tagging or bracket doubling, and outputs the results. The script also includes error handling to catch and display exceptions during execution. Additionally, it provides feedback to the user, such as the processed prompt, total cost, and model name. The CLI is designed to be flexible and user-friendly, allowing users to customize the preprocessing workflow for prompt files efficiently. The script is executed as a standalone program when run directly.",2025-01-01T07:18:16.216462+00:00
"context/process_csv_change_example.py","This Python code demonstrates the usage of a function called 'process_csv_change' from a module named 'pdd.process_csv_change'. The script is designed to process changes in code files based on CSV input. It initializes several parameters including the CSV file path, strength (0.8), temperature (0), code directory path, programming language (Python), file extension (.py), and a maximum budget of $10.00. The main function call is wrapped in a try-except block for error handling. If successful, it prints the total cost of changes, the AI model used, and displays modified prompts for each affected file. The code includes error handling for file not found scenarios and unexpected errors. The function appears to be part of a larger system that automates code modifications, possibly using AI models, while maintaining budget constraints and tracking costs.",2024-12-07T01:53:09.415833+00:00
"context/split_example.py","The provided Python script demonstrates the use of the 'split' function from the 'pdd.split' module. It begins by importing necessary libraries, including 'os' for environment variable management and 'rich.console' for enhanced console output. The main function sets the 'PDD_PATH' environment variable to the parent directory of the script's location. It defines an input prompt asking for a Python function to calculate the factorial of a number, along with a sample implementation of the factorial function and an example usage. The script then calls the 'split' function with these inputs, along with parameters for 'strength' and 'temperature', which likely influence the output's creativity and variability. The results, including the sub prompt, modified prompt, model name, and total cost of the operation, are printed to the console in a formatted manner. The script is wrapped in a try-except block to handle potential errors gracefully, providing feedback in case of exceptions. Overall, the script serves as a practical example of how to utilize the 'split' function for generating and modifying code prompts.",2025-02-09T08:10:57.880306+00:00
"context/split_main_example.py","The provided Python script, `example_split_usage.py`, is a command-line interface (CLI) tool that utilizes the Click library to facilitate the splitting of input prompts and code files. It imports the `split_main` function from a module named `pdd.split_main`, which is presumably responsible for the core functionality of splitting prompts. The script defines a command `split_cli` that requires three input files: an input prompt file, a generated code file, and an example code file. It also includes optional parameters for output file paths, a force overwrite flag, and a quiet mode to suppress console output. The command allows users to customize settings such as the strength of the split and randomness through temperature. Upon execution, it attempts to call the `split_main` function with the provided parameters and handles any exceptions that may arise, displaying relevant output if not in quiet mode. The script concludes by defining a top-level CLI group and adding the `split_cli` command to it, making it executable as a standalone program.",2025-02-09T17:23:23.548186+00:00
"context/summarize_directory_example.py","This Python script demonstrates the usage of the 'summarize_directory' module from the 'pdd' package. The main function showcases how to summarize Python files in a directory while handling existing summaries. It includes an example of existing CSV content and demonstrates calling the summarize_directory function with various parameters including directory path (with wildcard), strength (model capability), temperature (output determinism), verbosity, and existing CSV content. The script processes the files, generates summaries, and outputs the results including the generated CSV content, total cost in USD, and the model used. The results are both printed to console and saved to 'output.csv'. The code includes error handling and is structured as a standalone executable script with a main() function guard.",2024-12-07T01:53:09.471711+00:00
"context/tiktoken_example.py","This code snippet demonstrates the use of the tiktoken library for token counting in text processing. The code first imports the tiktoken library, then creates an encoding object using the 'cl100k_base' encoding scheme (which is commonly used with newer GPT models). Finally, it counts the number of tokens in a variable called 'preprocessed_prompt' by encoding the text and measuring the length of the resulting encoded sequence. The code is particularly useful for applications that need to track token usage when working with language models, as it provides a way to count tokens before sending text to an API. The 'cl100k_base' encoding is specifically mentioned, though the comment indicates other encoding options are available. This is a common pattern when working with OpenAI's GPT models, where understanding token count is important for both cost management and staying within model context limits.",2024-12-07T01:53:09.500027+00:00
"context/trace_example.py","The provided Python script demonstrates the use of the `trace` function from the `pdd.trace` module. It includes a `main` function that sets up an environment, defines example inputs, and calls the `trace` function to analyze a specific line of code within a given code snippet. The script uses the `rich.console` module for formatted console output. Example inputs include a Python code snippet, a prompt file with instructions, and parameters like `strength` and `temperature` for the LLM model. The `trace` function is called with these inputs, and its results, such as the corresponding prompt line, total cost, and model name, are displayed in the console. The script also includes error handling for `FileNotFoundError`, `ValueError`, and general exceptions, providing informative messages for each. The script is designed to be executed as a standalone program, with the `main` function serving as the entry point.",2025-01-01T07:18:25.658558+00:00
"context/trace_main_example.py","The provided code snippet demonstrates how to create a simple calculator using Python. It includes a prompt that outlines the requirements for a function that adds two numbers together. The function, named `add_numbers`, takes two float inputs, adds them, and returns the result. The code also includes a test case that calls this function with the numbers 5.0 and 3.0, printing the sum to the console. Additionally, the script utilizes the Click library to set up a command-line context for tracing the execution of the code. It configures parameters such as verbosity, file overwrite options, analysis strength, and randomness for a language model (LLM) analysis. The `trace_main` function is called to analyze the code, with the results being saved to a specified output file. The script handles exceptions gracefully, providing error messages if any issues arise during execution. Overall, this code serves as a practical example of integrating user prompts, function definitions, and command-line interfaces in Python.",2025-02-09T07:17:34.415338+00:00
"context/track_cost_example.py","This Python file implements a Command-Line Interface (CLI) tool called PDD using the Click library. The tool is designed for processing prompts and generating outputs with cost tracking functionality. The code consists of two main components: a base CLI group and a 'generate' command. The CLI accepts an optional '--output-cost' parameter to track costs and save usage details to a CSV file. The 'generate' command requires a prompt file input and has an optional output file parameter. It's decorated with a 'track_cost' decorator for cost monitoring. The command reads a prompt from the input file, processes it (currently using placeholder logic), and either writes the output to a specified file or prints it to the console. The function returns a tuple containing the generated output, the cost of execution (simulated at $0.05 per million tokens), and the model name (set to 'gpt-4'). The file includes proper type hints, documentation, and error handling through Click's built-in functionality. The main execution block demonstrates example usage of the CLI with specific command-line arguments.",2024-12-07T01:53:09.597187+00:00
"context/unfinished_prompt_example.py","This Python file demonstrates the usage of an 'unfinished_prompt' function from a 'pdd' module. The code consists of a main function that analyzes the completeness of a given text prompt using a Language Learning Model (LLM). The example uses a simple fairy tale beginning ('Once upon a time...') as the test prompt. The function takes several parameters including the prompt text, model strength (0.5), and temperature (0.0). The unfinished_prompt function returns four values: reasoning, whether the prompt is finished, the total cost of the analysis, and the model name used. The code includes error handling through a try-except block and is structured to run the main function only when the file is executed directly. This appears to be a demonstration or testing script for a larger prompt analysis system, likely used to determine if a given text prompt is complete or needs additional content.",2024-12-07T01:53:09.626009+00:00
"context/unrunnable_raw_llm_output.py","The provided content outlines the implementation of a Python function named `context_generator`. This function is designed to read a specified Python file, preprocess its content, generate a prompt for a model (specifically GPT-4), and write the output to a designated file. The implementation includes error handling for cases where the specified file does not exist, returning `False` in such instances. The function constructs a prompt that asks for a concise example of how to use the module based on the provided Python code. The usage of the function is demonstrated with an example call, indicating how to specify the input and output file names. Overall, the content serves as a guide for creating a utility that aids in generating usage examples for Python modules.",2025-02-09T07:17:41.579824+00:00
"context/update_main_example.py","The provided Python script demonstrates the usage of a CLI (Command Line Interface) tool built using the Click library. The script defines a command named 'update' that facilitates updating a prompt based on modified code. It accepts various options such as paths to input prompt files, modified code files, and optionally original code files. Users can also specify parameters like 'strength' (to control how strongly the model incorporates changes), 'temperature' (for randomness in text generation), verbosity, and whether to use Git history instead of an input code file. The script integrates with a function named 'update_main' from the 'pdd.update_main' module, which performs the core update operation. The results, including the updated prompt snippet, total cost, and model name, are displayed unless suppressed by the 'quiet' flag. Additional options include overwriting files ('force') and enabling verbose logging. The script also supports grouping commands using Click's 'group' functionality, allowing for future extensibility. Overall, the script serves as an example of how to build a flexible and user-friendly CLI for managing and updating text prompts based on code changes.",2025-01-01T07:18:31.507257+00:00
"context/update_prompt_example.py","The provided Python script demonstrates the usage of the `update_prompt` function from the `pdd.update_prompt` module. The script defines a `main` function that sets up input parameters, including an input prompt, input code, modified code, and parameters like strength and temperature for a language model. It then calls the `update_prompt` function with these parameters. The function is expected to return a modified prompt, the total cost of the operation, and the model name. If successful, the script prints these results; otherwise, it handles exceptions and prints an error message. The script is designed to be executed as a standalone program, with the `main` function being called when the script is run directly.",2024-12-21T03:10:40.913389+00:00
"context/xml_tagger_example.py","This Python script demonstrates the usage of an XML tagging functionality using a custom module 'pdd.xml_tagger'. The code imports the 'xml_tagger' function and the 'rich' library for enhanced console printing. The script takes a raw text prompt ('Write a story about a magical forest') and processes it with two parameters: 'strength' (set to 0.5, representing base model performance) and 'temperature' (set to 0.7, controlling output randomness). The code is wrapped in a try-except block for error handling. When successful, it outputs the XML-tagged version of the prompt, the total cost of processing, and the name of the model used. The output is formatted using rich's printing capabilities with color coding (green for success, red for errors) and bold text formatting. This appears to be a demonstration or example implementation of an AI-powered text processing tool that adds XML markup to input text.",2024-12-07T01:53:09.681130+00:00
