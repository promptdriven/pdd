full_path,file_summary,date
"context/DSPy_example.py","This file contains commented-out Python code that demonstrates how to set up and use DSPy, a framework for programming with language models. The code outlines a two-step process for selecting optimal examples from changes using DSPy.

The first step involves setting up the language model configuration. It imports a DSPy example module and configures an OpenAI GPT-3.5-turbo-instruct model with a maximum token limit of 250. The settings are then configured to use this turbo model as the default language model.

The second step defines a custom DSPy module called chainofthought that inherits from dspy.Module. This class implements a Chain of Thought reasoning pattern, which is a prompting technique that encourages the model to break down complex problems into intermediate reasoning steps. The class initializes a ChainOfThought program that maps questions to answers, and includes a forward method that processes questions through this program.

The file ends with an incomplete comment suggesting there would be additional code for compiling and optimizing the module. Overall, this appears to be a work-in-progress template or tutorial for implementing Chain of Thought prompting using the DSPy framework with OpenAI's language models.",2025-12-14T08:28:52.943712+00:00
"context/__init__example.py","This Python script demonstrates the usage of a command registration module for building a command-line interface (CLI) using the Click library. The file serves as an example of how to initialize and configure the main CLI entry point for a tool called PDD.

The script imports necessary modules including Click, sys, and os, and imports a `register_commands` function from the `pdd.commands` package. It defines a main CLI group using Click's `@click.group()` decorator, which creates the root command group that will contain all subcommands.

The core functionality is contained in the `run_example()` function, which performs three main tasks: (1) initializes the CLI group, (2) calls `register_commands()` to attach all subcommands defined in the pdd.commands package to the main CLI group, and (3) simulates running the CLI with the '--help' flag to verify that all commands were registered successfully.

The script uses `standalone_mode=False` when invoking the help command to prevent Click from calling `sys.exit()`, allowing the script to complete gracefully. This example is typically used as a reference for setting up the main entry point (e.g., `cli.py`) of an application that aggregates multiple command-line functionalities into a unified interface.",2025-12-14T08:29:01.960714+00:00
"context/addition_of_time_param.prompt","This file is a prompt template designed for the PDD CLI's `detect` command. Its purpose is to analyze other PDD CLI command prompt files to determine if they need modifications following the introduction of a new global `--time` parameter. The `--time` parameter (ranging from 0.0 to 1.0, defaulting to 0.25) controls the LLM's thinking effort or reasoning allocation when invoking the underlying `llm_invoke` Python function. The prompt explains how the `time` parameter behaves differently based on a model's `reasoning_type`: it's ignored for 'none' types, calculates a token budget for 'budget' types, and maps to qualitative effort levels (low/medium/high) for 'effort' types. The analysis task involves checking target prompts for: conflicting hardcoded reasoning instructions, sensitivity to speed/cost vs. reasoning quality trade-offs, user-facing documentation that may need updating, and whether the prompt can work transparently with the new global parameter. The goal is to identify prompts that either conflict with or are made redundant by the new `--time` functionality, versus those that require no changes because they're generic regarding LLM reasoning behavior.",2025-12-14T08:29:10.378317+00:00
"context/agentic_fix_example.py","This Python script demonstrates an agentic fix workflow, showcasing how an AI-powered agent can automatically debug and correct faulty code. The example uses a deliberately buggy `add` function that performs subtraction instead of addition, along with corresponding unit tests that will fail.

The script is organized into several key components:

1. **Setup Data**: Defines buggy code (an incorrect `add` function), test cases, and a prompt instructing the agent to fix the implementation.

2. **`setup_scenario` Function**: Creates a temporary project directory with the buggy code, tests, and prompt files. It also runs pytest to generate an initial error log capturing the test failures.

3. **`check_prerequisites` Function**: Validates that required API keys (Anthropic, Google/Gemini, or OpenAI) are available in the environment before proceeding.

4. **`main` Function**: Orchestrates the complete workflow by setting up the temporary environment, invoking the `run_agentic_fix` function from the `pdd.agentic_fix` module, and reporting results including success status, model used, estimated cost, and the corrected code.

The script uses temporary directories for isolation, handles cleanup properly, and provides informative console output with emoji indicators for progress tracking. It serves as a practical example of integrating LLM-based code repair agents into development workflows.",2025-12-14T08:29:18.135909+00:00
"context/agentic_langtest_example.py","This Python module provides utilities for detecting project structures and generating test verification commands across multiple programming languages. The main file `pdd/agentic_langtest.py` contains several key functions: `_which()` checks if command-line tools exist in the system PATH, `_find_project_root()` traverses directories upward to locate project markers like `build.gradle`, `pom.xml`, or `package.json`, and `default_verify_cmd_for()` generates appropriate shell commands to run tests for Python (using pytest), JavaScript/TypeScript (using npm), and Java (supporting Maven and Gradle build systems). The `missing_tool_hints()` function detects missing development tools and provides platform-specific installation instructions for macOS and Ubuntu, covering npm, Java JDK, JUnit, and g++. The accompanying `example.py` demonstrates these utilities by creating temporary mock JavaScript and Python projects, generating verification commands, and checking for missing tools. The module uses the `rich` library for formatted console output with colored panels. This toolset appears designed for an agentic testing framework that needs to automatically detect and execute tests across different language ecosystems.",2025-12-14T08:29:26.961294+00:00
"context/anthropic_counter_example.py","This file contains commented-out Python code that demonstrates how to use the Anthropic API client to count tokens in a text string. The code shows the following steps: First, it imports the anthropic library. Then, it initializes an Anthropic client object. A sample text variable is defined with placeholder content (Sample text). The code then calls the client's count_tokens() method, passing in the text to get a token count. Finally, it prints the total number of tokens. Importantly, there is a comment noting that this token counting method is not accurate for version 3.0 and above models of Anthropic's API. The entire code block is commented out (using # symbols), suggesting it may be deprecated, under development, or kept as a reference example. This snippet would be useful for developers who need to estimate token usage before making API calls, though they should be aware of the accuracy limitation mentioned for newer model versions. The code follows standard Python conventions and uses f-string formatting for the output message.",2025-12-14T08:29:34.521936+00:00
"context/anthropic_thinking_test.py","This Python script is a test utility for validating LiteLLM API calls with Anthropic's Claude models, specifically focusing on the thinking or reasoning feature. The script begins by setting up configuration, including defining the project root path, locating the `.env` file for environment variables, and specifying two Claude models: `claude-3-5-haiku` as the main model and `claude-3-7-sonnet` as the thinking model.

It loads environment variables using `dotenv` and checks for the presence of the `ANTHROPIC_API_KEY`, exiting with an error if not found. The script then prepares a LiteLLM completion call with a sample prompt asking to explain recursion using an analogy. The call parameters include enabling the thinking feature with a token budget of 1024 and a maximum output of 50,000 tokens.

After executing the API call, the script attempts to extract any reasoning or thinking content from the response, checking both hidden parameters and the message content. It handles potential errors gracefully, specifically catching `UnsupportedParamsError` to indicate if the thinking parameter isn't supported, and provides informative output using the `rich` library for formatted console printing throughout the process.",2025-12-14T08:29:41.049639+00:00
"context/anthropic_tool_example.py","This Python code demonstrates how to use the Anthropic API to interact with Claude, specifically utilizing a text editor tool capability. The script begins by importing the Anthropic library and initializing a client instance. It then creates a message request to the Claude 3.7 Sonnet model (version dated February 19, 2025) with a maximum token limit of 1024.

The key feature of this code is the integration of a specialized tool called str_replace_editor of type text_editor_20250124. This tool enables Claude to perform text editing operations, which is particularly useful for tasks like fixing errors in documents.

The user message in this example asks Claude for help fixing a syntax error in a file called patent.md. By providing the text editor tool, Claude would be able to view, analyze, and suggest or make corrections to the file's content.

Finally, the script prints the response from the API. This code serves as a basic template for developers looking to leverage Claude's tool-use capabilities for document editing and error correction tasks, showcasing the integration between conversational AI and practical file manipulation functionality.",2025-12-14T08:29:49.494997+00:00
"context/auto_deps_main_example.py","This Python script defines a command-line interface (CLI) tool using the Click library for automated dependency analysis. The tool is named 'auto-deps' and provides several configurable options including: force mode for file overwrites, quiet mode to suppress output, strength and temperature parameters for dependency analysis, paths for input prompt files, directory scanning, CSV dependency files, and output locations, plus a force-scan option to rescan directories.

The main function initializes a Click context with the provided command-line parameters and calls the `auto_deps_main` function from the `pdd.auto_deps_main` module. This function processes a prompt file and analyzes dependencies based on the specified configuration. Upon successful execution, the script displays the modified prompt content, the total cost of the operation (formatted as currency), and the name of the model used for analysis.

Error handling is implemented to catch exceptions during processing, displaying error messages and aborting the Click command gracefully. The script uses a default strength value imported from the pdd module and sets a default temperature of 0.0. The entry point allows the script to be run directly as a standalone program, making it suitable for integration into development workflows or automation pipelines for managing project dependencies.",2025-12-14T08:29:57.189578+00:00
"context/auto_include_example.py","This Python script demonstrates the usage of an `auto_include` function from a custom `pdd` (likely Prompt-Driven Development) module. The main function reads a CSV file containing project dependencies and defines a detailed input prompt for generating unit tests using an LLM (Large Language Model).

The input prompt outlines a multi-step process for a Python function called generate_test that leverages Langchain to: load prompt templates from environment-specified paths, preprocess prompts, create LCEL templates, use an LLM selector for model choice, execute the model with specific parameters (prompt, code, language), track token usage and costs, detect incomplete generations, and handle post-processing.

The script configures parameters including a directory path pattern for context files (`context/c*.py`), default strength and temperature values (0.5), and verbose mode enabled. It then calls `auto_include()` with these parameters, which returns dependencies, CSV output, total cost, and model name.

Finally, the script prints the extracted dependencies, the CSV output, the total cost formatted in dollars per million tokens, and the selected model name. This appears to be part of a larger framework for automated code generation and testing using LLM-powered tools with cost tracking capabilities.",2025-12-14T08:30:04.774270+00:00
"context/auto_update_example.py","This Python script demonstrates the usage of the `auto_update` function from the `pdd.auto_update` module. The code provides a practical example of how to check for and perform package updates in a Python environment.

The `main()` function showcases three different ways to use `auto_update`:

1. **Basic usage**: Calling `auto_update()` without arguments checks for updates to the default 'pdd' package.

2. **Specific package check**: Passing a `package_name` parameter (e.g., requests) allows checking updates for any installed package.

3. **Version comparison**: Providing both `package_name` and `latest_version` parameters enables comparison against a known version number.

The function's workflow involves four steps: checking the currently installed version, comparing it with the latest available version (either from PyPI or a user-specified version), prompting the user if an upgrade is available, and performing the upgrade via pip if the user confirms. If the package is already up to date, no output is produced.

This utility is useful for maintaining package dependencies and ensuring software stays current with the latest releases. The script follows standard Python conventions with a `if __name__ == __main__` guard for proper module execution.",2025-12-14T08:30:13.045028+00:00
"context/autotokenizer_example.py","This Python script demonstrates how to count tokens in a text string using the Hugging Face Transformers library. The code defines a function called `count_tokens` that takes two parameters: the text to be tokenized and an optional model name (defaulting to deepseek-ai/deepseek-coder-7b-instruct-v1.5).

The function works by first loading the appropriate tokenizer for the specified model using `AutoTokenizer.from_pretrained()`. The `trust_remote_code=True` parameter allows the execution of custom code from the model repository, which is sometimes required for certain models. Once the tokenizer is loaded, it processes the input text and converts it into tokens. The function then returns the count of tokens by measuring the length of the 'input_ids' list in the tokenized output.

The script includes an example usage section that demonstrates the function with a sample prompt asking to Write a quick sort algorithm in Python. It calls the `count_tokens` function with this text and prints the resulting token count.

This utility is useful for developers working with large language models who need to track token usage for API rate limiting, cost estimation, or ensuring prompts fit within model context windows.",2025-12-14T08:30:20.403386+00:00
"context/bug_main_example.py","This Python script demonstrates how to use the `bug_main` function from the `pdd` library to automatically generate unit tests based on discrepancies between observed and desired program outputs. The script sets up a Click context with configuration options including force overwrite, output verbosity, model strength, and temperature settings. It then creates several example files in an output directory: a prompt file describing requirements for a function that sums even numbers from a list, a Python code file containing the `sum_even_numbers` function implementation, a main program file that imports and uses this function, and two output files representing the current (buggy) output and the desired correct output. The example illustrates a bug scenario where the function doesn't properly handle empty lists—returning 12 instead of 0. The `bug_main` function is called with all these file paths along with optional parameters for output location and language specification. Finally, the script prints the generated unit test, the total API cost in USD, and the model name used. This serves as a practical example for developers wanting to leverage AI-assisted debugging and test generation capabilities within the pdd framework.",2025-12-14T08:30:27.791361+00:00
"context/bug_to_unit_test_example.py","This Python script demonstrates the usage of a `bug_to_unit_test` function from the `pdd` module. The main purpose is to convert bug information into unit tests using an LLM (Large Language Model). 

The script sets up several input parameters including `current_output` and `desired_output`, which contain debug trace logs showing differences between erroneous and expected behavior when matching prompt lines to code. It reads additional inputs from external files: a prompt file describing code generation instructions, the actual code under test from a module file, and a program context file.

Key configuration parameters include `strength` (LLM model strength), `temperature` (set to 0 for deterministic output), and `language` (Python). The script calls `bug_to_unit_test()` with these parameters to generate a unit test that captures the bug scenario.

The function returns three values: the generated unit test, the total API cost, and the model name used. Results are displayed using the Rich console library for formatted output. Error handling is implemented to catch and display any exceptions that occur during execution.

This appears to be part of a Prompt-Driven Development (pdd) toolset that helps developers create unit tests from observed bugs by analyzing discrepancies between actual and expected program outputs.",2025-12-14T08:30:35.754261+00:00
"context/bug_to_unit_test_failure_example.py","This file contains a Python function definition with a notable discrepancy between its name and implementation. The function is named add and accepts two parameters, x and y, suggesting it is intended to perform addition of two values. However, the actual implementation returns x-y, which performs subtraction rather than addition. This appears to be either a bug or an error in the code, as the function name implies addition but the operation performed is subtraction. The function is minimalistic, consisting of only two lines: the function definition line and a return statement. There are no docstrings, comments, type hints, or error handling included in this code snippet. If this function were called with arguments like add(5, 3), it would return 2 (the result of 5-3) instead of the expected 8 (5+3). This type of mismatch between function naming and actual behavior can lead to confusion and bugs in larger codebases, as developers would reasonably expect a function named add to perform addition. The code would benefit from either renaming the function to subtract or correcting the operation to use the addition operator (+) instead of subtraction (-).",2025-12-14T08:30:44.369048+00:00
"context/change_example.py","This Python script demonstrates the usage of a `change` function from the `pdd.change` module, which appears to be a tool for modifying code using a Large Language Model (LLM). The script imports necessary modules including `os` for environment variables and `rich.console` for formatted terminal output.

The `main()` function sets up example inputs including an original prompt describing a factorial function, the corresponding Python code implementation, and a change request to modify the function to return the square root of the factorial result. It also configures LLM parameters: `strength` (set to 0.5) and `temperature` (set to 0.0), both ranging from 0.0 to 1.0.

The script calls the `change` function with these parameters and expects three return values: the modified prompt, the total API cost, and the model name used. Results are displayed using Rich console formatting with bold labels. Error handling is implemented via a try-except block that catches and displays any exceptions in red bold text.

The code includes a commented-out line for setting a `PDD_PATH` environment variable, suggesting the module may require path configuration. The script follows standard Python conventions with a `if __name__ == __main__` guard for direct execution.",2025-12-14T08:30:52.133090+00:00
"context/change_main_example.py","This Python script demonstrates the usage of a `change_main` function from a command-line program called 'pdd'. The script showcases two operational modes: single-change mode and CSV batch-change mode for modifying code prompts using an LLM (Large Language Model).

The script begins by importing necessary libraries including `os`, `click`, `pathlib`, and `rich` for formatted console output. It sets up a Click context with various configuration parameters such as LLM strength (0.8), temperature (0), programming language (Python), file extension (.py), and a budget limit ($10).

In single-change mode, the script creates sample files including a change prompt requesting error handling for division by zero, a simple divide function, and an input prompt. It then calls `change_main` to process these files and outputs the modified prompt along with cost and model information.

For CSV batch-change mode, the script creates multiple code files (addition and subtraction functions) with corresponding prompts, then generates a CSV file specifying batch modifications like handling overflow errors and optimizing for large integers. The `change_main` function processes all changes in batch, outputting results to a specified directory.

The script serves as a practical example for developers learning to use the pdd tool's change functionality programmatically.",2025-12-14T08:30:59.960708+00:00
"context/cli_example.py","This Python script (demo_run_pdd_cli.py) is a minimal demonstration of how to programmatically invoke the PDD (Prompt-Driven Development) command-line interface. The script performs three main tasks: First, it creates a simple prompt file in an output directory that instructs an AI to write a Python function returning Hello, PDD!. Second, it uses Click's CliRunner to invoke the PDD CLI with specific flags including --local (to run without cloud access) and --quiet (to suppress progress bars), along with the generate command that processes the prompt file and outputs generated code. Third, it displays the results including the exit code, CLI output, and the first few lines of the generated Python file if successful. The script imports the CLI entry point from pdd.cli and uses pathlib for file handling. Key files involved are the input prompt (hello_python.prompt) and the output code (hello.py), both stored in the ./output/ directory. This demo is useful for developers who want to integrate PDD's code generation capabilities into their own Python applications or testing workflows without manually running CLI commands.",2025-12-14T08:31:08.608084+00:00
"context/cli_python_preprocessed.prompt","This document is a comprehensive specification for PDD (Prompt-Driven Development), a Python command-line tool designed to automate code generation, testing, and prompt management using AI models. Built with the Click library for CLI handling and Rich for pretty-printed output, PDD follows a specific prompt file naming convention (`<basename>_<language>.prompt`).

The tool offers eight main commands: **generate** (creates code from prompts), **example** (generates usage examples), **test** (produces unit tests), **preprocess** (processes prompt files with optional XML formatting), **fix** (corrects code errors with optional iterative loop mode), **split** (divides large prompts into smaller files), **change** (modifies prompts based on instructions), and **update** (revises prompts based on code changes).

Key features include multi-command chaining for complex workflows, global options for AI model configuration (strength, temperature), cost tracking via CSV output, and flexible output path specification through command options or environment variables. The document provides extensive examples of helper functions like `construct_paths`, `code_generator`, `context_generator`, `generate_test`, `preprocess`, `xml_tagger`, `fix_errors_from_unit_tests`, `fix_error_loop`, `split`, `change`, and `update_prompt` that implement the core functionality. Additional features include tab completion, colorized output, and progress indicators.",2025-12-14T08:31:16.090787+00:00
"context/click_example.py","This Python script implements a command-line image processing pipeline using the Click library and PIL (Pillow). It creates a chainable CLI tool called imagepipe that processes images in a Unix pipe-like fashion, where the output of one command feeds into the next.

The script defines several key components: a main CLI group with chaining enabled, helper decorators (`processor` and `generator`) for creating stream-based command handlers, and a result callback that evaluates the processing chain.

Available commands include:
- **open**: Loads one or multiple images for processing
- **save**: Saves processed images with customizable filename patterns
- **display**: Opens images in the system's image viewer
- **resize**: Resizes images while maintaining aspect ratio
- **crop**: Crops images by a specified border amount
- **transpose**: Rotates (90°, 180°, 270°) or flips (left-right, top-bottom) images
- **blur**: Applies Gaussian blur with configurable radius
- **smoothen**: Applies smoothening filter with multiple iterations
- **emboss**: Applies emboss effect
- **sharpen**: Enhances image sharpness by a factor
- **paste**: Composites one image onto another with offset positioning

Each command processes images as a stream, enabling efficient batch operations. The script demonstrates advanced Click patterns including command chaining, custom parameter callbacks, and decorator-based stream processing.",2025-12-14T08:31:26.658653+00:00
"context/cloud_function_call.py","This Python script demonstrates how to make an authenticated request to a Google Cloud Function using a Firebase authentication token. The code imports the 'requests' library and defines a function called 'call_cloud_function' that takes a Firebase token as its parameter. 

The function constructs an HTTP GET request to a specific Cloud Function URL hosted on Google Cloud Platform (us-central1-prompt-driven-development.cloudfunctions.net). It includes the Firebase token in the Authorization header using the Bearer token authentication scheme, which is a standard method for passing authentication credentials in HTTP requests.

The script includes a hardcoded JWT (JSON Web Token) that appears to be a Firebase ID token. This token contains encoded user information including a name (Greg Tanaka), a GitHub profile picture URL, email address, and Firebase authentication details indicating the user signed in via GitHub.

After defining the function, the script calls it with the provided token and prints the JSON response from the Cloud Function. This pattern is commonly used when building applications that need to verify user identity before accessing protected cloud resources. The token-based authentication ensures that only authenticated users can invoke the Cloud Function, providing a security layer for serverless backend operations.",2025-12-14T08:31:35.142547+00:00
"context/cmd_test_main_example.py","This Python file, `test_cli_example.py`, demonstrates how to build a Click-based command-line interface (CLI) for generating or enhancing unit tests using the `cmd_test_main` function from the `pdd` package. The CLI is structured with a top-level group command that accepts global options such as `--verbose`, `--strength`, `--temperature`, `--force`, and `--quiet`, which control logging, AI generation parameters, and output behavior. These options are stored in a shared context for use by subcommands.

The main subcommand, `test`, accepts two required arguments: `prompt_file` (a text file containing the prompt that generated the code) and `code_file` (the source code to be tested). It also supports several optional arguments including `--output` for specifying where to save generated tests, `--language` to override language detection, `--coverage-report` for providing an existing coverage report, `--existing-tests` for merging with existing test files, `--target-coverage` for setting a desired coverage percentage, and `--merge` to combine new tests with existing ones.

When executed, the `test` command invokes `cmd_test_main()` with the provided parameters and outputs the generated test code, the total cost of the AI operation, and the model name used. The script is designed to be run from the command line and serves as a practical example for integrating AI-powered test generation into development workflows.",2025-12-14T08:31:43.738594+00:00
"context/code_generator_example.py","This Python script demonstrates the usage of a `code_generator` function imported from a `pdd.code_generator` module. The main function serves as an entry point to showcase how to generate code using a language model (LLM).

The script reads a prompt from an external file located at prompts/generate_test_python.prompt rather than using a hardcoded prompt (a commented-out example shows a factorial function request). It then configures several parameters for the code generation process: the target language is set to Python, the LLM strength is set to 0.5 (on a 0.0-1.0 scale), temperature is set to 0.0 for deterministic output, and verbose mode is enabled for detailed logging.

The `code_generator` function is called with these parameters and returns three values: the generated runnable code, the total cost of the API call, and the model name used. These results are then printed to the console, with the cost formatted to six decimal places.

The script includes error handling via a try-except block to catch and display any exceptions that occur during code generation. The standard `if __name__ == __main__` pattern ensures the main function only runs when the script is executed directly.",2025-12-14T08:31:52.827325+00:00
"context/code_generator_main_example.py","This Python script demonstrates how to use the `code_generator_main` function from a PDD (Prompt-Driven Development) CLI package. The script is designed to be self-contained and predictable by using mocked versions of actual code generator functions, avoiding real LLM API calls and API key requirements.

The script includes mock functions for both local and incremental code generation (`mock_local_code_generator_func` and `mock_incremental_code_generator_func`), along with a `MockContext` class to simulate Click CLI context objects. It defines several constants typically used by the PDD package, including default strength, time, and environment variable names for Firebase and GitHub authentication.

The main example function (`main_example`) runs through four scenarios: (1) full local code generation for a simple Java program, (2) incremental code generation with an explicit original prompt for Python code modification, (2b) forced incremental generation even when the model suggests full regeneration, and (3) cloud generation attempt that falls back to local execution due to dummy credentials.

Each scenario creates prompt files, sets up CLI parameters, calls `code_generator_main`, and prints results. The script uses Python's `unittest.mock.patch` decorator to replace actual generator functions with mocks during execution. Output files are written to a designated directory that gets cleaned up at the start of each run.",2025-12-14T08:32:00.310410+00:00
"context/comment_line_example.py","This file documents the `comment_line` function from the `comment_line.py` module, which is designed to comment out lines of code based on specified comment characters. The function accepts two parameters: `code_line`, a string representing the line of code to be commented, and `comment_characters`, which defines how the commenting should be applied. The `comment_characters` parameter supports three modes: (1) a single comment character like # for Python-style comments, (2) a pair of start and end comment characters separated by a space (e.g., <!-- --> for HTML), or (3) the special string del which deletes the line entirely by returning an empty string. The function returns the appropriately commented line or an empty string if deletion is specified. The file includes practical examples demonstrating each use case: commenting a Python print statement with #, wrapping an HTML tag with <!-- -->, and deleting a line using del. This utility function is useful for code processing tools that need to programmatically comment out or remove lines across different programming languages with varying comment syntax conventions.",2025-12-14T08:32:09.907765+00:00
"context/config_example.py","This code snippet demonstrates a Python initialization pattern for configuration management. The file imports the `init_config` function from a utility module located at `utils.config`. Immediately after the import, it calls `init_config()` to initialize the application's configuration settings. The comment Initialize configuration FIRST emphasizes the importance of running this configuration setup before any other code executes, suggesting this is likely placed at the top of a main application file or entry point. This pattern is common in applications that need to load environment variables, set up logging, establish database connections, or configure other global settings before the rest of the application runs. The `utils.config` module path indicates a well-organized project structure with utility functions separated into their own directory. This approach ensures that all subsequent code has access to properly initialized configuration values, preventing errors that could occur if other modules attempted to access configuration before it was set up. The explicit call to `init_config()` rather than relying on import side effects follows Python best practices for explicit initialization.",2025-12-14T08:32:16.741064+00:00
"context/conflicts_in_prompts_example.py","This Python script demonstrates a conflict detection system for AI prompts within the PDD Cloud project. The main function defines two detailed prompts: the first instructs an AI to create an `auth_helpers.py` module for Firebase authentication, including token verification, user extraction, and security best practices; the second prompt requests a `User` dataclass model with fields like uid, email, credits, and contributor tier, along with methods for validation, serialization, Firestore interaction, and permission management.

The script imports a `conflicts_in_prompts` function from the `pdd.conflicts_in_prompts` module and uses the Rich library for formatted console output. It configures LLM parameters including strength (0.89), temperature (0), and verbose mode (True), then calls the conflict detection function with both prompts.

The output displays the model used, total API cost, and any detected conflicts with suggested changes for each prompt. This tool appears designed to identify inconsistencies or contradictions between multiple AI prompts before they are used in code generation, helping maintain coherence across different parts of a software project. The script serves as both a demonstration and testing utility for the prompt conflict detection functionality.",2025-12-14T08:32:23.655288+00:00
"context/conflicts_main_example.py","This Python script demonstrates the usage of a conflict detection system between two AI prompts. It imports the `conflicts_main` function from a module called `pdd.conflicts_main`. The script creates two example prompt files: one defining an AI assistant persona and another defining a helpful chatbot persona, both stored in an output directory.

A `MockContext` class is defined to simulate a Click command-line interface context, containing configuration parameters like 'force', 'quiet', 'strength', and 'temperature'. However, the implementation appears to have a bug where the `obj` dictionary is immediately overwritten with an empty dictionary.

The main execution creates a mock context instance and calls the `conflicts_main` function with the two prompt file paths, an output CSV file path, and verbose mode enabled. The function returns three values: detected conflicts, total processing cost, and the model name used for analysis.

Finally, the script prints the results including the conflicts found, the cost formatted to six decimal places, and the model name. The results are also saved to a CSV file named 'conflicts_output.csv'. This appears to be a testing or demonstration script for a prompt conflict detection tool, likely used to identify inconsistencies or contradictions between different AI system prompts.",2025-12-14T08:32:32.189571+00:00
"context/construct_paths_example.py","This Python script, `demo_construct_paths.py`, serves as a minimal end-to-end demonstration for the `construct_paths` function from the `pdd.construct_paths` module, which is part of a Prompt-Driven Development (PDD) toolkit.

The script follows a five-step workflow: First, it creates a temporary prompt file named Makefile_makefile.prompt containing a simple task description for generating a hello world function. Second, it prepares the arguments that would typically be supplied by a command-line interface, including the input file path, force and quiet flags, the command type (generate), and command options.

Third, the script invokes the `construct_paths` function with these parameters, which returns four values: a resolved configuration, input strings (the file contents that were read), output file paths (where PDD will write results), and the detected programming language for the operation.

Fourth, it prints these returned values to the console in a formatted manner, iterating through dictionaries to display keys and values with proper indentation for multiline content. Finally, the script cleans up by deleting the temporary prompt file to leave the working directory in its original state.

This demo is useful for developers wanting to understand how the PDD path construction mechanism works internally before integrating it into larger applications.",2025-12-14T08:32:40.435943+00:00
"context/context_generator_example.py","This Python script demonstrates the usage of a context generator module from a package called 'pdd'. The script begins by importing necessary dependencies: the 'os' module for environment variable access, a 'context_generator' function from 'pdd.context_generator', and 'print' from the 'rich' library for enhanced console output formatting.

The script first checks for the 'PDD_PATH' environment variable and raises an error if it's not set, ensuring proper configuration before execution. It then defines several input parameters for the context generator: a simple Python function that adds two numbers, a corresponding prompt describing the desired functionality, the target programming language (Python), and configuration values for 'strength' (0.5) and 'temperature' (0.0) which likely control the generation behavior.

The main functionality calls the 'context_generator' function with these parameters and verbose mode enabled, receiving three return values: generated example code, the total cost of the operation (suggesting this may use a paid API service), and the name of the model used for generation.

Finally, the script outputs the results using rich formatting, displaying the generated code, the monetary cost formatted to six decimal places, and the model name. This appears to be a code generation or documentation tool that leverages AI models.",2025-12-14T08:32:48.212554+00:00
"context/context_generator_main_example.py","This Python script demonstrates the usage of a context generator tool, likely part of a prompt-driven development (PDD) framework. The script imports the Click library for command-line interface handling and a custom `context_generator_main` function from the `pdd` module.

The code sets up a Click context object with specific parameters: `force`, `quiet`, and `verbose` flags are all set to False, meaning the tool won't overwrite existing files, will display output messages, but won't show verbose details. Additional AI model configuration is stored in the context object, including a `strength` parameter set to 0.575 and a `temperature` of 0.0, both controlling the behavior of an underlying AI model.

The script defines three file paths: a prompt file located at prompts/get_extension_python.prompt, a code file at pdd/get_extension.py, and an output destination at output/example_code.py. These are passed to the `context_generator_main` function, which processes the inputs and returns generated example code, the total cost of the operation, and the name of the AI model used.

Finally, the script prints the generated code, the monetary cost formatted to six decimal places, and the model name used for generation.",2025-12-14T08:32:56.182460+00:00
"context/continue_generation_example.py","This Python script demonstrates the usage of a `continue_generation` function imported from a `pdd` module. The main purpose is to continue text generation using a language model (LLM) and track associated costs.

The script performs the following operations:

1. **Loading Input Data**: It reads two files - a preprocessed prompt from context/cli_python_preprocessed.prompt and an unfinished LLM output fragment from context/llm_output_fragment.txt.

2. **Configuration Parameters**: Sets up LLM parameters including a strength value of 0.915 and temperature of 0 (deterministic output).

3. **Text Generation**: Calls the `continue_generation` function with the loaded prompt, partial output, and configuration parameters. The function returns the completed text, total cost, and model name used.

4. **Output Handling**: Prints the total cost and model name to the console, then saves the final generated output to context/final_llm_output.py.

5. **Error Handling**: Includes try-except blocks to catch FileNotFoundError and general exceptions, providing appropriate error messages.

The script follows a standard Python entry point pattern with `if __name__ == __main__`. It appears to be part of a larger system for iterative or continued text generation with cost tracking capabilities.",2025-12-14T08:33:04.461704+00:00
"context/crash_main_example.py","This Python script demonstrates the usage of a `crash_main` function from the `pdd.crash_main` module, which is designed to automatically fix errors in code that caused a crash. The script creates a complete test scenario by generating four example files: a prompt file describing the desired factorial function, a buggy factorial code module that lacks negative number validation, a main program that incorrectly calls the factorial function with a negative number, and an error log capturing the resulting RecursionError.

The demonstration sets up a Click context with configuration parameters including strength, temperature, and verbosity settings. It then invokes `crash_main` with iterative fixing enabled, allowing up to 5 attempts to fix the code within a $5.00 budget. The function takes the prompt, code, program, and error files as inputs and produces fixed versions of both the code module and the calling program.

The script outputs the results including whether the fix was successful, the number of attempts required, total cost incurred, the model used, and displays the corrected code. This appears to be part of a prompt-driven development (PDD) toolkit that uses AI to automatically diagnose and repair code crashes by analyzing error logs and the original prompts that generated the faulty code.",2025-12-14T08:33:11.914892+00:00
"context/create_gcp_credential.py","This Python script demonstrates how to retrieve and decode a Google Cloud Platform (GCP) service account credential from Azure Key Vault for use with Vertex AI services. The code imports the `base64` and `json` modules and includes a commented-out placeholder class for `AzureKeyVaultService` that shows the expected structure of a GCP service account JSON (containing fields like project_id, private_key, client_email, etc.).

The main workflow consists of two steps: First, it fetches a base64-encoded service account JSON string from Azure Key Vault using the secret name GCP-VERTEXAI-SERVICE-ACC. Second, it decodes the base64 string and parses it into a Python dictionary using `json.loads()`. The script includes error handling for JSON decoding errors and general exceptions.

The decoded credentials can then be used with Google's `service_account.Credentials.from_service_account_info()` method to authenticate with GCP services, particularly for initializing ChatVertexAI. Currently, the actual Key Vault service call is commented out, leaving the `vertexai_service_account_b64` variable empty, which causes the script to print a failure message. This appears to be a template or example code for integrating Azure Key Vault secret management with GCP authentication in a cross-cloud scenario.",2025-12-14T08:33:19.789438+00:00
"context/detect_change_example.py","This Python script is designed to analyze prompt files for potential changes using a change detection system. It imports the `detect_change` function from a module called `pdd.detect_change` and uses the Rich library's Console for formatted terminal output.

The script defines a list of four prompt files located in context and prompts directories, including files for Python preamble, code changes, error fixing, and code generation. There's also commented-out code that would dynamically gather all `.prompt` files from these directories.

The main purpose is to detect whether these prompt files need modifications based on a specific change description: making prompts more compact by utilizing a Python preamble context file. The script configures two LLM parameters: `strength` (set to 1, controlling model capability/cost) and `temperature` (set to 0, minimizing output randomness).

The `detect_change` function is called with these parameters and returns a list of detected changes, total cost, and the model name used. The script then displays the results using Rich's formatted console output, showing each prompt file name with its corresponding change instructions, along with the total API cost and model information. Error handling is implemented via a try-except block to catch and display any exceptions that occur during execution.",2025-12-14T08:33:28.540139+00:00
"context/detect_change_main_example.py","This Python script serves as a test harness or driver for the `detect_change_main` function from the `pdd.detect_change_main` module. The script simulates a command-line interface (CLI) environment using the Click library without actually running from the command line.

The main function creates a Click context object with model parameters including strength (0.5), temperature (0), and flags for force and quiet modes. It defines a list of four prompt files related to Python code generation and error handling, along with a change description file path.

The script writes a specific change instruction to the change file, directing the system to use a Python preamble context to make prompts more compact. It sets up an output path for results in CSV format and ensures the output directory exists.

The core functionality calls `detect_change_main()` with the configured context, prompt files, change file, and output path. This function returns a list of changes, total cost, and model name. The script then prints the model used, the total cost formatted to six decimal places, and iterates through each change to display the prompt name and change instructions.

Error handling is implemented with a try-except block to catch and display any exceptions that occur during execution. The script runs the main function when executed directly.",2025-12-14T08:33:36.229303+00:00
"context/device_flow.txt","This document explains GitHub's Device Flow authentication mechanism, designed for authorizing users in headless applications like CLI tools or Git Credential Manager. The process involves three main steps: First, the app requests device and user verification codes from GitHub via a POST request to the device/code endpoint, providing the client_id and optional scopes. This returns a device_code, user_code, verification_uri, expiration time (default 900 seconds), and polling interval. Second, the app displays the user_code and prompts the user to enter it at github.com/login/device. Third, the app polls GitHub's access_token endpoint until the user authorizes the device or codes expire, respecting the minimum polling interval to avoid rate limits. Upon successful authorization, the app receives an access token for API calls. The document details response formats (URL-encoded, JSON, or XML), rate limits (50 submissions per hour, with slow_down penalties for excessive polling), and various error codes including authorization_pending, slow_down, expired_token, unsupported_grant_type, incorrect_client_credentials, incorrect_device_code, access_denied, and device_flow_disabled. Device flow must be enabled in app settings before use.",2025-12-14T08:33:43.206937+00:00
"context/edit_file_example.py","This Python script serves as a test harness for demonstrating and verifying an `edit_file` module's functionality. It imports a `run_edit_in_subprocess` function from a separate module and uses it to programmatically edit text files based on natural language instructions.

The script requires the `PDD_PATH` environment variable to be set, pointing to a working directory. It creates a dummy example file with initial content and then applies specific editing instructions: changing original to UPDATED, replacing line 3 entirely, and adding a new line 5.

Key components include:
- **Configuration section**: Sets up paths and imports the editing function with error handling
- **`create_example_files()`**: Creates an output directory and populates a test file with sample content
- **`run_edit_file_test()`**: Executes the edit operation and optionally runs verification checks to confirm expected changes were applied
- **`run_example()`**: Main orchestration function that creates files and runs the test

The verification system checks for specific content changes (presence of UPDATED, replaced line 3, added line 5) and reports detailed pass/fail status with visual indicators. The script appears designed for testing an LLM-powered file editing system that uses MCP (Model Context Protocol) adapters, requiring API keys and external services to function properly.",2025-12-14T08:33:51.812223+00:00
"context/example.prompt","This document provides guidelines for creating code examples and documentation for a Python module system. Key instructions include: 1) Environment setup is pre-configured, so examples should focus on module usage rather than installation. 2) Examples will be placed in a 'pdd' directory and must use absolute imports in the format 'from pdd.module_name import module_name'. 3) All input/output units must be clearly documented (e.g., dollars per million tokens). 4) Click command-line examples should include arguments to enable execution without user interaction. 5) When file operations are involved, example input files should be created with appropriate content. 6) Error handling via try/except blocks should be avoided in examples. 7) All generated output files, including prompts and other artifacts, must be saved to the './output' directory. 8) The PDD_PATH environment variable is pre-set to point to the pdd directory location. These guidelines ensure consistent, runnable, and well-documented code examples across the module system.",2025-12-14T08:34:01.481061+00:00
"context/execute_bug_to_unit_test_failure.py","This file is a simple Python script that demonstrates the use of an imported function. The script imports a function called add from a module located at context.bug_to_unit_test_failure_example. This import path suggests the code is part of a larger project structure, likely related to testing or debugging examples, specifically dealing with unit test failures.

After importing the function, the script calls the add function with two integer arguments (2 and 3) and prints the result to the console. The expected output would be 5, assuming the add function performs standard addition of its two parameters.

The code appears to be a minimal test or demonstration script, possibly used to verify that the add function works correctly or to showcase a specific bug scenario in a unit testing context. The naming convention of the import path (bug_to_unit_test_failure_example) suggests this might be part of educational material or documentation showing how bugs can be identified through unit test failures. The script itself is straightforward with just two lines of executable code - one import statement and one print statement that calls the imported function.",2025-12-14T08:34:08.007223+00:00
"context/final_llm_output.py","This Python file implements a command-line interface (CLI) for PDD (Prompt-Driven Development), a tool that leverages AI to assist with code generation and management. Built using the Click library and Rich console for output formatting, the CLI provides several chainable commands:

**Core Commands:**
- `generate`: Creates runnable code from a prompt file
- `example`: Generates example code from existing code and its prompt
- `test`: Creates unit test files for given code
- `preprocess`: Preprocesses prompt files with optional XML tagging
- `fix`: Repairs errors in code and unit tests, with optional iterative loop mode
- `split`: Divides large prompts into smaller, manageable files
- `change`: Modifies prompts based on change requests
- `update`: Updates original prompts based on code modifications
- `install_completion`: Sets up shell autocompletion for bash, zsh, or fish
- `version`: Displays CLI version

**Global Options:**
The CLI supports flags for force overwrite, AI model strength/temperature settings, verbosity control, and cost tracking via CSV output.

**Architecture:**
Commands use a processor decorator pattern enabling Unix-like piping between subcommands. Each command handles file path construction, calls appropriate generator modules from the pdd package, writes outputs, and reports costs. Error handling with Rich-formatted messages is implemented throughout.",2025-12-14T08:34:15.513743+00:00
"context/find_section_example.py","This file provides documentation and example usage for the `find_section` function from the `pdd.find_section` module. The function is designed to parse text containing code blocks (typically from LLM output) and identify the locations of different code sections within the text.

The example demonstrates how to use the function by first importing it, then providing a sample string containing multiple code blocks in different programming languages (Python and JavaScript). The text is split into lines using `splitlines()`, and then passed to `find_section()` which returns a list of tuples identifying each code block.

The function accepts three parameters: `lines` (a required list of strings representing text lines), `start_index` (an optional integer defaulting to 0 for where to begin searching), and `sub_section` (an optional boolean flag defaulting to False).

The output is a list of tuples, each containing the programming language identifier, the starting line index, and the ending line index of each code block found. The example also shows loading content from an external file (`unrunnable_raw_llm_output.py`) and processing it the same way. This utility is useful for extracting and processing code snippets embedded within larger text outputs.",2025-12-14T08:34:25.165817+00:00
"context/firecrawl_example.py","This Python script demonstrates how to use the Firecrawl library to scrape web content. The code begins with a comment indicating that the firecrawl-py package should be installed using pip. It then imports the FirecrawlApp class from the firecrawl module along with the os module for environment variable access.

The script initializes a FirecrawlApp instance by retrieving an API key from the environment variable 'FIRECRAWL_API_KEY'. If this environment variable is not set, it falls back to a placeholder string 'YOUR_API_KEY', which would need to be replaced with an actual API key for the code to function properly.

The main functionality involves calling the scrape_url method on the app instance, targeting Google's homepage (https://www.google.com). The method is configured to return the scraped content in markdown format by passing ['markdown'] as the formats parameter. The result is stored in the scrape_result variable.

Finally, the script prints the markdown representation of the scraped content by accessing the markdown attribute of the scrape_result object. This is a simple example showcasing Firecrawl's capability to convert web pages into structured markdown format, which can be useful for content extraction, documentation, or data processing tasks.",2025-12-14T08:34:32.867374+00:00
"context/fix_code_loop_example.py","This Python script demonstrates the usage of a `fix_code_loop` function from the `pdd.fix_code_loop` module, which appears to be an automated code debugging/fixing utility. The main function creates a testing environment by generating two example files: a code file (`module_to_test.py`) containing a simple `calculate_average` function, and a verification file (`verify_code.py`) that intentionally triggers an error by passing a string instead of a list to the function.

The script then invokes `fix_code_loop` with several configuration parameters including: the code file path, the original prompt that generated the code, the verification program, a model strength setting (0.5), temperature (0 for deterministic output), maximum attempts (5), a budget limit ($5 USD), and an error log file path.

After execution, the script outputs the results including whether the fix succeeded, the number of attempts made, total cost incurred, the model used, and the final corrected code. There's also commented-out code suggesting an alternative use case for testing a different module called `get_extension`. The cleanup section for removing temporary files is commented out, likely for debugging purposes. This appears to be a development/testing utility for an AI-powered code correction system.",2025-12-14T08:34:40.290910+00:00
"context/fix_code_module_errors_example.py","This Python script demonstrates the usage of the `fix_code_module_errors` function from the `pdd.fix_code_module_errors` module, which is designed to automatically fix code errors using an AI/LLM-based approach.

The example sets up a scenario with a buggy program that attempts to calculate the sum of a string 123 instead of a list of numbers, resulting in a TypeError. The script defines the original erroneous program, the prompt that generated it, the problematic code module, and the error message received.

The `fix_code_module_errors` function is called with several parameters: the program with the error, the original prompt, the code module, the error message, a model strength setting (0.5), temperature (0 for deterministic output), and a verbose flag for debugging.

The function returns seven values: boolean flags indicating whether the program and code module need updates, the fixed versions of both, the raw LLM output from the fix attempt, the total API cost in USD, and the model name used.

Finally, the script prints all results including whether updates are needed, the corrected code, the raw fix output, the API cost, and which model was utilized. This serves as a practical example for developers wanting to integrate automated code error correction into their workflows.",2025-12-14T08:34:48.261219+00:00
"context/fix_error_loop_example.py","This Python script serves as a demonstration entry point for an automated error-fixing loop system. The main function showcases the usage of a `fix_error_loop` function imported from the `pdd.fix_error_loop` module, which appears to be part of a larger code generation or debugging framework.

The script configures several parameters for the error-fixing process, including paths to a unit test file, a code file, a verification program, and a prompt file. It also sets numerical parameters such as strength (0.85), temperature (1), maximum attempts (5), and a budget limit ($100) for the fixing process.

The `fix_error_loop` function is called with these parameters and returns multiple values: a success flag, the final unit test content, the final code, total attempts made, total cost incurred, and the model name used. The script uses the Rich library to display formatted output panels showing these results.

Error handling is implemented via a try-except block to catch and display any exceptions that occur during execution. The script follows a standard Python pattern with a `if __name__ == __main__` guard to ensure the main function only runs when executed directly. This appears to be part of a prompt-driven development (PDD) workflow for iteratively fixing code errors using AI assistance.",2025-12-14T08:34:56.181068+00:00
"context/fix_errors_from_unit_tests_example.py","This Python script serves as a demonstration for the `fix_errors_from_unit_tests` function from the `pdd` module. The main function sets up example inputs to showcase how the error-fixing utility works. It defines a sample unit test for an `add` function with an intentionally incorrect test case (asserting that `add(0, 0) == 1`), along with the corresponding implementation code that includes a deprecated NumPy function call (`np.asscalar`). The script also includes an error message string containing both an AssertionError and a DeprecationWarning. Configuration parameters include an error log file path, an LLM strength value of 0.85, and a temperature setting of 1.0. The `fix_errors_from_unit_tests` function is called with these inputs and returns multiple values: flags indicating whether the unit test and code were updated, the fixed versions of both, analysis results, total cost, and the model name used. The script uses the `rich` library to print formatted output displaying all these results. This appears to be part of a larger system that uses language models to automatically diagnose and fix errors in code based on failing unit tests.",2025-12-14T08:35:04.044052+00:00
"context/fix_main_example.py","This Python script defines a command-line interface (CLI) tool for automatically fixing errors in code and unit tests. It uses the Click library for argument parsing and Rich for formatted console output.

The main component is a `fix_command` function decorated with Click options that accepts several parameters: paths to a prompt file, code file, unit test file, and error log file; optional output paths for fixed tests, code, and results; a loop flag for iterative fixing; a verification program path; maximum fix attempts (default 3); a budget limit (default $5.00); an auto-submit flag; and an agentic fallback option.

When executed, the command calls `fix_main` from the `pdd.fix_main` module, passing all configuration parameters. The function returns success status, fixed test/code content, attempt count, cost, and model used. Results are displayed with color-coded messages—green for success showing attempts, cost, and model info, or red for failure.

The script's main block sets up a default context with strength, temperature, force, and quiet settings, then configures test arguments pointing to specific prompt, code, test, and verification files in a development environment. This appears to be part of a larger pdd (possibly prompt-driven development) system for AI-assisted code repair.",2025-12-14T08:35:11.924131+00:00
"context/fix_verification_errors_example.py","This Python script demonstrates the usage of the `fix_verification_errors` function from the `pdd` package, which automatically identifies and fixes errors in code modules using LLM (Large Language Model) analysis of verification output logs.

The script sets up a complete example scenario with several key components: a verification program that tests a calculator module, an original prompt describing the intended functionality, a buggy code module containing an intentional error (subtraction instead of addition), and sample verification output showing the failure.

The example uses a `calculator_module` with an `add_numbers` function that incorrectly performs subtraction. When the verification program runs, it compares expected results against actual results and outputs clear success/failure markers that the LLM can interpret.

The `fix_verification_errors` function accepts parameters including the program code, original prompt, buggy code, verification output, LLM strength (model capability), temperature (randomness control), and verbose mode settings. It returns a dictionary containing information about issues found, whether the program or code was updated, the fixed versions, LLM model used, total cost, and explanations of identified issues.

Prerequisites include having the pdd package installed, prompt template files accessible, LLM API keys configured, and proper environment variables set. The script uses the Rich library for formatted console output to display results.",2025-12-14T08:35:20.612650+00:00
"context/fix_verification_errors_loop_example.py","This Python script demonstrates the usage of the `fix_verification_errors_loop` function from the `pdd` module, which is designed to automatically fix bugs in code through iterative verification and correction. The example sets up a simple calculator module scenario with an intentional bug—a subtraction operation instead of addition in the `add_numbers` function.

The script creates three files: a main program (`program.py`) that tests the calculator module with command-line arguments, a buggy code module (`calculator_module.py`) containing the flawed function, and a verification program that asserts expected behavior. 

Key configuration parameters include: `strength` (0.5) for LLM model selection, `temperature` (0.0) for deterministic outputs, `max_attempts` (5) for retry limits, and a `budget` ($10) for cost control. The script also specifies output paths for fixed code and logging.

The `fix_verification_errors_loop` function is called with all these parameters, including program arguments [5, 3]. Upon completion, the script prints results including success status, final corrected program and code, total attempts made, total cost incurred, and the model name used. This demonstrates an automated debugging workflow using LLM-powered code correction with verification feedback loops.",2025-12-14T08:35:29.500398+00:00
"context/fix_verification_main_example.py","This Python script demonstrates the usage of a `fix_verification_main` function from a `pdd` package, designed for automated code verification and fixing using LLM (Large Language Model) capabilities. The script sets up a testing environment by creating example files including a prompt file, intentionally buggy calculator code (that subtracts instead of adds), a program to run the code, and a verification program with assertions.

The demonstration showcases two verification modes: Single Pass Mode and Loop Mode. In Single Pass Mode (`loop=False`), the LLM evaluates program output against the original prompt once and may propose fixes. In Loop Mode (`loop=True`), the system iteratively runs the program, uses the LLM to check output, applies fixes to the code/program, and runs verification tests until success is achieved or limits (max attempts or budget) are reached.

The script uses Click for CLI context simulation and includes configurable parameters like temperature, strength, budget limits, and maximum attempts. It creates a dummy Click context to pass parameters as if invoked from a command line. Output files track verification results, corrected code, and corrected programs. The Rich library provides formatted console output. This serves as a practical example for integrating LLM-based automated code verification and repair workflows into development pipelines.",2025-12-14T08:35:37.889808+00:00
"context/gcs_hmac_test.py","This Python script demonstrates how to upload files to Google Cloud Storage (GCS) using the boto3 library with HMAC (Hash-based Message Authentication Code) credentials. The script leverages the S3-compatible API that GCS provides, allowing developers to use familiar AWS SDK tools with Google's storage service.

The code begins by loading environment variables from a .env file using the dotenv library, specifically requiring three credentials: GCS_HMAC_ACCESS_KEY_ID, GCS_HMAC_SECRET_ACCESS_KEY, and GCS_BUCKET_NAME. It configures logging for debugging purposes and validates that all required environment variables are present before proceeding.

The main functionality creates an S3 client configured to point to Google's storage endpoint (https://storage.googleapis.com) instead of AWS. The script includes commented-out configuration options for newer versions of Boto3 (>1.35) that may require specific signature and checksum settings.

Once the client is established, the script uploads a simple test text file (test-hmac-upload.txt) to the specified GCS bucket using the put_object method. Comprehensive error handling is implemented to catch and log various failure scenarios, including missing buckets, authentication failures, and other unexpected errors. Upon successful upload, it provides a direct link to verify the uploaded file in the Google Cloud Console.",2025-12-14T08:35:46.982758+00:00
"context/gemini_may_pro_example.py","This code snippet demonstrates how to use the LiteLLM library to interact with Google's Vertex AI Gemini model. LiteLLM is a Python library that provides a unified interface for calling various Large Language Model (LLM) APIs using a consistent format similar to OpenAI's API structure.

The code imports the `completion` function from the `litellm` package, which is the primary method for generating text completions. It then makes an API call to the Vertex AI platform, specifically targeting the gemini-2.5-pro-preview-05-06 model, which is a preview version of Google's Gemini 2.5 Pro model.

The request follows a standard chat completion format with a messages array containing a single user message. The placeholder text Your prompt here indicates where users should insert their actual query or instruction for the model.

The response from the API call is stored in the `response` variable and then printed to the console. This basic example serves as a template for developers looking to integrate Vertex AI's Gemini models into their applications using LiteLLM's simplified interface, which abstracts away the complexity of directly calling Google Cloud's APIs while maintaining compatibility with OpenAI-style request formatting.",2025-12-14T08:35:55.099868+00:00
"context/generate_output_paths_example.py","This Python script demonstrates the usage of a `generate_output_paths` function from a module called `pdd.generate_output_paths`. The script begins by setting up the Python path to import the module from a parent directory. It then runs through eight different scenarios to showcase how the function handles various input configurations for generating output file paths.

The scenarios include: (1) using default settings for a 'generate' command, (2) specifying a custom output filename, (3) specifying an output directory, (4) using environment variables for a 'fix' command, (5) mixing user inputs with defaults, (6) handling commands with fixed extension defaults like 'preprocess', (7) handling unknown commands which return empty dictionaries, and (8) handling missing basename inputs.

Each scenario prints the inputs, the actual result from the function, and the expected output for verification. The script creates temporary directories as needed and cleans them up afterward. The function appears to resolve paths to absolute paths, handle both file and directory specifications, support multiple output keys per command (like output_test, output_code, output_results for 'fix'), and fall back to environment variables when user input is not provided. This serves as comprehensive documentation and testing for the path generation utility.",2025-12-14T08:36:02.975783+00:00
"context/generate_test_example.py","This Python script demonstrates the usage of a test generation utility from the 'pdd' package. The code imports necessary modules including 'os', a 'generate_test' function from 'pdd.generate_test', and 'rich' for enhanced console output formatting.

The script sets up parameters for generating unit tests automatically. It defines a sample prompt requesting a factorial function, along with the actual implementation of a recursive factorial function that handles base cases (0 and 1) and recursively calculates factorials for larger numbers.

Additional configuration parameters include a 'strength' value of 0.5, a 'temperature' of 0.0 (for deterministic output), and specifies Python as the target language.

The main functionality is wrapped in a try-except block that calls the 'generate_test' function with all defined parameters. Upon successful execution, it displays the generated unit test, the total cost of the API call (formatted to 6 decimal places), and the name of the AI model used. The output is styled using Rich's formatting capabilities with colored and bold text.

If an error occurs during test generation, the exception is caught and displayed in bold red text. The script also includes a commented-out line for setting a custom PDD_PATH environment variable.",2025-12-14T08:36:11.825113+00:00
"context/get_comment_example.py","This file provides documentation and an example usage for the `get_comment` function from the `pdd.get_comment` module. The function retrieves the comment character(s) associated with a specified programming language. It takes a single input parameter, `language`, which is a case-insensitive string representing the programming language name. The function returns a string containing the comment character(s) for that language (e.g., `#` for Python, `//` for Java). If the `PDD_PATH` environment variable is not set, the language is not found in the CSV data source, or any error occurs, the function returns the default string `'del'`. The example demonstrates calling the function with various languages including Python, Java, JavaScript, and an unknown language. Important prerequisites include setting the `PDD_PATH` environment variable to point to the directory containing `data/language_format.csv`, which must have at least two columns: `language` and `comment`. This utility is useful for applications that need to dynamically determine comment syntax across different programming languages.",2025-12-14T08:36:20.512034+00:00
"context/get_extension_example.py","This Python code snippet demonstrates the usage of a utility function called `get_extension` imported from a module named `pdd.get_extension`. The function appears to be designed to return the appropriate file extension associated with a given programming language or file type name.

The code shows three example calls to the `get_extension` function:

1. When passed Python, it returns .py - the standard file extension for Python source files.
2. When passed Makefile, it returns Makefile - since Makefiles typically don't have an extension and are named exactly Makefile.
3. When passed JavaScript, it returns .js - the standard extension for JavaScript files.

This utility function is likely part of a larger package (pdd) that deals with programming language detection or file type management. It provides a convenient way to map human-readable language names to their corresponding file extensions, which could be useful in various scenarios such as code generation, file creation, syntax highlighting configuration, or build system automation. The function handles both cases where languages have traditional extensions and special cases like Makefile where the filename itself is the identifier.",2025-12-14T08:36:27.325118+00:00
"context/get_jwt_token_example.py","This Python script is a CLI authentication utility for a Prompt Driven Development (PDD) application that handles Firebase authentication using GitHub's Device Flow OAuth. The script begins by loading the Firebase API key from environment variables or appropriate `.env` files based on the target environment (local, staging, or production). It dynamically configures the application name and token variable names based on the `PDD_ENV` environment variable.

The main async function orchestrates the authentication process by calling `get_jwt_token()` with Firebase and GitHub credentials. It includes comprehensive error handling for various failure scenarios including authentication errors, network issues, token problems, rate limiting, and user cancellation.

Upon successful authentication, the script updates the local `.env` file with the obtained JWT token. It writes both an environment-specific token variable (e.g., `JWT_TOKEN_STAGING`) and a generic `JWT_TOKEN` fallback, either replacing existing values or appending new ones.

Key dependencies include the `pdd.get_jwt_token` module which provides the core authentication logic and custom exception classes. The script uses asyncio for asynchronous execution and pathlib for cross-platform file path handling. This utility is designed to streamline developer authentication workflows across different deployment environments.",2025-12-14T08:36:34.875563+00:00
"context/get_language_example.py","This Python script serves as a demonstration module for the `get_language` function imported from the `pdd.get_language` package. The main purpose of the code is to showcase how to use the `get_language` utility to identify programming languages based on file extensions.

The script defines a `main()` function that performs the following operations:

1. **Sets up a test case**: It initializes a sample file extension variable (`.py`) to use as input for the demonstration.

2. **Calls the get_language function**: The function is invoked with the file extension to retrieve the corresponding programming language name.

3. **Handles the output**: The script includes conditional logic to print appropriate messages based on whether a language was successfully identified or not.

4. **Implements error handling**: A try-except block wraps the main logic to gracefully catch and display any exceptions that might occur during execution.

5. **Uses standard Python entry point**: The `if __name__ == __main__:` guard ensures the `main()` function only runs when the script is executed directly, not when imported as a module.

The code follows Python best practices including type hints for variables and a docstring for the main function. It appears to be part of a larger project (pdd) that likely deals with programming language detection or file type identification.",2025-12-14T08:36:42.707634+00:00
"context/get_run_command_example.py","This Python script demonstrates the usage of the `get_run_command` module, which retrieves execution commands for various programming languages based on file extensions. The module reads configuration data from a CSV file located at `$PDD_PATH/data/language_format.csv`, which maps file extensions to their corresponding run command templates (e.g., `.py` maps to `python {file}`).

The script showcases six key examples: (1) retrieving a run command template for Python files using the `.py` extension; (2) demonstrating extension normalization, where the function handles inputs with or without leading dots and is case-insensitive; (3) generating complete executable commands for specific file paths by replacing the `{file}` placeholder with actual paths; (4) handling unknown extensions gracefully by returning empty strings; (5) managing files without extensions like Makefiles; and (6) iterating through multiple language extensions including Python, JavaScript, Ruby, Shell, and Java.

The module requires the `PDD_PATH` environment variable to be set and exposes two main functions: `get_run_command()` for retrieving command templates by extension, and `get_run_command_for_file()` for generating complete commands for specific files. This utility is useful for build systems or development tools that need to dynamically execute files across different programming languages.",2025-12-14T08:36:50.513898+00:00
"context/git_update_example.py","This Python script demonstrates the usage of a `git_update` function from a custom `pdd` module. The main function reads an input prompt from a file located in a prompts directory, specifically targeting a file named fix_error_loop_python.prompt. It then calls the `git_update` function with several parameters: the loaded input prompt, a path to a modified code file (`pdd/fix_error_loop.py`), a default strength value imported from the pdd module, and a temperature setting of 0 for the LLM (Large Language Model).

The function returns a tuple containing a modified prompt, the total cost of the operation (displayed in dollars), and the model name used. If successful, these values are printed to the console, and the modified prompt is saved back to the original prompt file, effectively updating it. The script includes error handling for both ValueError exceptions (for input errors) and general exceptions.

This appears to be part of a prompt-driven development (PDD) workflow tool that uses AI/LLM capabilities to update code based on prompts, with Git integration for version control. The strength and temperature parameters suggest fine-tuning control over the LLM's output behavior.",2025-12-14T08:36:58.066274+00:00
"context/increase_tests_example.py","This Python script demonstrates the usage of an `increase_tests` function from a `pdd` module, which appears to be a tool for automatically generating additional unit tests to improve code coverage. The script defines an `example_usage()` function that showcases both basic and advanced usage patterns.

The example works with a simple `calculate_average` function that computes the average of a list of numbers. It includes an existing minimal unit test and a coverage report showing 60% code coverage with 2 missed statements.

The script calls `increase_tests()` twice: first with basic parameters (existing tests, coverage report, source code, and the original prompt), and then with advanced parameters including language specification, strength setting (using a default value), temperature for controlling randomness, and verbose mode enabled.

The function returns three values: newly generated tests, the total cost (suggesting this may use an AI/LLM service), and the model name used for generation. Error handling is implemented to catch both validation errors and unexpected exceptions.

This appears to be part of a prompt-driven development (PDD) toolkit that leverages AI to automatically expand test suites based on coverage gaps, helping developers achieve more comprehensive testing with less manual effort.",2025-12-14T08:37:05.574553+00:00
"context/incremental_code_generator_example.py","This Python script demonstrates the usage of an incremental code generator from the `pdd` package. The main function showcases how to update existing code based on prompt changes using language model (LLM) tools. 

The script sets up several input parameters including: an original prompt (calculating factorial), a new prompt (adding input validation), the existing code implementation, and various configuration options like language (Python), strength, temperature, time budget, and flags for forcing incremental updates and verbose output.

The core functionality calls `incremental_code_generator()` which analyzes the difference between the original and updated prompts, then decides whether to apply a minimal structured diff (patch) to the existing code or recommend full regeneration. This approach optimizes code updates by avoiding complete rewrites when only small changes are needed.

The output section uses the Rich library for formatted console display, showing whether incremental patching was successful or if full regeneration is recommended. It also displays the updated code (if patching succeeded), the total cost incurred from LLM API calls, and the model name used.

This utility is useful for development workflows where code needs iterative refinement based on evolving requirements, minimizing computational costs and preserving existing code structure when possible.",2025-12-14T08:37:13.430383+00:00
"context/insert_includes_example.py","This Python script demonstrates example usage of the `insert_includes` module from the `pdd` package, which processes dependencies for code generation tasks. The script imports necessary libraries including `pathlib` for file handling, `rich` for enhanced console output, and the core `insert_includes` function.

The main function `example_usage()` showcases how to set up and execute dependency processing. It initializes a Rich console for formatted output and defines three key input parameters: an input prompt describing a Python function to write (one that reads CSV, processes data, and outputs results), a directory path pattern for context files, and a CSV filename for dependency information.

The script calls `insert_includes()` with custom parameters including a high strength value (0.93) for focused output and zero temperature for consistent results. The function returns four values: a modified prompt with dependencies, CSV output, total processing cost, and the model name used.

Results are displayed using Rich's formatted console output with color-coded sections. The script includes error handling for missing files and general exceptions. Finally, it saves the generated CSV output to a file. The script runs the example function when executed directly via the standard `if __name__ == __main__` pattern.",2025-12-14T08:37:21.946831+00:00
"context/install_completion_example.py","This Python script demonstrates the usage of the PDD (Python Development Directory) shell completion installation module. It showcases two main functions from the pdd package: `get_local_pdd_path()`, which returns the absolute path to the PDD directory, and `install_completion()`, which installs shell completion for the PDD CLI by detecting the current shell, determining the appropriate completion script, and appending a source command to the user's shell RC file.

The script includes a `setup_example_environment()` function that creates a safe, isolated testing environment by setting up dummy directories and files. It forces the shell to bash, creates a dummy home directory (example_home) to avoid modifying the actual ~/.bashrc, establishes a dummy PDD_PATH directory, and generates placeholder completion and RC files.

The `main()` function orchestrates the example by first setting up the environment, then retrieving and displaying the PDD path, and finally calling the installation function. The script uses the Rich library for formatted console output with colored messages.

This example is designed for demonstration purposes, allowing developers to understand how the shell completion installation works without affecting their actual system configuration. Users can run it directly via `python example_install_completion.py`.",2025-12-14T08:37:29.425798+00:00
"context/langchain_lcel_example.py","This Python script demonstrates the use of LangChain, a framework for building applications with large language models (LLMs). It showcases integration with multiple LLM providers including OpenAI (GPT-4o-mini, GPT-3.5-turbo, o1), Google (Gemini via both GenAI and VertexAI), Anthropic (Claude), Azure OpenAI, DeepSeek, Fireworks, Groq, Together.ai, Ollama, MLX (for local Apple Silicon models), and AWS Bedrock.

Key features demonstrated include: setting up SQLite caching to reduce API costs and improve response times; creating prompt templates with variable placeholders; using output parsers (JSON and Pydantic) to structure LLM responses into defined schemas like a Joke class with setup and punchline fields; implementing a custom callback handler to track completion status, finish reasons, and token usage metrics.

The script also illustrates LangChain Expression Language (LCEL) for chaining components using the pipe operator, configurable alternatives for switching between models, fallback mechanisms for reliability, and structured output enforcement. Advanced features like Anthropic's thinking mode for reasoning tasks are shown. Throughout, various chains are invoked with different queries (jokes, company names, code generation) to demonstrate the flexibility of the framework across different use cases and model providers.",2025-12-14T08:37:37.342559+00:00
"context/litellm_bedrock_sonnet.py","This Python code snippet demonstrates how to use the LiteLLM library to interact with Amazon Bedrock's Claude 3.7 Sonnet model. The code imports the `completion` function from the `litellm` package and includes commented-out environment variable configurations for AWS credentials (access key ID, secret access key, and region name), which would need to be set for authentication with AWS services.

The main functionality sends a simple query to the Claude model asking What is the capital of France? The API call specifies the Bedrock model path using the format bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0, indicating it's using the US-hosted Anthropic Claude 3.7 Sonnet model through Bedrock. The `reasoning_effort` parameter is set to low, which likely controls the depth of reasoning the model applies to generate its response.

Finally, the script prints the response object returned by the completion call. This is a basic example of integrating LiteLLM as a unified interface for accessing various LLM providers, in this case specifically for AWS Bedrock-hosted Anthropic models, simplifying the process of making API calls to different AI services through a consistent interface.",2025-12-14T08:37:47.593005+00:00
"context/llm_invoke_example.py","This Python script demonstrates the usage of an `llm_invoke` function for interacting with language models. It imports utilities from a `pdd.llm_invoke` module and uses Pydantic for structured data validation.

The code defines a `Joke` Pydantic model with `setup` and `punchline` fields for structured output parsing. The `calculate_model_ranges` function samples strength values from 0.0 to 1.0 to determine the boundaries where different LLM models are selected, returning a list of dictionaries containing each model's name, start/end range, and midpoint.

The `main` function serves as a demonstration entry point. It first calculates and displays the strength ranges for available models, then iterates through each model, invoking it at its midpoint strength value. For each model, it runs two examples: an unstructured joke generation about programmers and a structured joke generation about data scientists using the Pydantic model to parse JSON output.

The script showcases key features including dynamic model selection based on strength parameters, cost tracking for API calls, temperature control, verbose mode toggling, and both unstructured text and structured Pydantic-validated responses. Error handling is included for the structured output calls. This appears to be a testing or demonstration utility for a multi-model LLM invocation system.",2025-12-14T08:37:55.952595+00:00
"context/llm_output_fragment.txt","This Python file implements a command-line interface (CLI) tool called pdd (Prompt-Driven Development) using the Click library. The program provides several AI-powered code generation and manipulation commands. Key features include: 1) A main CLI group with global options for force overwrite, AI model strength/temperature settings, verbosity controls, and cost tracking. 2) Multiple subcommands: generate creates runnable code from prompt files, example generates example code from existing code and prompts, test creates unit test files, preprocess handles prompt file preprocessing with optional XML tagging, fix repairs code errors with optional iterative loop mode, and split divides complex prompts into smaller files. The tool uses the Rich library for formatted console output and imports various helper modules for path construction, code generation, context generation, test generation, preprocessing, XML tagging, and error fixing. Each command follows a consistent pattern of constructing file paths, calling the appropriate generator function, writing outputs, and displaying cost information. The architecture supports command chaining through a processor decorator pattern similar to Unix pipes.",2025-12-14T08:38:04.936885+00:00
"context/llm_selector_example.py","This Python script demonstrates the usage of an `llm_selector` function imported from a `pdd.llm_selector` module. The main function iterates through different strength values (starting at 0.5 and incrementing by 0.05 until exceeding 1.1) while keeping the temperature fixed at 1.0. For each strength value, it calls the `llm_selector` function, which returns five values: an LLM object, a token counter function, input cost, output cost, and the model name. The script then prints details about the selected model, including its name and the costs per million tokens for both input and output. It also demonstrates the token counting functionality by counting tokens in a sample text string. The code includes error handling for `FileNotFoundError` and `ValueError` exceptions, suggesting the `llm_selector` function may read configuration files or validate input parameters. This appears to be a testing or demonstration utility for exploring how different strength parameters affect LLM model selection, likely used in a larger system where model selection is dynamically determined based on task requirements or resource constraints.",2025-12-14T08:38:13.779267+00:00
"context/load_prompt_template_example.py","This Python script demonstrates a simple utility for loading and displaying prompt templates. The code imports two key components: a custom function called `load_prompt_template` from the `pdd.load_prompt_template` module, and the `print` function from the `rich` library, which enables formatted console output with styling capabilities.

The main function defines a prompt name variable set to generate_test_LLM, which represents the name of a prompt template file (without its file extension). It then calls the `load_prompt_template` function with this name to retrieve the corresponding template content.

If the prompt is successfully loaded (i.e., the returned value is truthy), the script prints a styled header message in blue text reading Loaded Prompt Template: followed by the actual prompt content. The rich library's print function handles the color formatting using bracket notation.

The script follows Python's standard entry point pattern with the `if __name__ == __main__:` guard, ensuring the main function only executes when the script is run directly rather than imported as a module.

Overall, this appears to be a testing or demonstration utility within a larger prompt-driven development (PDD) framework, likely used for verifying that prompt templates are correctly stored and retrievable.",2025-12-14T08:38:20.862437+00:00
"context/logo_animation_example.py","This Python script demonstrates the usage of a PDD (presumably a product or project name) branding animation module. The code imports functions from a custom 'pdd.logo_animation' package to control a terminal-based logo animation.

The main function serves as a demonstration, starting with informational print statements before launching the animation via `start_logo_animation()`, which runs in a background thread. The animation sequence consists of three phases: logo formation (1.5 seconds), logo hold (1.0 seconds), and logo-to-box transition (1.5 seconds), totaling approximately 4 seconds.

The script allows the animation to run for 7 seconds using a simple loop with `time.sleep()` calls, simulating concurrent main program work. An important note in the comments explains that any print statements during this period won't be visible because the animation uses full-screen terminal mode (screen=True), which temporarily takes over the display.

After the viewing duration, the script calls `stop_logo_animation()` to halt the animation, clean up the background thread, and restore normal terminal control. The code follows Python best practices with type hints, docstrings, and a standard `if __name__ == __main__` entry point pattern. This appears to be example or demo code for integrating the PDD branding animation into other applications.",2025-12-14T08:38:28.713395+00:00
"context/no_include_conflicts_in_prompts_python.prompt","This file is a prompt specification for generating a Python function called conflicts_in_prompts that analyzes two prompts for conflicts and suggests resolutions. The function uses LangChain and an LLM to perform the analysis.

**Inputs:** The function accepts two prompts (prompt1, prompt2), a strength parameter (default 0.5), and a temperature parameter (default 0).

**Outputs:** It returns a list of conflict dictionaries (each containing description, explanation, and resolution suggestions for both prompts), total cost, and model name.

**Implementation Steps:**
1. Load prompt templates from files specified via the $PDD_PATH environment variable (conflict_LLM.prompt and extract_conflict_LLM.prompt)
2. Create a LangChain LCEL template from the conflict detection prompt
3. Use an imported llm_selector module to select the model
4. Run the prompts through the model, passing PROMPT1 and PROMPT2 as parameters, while displaying token counts and costs to the user
5. Create a second LCEL chain using the extraction prompt with 0.8 strength, outputting JSON. This processes the LLM output from step 4, calculates tokens/costs, and extracts the conflicts list
6. Return the conflicts list with nested fields, total cost, and model name

The specification includes a reference to an external Python preamble file for context.",2025-12-14T08:38:37.077536+00:00
"context/o4-mini-test.ipynb","This Jupyter Notebook demonstrates how to use the Azure OpenAI API to create a conversational chatbot experience. The code imports the AzureOpenAI client library and configures it with an Azure endpoint, API key, and the o4-mini model deployment. The notebook showcases a multi-turn conversation about Paris travel recommendations. The conversation flow includes: a system prompt establishing the assistant's helpful role, a user asking for Paris sightseeing suggestions, an assistant response listing top attractions (Eiffel Tower, Louvre Museum, Notre-Dame Cathedral), and a follow-up user question asking specifically about the Eiffel Tower. The output displays a detailed response explaining why the Eiffel Tower is remarkable, covering its architectural and engineering significance as a revolutionary 300-meter iron structure from 1889, its status as an iconic global symbol of Paris and French culture, the spectacular 360-degree panoramic views from its three observation levels, the day-to-night experience with its famous hourly light show featuring 20,000+ bulbs, and its role as a cultural hub hosting exhibitions and seasonal events. The notebook runs on Python 3.12.9 and uses the 2024-12-01-preview API version.",2025-12-14T08:38:45.780418+00:00
"context/pdd_discussiion.txt","This transcript captures an interview-style conversation about Prompt-Driven Development (PDD), a methodology where prompts replace code as the primary development artifact. The interviewee, who created an open-source PDD CLI tool and MCP server, explains that traditional code maintenance accounts for 80-90% of development costs, and PDD addresses this by allowing developers to regenerate clean code from modified prompts rather than continuously patching legacy code.

Key concepts discussed include: the evolution paralleling how HDL languages replaced netlists in chip design; the fundamental PDD unit consisting of four synchronized files (prompt, code, example, and test); token efficiency compared to agentic tools like Cursor; batch processing capabilities that reduce human babysitting; and improved collaboration through human-readable prompts.

The methodology involves creating a PRD, breaking it into module-specific prompts, generating code with examples and tests, then back-propagating learnings to keep all documentation synchronized. Future developments include PDD Cloud—a marketplace for few-shot examples to improve code generation quality—and a VS Code plugin for prompt formatting.

Challenges discussed include the learning curve for writing effective prompts, managing dependencies between modules, and knowing when to use PDD versus quick direct fixes. The interviewee emphasizes that PDD complements rather than replaces tools like Cursor, offering more control and consistency for complex, evolving projects while enabling non-technical stakeholders to participate in development.",2025-12-14T08:38:54.816456+00:00
"context/postprocess_0_example.py","This file provides an example usage guide for the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The function is designed to process output from a language model (LLM) that contains multiple code sections in various programming languages. 

The example demonstrates how to use `postprocess_0` by passing it a sample LLM output string containing mixed content—including Python code blocks, Java code, and plain text—along with a specified target language (python). The function processes this input and returns a modified string where only the largest code section in the specified language remains uncommented, while all other text and code sections are commented out using the appropriate comment syntax for that language.

The documentation outlines two input parameters: `llm_output` (the raw LLM string output) and `language` (the target programming language to focus on). The output is a processed string with selective commenting applied.

The file also notes that helper functions `get_comment`, `comment_line`, and `find_section` must be properly implemented and accessible for the `postprocess_0` function to work correctly. This utility appears useful for extracting and isolating relevant code from mixed LLM outputs while preserving context through commenting.",2025-12-14T08:39:05.777701+00:00
"context/postprocess_example.py","This Python script demonstrates the usage of the `postprocess` function from the `pdd.postprocess` module, which is designed to extract code from Large Language Model (LLM) text outputs. The example showcases two distinct extraction scenarios:

**Scenario 1 (Simple Extraction, strength=0):** Uses basic string manipulation to find code blocks enclosed in triple backticks. This method is fast, cost-free, and language-agnostic, returning the first code block found.

**Scenario 2 (LLM-based Extraction, strength>0):** Leverages an LLM for more sophisticated code extraction. This approach is more powerful but incurs API costs and processing time. The script uses `unittest.mock` to simulate the behavior of internal dependencies (`load_prompt_template` and `llm_invoke`), allowing the example to run without actual LLM API calls.

The `postprocess` function accepts parameters including the LLM output text, target programming language, strength (0-1 scale determining extraction method), temperature, time (thinking effort), and verbose flag. It returns a tuple containing the extracted code, total cost, and model name used.

The script requires the `pdd` package and `rich` library for formatted console output. Mock verification ensures correct function calls and parameter passing, making this an educational example for understanding the postprocessing workflow.",2025-12-14T08:39:13.082325+00:00
"context/postprocessed_runnable_llm_output.py","This file contains the implementation of a `context_generator` function in Python, designed to automate the generation of usage examples for Python modules. The function accepts three parameters: `python_filename` (the input Python file to analyze), `output_filename` (where to write the generated output), and an optional `force` boolean flag defaulting to False.

The implementation follows a multi-step workflow: First, it reads the contents of the specified Python file, handling potential FileNotFoundError exceptions gracefully by printing an error message and returning False. Next, it preprocesses the file content (though the preprocessing function itself is referenced but not shown). The function then constructs a prompt for GPT-4, positioning it as an expert Python engineer tasked with generating concise usage examples based on the provided code.

The file includes explanatory comments describing each step of the process, along with usage examples showing how to call the function with appropriate arguments. The code appears to be incomplete or a work-in-progress, as some referenced components like the `preprocess` function and the actual model invocation logic are not fully implemented in the visible code. The function returns a boolean indicating success or failure of the operation.",2025-12-14T08:39:21.928602+00:00
"context/preprocess_example.py","This Python script demonstrates the usage of a preprocessing module from a package called 'pdd'. It imports a 'preprocess' function and uses the Rich library's Console for formatted terminal output.

The script defines a multi-line prompt string containing various XML-like tags including shell commands, PDD comments, web URLs, placeholder variables in curly braces ({test}, {test2}), and a markdown file reference. It also includes multi-line PDD comments that are noted as content that should not appear in the output.

Three configuration variables are set: 'recursive' is set to False, 'double_curly_brackets' is enabled (True), and 'exclude_keys' is defined as a list containing test2 to prevent that specific key from being processed with double curly brackets.

The script then prints debug information showing the excluded keys using Rich's styled console output with bold yellow formatting. Finally, it calls the preprocess function with the prompt and configuration options, then displays the processed result with bold white styling.

This appears to be a test or demonstration script for a prompt preprocessing system that handles special tags, variable substitution, and formatting options for template-like prompt strings.",2025-12-14T08:39:28.990143+00:00
"context/preprocess_main_example.py","This Python file implements a command-line interface (CLI) tool using the Click library for preprocessing prompt files. The CLI provides several configurable options: a required `--prompt-file` path specifying the input file, an optional `--output` path for saving results, and multiple boolean flags including `--xml` for adding XML delimiters, `--recursive` for processing referenced prompt files, `--double` for doubling curly brackets, and `--verbose` for detailed logging. The `--exclude` option allows specifying multiple keys to exempt from bracket doubling.

The main `cli` function initializes a context object with default parameters (strength, temperature, verbose setting) and calls `preprocess_main` from the `pdd.preprocess_main` module. This function handles the actual preprocessing logic and returns three values: the processed prompt content, total processing cost, and model name used.

Upon successful execution, the tool displays the preprocessing results including the processed prompt, cost, and model information. Error handling is implemented to catch and display any exceptions that occur during processing. The script can be run directly as a standalone program, making it a utility for transforming and preparing prompt files with various formatting options for downstream use in prompt-driven development workflows.",2025-12-14T08:39:36.941842+00:00
"context/process_csv_change_example.py","This Python script, `run_demo.py`, serves as a concise usage example demonstrating how to use the `process_csv_change` function from the `pdd.process_csv_change` module. The script walks through a complete workflow in four main steps:

1. **Imports**: It imports necessary modules including `Path` from pathlib and the target function `process_csv_change`.

2. **Workspace Setup**: Creates a demo workspace directory structure containing:
   - A sample Python source file (`factorial.py`) with a recursive factorial function
   - A prompt file (`factorial_python.prompt`) following a specific naming convention
   - A CSV file (`tasks.csv`) containing instructions for modifying the prompt

3. **Function Call**: Invokes `process_csv_change` with various parameters including the CSV file path, strength (0.5), temperature (0.0), code directory, language (python), file extension (.py), and a budget limit (0.25 USD).

4. **Output Inspection**: Displays the returned values including success status, total cost, model name (gpt-5-nano), and a list of JSON objects containing the modified prompt content.

The example output shows the function successfully adding an example test section to the original prompt, demonstrating how the tool can automatically enhance prompts based on CSV-defined instructions using an LLM.",2025-12-14T08:39:44.862789+00:00
"context/prompt_caching.ipynb","This Jupyter notebook demonstrates how to use prompt caching with the Anthropic API to optimize performance and reduce costs when working with large documents. The cookbook uses Pride and Prejudice by Jane Austen (~187,000 tokens) as sample content to illustrate two main use cases.

The first example compares cached versus non-cached API calls for single-turn interactions. Results show dramatic latency improvements—from 21 seconds down to approximately 3 seconds—when leveraging cached prompts. The implementation uses the cache_control attribute with ephemeral type and requires the prompt-caching-2024-07-31 beta header.

The second example demonstrates multi-turn conversations with incremental caching using a ConversationHistory class that manages conversation turns and applies cache breakpoints to the last two user messages. This approach achieves nearly 100% cache hit rates after the initial setup, reducing response times from 24 seconds to 8-9 seconds while maintaining response quality.

Key benefits highlighted include over 2x latency reduction and up to 90% cost savings, making it practical to include detailed instructions and examples in prompts. The notebook provides complete Python code using the anthropic library, BeautifulSoup for content fetching, and demonstrates proper API call structure for enabling prompt caching.",2025-12-14T08:39:52.186970+00:00
"context/pytest_example.py","This Python script implements a custom pytest test runner with result collection capabilities. It defines a `TestResultCollector` class that acts as a pytest plugin to track test outcomes including failures, errors, and warnings. The collector uses pytest hooks (`pytest_runtest_logreport` and `pytest_sessionfinish`) to capture test results during different phases (setup, call, and teardown). It also includes functionality to redirect stdout and stderr to an in-memory StringIO buffer for log capture.

The `run_pytest()` function creates a collector instance, captures logs, and executes pytest with verbose output on a specific test file (`tests/test_get_extension.py`). It returns the counts of failures, errors, warnings, and the captured log output.

The main block includes extensive debug print statements to trace execution flow, displaying the current working directory and status messages at various stages. This appears to be a development or debugging version of a test runner utility, designed to programmatically run pytest tests and collect detailed results for further processing or reporting. The script is useful for integrating pytest into larger automation workflows where programmatic access to test results is needed.",2025-12-14T08:40:02.153924+00:00
"context/pytest_output_example.py","This Python script demonstrates the usage of a custom `pytest_output` module for running and capturing pytest test results. It imports several libraries including argparse, json, pytest, and rich (for console formatting). The script defines a helper function `create_dummy_test_file()` to generate test files with specified content.

The main function `main_example()` showcases six different usage examples:

1. **Using the main function**: Simulates command-line arguments to run pytest on a dummy test file containing passing, failing, error, and warning test cases.

2. **Capturing and saving output**: Demonstrates `run_pytest_and_capture_output()` to capture test results and `save_output_to_json()` to save them to a JSON file.

3. **Non-existent file handling**: Shows error handling when attempting to run tests on a file that doesn't exist.

4. **Non-Python file handling**: Tests behavior when given a non-Python file (`.txt` extension).

5. **Empty test file**: Demonstrates handling of a Python file containing no test functions.

6. **Advanced TestResultCollector usage**: Shows direct usage of the `TestResultCollector` class to capture stdout, stderr, failures, errors, warnings, and passed tests.

The script creates test files in an output directory and serves as both documentation and a functional example for the pytest_output module's capabilities.",2025-12-14T08:40:10.362700+00:00
"context/python_env_detector_example.py","This Python script serves as an example demonstrating the usage of the `python_env_detector` module from the `pdd` package. The script imports four key functions from the module: `detect_host_python_executable`, `get_environment_info`, `is_in_virtual_environment`, and `get_environment_type`. 

The main function showcases each of these utilities in sequence. First, it detects and prints the path to the host Python executable. Then, it checks whether the script is running within a virtual environment and displays the boolean result. Next, it identifies and outputs the type of Python environment being used (such as venv, conda, or system Python). Finally, it retrieves comprehensive environment information as a dictionary and iterates through it to display each key-value pair, providing detailed insights about the current Python setup.

The script follows standard Python conventions with a shebang line for direct execution, a docstring explaining its purpose, and the `if __name__ == __main__` guard to ensure the main function only runs when the script is executed directly. This example is useful for developers who need to programmatically detect and understand their Python environment configuration, particularly when working across different virtual environment setups or deployment scenarios.",2025-12-14T08:40:18.593539+00:00
"context/python_preamble.prompt","The file contents provide development guidelines for creating a Python function within a package structure. Key requirements include: (1) Using relative imports with single-dot notation for internal modules (e.g., 'from .module_name import module_name'), which is standard practice for Python packages. (2) Implementing console output using the Python Rich library for pretty-printed, visually formatted text. (3) Incorporating robust error handling for edge cases, particularly for missing inputs and model errors, with clear and informative error messages for users. Additionally, the file references a specific package structure where the './pdd/__init__.py' file contains global constants such as EXTRACTION_STRENGTH, DEFAULT_STRENGTH, and DEFAULT_TIME. These constants should be imported using relative import syntax (e.g., 'from . import DEFAULT_STRENGTH'). This setup suggests a modular Python application where configuration values are centralized in the package's initialization file, promoting maintainability and consistency across the codebase. The guidelines emphasize clean code organization, proper package conventions, and user-friendly output formatting.",2025-12-14T08:40:26.112916+00:00
"context/regression_example.sh","This Bash script is a regression testing suite for a tool called PDD (likely a code generation or development tool). The script sets up various path variables and filenames for prompts, scripts, and test files, then executes a comprehensive series of PDD commands to validate the tool's functionality.

The script begins by defining environment variables for staging paths, prompt files, and Python scripts related to file extension handling and code generation. It includes utility functions for logging with timestamps and running PDD commands with error handling.

The regression tests cover multiple PDD operations: generate creates Python scripts from prompts, example produces verification programs, test generates test files, preprocess handles prompt preprocessing (including XML output), and update modifies prompts based on script changes. Additional commands include change for code modifications, fix for correcting failing tests (with a loop option for iterative fixes), split for dividing prompts into sub-components, detect for analyzing multiple prompts, conflicts for identifying issues between prompts, and crash for handling runtime errors.

Throughout execution, the script logs all operations to a regression log file and tracks costs in a CSV file. Upon completion, it calculates and displays the total operational cost. The script exits immediately if any command fails, ensuring reliable regression testing.",2025-12-14T08:40:33.537001+00:00
"context/render_mermaid_example.py","This Python script demonstrates how to use the `render_mermaid.py` module to convert architecture.json files into interactive HTML Mermaid diagrams for visualizing software architecture. The file contains several example functions showcasing different usage patterns.

The script includes a `create_sample_architecture()` function that generates a sample architecture definition with five modules: a FastAPI main application, database configuration, app settings, React frontend components, and an API client. Each module includes metadata like dependencies, priority, file paths, and tags.

Four example functions demonstrate various use cases: (1) basic command-line usage, (2) programmatic usage showing how to generate Mermaid code and HTML output directly, (3) loading and processing existing architecture.json files from common locations, and (4) describing customization features like automatic module categorization into Frontend/Backend/Shared groups, priority indicators, dependency arrows, color-coded subgraphs, interactive tooltips, and responsive HTML output.

The main function orchestrates all examples, providing a comprehensive demonstration of the render_mermaid module's capabilities for creating visual architecture diagrams from JSON configuration files.",2025-12-14T08:40:42.586210+00:00
"context/simple_math.py","The file contains a single comment indicating that it has been intentionally left blank for pdd generate. This suggests the file serves as a placeholder or template that will be populated later by an automated process or tool called pdd (which likely stands for Puzzle Driven Development or a similar development methodology). The file currently holds no substantive content, code, or data—only this explanatory note. Such placeholder files are commonly used in software development projects to establish file structure and organization before actual implementation begins. The pdd generate reference indicates this file is part of a workflow where content will be automatically generated or inserted at a later stage in the development process. This approach helps maintain project structure while allowing for incremental development and automated code generation. The blank state of the file is deliberate and documented, ensuring that other developers or team members understand its purpose and do not mistakenly assume the file is incomplete or missing content due to an error.",2025-12-14T08:40:50.434043+00:00
"context/split_example.py","This Python script demonstrates the usage of a `split` function from the `pdd.split` module. The code sets up a main function that configures the environment and calls the split function with specific parameters. 

The script begins by importing necessary modules: `os` for environment variable management, `Console` from the `rich` library for formatted terminal output, and the `split` function from `pdd.split`. 

In the `main()` function, it first sets the `PDD_PATH` environment variable to the parent directory of the script. It then defines several input parameters: an `input_prompt` requesting a factorial function, sample `input_code` containing a recursive factorial implementation, `example_code` showing how to use the factorial function, and two float parameters (`strength` and `temperature`) both ranging from 0 to 1.

The `split` function is called with these parameters plus a `verbose=True` flag. It returns a tuple containing a result tuple (with `sub_prompt` and `modified_prompt`), a `total_cost` value, and the `model_name` used. The results are then printed to the console using Rich's formatting capabilities, displaying the sub-prompt, modified prompt, model name, and cost. Error handling is implemented to catch and display any exceptions that occur during execution.",2025-12-14T08:40:56.932433+00:00
"context/split_main_example.py","This Python file, `example_split_usage.py`, demonstrates how to create a command-line interface (CLI) using the Click library to interact with a `split_main` function from a module called `pdd.split_main`. The CLI tool is designed to split and process prompt files used in code generation workflows.

The script defines a `split_cli` command with several options: required paths for an input prompt file, an input code file, and an example code file, along with optional paths for saving sub-prompts and modified prompts. Additional flags include `--force` for overwriting existing files and `--quiet` for suppressing output.

The command configures custom settings like strength (controlling how much content is extracted into a sub-prompt) and temperature (controlling randomness) via the Click context object. When executed, it calls `split_main()` which returns a result dictionary containing sub-prompt and modified prompt content, along with the total operation cost and model name used.

The output displays the generated content, file save locations, model information, and cost unless quiet mode is enabled. Error handling is included to catch and report exceptions. The script uses Click's group/command pattern, making it extensible for additional CLI commands.",2025-12-14T08:41:04.701150+00:00
"context/summarize_directory_example.py","This Python script demonstrates the usage of a `summarize_directory` module from the `pdd` package. The main function showcases a workflow for generating file summaries in CSV format. It begins by defining sample existing CSV content containing file paths, summaries, and dates. The script then calls the `summarize_directory` function with several parameters: a wildcard directory path pattern (`context/c*.py`) to locate Python files, a strength value of 0.5 that influences model selection (higher values use more capable but expensive models), a temperature of 0.0 for deterministic output, verbose mode enabled for progress tracking, and the existing CSV content for reference. The function returns three values: the generated CSV output, total processing cost, and the model name used. After processing, the script prints the results including the CSV content, cost in USD, and model information. It then ensures an output directory exists (creating it if necessary) and saves the generated CSV to `output/output.csv`. The entire operation is wrapped in a try-except block for error handling. The script follows standard Python conventions with a `__name__ == __main__` guard for direct execution.",2025-12-14T08:41:12.698067+00:00
"context/sync_animation_example.py","This Python script demonstrates how to use a `sync_animation` module for displaying a terminal-based animation that tracks the progress of a simulated PDD (Prompt-Driven Development) workflow. The script showcases multi-threaded programming with shared state management between a main application thread and an animation thread.

Key components include:
1. **Shared State Variables**: Mutable lists and a `threading.Event` are used to communicate between threads. These include references for the current function name, accumulated cost, file paths (prompt, code, example, tests), and box colors for the animation display.

2. **Static Parameters**: Configuration values like the project basename (handpaint), budget ($3.00), and initial color settings for UI boxes.

3. **Mock Workflow Simulation**: The `mock_pdd_main_workflow()` function simulates various PDD commands (checking, auto-deps, generate, example, crash, verify, test, fix, update) with timed delays, updating the shared state to trigger animation changes.

4. **Thread Management**: The animation runs as a daemon thread, allowing the main workflow to proceed independently. The script properly signals termination via `stop_event.set()` and joins the thread with a timeout.

5. **Cleanup and Reporting**: After completion, the script reports total elapsed time and accumulated cost, demonstrating proper resource management and thread synchronization patterns.",2025-12-14T08:41:20.004164+00:00
"context/sync_determine_operation_example.py","This Python script demonstrates the usage of a `sync_determine_operation` function from a module called `pdd` (likely Prompt-Driven Development). The script simulates a project environment to showcase how the synchronization system determines what operation to perform on code units based on their current state.

The script sets up a simulated project in an `./output` directory and runs through four distinct scenarios:

1. **New Unit**: A fresh prompt file exists with no associated code or history, triggering a new unit creation workflow.

2. **Test Failure**: Simulates a situation where test execution has failed (exit code 1, one test failed), demonstrating how the system detects and responds to test failures.

3. **Manual Code Change**: Shows what happens when a developer manually modifies code, causing the file's hash to differ from the stored fingerprint, indicating the code is out of sync.

4. **Unit Synchronized**: Represents the ideal state where all file hashes (prompt, code, example, tests) match the fingerprint and tests have passed with full coverage.

The script uses SHA256 hashing to track file changes and stores metadata (fingerprints and run reports) as JSON files in a `.pdd/meta` directory. Key data structures include `Fingerprint` (tracking file versions) and `RunReport` (storing test execution results). This appears to be a development tool for maintaining synchronization between prompts, generated code, and tests.",2025-12-14T08:41:29.272238+00:00
"context/sync_main_example.py","This Python script demonstrates and tests the `sync_main` function from the `pdd` (Prompt-Driven Development) framework in a controlled, isolated environment. The code sets up a mock project structure with prompt files for multiple programming languages (Python and JavaScript) and simulates the behavior of the `pdd sync` CLI command.

The script consists of two main functions: `setup_mock_project` creates a temporary project directory structure containing prompt files that instruct code generation for greeting functions in different languages. The `main` function orchestrates the demonstration by: (1) setting up the mock project, (2) creating a simulated Click context object with default configuration options like temperature, strength, and verbosity settings, and (3) patching internal dependencies (`construct_paths` and `sync_orchestration`) with mock implementations.

The mocked `sync_orchestration` function simulates different outcomes—returning success for Python code generation with passing tests, and failure for JavaScript due to a simulated test error. This allows testing the aggregation logic of `sync_main` without requiring actual LLM API calls or test execution.

The script uses the Rich library for formatted console output and demonstrates how the sync command processes multiple language targets sequentially, aggregating results and costs across all processed languages before displaying a final JSON summary.",2025-12-14T08:41:39.496822+00:00
"context/sync_orchestration_example.py","This Python script demonstrates the usage of the `sync_orchestration` module from a PDD (Prompt-Driven Development) CLI tool. The code consists of two main parts:

1. **Project Setup Function (`setup_example_project`)**: This function creates a mock project structure within an output directory, including subdirectories for prompts, source code, examples, and tests. It also generates a sample prompt file for a calculator function and ensures the PDD metadata directory exists.

2. **Main Execution Block**: The script showcases two primary functionalities:
   - **Running a Full Sync Process**: Simulates the `pdd sync calculator` command by calling `sync_orchestration()` with various parameters including basename, language, directory paths, and a budget limit of $5.00. The function determines necessary steps (generate, example, test) based on file states and executes them.
   - **Viewing Sync Logs**: Demonstrates how to retrieve and display logs from a previous sync operation by setting the `log=True` flag.

The script outputs JSON-formatted results showing the sync operation's success status and any errors encountered. It serves as a practical example for developers learning to integrate the PDD sync orchestration module into their projects, illustrating both execution and logging capabilities.",2025-12-14T08:41:48.419411+00:00
"context/test.prompt","This file contains development guidelines and configuration notes for a project structure involving test generation and module organization. Key points include: 1) Test files are located in a separate 'tests' directory from the main module code in the 'pdd' directory, requiring absolute import references. The naming convention dictates that module file names match their corresponding function names. 2) Any files created during execution should be placed in the 'output' directory. 3) The project includes pre-existing data files (language_format.csv and llm_model.csv) stored in the PDD_PATH/data directory. These files contain information about popular programming languages and LLM models, and developers are instructed not to overwrite them. They can be utilized for testing purposes. 4) The PDD_PATH environment variable is pre-configured and available for use. These guidelines appear to be instructions for developers working on a code generation or testing framework, emphasizing proper file organization, import handling, and preservation of existing data resources.",2025-12-14T08:41:56.645588+00:00
"context/thinking_tokens.md","This document provides a detailed comparison of the maximum internal reasoning tokens (chain-of-thought tokens used for intermediate steps) allowed by various large language models. The models analyzed include offerings from OpenAI (GPT-4.1, o3, o4-mini), Anthropic (Claude 3.5 Haiku, Claude 3.7 Sonnet), Google (Gemini 2.5 Flash and Pro), xAI (Grok 3 Beta), and DeepSeek (DeepSeek Coder, DeepSeek-R1 variants).

Key findings show significant variation across models. OpenAI's o3 and o4-mini support the highest reasoning budgets at approximately 100K tokens. Claude 3.7 Sonnet offers up to 64K reasoning tokens through its extended thinking mode. DeepSeek-R1 models (including distilled variants) cap at around 32K reasoning tokens. Gemini 2.5 models support tens of thousands of reasoning tokens, comparable to their output limits.

Notably, several models lack dedicated reasoning token capabilities: GPT-4.1, GPT-4.1-nano, Claude 3.5 Haiku, and DeepSeek Coder operate as standard single-pass models without hidden chain-of-thought processing. Grok 3 Beta supports extended reasoning but has no published fixed limit.

The document draws from official documentation, model cards, and benchmark reports to provide these estimates, distinguishing internal reasoning budgets from input context and output length limits.",2025-12-14T08:42:03.146311+00:00
"context/tiktoken_example.py","This code snippet demonstrates how to count tokens in a text string using the tiktoken library, which is OpenAI's tokenizer. The code performs three main operations: First, it imports the tiktoken library, which is a fast BPE (Byte Pair Encoding) tokenizer commonly used with OpenAI's language models. Second, it retrieves a specific encoding scheme called cl100k_base using the get_encoding() function. The cl100k_base encoding is the tokenizer used by GPT-4 and GPT-3.5-turbo models. The comment suggests that other encoding names could be used as alternatives depending on the model being targeted. Third, it calculates the token count by encoding a variable called preprocessed_prompt and measuring the length of the resulting token list. This is useful for managing API costs, staying within context window limits, or understanding how text will be processed by language models. Token counting is essential when working with LLMs since they have maximum token limits and pricing is often based on token usage. This simple three-line implementation provides an efficient way to determine exactly how many tokens a given text will consume when sent to an OpenAI model.",2025-12-14T08:42:13.030175+00:00
"context/trace_example.py","This Python script serves as a demonstration for the `trace` function from the `pdd.trace` module. It imports necessary dependencies including `os`, the `trace` function, `Console` from the `rich` library for formatted output, and a `DEFAULT_STRENGTH` constant from the `pdd` package.

The `main()` function sets up example inputs to showcase the tracing functionality. It defines a sample code snippet containing a simple `hello_world()` function that prints a greeting and returns 42. The script specifies line 3 as the target line to trace, along with a prompt file that describes the expected behavior of the code.

Configuration parameters include a default strength value and a temperature of 0.0 for LLM generation. The script calls the `trace()` function with these inputs in verbose mode, which returns three values: the corresponding prompt line, total cost, and model name used.

Results are displayed using Rich console formatting with colored output. The script includes comprehensive error handling for `FileNotFoundError`, `ValueError`, and general exceptions, each displaying appropriate error messages in red. The script follows standard Python conventions with a `if __name__ == __main__` guard to ensure the main function only runs when executed directly.",2025-12-14T08:42:20.323786+00:00
"context/trace_main_example.py","This Python script demonstrates the usage of a tracing utility from the 'pdd' package, specifically the `trace_main` function. The script creates two example files: a prompt file (calculator.prompt) containing instructions for building a simple calculator that adds two numbers, and a Python code file (calculator.py) implementing an `add_numbers` function that takes two floats and returns their sum.

The script uses the Click library to set up a command context with configuration options including: 'quiet' mode (disabled), 'force' mode (enabled to overwrite files), 'strength' parameter (0.7 for LLM analysis intensity), and 'temperature' (0.0 for deterministic LLM output).

The main functionality calls `trace_main()` with the prompt file, code file, a specific line number (2) to trace, and an output file path. This appears to be a tool for tracing relationships between prompts and generated code, likely for debugging or analysis purposes in prompt-driven development workflows.

The function returns three values: the corresponding prompt line number, the total cost of the LLM analysis in USD, and the model name used. Error handling is included to catch and display any exceptions that occur during execution.",2025-12-14T08:42:28.030536+00:00
"context/track_cost_example.py","This Python file implements a command-line interface (CLI) for a tool called PDD using the Click library. The CLI is designed for processing prompts and generating outputs with built-in cost tracking functionality.

The code defines two main components:

1. **Main CLI Group (`cli`)**: The entry point that accepts an optional `--output-cost` parameter to enable cost tracking and export usage details to a CSV file. It uses Click's context object to pass configuration between commands.

2. **Generate Command (`generate`)**: A subcommand decorated with `@track_cost` that processes prompt files. It accepts two parameters:
   - `--prompt-file` (required): Path to an input file containing the prompt
   - `--output` (optional): Path for the output file; if omitted, results print to console

The generate function reads the prompt file, simulates processing (currently a placeholder), and returns a tuple containing the generated output, execution cost ($0.05 per million tokens), and the model name (gpt-4). The code uses Rich library for enhanced console printing.

The file includes a main block demonstrating example usage with sample arguments. This appears to be a framework or template for building AI/LLM-powered text generation tools with cost monitoring capabilities.",2025-12-14T08:42:35.290899+00:00
"context/unfinished_prompt.txt","This Python file implements a command-line interface (CLI) for PDD (Prompt-Driven Development) using the Click and Rich libraries. The CLI provides tools for AI-assisted code generation and development workflows.

The implementation includes several key components:

1. **Library Imports**: The code imports Click for CLI functionality, Rich for enhanced console output (tables, panels, markdown), and various custom modules for code generation, testing, error fixing, and prompt processing.

2. **Cost Tracking**: Global variables and a callback function enable optional cost tracking, which logs AI model usage to a CSV file with timestamps, model information, commands executed, costs, and input/output files.

3. **Main CLI Group**: The `@click.group(chain=True)` decorator creates a chainable command group with several global options:
   - `--force`: Overwrite files without confirmation
   - `--strength`: AI model strength (0.0-1.0)
   - `--temperature`: AI model temperature setting
   - `--verbose`/`--quiet`: Output verbosity controls
   - `--output-cost`: Enable cost tracking with CSV output

4. **Generate Command**: The primary command shown creates runnable code from a prompt file. It uses helper functions to construct file paths, generate code via an AI model, and save the output.

The file appears to be part of a larger PDD toolkit for automating code generation through AI-powered prompt processing.",2025-12-14T08:42:42.700442+00:00
"context/unfinished_prompt_example.py","This Python script demonstrates how to use the `unfinished_prompt` function from the `pdd.unfinished_prompt` module. The function analyzes whether a given prompt text is complete or requires continuation, leveraging a Large Language Model (LLM) for assessment.

The script begins with detailed prerequisites: the `pdd` package must be accessible in the Python environment, a prompt template file named unfinished_prompt_LLM must exist, and the LLM invocation must be configured with appropriate API keys (e.g., `OPENAI_API_KEY`).

The example uses an intentionally incomplete prompt about baking sourdough bread to showcase the function's detection capabilities. The `unfinished_prompt` function accepts four parameters: `prompt_text` (required), `strength` (optional, 0.0-1.0), `temperature` (optional, 0.0-1.0), and `verbose` (optional boolean for logging).

The function returns a tuple containing: the LLM's reasoning explanation, a boolean indicating completeness, the estimated API call cost, and the model name used. Results are displayed using the `rich` library for formatted console output.

A commented-out section shows how to call the function with default parameters using a simpler, complete prompt example. This script serves as a practical reference for integrating prompt completeness analysis into applications.",2025-12-14T08:42:51.485759+00:00
"context/unrunnable_raw_llm_output.py","This file contains the implementation of a `context_generator` function in Python, designed to automate the generation of usage examples for Python modules. The function takes three parameters: a Python filename to read, an output filename for writing results, and an optional `force` boolean flag. The implementation demonstrates a workflow that includes: (1) reading a Python source file with error handling for missing files, (2) preprocessing the file contents, and (3) generating a prompt for GPT-4 that requests the model to create concise usage examples based on the provided code. The function returns a boolean indicating success or failure. The code includes try-except blocks for file operations and returns False if the input file doesn't exist. The file also provides documentation explaining each step of the process and includes a usage example showing how to call the function with sample filenames. Note that the implementation appears incomplete, as it references a `processed_content` variable and `preprocess` function that aren't fully defined in the visible code, and the model invocation and output writing steps mentioned in the description are not shown in the provided snippet.",2025-12-14T08:42:59.685127+00:00
"context/update_main_example.py","This Python script demonstrates a command-line interface (CLI) example using the Click library to update prompts based on modified code. The script imports an `update_main` function from a `pdd` module and wraps it in a CLI command called update.

The CLI accepts several options including: required paths for an input prompt file and modified code file, an optional original code file path, an output path for the updated prompt, a Git flag to retrieve original code from version history instead of a file, strength and temperature parameters for controlling model behavior, and flags for quiet mode, force overwrite, and verbose logging.

The `update` command function stores configuration parameters in Click's context object and calls `update_main()` with the provided arguments. The function returns three values: the updated prompt text, total cost, and model name used. Unless quiet mode is enabled, results are displayed using Rich's formatted printing, showing a truncated prompt snippet, the total cost, and the model name.

The script uses Click's group decorator to organize commands and includes example usage in the docstring showing how to run the update command with either explicit file paths or Git history integration. This serves as a practical template for building CLI tools that interface with prompt-updating functionality.",2025-12-14T08:43:06.212424+00:00
"context/update_model_costs_example.py","This Python script, `example_update_model_costs.py`, serves as an end-to-end demonstration for using the `update_model_costs.py` helper from the PDD package. The script teaches users how to create or reference an `llm_model.csv` file, invoke the `update_model_data` function either directly from Python or via the command-line interface (CLI), and understand the expected inputs and outputs of the function.

The prerequisites include having the PDD package on the PYTHONPATH, configured environment variables for LiteLLM API keys (e.g., OPENAI_API_KEY), and installed dependencies like `pandas`, `rich`, and `litellm`.

The script outlines the expected CSV schema, which includes fields for provider, model, input/output costs (per million tokens), coding arena ELO ratings, API configuration details, token limits, and structured output flags.

The demonstration workflow involves four steps: (1) creating a minimal sample CSV file with OpenAI and Anthropic model entries where cost fields are intentionally left blank, (2) calling the `update_model_data` function to populate the missing cost and structured output information in-place, (3) displaying the updated CSV results, and (4) optionally invoking the same functionality through the CLI using subprocess. This example provides a practical guide for automating LLM model cost updates.",2025-12-14T08:43:14.763227+00:00
"context/update_prompt_example.py","This Python script serves as a demonstration file for the `update_prompt` function from the `pdd` package. The main purpose is to showcase how to use this function to modify prompts based on code changes.

The script imports necessary modules including `os`, `tabnanny`, and components from the `pdd` package. It defines a `main()` function that sets up example parameters: an initial prompt asking to add two numbers, original code for an addition function, and modified code that changes the function to multiplication.

Key parameters configured include:
- `strength`: Uses a default value from the pdd package
- `temperature`: Set to 0 for deterministic LLM output
- `verbose`: Enabled for detailed output

The script calls `update_prompt()` with these parameters, which returns three values: the modified prompt, the total cost of the API call, and the model name used. Error handling is implemented using a try-except block to catch and display any exceptions that occur during execution.

If successful, the script prints the modified prompt, the cost formatted to six decimal places, and the model name. If the function returns None, it indicates a failure. The script follows standard Python conventions with a `if __name__ == __main__` guard for direct execution.",2025-12-14T08:43:23.189040+00:00
"context/vertex_ai_litellm.py","This Python script demonstrates how to use the LiteLLM library to interact with Google's Vertex AI Gemini model. The code begins by importing necessary modules: `litellm` for API completion calls, `json` for handling JSON data, and `os` for environment variable access. It enables debug logging by setting the `LITELLM_LOG` environment variable to DEBUG.

The script retrieves Vertex AI credentials from a file path stored in the `VERTEX_CREDENTIALS` environment variable. It reads and parses the JSON credentials file, then converts the credentials back to a JSON string format required by the LiteLLM library.

The main functionality calls the `completion` function to send a request to the Vertex AI Gemini 2.5 Pro model (preview version 05-06). The request includes a simple conversation with a system message defining the bot's persona (You are a good bot) and a user greeting (Hello, how are you?). The function is configured with the project ID meta-plateau-401521 and location us-central1 for the Vertex AI endpoint.

Finally, the script prints the model's response. This serves as a basic example of integrating LiteLLM with Google Cloud's Vertex AI for generative AI applications.",2025-12-14T08:43:31.148937+00:00
"context/whitepaper.md","This white paper introduces Prompt-Driven Development (PDD), a new software engineering paradigm that positions high-level prompts as the central development artifact instead of traditional code. PDD addresses limitations of conventional and AI-assisted coding, including cognitive overload, misalignment with business objectives, scalability issues, and technical debt accumulation. The methodology works by having developers craft detailed prompts specifying functional requirements, constraints, and context, which advanced AI models then interpret to generate code, tests, and documentation. Key advantages include enhanced stakeholder collaboration, increased productivity through automation, improved code consistency, and better alignment with business goals. The paper addresses potential challenges such as debugging AI-generated code, making small code changes, and learning curves, offering built-in PDD commands like `pdd trace`, `pdd update`, and `pdd generate` to mitigate these issues. The document provides adoption strategies, including pilot projects, training programs, and workflow integration recommendations. It concludes that PDD represents a fundamental advancement in software engineering, transforming developer roles toward design thinking and strategic oversight while leveraging AI capabilities for code generation.",2025-12-14T08:43:38.801108+00:00
"context/xml_tagger_example.py","This Python script demonstrates the usage of an XML tagging utility from a custom module called `pdd.xml_tagger`. The code imports the `xml_tagger` function and the `rich` library's print function for enhanced console output formatting.

The script sets up three key parameters: a raw prompt containing the text Write a story about a magical forest, a strength value of 0.5 (which controls the LLM model selection on a scale where 0 represents the cheapest option and 1 represents the highest ELO rating), and a temperature of 0.7 (controlling randomness in the model's output on a 0.0 to 1.0 scale).

Within a try-except block, the script calls the `xml_tagger` function with these parameters, which returns three values: the XML-tagged version of the prompt, the total cost of the operation, and the name of the model used. Upon successful execution, the script uses rich formatting to display a success message in bold green, followed by the tagged prompt, the cost formatted to six decimal places, and the model name. If an error occurs during execution, it catches the exception and displays the error message in bold red text. This appears to be a demonstration or testing script for an LLM-based XML tagging service.",2025-12-14T08:43:45.450532+00:00
