full_path,file_summary,date
"context/DSPy_example.py","The provided Python code snippet is an introductory example of how to use the DSPy library for programming with language models. The script begins by setting up the environment, which includes importing DSPy and configuring it to use OpenAI's 'gpt-3.5-turbo-instruct' model as the language model (LM). The main part of the code defines a custom module named 'chainofthought' that inherits from 'dspy.Module'. Inside this module, it initializes a 'dspy.ChainOfThought' program with a simple 'question->answer' signature. This program is designed to prompt the LM to generate a rationale before producing the final answer. The module's 'forward' method is defined to pass a given question to this Chain of Thought program. The script appears to be incomplete, as it ends with a comment indicating that the next steps would be to compile and optimize the module, which are key features of the DSPy framework for improving program performance. The code successfully lays out the basic structure but does not include the execution or optimization phases.",2025-09-01T19:20:02.410227+00:00
"context/anthropic_counter_example.py","The provided file contains a Python script that demonstrates how to count the number of tokens in a given text string using the Anthropic library. The script first imports the `anthropic` library and initializes its client. It then uses the `client.count_tokens()` method on a sample text. A significant comment in the code points out that this method is not accurate for newer models, specifically version 3.0 and above. Finally, the script prints the calculated number of tokens to the console. This code snippet serves as a basic example for token counting with the Anthropic API, while also providing a critical warning about its compatibility and accuracy with the latest generation of models.",2025-09-01T19:20:13.181613+00:00
"context/anthropic_tool_example.py","This Python script demonstrates how to use the Anthropic API to interact with an AI model. The script begins by importing the `anthropic` library and initializing an `Anthropic` client. It then constructs a request to the `claude-3-7-sonnet-20250219` model using the `client.messages.create` method. The request is configured with a maximum token limit of 1024 and specifies a tool named `str_replace_editor` of type `text_editor_20250124`. The content of the message sent to the model is a user prompt asking for assistance in fixing a syntax error within a file named `patent.md`. Finally, the script executes the API call and prints the entire response object received from the model to the console. The code serves as a basic example of making a tool-use request to a Claude model.",2025-09-01T19:20:20.315361+00:00
"context/auto_deps_main_example.py","This Python script defines a command-line interface (CLI) tool named `auto-deps` using the `click` library. The tool is designed to process a prompt file and analyze its dependencies. It accepts several command-line options to configure its behavior, including flags for forcing file overwrites (`--force`), suppressing output (`--quiet`), and forcing a directory rescan (`--force-scan`). It also takes parameters for dependency analysis strength and temperature, as well as paths for the input prompt file, the directory containing potential dependency files, an output CSV for dependency information, and the output directory for the modified prompt. The main function collects these options, initializes a `click` context, and calls the `auto_deps_main` function, which performs the core logic. After execution, the script prints the modified prompt, the total cost (suggesting an API call, likely to a large language model), and the model name used. The script includes basic error handling and is intended to be run as a standalone program.",2025-09-01T19:20:26.665356+00:00
"context/auto_include_example.py","This Python script serves as an example of how to use the `auto_include` function from the `pdd` library. The `main` function initializes several parameters, including a detailed `input_prompt` which instructs an LLM on how to create a Python function named `generate_test` for generating unit tests. This prompt notably contains an `<include>` directive. The script also defines a directory path, reads a CSV file named `project_dependencies.csv`, and sets configuration values like `strength` and `temperature` for an AI model. It then calls the `auto_include` function with these parameters. The function is expected to process the prompt, handle the include directive, and interact with an LLM. Upon completion, it returns the identified dependencies, a new CSV output, the total cost of the LLM operation, and the model's name. Finally, the script prints these results to the console, demonstrating a complete workflow.",2025-09-01T19:20:33.548372+00:00
"context/auto_update_example.py","This Python script serves as an example of how to use the `auto_update` function from the `pdd.auto_update` module. The primary purpose of the `auto_update` function is to check if a newer version of a specified Python package is available and facilitate an upgrade. The script's docstring explains that the function works by comparing the currently installed version of a package with the latest version available on PyPI or a specific version provided by the user. If the installed version is outdated, it prompts the user for confirmation to upgrade. If the user agrees, the function proceeds to upgrade the package using `pip`. The `main` function demonstrates three distinct use cases: a basic call to check the default package ('pdd'), a call to check a specific package by name ('requests'), and a call to check a package against a known target version ('pandas' version '2.0.0'). The script is designed to be executed directly to showcase this functionality.",2025-09-01T19:20:42.598991+00:00
"context/autotokenizer_example.py","This Python script defines a function, `count_tokens`, that calculates the number of tokens in a given text string using a specified pre-trained model from the Hugging Face transformers library. The function leverages `AutoTokenizer` to load the tokenizer for the model, which defaults to 'deepseek-ai/deepseek-coder-7b-instruct-v1.5'. It then tokenizes the input text and returns the length of the resulting 'input_ids' list, which represents the token count. The script also includes a simple example that demonstrates its usage by counting the tokens in the phrase 'Write a quick sort algorithm in Python.' and printing the result to the console.",2025-09-01T19:20:51.108139+00:00
"context/bug_main_example.py","This Python script demonstrates the usage of the `bug_main` function from the `pdd` library to automatically generate a unit test for a bug. It sets up a test case by programmatically creating several input files in an 'output' directory. These files include a prompt describing a function's requirements, a Python file with a buggy implementation of that function, a main program that uses the function, and two text files representing the current (incorrect) program output and the desired (correct) output. The script then calls `bug_main`, passing the paths to these files and some configuration options. The function analyzes the discrepancy between the observed and desired behavior to generate a unit test that specifically targets the bug. Finally, the script prints the generated unit test content, the total cost of the AI operation, and the name of the model that was used.",2025-09-01T19:20:59.659844+00:00
"context/bug_to_unit_test_example.py","This Python script serves as a command-line interface or a demonstration for a function named `bug_to_unit_test`. The script's `main` function orchestrates the process of generating a unit test based on a discrepancy between a program's current and desired outputs. It initializes several parameters, including hardcoded strings for `current_output` (which shows an error) and `desired_output` (which shows a successful run). It then reads the prompt used for code generation, the code under test, and a program to run the code from corresponding files (`prompts/trace_python.prompt`, `pdd/trace.py`, `context/trace_example.py`). These inputs, along with configuration settings like `strength` and `temperature`, are passed to the `bug_to_unit_test` function. Finally, it uses the `rich` library to print the generated unit test, the total cost, and the AI model used, while also handling any potential exceptions.",2025-09-01T19:21:09.694046+00:00
"context/bug_to_unit_test_failure_example.py","The provided file contains a single Python function definition named `add`. This function accepts two arguments, `x` and `y`. Although the function is named `add`, which implies it should perform addition, its implementation actually performs subtraction. The function returns the result of `x - y`, which is the difference between the two inputs, not their sum. This discrepancy between the function's name and its behavior is a potential source of bugs, as it violates the principle of least astonishment and would likely lead to incorrect calculations if used by a developer expecting it to add the numbers. The code is syntactically valid but logically flawed given its naming convention.",2025-09-01T19:21:18.473373+00:00
"context/change_example.py","This Python script serves as a demonstration for the `change` function from the `pdd.change` module. It imports necessary libraries, including `os`, `rich.console`, and the `change` function itself. The main logic is encapsulated within a `main` function. Inside `main`, it defines a set of example inputs: an initial prompt to write a factorial function, the corresponding Python code for that function, and a `change_prompt` requesting a modification to calculate the square root of the factorial's result. It also sets parameters like `strength` and `temperature`, likely for controlling a language model's output. The script then calls the `change` function with these inputs. Upon successful execution, it prints the modified prompt, the total cost of the operation (presumably an API call), and the model used, using the `rich` library for formatted console output. The script includes basic error handling and is designed to be run directly, as indicated by the `if __name__ == __main__:` block.",2025-09-01T19:21:27.568308+00:00
"context/change_main_example.py","This Python script serves as a comprehensive example for using the `change_main` function from the `pdd.change_main` module. It demonstrates two primary modes of operation: single-change and CSV batch-change. The script begins by setting up a simulated `click.Context` to configure various parameters like LLM strength, temperature, budget, and file settings, mimicking command-line arguments. For the single-change mode example, it creates a sample code file, an initial prompt, and a change instruction file. It then calls `change_main` to generate a modified prompt based on these inputs and prints the result, cost, and model used. For the CSV batch-change mode, it creates a directory with multiple code and prompt files. A CSV file is then generated, with each row specifying a target file and a corresponding modification instruction. The script calls `change_main` again with the `use_csv` flag enabled, processing all changes in batch. Finally, it displays the outcome of the batch operation, including a summary message, total cost, and the output location. The script uses the `rich` library for formatted console output and is self-contained, creating all necessary files and directories for the demonstration.",2025-09-01T19:21:34.345672+00:00
"context/cli_example.py","This Python script, `demo_run_pdd_cli.py`, is a self-contained demonstration of how to programmatically call the PDD (Prompt-Driven Development) command-line interface. The script's main goal is to generate a Python source file from a text prompt. It begins by creating an `output` directory and writing a simple prompt file (`hello_python.prompt`) that asks for a function to return Hello, PDD!. It then uses the `CliRunner` from the Click testing library to invoke the PDD CLI with the command `pdd --local generate`. The `--local` flag is specifically used to ensure the script runs without network access by using a local model. The script specifies the output path for the generated code as `output/hello.py`. Finally, it prints the exit code and console output from the CLI execution. It also checks if the output file was successfully created, and if so, displays its path and the first few lines of its content, providing a complete example of integrating the PDD CLI into a Python program.",2025-09-01T19:21:44.932209+00:00
"context/click_example.py","This Python script implements a command-line image processing pipeline using the `click` and `Pillow` (PIL) libraries. It defines a main command group `cli` with `chain=True`, allowing multiple subcommands to be linked together, simulating a Unix-style pipe where the output of one command feeds into the next. A `result_callback` function, `process_commands`, orchestrates this flow by iterating through functions returned by each subcommand. The script uses custom `processor` and `generator` decorators to wrap image manipulation functions, enabling them to operate on a stream of images. It provides a comprehensive set of image operations as subcommands, including `open` to load images, `save` to write them to disk, and `display` to show them. Various transformation commands are available, such as `resize`, `crop`, `transpose` (for rotation and flipping), `blur`, `smoothen`, `emboss`, and `sharpen`. There is also a `paste` command to overlay one image onto another. Each command processes a stream of images and yields the modified images for the next command in the chain, creating a powerful and flexible tool for batch image processing directly from the terminal.",2025-09-01T19:21:54.074173+00:00
"context/cmd_test_main_example.py","This Python script, `test_cli_example.py`, demonstrates how to build a command-line interface (CLI) using the `click` library to automate unit test generation. The script defines a main `cli` group that manages global settings like verbosity, AI strength, and temperature. The core functionality resides in a `test` subcommand, which takes a source code file and its original prompt file as required inputs. It then calls an external function, `cmd_test_main`, to handle the actual test generation or enhancement. The CLI supports numerous options, allowing users to specify an output file, provide an existing test file to enhance, use a coverage report to guide test creation, set a target coverage percentage, and merge new tests with existing ones. After execution, the script prints the generated test code, the total cost of the AI operation, and the name of the model used. It serves as a comprehensive example for integrating automated testing into a development workflow via a custom CLI tool.",2025-09-01T19:22:03.874326+00:00
"context/code_generator_example.py","This Python script demonstrates the use of the `code_generator` function from the `pdd.code_generator` module. The `main` function reads a prompt from an external file (`prompts/generate_test_python.prompt`), sets parameters such as language, strength, and temperature, and then calls the `code_generator`. The script is designed to receive the generated code, the total cost of the operation, and the model name used. Finally, it prints these results to the console. The script includes a `try...except` block for basic error handling and is executed when run as the main program.",2025-09-01T19:22:16.739813+00:00
"context/code_generator_main_example.py","This Python script serves as a comprehensive example for using the `code_generator_main` function from a package named `pdd`. The script's primary purpose is to demonstrate various code generation scenarios in a controlled, self-contained environment without requiring actual API keys or network calls. It achieves this by extensively using mocking. Key functions responsible for local and incremental code generation (`local_code_generator_func`, `incremental_code_generator_func`) are replaced with mock versions that return predictable, hardcoded results. The main example function, `main_example`, executes several distinct scenarios: 1) a full, from-scratch code generation for a Java file; 2) an incremental update to an existing Python file; 3) a forced incremental update to demonstrate overriding the model's suggestion; and 4) a simulated cloud generation attempt that gracefully falls back to the local mock. The script uses `unittest.mock.patch` to apply these mocks and manages all file input/output, creating prompt files and writing the generated code to a dedicated output directory. Overall, it acts as a usage guide or an integration test for the `code_generator_main` function, showcasing its core features like local/cloud execution and full/incremental generation.",2025-09-01T19:22:22.766078+00:00
"context/comment_line_example.py","This document provides a detailed explanation and example usage for a Python function named `comment_line` from the `comment_line.py` module. The function is designed to comment out a single line of code. It accepts two string parameters: `code_line`, which is the line of code to be commented, and `comment_characters`, which specifies the commenting style. The behavior of the function varies based on the `comment_characters` argument. If it's a single character or string (e.g., '#'), it's prepended to the code line. If it contains a space (e.g., '<!-- -->'), it's treated as start and end comment tags that wrap the code line. If the argument is the specific string 'del', the function returns an empty string, effectively deleting the line. The document includes the full source code for the function, complete with docstrings, and provides clear examples demonstrating its use for Python-style comments, HTML-style comments, and line deletion. An explanation of the input and output parameters is also included to ensure clarity for the user.",2025-09-01T19:22:32.861017+00:00
"context/config_example.py","This Python code snippet's primary function is to initialize the application's configuration at the very beginning of its execution. It imports the `init_config` function from a `utils.config` module. Immediately following the import, it calls this `init_config()` function. A prominent comment, `# Initialize configuration FIRST`, underscores the critical importance of this step, indicating that it is a prerequisite for other parts of the application to function correctly. The code's singular focus suggests that subsequent modules or processes depend on the configuration values that are set up by this initialization routine.",2025-09-01T19:22:40.810300+00:00
"context/conflicts_in_prompts_example.py","This Python script demonstrates the functionality of a `conflicts_in_prompts` function, which is designed to analyze and identify potential conflicts between two AI code generation prompts. The script defines two extensive string variables, `prompt1` and `prompt2`. `prompt1` details the requirements for generating a Firebase authentication helper module (`auth_helpers.py`), while `prompt2` specifies the creation of a `User` dataclass for the application's data model. The `main` function calls `conflicts_in_prompts` with these two prompts as input, along with parameters like `strength` and `temperature`. The script then prints the results of this analysis, including the language model used, the operational cost, and any suggested changes to resolve inconsistencies between the prompts. It utilizes the `rich` library for formatted console output. The overall purpose is to showcase a tool that helps developers ensure consistency and coherence when using multiple AI prompts to generate different but related parts of a software project.",2025-09-01T19:22:48.956566+00:00
"context/conflicts_main_example.py","This Python script serves as a test or demonstration for the `conflicts_main` function, which is imported from `pdd.conflicts_main`. The script begins by programmatically creating two text files, `prompt1_LLM.prompt` and `prompt2_LLM.prompt`, each containing a slightly different system prompt for an AI assistant. To facilitate the testing of `conflicts_main` without a command-line interface, it defines a `MockContext` class that simulates a Click context object. An instance of this mock context is created. The script then invokes the `conflicts_main` function, providing it with the mock context, the paths to the two prompt files, and a path for an output CSV file (`outputconflicts_output.csv`). After the function executes, the script prints the returned values, which include the identified conflicts, the total operational cost, and the name of the language model used. The detailed results of the conflict analysis are saved to the specified CSV file.",2025-09-01T19:23:01.049009+00:00
"context/construct_paths_example.py","This Python script, `demo_construct_paths.py`, serves as a demonstration for the `pdd.construct_paths.construct_paths` function. The script begins by creating a temporary prompt file named `Makefile_makefile.prompt` containing a simple development task. It then simulates the arguments that would typically be passed from a command-line interface, including the input file path, a 'force' overwrite flag, and the command being executed ('generate'). The core of the script is the call to `construct_paths`, which processes these inputs to read the prompt file's contents, determine the appropriate output file path for the generated code, and detect the relevant programming language from the file name. After the function returns, the script prints the retrieved input string, the calculated output file path, and the detected language ('makefile') to the console. Finally, it performs a cleanup operation by deleting the temporary prompt file, ensuring the directory is left in its original state. The script effectively illustrates how the `construct_paths` helper prepares file paths and content for a Prompt-Driven Development tool.",2025-09-01T19:23:08.496513+00:00
"context/context_generator_example.py","This Python script serves as an example of how to use the `context_generator` function from the `pdd.context_generator` module. The script first validates that the `PDD_PATH` environment variable is set, indicating a required configuration for the PDD tool. It then defines a set of input parameters, including a simple Python function (`add`), a natural language prompt describing it, the programming language, and model-specific settings like `strength` and `temperature`. The core functionality involves calling `context_generator` with these inputs. This function, likely interacting with a language model API, returns a generated code example, the total cost of the API call, and the name of the model used. Finally, the script utilizes the `rich` library to print these results to the console, providing a clear, formatted output of the generated code, its associated cost, and the model that produced it. The script essentially demonstrates a single, verbose run of the context generation process.",2025-09-01T19:23:15.440351+00:00
"context/context_generator_main_example.py","This Python script demonstrates the usage of the `context_generator_main` function from the `pdd.context_generator_main` module. It begins by importing the necessary `click` library and the main function. It then manually constructs a `click.Context` object, setting various parameters such as `force`, `quiet`, and `verbose` to control the execution flow. Additionally, it configures AI model-specific settings like `strength` and `temperature` within the context's object attribute. The script defines file paths for an input prompt (`prompts/get_extension_python.prompt`), a source code file (`pdd/get_extension.py`), and an output file (`output/example_code.py`). The core of the script is the call to `context_generator_main`, passing these configurations and file paths. Finally, it captures the returned generated code, total cost, and the AI model name, printing these results to the standard output. The script serves as an example of how to programmatically invoke the code generation process.",2025-09-01T19:23:24.016839+00:00
"context/continue_generation_example.py","This Python script defines a `main` function that orchestrates a text generation task. It imports a `continue_generation` function from the `pdd` library. The script's primary role is to prepare inputs for this function by reading a preprocessed prompt from `context/cli_python_preprocessed.prompt` and a partial language model output from `context/llm_output_fragment.txt`. It then calls `continue_generation` with these inputs, along with specified `strength` and `temperature` parameters, to complete the text generation. After the generation is finished, the script prints the total cost and the name of the model used. The final, completed text output is then written to a new file named `context/final_llm_output.py`. The entire process is wrapped in a try-except block for basic error handling, and the script is executed via the standard `if __name__ == __main__:` guard.",2025-09-01T19:23:29.430279+00:00
"context/crash_main_example.py","This Python script demonstrates the functionality of a `crash_main` function, which appears to be a tool for automatically debugging and fixing code. The script sets up a controlled test environment by creating several files in an 'output' directory. These files include: a prompt defining a factorial function, a buggy implementation of this function that fails to handle negative numbers, a main program that calls the function with a negative number, and a log file containing the resulting `RecursionError` traceback. The script then invokes `crash_main`, providing it with the paths to all these files. It configures the function to perform iterative fixing with a maximum of five attempts and a budget. Finally, it prints the outcome of the debugging process, including whether the fix was successful, the number of attempts, the cost, the AI model used, and the contents of the newly generated, corrected code and program files. The overall purpose is to showcase an automated crash-fixing workflow.",2025-09-01T19:23:37.907616+00:00
"context/detect_change_example.py","This Python script uses the `detect_change` function from a `pdd` library to analyze a specified list of prompt files. The script's objective is to identify changes that would make these prompts more compact by incorporating a common preamble, as described in the `change_description` variable. It configures parameters for a language model, such as `strength` and `temperature`, before calling the analysis function. Upon completion, it uses the `rich` library to print the suggested changes for each file, along with the total cost of the operation and the name of the model that was used. The script includes error handling to catch and display any exceptions that may occur during the process.",2025-09-01T19:23:46.796565+00:00
"context/detect_change_main_example.py","This Python script acts as a test driver for a function named `detect_change_main`. It uses the `click` library to simulate command-line arguments, setting up a context with model parameters like `strength` and `temperature`. The script defines a list of input prompt files and a specific change description, which it writes to a file. The core action is calling `detect_change_main` with these predefined inputs, along with a specified output CSV file path. After the function call, it processes the results by printing the language model used, the total cost incurred, and a detailed list of the changes detected for each prompt file. The script is self-contained, running its `main` function when executed, and includes basic error handling. Its primary purpose is to programmatically test or demonstrate the functionality of `detect_change_main` in a controlled environment.",2025-09-01T19:23:53.395256+00:00
"context/edit_file_example.py","This Python script serves as a test harness and example for a function named `run_edit_in_subprocess`, which is designed to edit files based on natural language instructions. The script begins by configuring its environment, dynamically adjusting the system path to import the target function from a parent directory and verifying that a `PDD_PATH` environment variable is set. A helper function, `create_example_files`, prepares the test by creating a dummy text file with predefined initial content. The core of the script is the `run_edit_file_test` function, which calls `run_edit_in_subprocess` with the path to the dummy file and a set of specific editing instructions. After the edit attempt, the script reads the modified file's content and performs a detailed verification to check if the requested changes—such as replacing words, rewriting lines, and adding new lines—were successfully applied. Finally, it prints a comprehensive report detailing the success status, any errors, the final file content, and the verification results. The main execution block orchestrates this entire process, providing a clear demonstration of the file editing function's capabilities and its prerequisites, which include API keys and external services.",2025-09-01T19:24:01.380189+00:00
"context/find_section_example.py","This document serves as a guide for the `find_section` function, located in the `pdd.find_section` Python module. The function's main purpose is to scan a list of text lines and identify embedded code blocks. The documentation outlines the function's inputs, which include a list of strings (`lines`) and optional parameters like `start_index` and `sub_section`. The output is a list of tuples, where each tuple details a found code block by specifying its programming language, start line index, and end line index.

Two practical examples are provided to demonstrate its usage. The first example processes a hardcoded multi-line string containing both Python and JavaScript code, showing how to call the function and interpret the results. The second example extends this by reading content from an external file (`unrunnable_raw_llm_output.py`) and applying the `find_section` function to locate code sections within it. The file also includes a sample output, which clarifies the expected format of the returned data, confirming the function's ability to correctly identify the language and line numbers for each code section.",2025-09-01T19:24:12.131127+00:00
"context/firecrawl_example.py","This Python script demonstrates the basic usage of the `firecrawl-py` library for web scraping. The script begins by importing the `FirecrawlApp` class and the `os` module. It then initializes the `FirecrawlApp` by retrieving an API key from an environment variable named `FIRECRAWL_API_KEY`, with a fallback to a placeholder key. The core of the script is a call to the `app.scrape_url` method, which is used to scrape the content of 'https://www.google.com'. A specific parameter, `formats=['markdown']`, is passed to request the scraped data in Markdown format. Finally, the script accesses the `markdown` attribute from the result of the scrape operation and prints it to the console. A comment at the top of the file indicates that the library can be installed using `pip install firecrawl-py`. This code serves as a simple, functional example for getting started with the Firecrawl service in a Python environment.",2025-09-01T19:24:21.508231+00:00
"context/fix_code_loop_example.py","This Python script serves as a demonstration for an automated code-fixing function called `fix_code_loop`. The `main` function sets up a test scenario by creating two files: a Python module with a `calculate_average` function and a verification script designed to fail by calling that function with an invalid argument (a string). It also defines the original natural language prompt that would have generated the initial code. The script then calls `fix_code_loop`, providing it with the buggy code, the verification program, the prompt, and configuration parameters such as model strength, temperature, and a maximum number of attempts. The purpose of `fix_code_loop` is to automatically debug and correct the code until the verification script runs without errors. After the process completes, the script prints a summary of the results, including whether the fix was successful, the number of attempts, the total cost, the model used, and the final, corrected code. The script effectively showcases a workflow for an automated debugging and code repair tool.",2025-09-01T19:24:30.263823+00:00
"context/fix_code_module_errors_example.py","This Python script serves as an example of how to use the `fix_code_module_errors` function from the `pdd.fix_code_module_errors` module. The script sets up a scenario with a buggy program, which incorrectly tries to calculate the sum of a string instead of a list of numbers. It defines the problematic program, the original natural language prompt that generated the code, the isolated code module containing the function, and the `TypeError` message produced at runtime. These elements are then passed as arguments to the `fix_code_module_errors` function. The function processes these inputs to generate corrected versions of the program and the code module. Finally, the script prints the results, including boolean flags indicating whether updates were necessary, the fixed code itself, the total cost of the API call, and the name of the model used for the fix. This demonstrates a complete workflow for automatically debugging and repairing code based on runtime errors.",2025-09-01T19:24:39.908398+00:00
"context/fix_error_loop_example.py","This Python script serves as a demonstration for a function named `fix_error_loop`, which is imported from the `pdd.fix_error_loop` module. The `main` function initializes a set of parameters required by `fix_error_loop`, including paths to a unit test file, a code file to be debugged, a prompt file, and a verification program. It also defines configuration settings like `strength`, `temperature`, `max_attempts`, and a `budget` for the error-fixing process. The script then calls `fix_error_loop` with these parameters. The results of this call, such as the success status, total attempts, cost, the model used, and the final versions of the code and unit tests, are then printed to the console in a formatted manner using the `rich` library. The entire process is wrapped in a try-except block to handle and report any potential errors that may occur during execution. The script is intended to be run directly, as indicated by the `if __name__ == __main__:` block.",2025-09-01T19:24:48.034832+00:00
"context/fix_errors_from_unit_tests_example.py","This Python script serves as a demonstration for a function named `fix_errors_from_unit_tests`. The script defines a `main` function that sets up example inputs, including a piece of Python code for an `add` function, a corresponding unit test with a deliberate failure, and the resulting error message. The `add` function also contains a deprecated NumPy call. The script then calls `fix_errors_from_unit_tests` with this buggy code, the failing test, the error string, and other parameters like LLM strength and temperature. The purpose of this function is presumably to automatically debug the code and/or the unit test. Finally, the script prints the results of the debugging process, which include the potentially fixed code and unit test, analysis results, the cost of the operation, and the model used. The script is designed to be run directly to showcase this automated error-fixing capability.",2025-09-01T19:24:56.885923+00:00
"context/fix_main_example.py","This Python script defines a command-line interface (CLI) tool using the `click` library to automatically fix errors in code and its corresponding unit tests. The primary function, `fix_command`, is decorated as a Click command and accepts numerous options, including paths to a prompt file, the code file to be fixed, the unit test file, and an error log. It also takes parameters for output locations, iterative fixing (`--loop`), a verification program, maximum attempts, a budget, and an auto-submit flag. The command orchestrates the fixing process by calling a `fix_main` function from another module, passing along all the provided arguments. After `fix_main` completes, the script prints a success or failure message to the console, along with details like the number of attempts and the cost. The `if __name__ == '__main__':` block demonstrates how to run the command programmatically, setting up a default context and hardcoding example arguments to test the fixing functionality on a specific set of files.",2025-09-01T19:25:04.280115+00:00
"context/fix_verification_errors_example.py","This Python script serves as a comprehensive example for using the `fix_verification_errors` function from a package named `pdd`. The function's purpose is to automatically identify and correct errors in a Python code module by leveraging Large Language Models (LLMs). The script meticulously sets up a test case by defining several key inputs: a `program_code` that runs and verifies a separate module, an `original_prompt` that describes the module's intended functionality, a `buggy_code_module` containing an intentional error (subtraction instead of addition), and the `verification_output` from the program which clearly indicates a failure. It also configures LLM parameters like strength and temperature. The script then calls `fix_verification_errors` with these inputs and prints the results, which include the potentially corrected code, flags indicating whether the code was updated, and metadata such as the LLM model used and the associated cost. The example effectively demonstrates a complete workflow for automated, LLM-driven code debugging based on verification logs.",2025-09-01T19:25:11.227352+00:00
"context/fix_verification_errors_loop_example.py","This Python script serves as a comprehensive example for using the `fix_verification_errors_loop` function, which is designed to automatically debug code. The script begins by setting up a test scenario. It programmatically creates several files: a `program.py` that acts as a test harness, a `calculator_module.py` containing an intentional bug (subtraction instead of addition), and a secondary `verification_program.py` with assertions. It defines all the necessary parameters for the main function, including file paths, the original prompt used for code generation, LLM settings like strength and temperature, and loop controls such as maximum attempts and budget. The core of the script is the call to `fix_verification_errors_loop`, passing in all the configured parameters. This function is expected to iteratively attempt to fix the bug in `calculator_module.py` until the verification process in `program.py` succeeds. Finally, the script prints the results of the debugging process, including a success flag, the final corrected code, the total number of attempts, and the associated cost. This provides a clear demonstration of the function's inputs, execution, and outputs.",2025-09-01T19:25:19.724567+00:00
"context/fix_verification_main_example.py","This Python script serves as a comprehensive example for using the `fix_verification_main` function, presumably from a larger 'pdd' package. The core purpose of this function is to automatically verify and fix buggy code using a Large Language Model (LLM) based on a provided prompt. The script sets up a demonstration by creating several files: a prompt asking for a calculator's 'add' function, an intentionally buggy version of the code that subtracts instead of adds, a program to run the code, and a verification script with assertions. It then showcases two distinct operational modes. The first is a 'single pass' mode where the LLM attempts to fix the code in one shot by judging its output against the prompt. The second is an 'iterative loop' mode where the function repeatedly attempts a fix and then runs the verification script to confirm correctness, continuing until the code passes, a maximum number of attempts is reached, or a specified budget is exceeded. The script simulates a command-line interface context using a dummy Click object and prints detailed results for both modes, including success status, cost, and the final corrected code.",2025-09-01T19:25:29.928180+00:00
"context/gemini_may_pro_example.py","The provided Python code snippet demonstrates a basic usage of the `litellm` library to generate a text completion from a large language model. The script imports the `completion` function from `litellm` and then calls it. The call specifies the model to be used as `vertex_ai/gemini-2.5-pro-preview-05-06`, which corresponds to a specific version of Google's Gemini Pro model available through Vertex AI. The prompt sent to the model is a simple user message with the placeholder content `Your prompt here`. The entire response object returned by the `completion` function is then stored in the `response` variable and subsequently printed to the standard output. This example illustrates a minimal implementation for making an API call to an LLM through the `litellm` abstraction layer.",2025-09-01T19:25:41.188356+00:00
"context/generate_output_paths_example.py","This Python script serves as a comprehensive example and test suite for a function named `generate_output_paths`. The script's primary purpose is to demonstrate how this function determines absolute file paths for various outputs based on a combination of inputs. It begins by adjusting the system path to import the target function from a parent directory, simulating a typical module structure. The core of the script consists of eight distinct scenarios that test the function's logic. These scenarios cover default behavior, user-specified filenames and directories, the use of environment variables to override defaults, and the handling of commands with multiple, distinct outputs. It also showcases how some commands have fixed default extensions and how the function gracefully handles edge cases like unknown commands or missing input basenames. For each scenario, the script clearly prints the inputs, the function's actual output, and the expected output, making it an effective validation and usage guide for the `generate_output_paths` function.",2025-09-01T19:25:47.249180+00:00
"context/generate_test_example.py","This Python script is an example demonstrating the usage of the `generate_test` function from the `pdd.generate_test` library. It begins by importing necessary modules, including `os`, `generate_test`, and `print` from the `rich` library for enhanced terminal output. The script defines a set of input parameters: a natural language `prompt` describing a factorial function, the `code` for the function itself, and configuration values for `strength` and `temperature`. It then calls the `generate_test` function with these parameters to automatically generate a unit test. The script is designed to handle both successful execution and potential errors. Upon success, it prints the generated unit test, the total cost of the API call, and the model used, all formatted with `rich`. If an exception occurs, it prints a formatted error message. A commented-out line suggests that setting a `PDD_PATH` environment variable might be a necessary configuration step for the library.",2025-09-01T19:25:58.670718+00:00
"context/get_comment_example.py","This document provides an example and documentation for the `get_comment` Python function, located in the `pdd.get_comment` module. The function's purpose is to retrieve the comment character(s) for a specified programming language. It takes a single, case-insensitive string argument, `language`, and returns the corresponding comment symbol (e.g., '#' for Python). The function's operation depends on an environment variable, `PDD_PATH`, which must point to a directory containing a `data/language_format.csv` file. This CSV file serves as a lookup table, mapping language names to their comment syntax. If the `PDD_PATH` is not set, the language is not found in the CSV, or any other error occurs, the function returns the default string 'del'. The provided code snippet demonstrates how to import and use the function, showing expected outputs for both valid and invalid language inputs.",2025-09-01T19:26:06.391934+00:00
"context/get_extension_example.py","This Python script demonstrates the usage of a function named `get_extension`, which is imported from the `pdd.get_extension` module. The script does not define the function itself but rather shows how to use it through three example calls with different inputs. The examples provided are `get_extension(Python)`, `get_extension(Makefile)`, and `get_extension(JavaScript)`. Comments in the code indicate the expected output for each call, suggesting that the function's purpose is to return the corresponding file extension or conventional filename for a given language or file type. For instance, it is expected to return `.py` for Python, `.js` for JavaScript, and the string Makefile for the input Makefile. The file serves as a clear, concise usage example or a test script for the `get_extension` functionality.",2025-09-01T19:26:14.272644+00:00
"context/get_jwt_token_example.py","This Python script is an asynchronous command-line tool designed to handle user authentication. It leverages the GitHub Device Flow to obtain a JSON Web Token (JWT) from Firebase. The script starts by importing necessary libraries and a custom `get_jwt_token` function, then retrieves configuration constants like the Firebase API key and GitHub client ID from environment variables. The main asynchronous function orchestrates the authentication process, calling `get_jwt_token` and implementing comprehensive error handling for various potential issues, including authentication failures, network problems, token errors, and rate limiting. If the authentication is successful, the script prints the retrieved Firebase ID token. It then automatically updates a local `.env` file, replacing any existing `JWT_TOKEN` variable with the new token or adding it if it doesn't exist. This provides a convenient way to manage and refresh authentication credentials for a development environment.",2025-09-01T19:26:22.026392+00:00
"context/get_language_example.py","This Python script serves as a demonstration for the `get_language` function, which it imports from the `pdd.get_language` module. The script's entry point is the `main` function, executed when the file is run directly. Inside `main`, it defines a sample file extension, `'.py'`, and passes it to the `get_language` function to identify the associated programming language. The entire process is enclosed in a `try...except` block for error handling. Based on the function's return value, the script prints one of three outcomes: the identified programming language, a message indicating that no language was found for the given extension, or an error message if an exception occurred during execution. In essence, the file is a simple, self-contained example illustrating the usage and expected output of the `get_language` utility.",2025-09-01T19:26:29.936959+00:00
"context/git_update_example.py","This Python script demonstrates the usage of the `git_update` function from the `pdd.git_update` module. The `main` function orchestrates the process. It begins by defining input parameters, notably reading an initial prompt from a file named `prompts/fix_error_loop_python.prompt` and specifying a target code file, `pdd/fix_error_loop.py`, to be modified. It then calls the `git_update` function with the prompt, the file path, and other settings like strength and temperature for the language model. If the function executes successfully, it prints the results, which include the modified prompt, the total cost of the API call, and the name of the model used. Crucially, the script then overwrites the original prompt file with the new, modified prompt returned by the function. The entire process is wrapped in a try-except block to handle potential `ValueError` exceptions and other unexpected errors, printing informative messages if any occur. The script is designed to be run directly, as indicated by the `if __name__ == __main__:` block.",2025-09-01T19:26:39.039595+00:00
"context/increase_tests_example.py","This Python script serves as a usage example for the `increase_tests` function from the `pdd` library. It defines a function `example_usage` that demonstrates how to automatically generate more comprehensive unit tests for a given piece of code to improve its test coverage. The script sets up sample inputs, including existing code, a basic unit test, a coverage report indicating incomplete testing, and the original prompt that generated the code. It then showcases two calls to `increase_tests`: a basic call using default parameters and an advanced call specifying custom parameters like `language`, `strength`, and `temperature`. The script is designed to print the results, including the newly generated tests, the operational cost, and the model used. It also includes a `try...except` block to handle potential errors during the function execution. The main execution block ensures the example is run when the script is executed directly.",2025-09-01T19:26:49.473596+00:00
"context/incremental_code_generator_example.py","This Python script demonstrates the usage of the `incremental_code_generator` function from the `pdd` package. It sets up an example scenario to showcase how to update an existing piece of code based on a revised prompt. The script defines an `original_prompt` (to create a factorial function), the `existing_code` generated from it, and a `new_prompt` which adds an input validation requirement. It then calls the `incremental_code_generator` with these inputs, along with other configuration parameters like language, strength, and temperature. The function's purpose is to determine whether to apply a minimal, incremental patch to the code or to perform a full regeneration. Finally, the script uses the `rich` library to print the results to the console, indicating whether the update was incremental, displaying the updated code, and reporting the operational cost and the language model used. This serves as a practical example of evolving code with LLMs in a structured and potentially more efficient way than complete regeneration.",2025-09-01T19:26:59.258700+00:00
"context/insert_includes_example.py","This Python script provides an example usage of the `insert_includes` function from the `pdd.insert_includes` module. The script defines a sample `input_prompt` for an AI, a directory path containing context files, and an output CSV filename. It then calls `insert_includes`, passing these parameters along with custom settings for `strength` and `temperature`. The function processes the inputs to generate a modified prompt that includes relevant code dependencies, a CSV string of these dependencies, and cost/model information. The script uses the `rich` library to print the original prompt, the modified prompt, the CSV output, and other metadata to the console in a formatted way. It also includes error handling for potential issues like a `FileNotFoundError` or other exceptions during processing and saves the generated CSV content to a file.",2025-09-01T19:27:10.325823+00:00
"context/install_completion_example.py","This Python script provides a working example of how to use the shell completion installation module from the `pdd` package. It showcases two main functions: `get_local_pdd_path()`, which retrieves the absolute path to the `PDD_PATH` directory, and `install_completion()`, which automatically installs shell completion for the PDD command-line interface. A key feature of this example is its safe execution environment. To avoid modifying the user's actual system files, the script creates temporary 'home' and 'pdd' directories. It then sets the `HOME` and `PDD_PATH` environment variables to point to these dummy locations. Within this sandboxed environment, it creates a fake shell RC file (e.g., `.bashrc`) and a dummy completion script. The `main` function first sets up this safe environment and then calls the `pdd` functions to demonstrate their behavior. The script uses the `rich` library to print color-coded, informative messages to the console, showing the steps being performed, such as locating the `PDD_PATH` and updating the RC file. The example is self-contained and can be run directly to observe the installation logic without affecting the user's configuration.",2025-09-01T19:27:19.286082+00:00
"context/langchain_lcel_example.py","This Python script serves as a comprehensive demonstration of the LangChain library's capabilities for interacting with a wide array of Large Language Models (LLMs). It showcases the integration of numerous models from various providers, including OpenAI, Google (Gemini, VertexAI), Anthropic, Groq, Together, Azure, DeepSeek, and local models via Ollama and MLX. The script heavily utilizes the LangChain Expression Language (LCEL) to construct processing chains, typically combining a prompt template, an LLM, and an output parser. It highlights different parsing techniques, such as `StrOutputParser` for simple text, and `JsonOutputParser` and `PydanticOutputParser` for generating structured JSON output based on a Pydantic `Joke` model. A key feature is a custom `CompletionStatusHandler` callback, which is used to track API call metrics like token usage and completion status. The script also demonstrates setting up a SQLite cache to improve performance and reduce costs, along with advanced features like configurable model fallbacks and the `with_structured_output` method for OpenAI models. Overall, it's a practical collection of examples for building LLM-powered applications.",2025-09-01T19:27:29.218931+00:00
"context/llm_invoke_example.py","This Python script demonstrates the usage of a function named `llm_invoke`, which appears to be a wrapper for calling large language models. The script defines a Pydantic model `Joke` to enforce a specific JSON structure for the output. The `main` function iterates through a `strength` parameter from 0.0 to 1.0, calling `llm_invoke` in each iteration to generate jokes. It showcases two primary use cases: first, generating an unstructured text response, and second, generating a structured JSON response that is parsed into the `Joke` Pydantic model. The script also tracks and records which underlying language model is used for different ranges of the `strength` parameter. At the end of its execution, it prints a summary of these model-to-strength mappings, illustrating how the `strength` value likely influences model selection for cost or performance optimization. The examples involve generating jokes about programmers and data scientists.",2025-09-01T19:27:39.478135+00:00
"context/llm_selector_example.py","This Python script serves as a demonstration for the `llm_selector` function, which is imported from the `pdd.llm_selector` module. The `main` function initializes two parameters, `strength` and `temperature`, and then enters a loop. This loop iterates through a range of `strength` values from 0.5 to 1.1, incrementing by 0.05 in each step. Inside the loop, it calls the `llm_selector` function with the current `strength` and a fixed `temperature`. The function returns the selected language model, a token counting function, input/output costs, and the model's name. The script then prints these details to the console, effectively showing how different strength values map to different underlying LLM models and their associated costs. It also includes a practical example of using the returned `token_counter` function on a sample string. The script is wrapped in a standard `if __name__ == __main__:` block to ensure it runs when executed directly and includes basic error handling for `FileNotFoundError` and `ValueError`.",2025-09-01T19:27:45.721050+00:00
"context/load_prompt_template_example.py","This Python script serves as an executable example for loading and displaying a prompt template. It imports a custom function, `load_prompt_template`, from a local module `pdd.load_prompt_template`, and utilizes the `rich` library for colored console output. The `main` function is designed to load a specific prompt template named generate_test_LLM. If the template is found and loaded successfully, the script prints a blue-colored header to the console, followed by the contents of the loaded prompt. The standard `if __name__ == __main__:` block ensures that the `main` function is called when the script is run directly, making it a self-contained demonstration of the prompt loading functionality.",2025-09-01T19:27:52.415510+00:00
"context/logo_animation_example.py","This Python script serves as a demonstration for a terminal-based branding animation library, specifically from a package named 'pdd'. The script imports `start_logo_animation` and `stop_logo_animation` functions. The `main` function first calls `start_logo_animation` to initiate a full-screen animation that runs in a background thread. It then pauses the main program's execution for 7 seconds, allowing the animation sequence—which includes formation, hold, and expansion phases—to complete and hold its final state. During this pause, the script simulates ongoing work, noting that any standard output would be obscured by the full-screen animation. Finally, it calls `stop_logo_animation` to terminate the animation, clean up the background thread, and restore normal terminal functionality. The script's primary purpose is to illustrate the basic usage of starting and stopping the PDD logo animation within a larger application.",2025-09-01T19:28:00.883812+00:00
"context/postprocess_0_example.py","This Python script serves as a usage example for the `postprocess_0` function, located in the `staging.pdd.postprocess_0` module. The script demonstrates how to use this function to process the output from a Large Language Model (LLM) and isolate a specific block of code. The `postprocess_0` function accepts two string arguments: `llm_output`, which is the raw text from an LLM, and `language`, which specifies the programming language of interest (e.g., 'python'). The function's core logic involves identifying all code sections of the specified language within the input, selecting the largest one, and then returning a modified string. In this new string, the largest code block is left intact, while all other text and code sections are commented out using the appropriate comment syntax for the target language. The example provided in the file initializes a sample LLM output containing mixed text, Python, and Java code, and then uses `postprocess_0` to extract and preserve only the largest Python code segment.",2025-09-01T19:28:08.579583+00:00
"context/postprocess_example.py","This Python script demonstrates the usage of a `postprocess` function from a `pdd.postprocess` module, which is designed to extract code blocks from text, such as the output from an LLM. The example showcases two distinct scenarios. The first is a 'simple extraction' mode (when `strength=0`), which uses basic string processing to find code enclosed in triple backticks. This method is fast and has no associated cost. The second is an 'advanced extraction' mode (when `strength > 0`), which leverages an LLM for more sophisticated and accurate code extraction, a method that is more powerful but incurs both time and monetary costs. A key aspect of the script is its use of Python's `unittest.mock` to simulate the functions (`load_prompt_template` and `llm_invoke`) required for the LLM-based scenario. This allows the example to run and illustrate the function's logic without needing actual API keys, network access, or external prompt files. The script uses the `rich` library for formatted output and verifies that the mocked functions are called as expected, serving as a self-contained and clear demonstration.",2025-09-01T19:28:18.159096+00:00
"context/preprocess_example.py","This Python script demonstrates the functionality of a `preprocess` function from a library named `pdd.preprocess`. The script defines a multi-line string variable called `prompt` which serves as a template. This template contains various XML-like tags such as `<shell>`, `<pdd>`, and `<web>`, as well as placeholders enclosed in curly brackets like `{test}` and `{test2}`. The core of the script is the call to the `preprocess` function, passing the `prompt` string and several configuration flags. Specifically, it sets `double_curly_brackets` to `True`, which likely converts single-bracket placeholders to double-bracket ones (e.g., `{test}` to `{{test}}`), but it also provides an `exclude_keys` list to prevent `{test2}` from being modified. The script uses the `rich` library to print debugging information and the final, processed string to the console, effectively showcasing how the `preprocess` function transforms the input template based on the given rules.",2025-09-01T19:28:29.874062+00:00
"context/preprocess_main_example.py","This Python script utilizes the `click` library to create a command-line interface (CLI) for preprocessing text files, referred to as 'prompt files'. The main function, `cli`, is decorated with several options that allow users to customize the preprocessing. Key functionalities include specifying an input file (`--prompt-file`) and an optional output file (`--output`). Users can enable features such as inserting XML delimiters (`--xml`), recursively processing nested prompt files (`--recursive`), and doubling curly brackets within the text (`--double`), while also allowing specific keys to be excluded from this doubling (`--exclude`). A `--verbose` flag is available for more detailed logging. The script calls an external function, `preprocess_main`, to perform the actual processing, passing along the user-defined options. Upon completion, it prints the processed prompt, the total cost, and the model name to the console, and includes basic error handling.",2025-09-01T19:28:37.851235+00:00
"context/process_csv_change_example.py","This Python script, `run_demo.py`, serves as a concise usage example for the `process_csv_change` function from a library named `pdd`. The script begins by setting up a temporary workspace on the disk. Within this workspace, it creates three key files: a Python source file (`factorial.py`) containing a factorial function, a prompt file (`factorial_python.prompt`) with an initial instruction to write a test for the function, and a CSV file (`tasks.csv`) that specifies a modification to be made to the prompt file—specifically, to add an example section. The core of the script is the call to `process_csv_change`. This function is invoked with paths to the created files and several configuration parameters like `strength`, `temperature`, and `budget`, suggesting it uses a language model to perform the modification. Finally, the script prints the results returned by the function, including a success flag, total cost, the model name used, and a JSON object detailing the modified prompt. The example output shows that the prompt file was successfully updated to include a sample test case, as instructed by the CSV.",2025-09-01T19:28:45.040614+00:00
"context/pytest_example.py","This Python script provides a mechanism for programmatically running pytest and capturing detailed results, including failures, errors, warnings, and log output. It defines a `TestResultCollector` class that acts as a pytest plugin. This class uses pytest hooks like `pytest_runtest_logreport` and `pytest_sessionfinish` to count test failures, errors, and warnings during a test run. A key feature of this class is its ability to capture standard output and standard error by redirecting `sys.stdout` and `sys.stderr` to an in-memory `io.StringIO` buffer. The `run_pytest` function orchestrates the process: it initializes the collector, starts log capture, executes `pytest.main` on a specified test file, and then retrieves the captured logs, ensuring standard output streams are reset in a `finally` block. The main execution block (`if __name__ == __main__:`) calls this function and prints the final aggregated counts of failures, errors, warnings, and the complete captured log content to the console, demonstrating a self-contained method for test execution and result aggregation.",2025-09-01T19:28:54.132184+00:00
"context/pytest_output_example.py","This Python script is a comprehensive example file demonstrating the usage of a custom module named `pdd.pytest_output`. The primary function of this module is to run `pytest` on specified test files, capture the output, and structure the results. The script first creates a dummy test file containing tests designed to pass, fail, raise an error, and issue a warning. It then proceeds through several examples showcasing the module's capabilities. These include: running the module as a command-line script, programmatically calling functions like `run_pytest_and_capture_output` to get a results dictionary, and saving these results to a JSON file. The examples also demonstrate robust error handling for various edge cases, such as non-existent files, non-Python files, and files with no tests. An advanced usage section shows how to use the underlying `TestResultCollector` class directly to gain granular control over capturing stdout, stderr, and categorized test outcomes. The script utilizes the `rich` library for enhanced console output to clearly delineate each example.",2025-09-01T19:29:04.257873+00:00
"context/split_example.py","This Python script serves as a demonstration for the `split` function located in the `pdd.split` module. It uses the `rich` library to provide formatted console output. The script's `main` function sets up the necessary environment variables and defines input parameters, including a prompt, a code snippet for a factorial function, and an example of its usage. It also sets `strength` and `temperature` values. The core of the script is the call to the `pdd.split.split` function, which processes these inputs. The function's return values—a sub-prompt, a modified prompt, the model name, and the total cost—are then unpacked and printed to the console in a user-friendly format. The script includes basic error handling and is intended to be executed directly to showcase the `split` function's capabilities.",2025-09-01T19:29:15.856638+00:00
"context/split_main_example.py","This Python script, `example_split_usage.py`, serves as a command-line interface (CLI) wrapper to demonstrate the usage of a `split_main` function imported from `pdd.split_main`. It utilizes the `click` library to create a command named `split-cli`. This command requires three file paths as input: an input prompt, a code file generated from that prompt, and an example code file. It also accepts optional output paths for a 'sub-prompt' and a 'modified prompt'. The script's main purpose is to call the `split_main` function with these file paths and custom settings like 'strength' and 'temperature', which are passed via a context object. After execution, `split_main` returns the resulting prompt content, the total cost of the operation, and the model name used. The script then prints this information to the console, including the content of the generated prompts and their save locations, unless the `--quiet` flag is specified. It also includes a `--force` flag to allow overwriting existing output files and basic error handling.",2025-09-01T19:29:24.335767+00:00
"context/summarize_directory_example.py","This Python script serves as a practical example for using the `summarize_directory` function from the `pdd.summarize_directory` module. The script's `main` function demonstrates a complete workflow. It starts by defining a sample string that mimics an existing CSV file containing outdated file summaries. It then invokes `summarize_directory` to process Python files matching the pattern `context/c*.py`, passing parameters for model strength, temperature, and verbosity. A key feature demonstrated is the ability to provide existing summary data, allowing the function to update summaries intelligently. Upon completion, the script retrieves the newly generated CSV content, the total operational cost, and the name of the AI model used. It prints this information to the console and then saves the CSV output to a file named `output.csv` within an `output` directory, which it creates if necessary. The entire operation is enclosed in a `try...except` block for error handling.",2025-09-01T19:29:33.165958+00:00
"context/sync_animation_example.py","This Python script serves as a comprehensive example for using the `sync_animation` module. Its primary purpose is to demonstrate how to run a terminal-based animation in a separate thread, driven by state changes in a main application. The script sets up several shared, mutable state variables, including references for the current function name, accumulated cost, and file paths. It also uses a `threading.Event` to signal for graceful termination. The core of the demonstration is the `mock_pdd_main_workflow` function, which simulates a main application's process by sequentially updating these shared variables over time, mimicking stages like 'generate', 'test', and 'fix'. The `sync_animation` function is launched in a daemon thread, receiving these shared variables as arguments. This allows the animation to react in real-time to changes made by the main workflow without blocking it. The script concludes by setting the stop event and joining the animation thread, ensuring a clean shutdown. Overall, it illustrates a robust pattern for creating responsive, non-blocking command-line interfaces with dynamic visual feedback.",2025-09-01T19:29:42.030369+00:00
"context/sync_determine_operation_example.py","This Python script is a demonstration for a function named `sync_determine_operation`, which is part of a 'pdd' module. The script's main purpose is to showcase how this function decides on the next required action for a software unit by analyzing its state. It simulates a project environment within an './output' directory, creating and modifying files to represent different development stages. The script executes four distinct scenarios: 1) A new unit with only a prompt file, which should trigger a 'generate' operation. 2) A unit with a run report indicating test failures, triggering a 'fix' operation. 3) A unit where the source code has been manually modified, detected via a hash mismatch with a stored fingerprint, triggering an 'update'. 4) A fully synchronized unit where all files are up-to-date and tests pass, resulting in a 'skip' operation. For each scenario, the script sets up the necessary files (prompts, code, metadata), calls the function, and prints the resulting decision and the reason behind it, effectively illustrating the function's core logic.",2025-09-01T19:29:50.247311+00:00
"context/sync_main_example.py","This Python script serves as a demonstration and test for a function named `sync_main`, which is part of a command-line tool built with the Click library. The primary goal is to show how `sync_main` can be called programmatically in a controlled environment, isolating it from external dependencies like Large Language Model (LLM) APIs. The script first sets up a mock project directory with prompt files for both Python and JavaScript. It then uses Python's `unittest.mock` library to replace two key internal functions: `construct_paths` and `sync_orchestration`. The `mock_construct_paths` function simulates file path discovery, while `mock_sync_orchestration` returns hardcoded success and failure results, mimicking the outcome of code generation and testing for the different languages. By creating a mock `click.Context` object and patching these dependencies, the script executes `sync_main` as if it were run from the command line. Finally, it prints the aggregated results, demonstrating how the function processes multiple languages, calculates total costs, and reports on the overall success of the operation.",2025-09-01T19:30:04.119540+00:00
"context/sync_orchestration_example.py","This Python script serves as a demonstration for a `sync_orchestration` module, likely part of a Prompt-Driven Development (PDD) command-line tool. The script showcases two primary functionalities. First, it sets up a mock project structure by creating directories for prompts, source code, examples, and tests, along with a sample prompt file that asks for a Python function to add two numbers. Then, it simulates running a full PDD sync process by calling the `sync_orchestration` function. This call orchestrates the generation of code, examples, and tests based on the initial prompt, mimicking a command like `pdd sync calculator`. The script prints a JSON summary of this operation, indicating its success or failure. The second functionality demonstrated is viewing the log of a completed sync. It calls the same `sync_orchestration` function but with a flag to retrieve and display the log file generated by the previous run, simulating a command like `pdd sync --log calculator`. The script concludes by printing the JSON results for both the sync operation and the log retrieval, providing a clear example of how to programmatically use the PDD workflow.",2025-09-01T19:30:13.643115+00:00
"context/tiktoken_example.py","This Python code snippet demonstrates the use of the `tiktoken` library to count the number of tokens in a string. It first imports the library and then retrieves a specific encoding model, cl100k_base. This encoding is commonly used for newer generation OpenAI models. The code then uses the `encode` method from the selected encoding object to convert a string variable, `preprocessed_prompt`, into a sequence of tokens. Finally, it calculates the total number of tokens by getting the length of the resulting encoded list and stores this value in the `token_count` variable. This is a fundamental operation for managing context window size and estimating API costs when working with large language models.",2025-09-01T19:30:24.375292+00:00
"context/trace_example.py","This Python script serves as a demonstration for the `trace` function, which is imported from a `pdd.trace` module. The script's `main` function sets up and executes a call to this `trace` function to showcase its usage. It defines several example inputs: a string containing a small Python program, a specific line number within that code to be analyzed, and a string containing a natural language prompt that describes the code's functionality. It also defines parameters like `strength` and `temperature`, suggesting the `trace` function likely interacts with a Large Language Model (LLM). The script calls `trace` with these inputs and then prints the results, which include the corresponding line number in the prompt file, the total cost of the operation, and the name of the model used. The output is formatted for the console using the `rich` library, and the script includes a `try...except` block for basic error handling.",2025-09-01T19:30:30.104040+00:00
"context/trace_main_example.py","This Python script demonstrates the usage of a function named `trace_main` from a module `pdd.trace_main`. The script begins by creating two example files: a prompt file (`calculator.prompt`) containing a natural language request to create a simple calculator function, and a code file (`calculator.py`) containing the Python implementation of that function. It then configures and creates a context object using the `click` library, setting parameters like `quiet`, `force`, `strength`, and `temperature`, which appear to control the behavior of an underlying analysis tool, likely involving a Large Language Model (LLM). The core of the script is a call to `trace_main`, passing the paths to the prompt and code files, and specifying a particular line of code to analyze. The purpose is to trace the code line back to its corresponding instruction in the prompt file. Finally, the script prints the results of this analysis, including the identified prompt line number, the monetary cost of the analysis, and the name of the LLM used, handling any potential errors during the process.",2025-09-01T19:30:40.960526+00:00
"context/track_cost_example.py","This Python script defines a command-line interface (CLI) using the `click` library. The tool, named PDD, is designed for processing prompts and generating outputs while tracking associated costs. The main command group, `cli`, includes a global option `--output-cost` to specify a CSV file for logging usage and cost details. The primary subcommand, `generate`, takes a mandatory `--prompt-file` as input and an optional `--output` file path. If an output path is not provided, the generated content is printed to the console using the `rich` library. The `generate` function is decorated with a custom `@track_cost` decorator, which intercepts the function's return values (output, cost, and model name) to perform the cost logging. The current implementation simulates the generation process by reading the prompt file and returning placeholder data for the output, cost, and model used. The script concludes with an example of how to run the CLI programmatically, demonstrating its intended usage.",2025-09-01T19:30:47.993542+00:00
"context/unfinished_prompt_example.py","This Python script provides a comprehensive example of how to use the `unfinished_prompt` function from a custom `pdd` package. The function's purpose is to analyze a given text prompt using a Large Language Model (LLM) to determine if it is complete or unfinished. The script first details the necessary prerequisites for running the example, including setting up the `PYTHONPATH`, ensuring access to an internal prompt template file, and configuring LLM API keys. The core of the example defines an intentionally incomplete prompt about baking bread and calls `unfinished_prompt` with it, specifying parameters like `strength`, `temperature`, and `verbose`. It then captures the returned tuple, which includes the LLM's reasoning, a boolean completeness flag, the API call cost, and the model used. The results are then printed to the console in a formatted manner using the `rich` library. A second, commented-out example demonstrates a simpler call with a complete prompt and default parameters, illustrating the function's versatility.",2025-09-01T19:30:58.775446+00:00
"context/update_main_example.py","This Python script provides a command-line interface (CLI) example built with the `click` library. Its primary purpose is to demonstrate how to use an external function, `update_main`, which is responsible for updating a text prompt based on modifications to a code file. The script defines a single command, `update`, which accepts several options. These include paths to an original prompt file, a modified code file, and an optional original code file. Alternatively, it can use Git history to find the original code if the `--git` flag is used. Users can also specify an output file for the new prompt and adjust model behavior with `--strength` and `--temperature` parameters. The script gathers these command-line arguments, passes them to the `update_main` function, and then prints the results, such as the updated prompt, the total cost, and the model name, to the console using the `rich` library for formatted output. It serves as a wrapper to make the core prompt-updating functionality accessible and configurable from the command line.",2025-09-01T19:31:08.098062+00:00
"context/update_model_costs_example.py","This Python script, `example_update_model_costs.py`, is a demonstration of the `update_model_costs.py` utility from the `pdd` package. It illustrates how to automatically update a CSV file containing language model data, specifically focusing on populating cost and structured output information.

The script first creates a sample `llm_model.csv` with models like GPT-4o-mini and Claude 3 Haiku, but with empty cost columns. It then showcases two ways to run the updater. The primary method involves importing and calling the `update_model_data` function directly from Python, which modifies the CSV file in-place. The script prints the file's contents before and after the update to show the newly added cost data.

Additionally, a commented-out section demonstrates how to achieve the same result via the command-line interface (CLI). The file's docstring outlines the prerequisites, such as having the `pdd` package installed and API keys configured, and specifies the required CSV schema, which includes columns for provider, model, input/output costs, and other metadata.",2025-09-01T19:31:18.634732+00:00
"context/update_prompt_example.py","This Python script serves as a demonstration for a function named `update_prompt` from the `pdd.update_prompt` module. The script's `main` function initializes a set of example inputs: an original natural language prompt asking for a function to add numbers, a corresponding Python function that performs addition, and a modified version of the function that performs multiplication instead. It then calls `update_prompt` with these inputs, along with LLM parameters like `strength` and `temperature`. The goal is to generate a new prompt that reflects the change in the code's functionality (from addition to multiplication). Finally, the script prints the resulting modified prompt, the total cost associated with the API call, and the name of the model used, while also including basic error handling. The script is designed to be executed directly to showcase how the `update_prompt` function works.",2025-09-01T19:31:28.500668+00:00
"context/xml_tagger_example.py","This Python script serves as an example of how to use the `xml_tagger` function from the `pdd.xml_tagger` library. The script imports the necessary function along with `rprint` from the `rich` library for formatted console output. It defines a sample `raw_prompt` string, a `strength` parameter (0.5), and a `temperature` parameter (0.7), which are likely used to configure a large language model. The main logic is contained within a `try...except` block. It calls the `xml_tagger` function with the defined prompt and parameters. If successful, the function returns the XML-tagged version of the prompt, the total cost of the API call, and the name of the model used. These results are then printed to the console. The `except` block handles and prints any potential errors that occur during the execution of the function, ensuring robust error handling.",2025-09-01T19:31:35.911279+00:00
