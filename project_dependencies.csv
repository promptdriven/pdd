full_path,file_summary,date
"context/DSPy_example.py","This code snippet demonstrates the initial setup and implementation of a DSPy-based application focused on example selection. The code begins with importing necessary DSPy components and configuring OpenAI's GPT-3.5-turbo-instruct model as the language model backend, with a maximum token limit of 250. The core of the implementation is a 'chainofthought' class that inherits from dspy.Module. This class is designed to implement a chain-of-thought reasoning process, with a basic structure that includes initialization and a forward method to process questions. The class uses DSPy's ChainOfThought feature to map questions to answers. However, the code appears to be incomplete, as indicated by empty spaces and an unfinished comment about compilation and optimization. The code represents a basic framework for implementing chain-of-thought reasoning in a DSPy environment, though it needs additional implementation details to be fully functional.",2024-12-07T01:53:08.162193+00:00
"context/anthropic_counter_example.py","This Python code demonstrates how to count tokens in a text string using the Anthropic API. The code begins by importing the Anthropic library and initializing an Anthropic client. It then defines a sample text string and uses the client's count_tokens method to calculate the number of tokens in that text. The result is printed to the console. However, there's an important note in the comments indicating that this token counting method is not accurate for Anthropic models version 3.0 and above. The code is straightforward and serves as a basic example of token counting functionality, which is often useful when working with language models to stay within token limits or estimate API costs.",2024-12-07T01:53:08.238576+00:00
"context/auto_deps_main_example.py","The provided Python script is a command-line tool implemented using the Click library. It defines a command named 'auto-deps' that facilitates dependency analysis and processing of prompt files. The script includes several command-line options, such as '--force' to overwrite files, '--quiet' to suppress output, '--strength' and '--temperature' to configure dependency analysis parameters, and paths for input prompt files, directories, and output files. The main function initializes the Click context with these options and invokes the 'auto_deps_main' function from the 'pdd.auto_deps_main' module. This function processes the specified prompt file, analyzes dependencies, and generates a modified prompt file. The script also calculates and displays the total cost of the operation and the model used. Error handling is implemented to catch exceptions and abort execution if necessary. The script is designed to be executed as a standalone program.",2024-12-13T21:07:14.439152+00:00
"context/auto_include_example.py","The provided Python script demonstrates the usage of the auto_include function from the pdd.auto_include module. The script is designed to process project dependencies and generate unit tests using a language model (LLM) via Langchain. It begins by loading a CSV file named project_dependencies.csv and defining an input prompt that specifies the requirements for generating unit tests. The input prompt includes details about the inputs, outputs, and steps involved in the process, such as preprocessing the prompt, creating a Langchain LCEL template, selecting an LLM model, and running the inputs through the model. The script also includes steps for handling incomplete generations, postprocessing the output, and calculating the total cost of the operation. Key parameters like strength, temperature, and verbosity are defined for the auto_include function. The function is then called with the specified parameters, and the results, including dependencies, CSV output, total cost, and model name, are printed. The script is structured with a main function and is intended to be executed as a standalone program.",2025-01-01T07:17:23.527000+00:00
"context/auto_update_example.py","The file provides an example of how to use the 'auto_update' function from the 'pdd.auto_update' module. It includes a Python script that demonstrates various ways to utilize the function. The 'auto_update' function is designed to check the current installed version of a package, compare it with the latest version (either from PyPI or a specified version), and prompt the user to upgrade if a newer version is available. If the user confirms, the function performs the upgrade using pip. The script showcases basic usage for checking updates for the 'pdd' package, checking updates for a specific package like 'requests,' and comparing against a known version, such as 'pandas' version 2.0.0. The script is structured with a 'main' function that encapsulates these examples, and it is executed when the script is run directly. The file serves as a guide for implementing and understanding the 'auto_update' function's capabilities.",2025-01-01T07:17:26.468121+00:00
"context/autotokenizer_example.py","This Python code defines a function called 'count_tokens' that calculates the number of tokens in a given text using a specified language model's tokenizer. The function uses the Hugging Face transformers library, specifically the AutoTokenizer class. By default, it uses the 'deepseek-ai/deepseek-coder-7b-instruct-v1.5' model, but this can be changed through the model_name parameter. The function works by first loading the appropriate tokenizer, then tokenizing the input text, and finally returning the length of the resulting input_ids. The code includes a simple example that demonstrates how to use the function by counting tokens in the text 'Write a quick sort algorithm in Python.' This utility is particularly useful for working with language models where understanding token count is important for managing context windows and input limitations.",2024-12-07T01:53:08.299472+00:00
"context/bug_main_example.py","The provided script is a Python program that utilizes the Click and Rich libraries to create a context for debugging and testing a Python function. The script sets up parameters for an AI model, such as strength and temperature, and defines file paths for a prompt, code, and a main program. It creates example files: a prompt file describing a factorial function, a Python file implementing the factorial function, and a main program file that imports and tests the function. The script specifies observed and desired outputs for the factorial function and calls a function named `bug_main` from the `pdd.bug_main` module. This function generates a unit test to identify and fix potential bugs in the factorial implementation. The generated unit test is saved to a specified file, and the script prints the unit test, the total cost of the operation, and the AI model used. The script appears to focus on debugging and validating the correctness of the factorial function using AI-assisted tools.",2025-01-01T07:17:29.556391+00:00
"context/bug_to_unit_test_example.py","This Python script demonstrates the usage of a bug_to_unit_test function for automated test generation. The main function serves as the entry point and orchestrates the process of generating unit tests from bug reports. The script reads input from three different files: a prompt file (from 'prompts' directory), a code file (from 'pdd' directory), and a context file (from 'context' directory). It processes two types of outputs (current and desired) that contain debug information and JSON-formatted explanations. The function takes several parameters including the outputs, prompt, code under test, program to run the code, strength (0.9), temperature (0), and language ('Python'). The script uses the Rich library for enhanced console output formatting, displaying the generated unit test, total cost, and model name used. Error handling is implemented to catch and display any exceptions that occur during execution. The code appears to be part of a larger system for automated testing and debugging, possibly using AI/LLM models given the presence of temperature and strength parameters.",2024-12-07T01:53:08.330225+00:00
"context/bug_to_unit_test_failure_example.py","This code snippet contains a Python function named 'add' that takes two parameters 'x' and 'y'. However, there appears to be a logical error in the implementation. Despite the function name suggesting addition, the function actually performs subtraction, returning 'x-y' instead of 'x+y'. This is likely a bug that needs to be fixed to match the intended functionality implied by the function name. The code is very simple, consisting of just two lines: the function definition and the return statement. The indentation suggests this might be part of a larger codebase, but only this single function is shown in the provided content.",2024-12-07T01:53:08.365017+00:00
"context/change_example.py","The provided Python script demonstrates the use of a `change` function from the `pdd.change` module. It includes a `main` function that sets up environment variables, defines input parameters, and calls the `change` function. The script outlines an example scenario where a function to calculate the factorial of a number is modified based on a prompt to take the square root of the factorial output. Parameters such as `strength` and `temperature` are used to control the behavior of the `change` function. The script also handles potential exceptions and prints the modified prompt, total cost, and model name using the `rich.console` module for formatted output. The `main` function is executed when the script is run directly.",2024-12-21T03:10:15.598748+00:00
"context/change_main_example.py","The provided Python script demonstrates the usage of the 'change_main' function from the 'pdd' module for modifying code and prompts in two modes: single-change and CSV batch-change. The script begins by setting up a Click context with parameters such as verbosity, language, and budget. It creates necessary directories and sample input files for both modes. In single-change mode, it modifies a Python function to include error handling for division by zero and displays the modified prompt, total cost, and model used. In CSV batch-change mode, it processes multiple files specified in a CSV file, applying changes like handling overflow errors and optimizing functions for large integers. The script uses the 'rich' library for formatted output and ensures all prerequisites, such as environment variables and dependencies, are met. Outputs for both modes, including modified prompts and costs, are displayed. The script is structured to be executed as a standalone program with a main function.",2025-01-01T07:17:32.186804+00:00
"context/cli_example.py","This Python script demonstrates the usage of the 'generate' command from the PDD CLI module. The main purpose is to generate runnable code from a prompt file. The script defines a main function that sets up the necessary parameters and executes the code generation process. It uses two key file paths: a prompt file ('prompts/get_extension_python.prompt') and an output file ('get_extension.py'). The generation process is configured with specific parameters including a strength of 0.5 and a temperature of 0, with a force flag to bypass user confirmation. The script uses command-line arguments formatted as a list to control the generation process. Error handling is implemented through a try-except block that catches and prints any exceptions that occur during execution. The script is designed to be run directly and assumes that environment variables are set and required packages are installed. The output includes both the generated code saved to the specified file and console output showing the total cost and model name used for generation.",2024-12-07T01:53:08.480560+00:00
"context/click_example.py","This Python script implements a command-line image processing tool using Click and Pillow (PIL) libraries. It follows a Unix-like pipe pattern where multiple image processing commands can be chained together. The script includes various image manipulation functions such as opening, saving, displaying, resizing, cropping, transposing (rotation and flipping), blurring, smoothening, embossing, sharpening, and pasting images. Each command is implemented as a Click command group with specific options and parameters. The script uses decorator patterns (@processor and @generator) to handle the command chain processing. Key features include:

1. Command chaining functionality similar to Unix pipes
2. Support for multiple input images
3. Various image transformation options
4. Error handling for image operations
5. Flexible file naming for output
6. Preservation of image filenames through transformations

The script is well-documented with docstrings and comments, making it clear how to use each command and what it does. Example usage shows commands can be combined like 'imagepipe open -i example.jpg resize -w 128 display' to process images in sequence.",2024-12-07T01:53:08.509553+00:00
"context/cmd_test_main_example.py","The file 'test_cli_example.py' provides an example of integrating the 'cmd_test_main' function into a Click-based command-line interface (CLI) for generating or enhancing unit tests. It includes a detailed explanation of its usage, required and optional arguments, and the expected output. The script defines a CLI with global options such as verbosity, AI generation strength, temperature, force overwrite, and quiet mode. It also includes a 'test' command that accepts arguments like 'prompt_file' (path to a text file), 'code_file' (path to the source code), and optional parameters such as output path, programming language override, coverage report, existing tests, target coverage percentage, and a merge flag. The 'test' command invokes the 'cmd_test_main' function to generate or enhance unit tests, returning the generated test code, total cost, and model name. The script demonstrates how to run the CLI from the command line with examples and provides flexibility for customizing test generation. It uses the Click library for CLI creation and ensures shared context across commands. The script is designed for developers looking to automate or improve their unit testing process using AI-driven tools.",2025-01-01T07:17:35.334047+00:00
"context/code_generator_example.py","The provided Python script demonstrates the usage of a function called 'code_generator' from the 'pdd.code_generator' module. The script's main function reads a prompt from a file ('prompts/generate_test_python.prompt') and uses it to generate Python code with the help of a language model. Key parameters for the 'code_generator' function include the programming language ('python'), model strength (0.5), temperature (0.0), and verbosity (True). The script attempts to generate runnable code, calculate the total cost, and identify the model name. If successful, it prints the generated code, cost, and model name. In case of an error, it catches and prints the exception. The script is designed to be executed as a standalone program with the main function serving as the entry point.",2024-12-21T03:10:19.903389+00:00
"context/code_generator_main_example.py","The provided Python script demonstrates the use of a code generation tool to create a Python function for calculating the factorial of a number. It utilizes the Click library to simulate command-line interface (CLI) parameters and sets up a context object with specific parameters such as 'force', 'quiet', 'strength', and 'temperature'. The script defines a prompt file containing instructions for generating the factorial function, including handling edge cases like negative numbers and zero. This prompt is written to a file named 'example_prompt.prompt'. The script then specifies an output file path ('factorial.py') and calls the 'code_generator_main' function from the 'pdd.code_generator_main' module, passing the context, prompt file, and output file as arguments. The function generates the code, calculates the total cost, and identifies the AI model used. Finally, the script prints the generated code, total cost, model name, and the output file path using the Rich library for formatted output.",2025-01-01T07:17:38.534335+00:00
"context/comment_line_example.py","This file provides documentation and examples for a Python function called `comment_line` that handles code commenting across different programming languages. The function takes two parameters: `code_line` (the line to be commented) and `comment_characters` (the commenting syntax to use). The function supports three commenting styles: single-character comments (like Python's #), paired comments (like HTML's <!-- -->), and line deletion (using 'del'). The documentation includes the function's definition with detailed parameter descriptions and return value information. The file also contains practical examples demonstrating all three use cases: commenting a Python print statement, commenting an HTML tag, and deleting a line of code. Each example is accompanied by its expected output. The function is designed to be flexible and can accommodate different programming language commenting syntaxes by appropriately formatting the input line based on the specified comment characters.",2024-12-07T01:53:08.574892+00:00
"context/conflicts_in_prompts_example.py","The provided Python script demonstrates the use of a function named `conflicts_in_prompts` to detect and resolve conflicts between two example prompts. The script imports necessary modules, including `conflicts_in_prompts` from `pdd.conflicts_in_prompts` and `print` from the `rich` library for formatted output. The main function defines two detailed prompts: one for creating an `auth_helpers.py` module for Firebase authentication and another for designing a `User` class for the PDD Cloud platform. Each prompt includes specific requirements, such as functionality, dependencies, error handling, and coding standards. The script sets parameters like `strength`, `temperature`, and `verbose` for the conflict detection process. It then calls the `conflicts_in_prompts` function with the prompts and parameters, capturing the suggested changes, total cost, and model name. The results are displayed using formatted output, highlighting any detected conflicts and proposed changes. If no conflicts are found, a corresponding message is printed. The script is structured to showcase the functionality of the `conflicts_in_prompts` function while adhering to Python best practices, including type hints, docstrings, and modular design.",2024-12-21T03:10:21.724675+00:00
"context/conflicts_main_example.py","The provided script is a Python program that utilizes the 'click' library and a custom module 'pdd.conflicts_main' to analyze conflicts between two AI-generated prompts. It begins by defining two prompt files ('prompt1_LLM.prompt' and 'prompt2_LLM.prompt') with predefined text for AI assistance. These files are created and written to disk. A mock Click context class ('MockContext') is then defined to simulate a command-line interface context, including parameters like 'force', 'quiet', 'strength', and 'temperature'. The script uses the 'conflicts_main' function from the 'pdd.conflicts_main' module, passing the mock context, the two prompt files, and an output file ('conflicts_output.csv') as arguments. The function returns the number of conflicts, the total cost, and the model name used in the analysis. These results are printed to the console and saved in the specified output file. The script also includes verbose output for detailed logging. Overall, the program is designed to evaluate and report conflicts between AI-generated prompts, providing insights into their compatibility and associated costs.",2024-12-21T03:10:24.366081+00:00
"context/construct_paths_example.py","The provided Python script demonstrates the usage of the `construct_paths` function from the `pdd.construct_paths` module. The script defines a `main` function that sets up input parameters, calls the `construct_paths` function, and handles its output. The `construct_paths` function is invoked multiple times with different configurations of input file paths, command options, and commands such as 'example', 'generate', and 'crash'. The script includes error handling to catch and print exceptions during function execution. It also prints the results of the function calls, including input strings, output file paths, and the language used. The script showcases how to manage file paths, read file contents, and handle command options dynamically. Additionally, it demonstrates the use of parameters like `force` and `quiet` to control file overwriting and output suppression. The script concludes by executing the `main` function when run as the main module.",2025-01-01T07:17:41.621721+00:00
"context/context_generator_example.py","The file is a Python script that utilizes a context generation function from the 'pdd.context_generator' module to generate example code based on specified parameters. It begins by checking if the 'PDD_PATH' environment variable is set, raising an error if it is not. The script defines input parameters, including a code module (a simple 'add' function), a prompt describing the task, the programming language ('python'), and additional parameters like 'strength' and 'temperature' for the context generation process. The 'context_generator' function is then called with these inputs, and it returns the generated example code, the total cost of the operation, and the model name used. The script concludes by printing the generated code, the cost, and the model name using the 'rich' library for formatted output.",2025-01-01T07:17:44.759820+00:00
"context/context_generator_main_example.py","The provided Python script utilizes the Click library to create a command-line interface (CLI) context for executing a function named `context_generator_main` from the `pdd.context_generator_main` module. The script initializes a Click context object with specific parameters and attributes, such as `force`, `quiet`, `verbose`, `strength`, and `temperature`, which control the behavior of the CLI and the AI model. It specifies input and output file paths, including a prompt file, a code file, and an output file where the generated code will be saved. The `context_generator_main` function is then called with the configured context and file paths, and it returns the generated code, the total cost of the operation, and the name of the AI model used. Finally, the script prints the generated code, the total cost, and the model name to the console. The script appears to be part of a larger system for generating Python code based on prompts and AI model configurations.",2025-01-01T07:17:47.378733+00:00
"context/continue_generation_example.py","The provided Python script demonstrates the usage of the `continue_generation` function, which extends text generation using a language model and calculates associated costs. The script begins by loading input data from external files: a formatted input prompt from `context/cli_python_preprocessed.prompt` and an initial LLM output fragment from `context/llm_output_fragment.txt`. It sets parameters for the language model, including `strength` and `temperature`, and then calls the `continue_generation` function with these inputs. The function returns the final generated output, the total cost of the operation, and the model name. The script prints the total cost and model name to the console and writes the final generated output to a file named `context/final_llm_output.py`. Error handling is implemented to catch and report file-related or general exceptions. The script is designed to be executed as a standalone program with the `main` function serving as the entry point.",2024-12-21T03:10:26.974173+00:00
"context/crash_main_example.py","The provided Python script demonstrates the use of the `crash_main` function to iteratively fix errors in a code module and its associated program. The script begins by creating an output directory and generating example files for testing. These include a prompt file describing the task (calculating the factorial of a number), a faulty code module (`factorial.py`) that lacks error handling for negative inputs, a program (`main.py`) that calls the faulty module and crashes, and an error log capturing the crash details.

The script then sets up a mock Click context with parameters for the `crash_main` function, such as model strength, verbosity, and file paths. The `crash_main` function is invoked with iterative fixing enabled, a maximum of three attempts, and a budget of $5.0. It attempts to fix the errors in the code module and program, producing corrected versions (`fixed_factorial.py` and `fixed_main.py`) if successful. The script concludes by printing the results, including whether the fix was successful, the number of attempts, the total cost, and the corrected code and program.

This script serves as an example of automated debugging and code correction using iterative refinement and contextual error analysis.",2025-01-01T07:44:04.293454+00:00
"context/detect_change_example.py","The provided script is a Python program designed to analyze and detect changes in prompt files using a machine learning model. It imports necessary modules, including 'detect_change' from 'pdd.detect_change', and utilizes the 'rich.console' library for formatted console output. The script defines a list of prompt files located in 'context' and 'prompts' directories, which are analyzed for changes based on a given description. The change description specifies the goal of making prompts more compact using a specific file ('context/python_preamble.prompt').

The script sets parameters for the machine learning model, such as 'strength' (determining model intensity) and 'temperature' (controlling randomness in output). It then attempts to detect changes in the prompt files by calling the 'detect_change' function with the specified parameters. If successful, it prints the detected changes, including the prompt name and change instructions, along with the total cost of the operation and the model used. In case of an error, it catches the exception and displays an error message.

Overall, the script automates the process of analyzing and updating prompt files to improve their compactness and efficiency, while providing detailed feedback and cost analysis.",2025-01-01T07:17:53.244881+00:00
"context/detect_change_main_example.py","The provided Python script defines a main function that simulates a command-line interface (CLI) for detecting changes in code using the `detect_change_main` function from the `pdd.detect_change_main` module. It sets up a Click context with default CLI options and model parameters, specifies a list of prompt files, and writes a change description to a designated file. The script ensures the output directory exists and calls the `detect_change_main` function with the context, prompt files, change file, and output file path as arguments. The function returns a list of changes, the total cost, and the model name, which are then printed to the console. If an error occurs during execution, it is caught and displayed. The script is designed to be executed as a standalone program.",2025-01-01T07:17:56.291667+00:00
"context/find_section_example.py","This file provides documentation and examples for using the `find_section` function from the `find_section.py` module. The function is designed to identify and extract code blocks from text content. The documentation includes a complete example that demonstrates how to use the function with a sample input containing multiple code blocks in different programming languages (Python and JavaScript). The file details both the input parameters (`lines`, `start_index`, and `sub_section`) and the output format (a list of tuples containing language, start line, and end line information). The example shows how to process a string containing code blocks by first splitting it into lines and then using the find_section function to locate the code sections. It also includes a practical example of reading from a file ('unrunnable_raw_llm_output.py') and processing its contents. The documentation concludes with sample output showing how the function identifies code blocks and their locations within the text. The function appears to be particularly useful for parsing and processing text that contains embedded code blocks in various programming languages.",2024-12-07T01:53:08.778520+00:00
"context/fix_code_loop_example.py","The provided Python script demonstrates the use of a function called `fix_code_loop` to iteratively debug and fix a code file. The script begins by creating example files: a Python module (`module_to_test.py`) containing a function `calculate_average` that calculates the average of a list of numbers, and a verification script (`verify_code.py`) that tests this function with an erroneous input (a string instead of a list). The script also defines a prompt describing the intended functionality of the code. The `fix_code_loop` function is then invoked with parameters such as the code file, verification program, prompt, and constraints like model strength, temperature, maximum attempts, and budget. The loop attempts to fix the code iteratively, logging errors and outputs, until the code passes the verification or the constraints are exhausted. The results, including success status, number of attempts, cost, and the final code, are printed. The script also includes commented-out code for cleaning up the generated files. This script serves as an example of automated debugging and code correction using a predefined loop mechanism.",2025-01-01T07:17:58.918831+00:00
"context/fix_code_module_errors_example.py","The provided Python script demonstrates a process for identifying and fixing errors in a code module using a function called `fix_code_module_errors`. The script begins with a sample program that contains an error, where a function `calculate_sum` attempts to sum a string instead of a list of numbers, resulting in a `TypeError`. The script also includes the original prompt that generated the code, the erroneous code module, and the error message encountered during execution. The `fix_code_module_errors` function is then called with these inputs, along with parameters such as model strength and temperature, to generate a corrected version of the program and code module. The function returns several outputs, including whether updates are needed, the fixed program and code, the total cost of the API calls, and the name of the model used. The script concludes by printing these results, providing a clear demonstration of how the error was identified and resolved. This script serves as an example of automated debugging and code correction using external tools or APIs.",2025-01-01T07:18:02.055770+00:00
"context/fix_error_loop_example.py","The provided Python script demonstrates the usage of a function called `fix_error_loop` from the `pdd.fix_error_loop` module. The script is designed to automate the process of fixing errors in code by iteratively testing and refining it. It sets up various parameters, including paths to unit test files, code files, prompts, and a verification program. Additionally, it defines parameters such as strength, temperature, maximum attempts, and budget for the error-fixing process. The script reads a prompt from a file, executes the `fix_error_loop` function, and captures the results, including success status, final unit test, final code, total attempts, total cost, and the model used. The results are displayed using the `rich` library for formatted output. If an error occurs during execution, it is caught and displayed. The script is intended to be run as a standalone program, with the main function serving as the entry point.",2025-01-01T07:44:07.238828+00:00
"context/fix_errors_from_unit_tests_example.py","This Python script demonstrates the usage of a function called 'fix_errors_from_unit_tests' which is designed to automatically fix errors in code and unit tests. The main function sets up example inputs including a simple unit test for an addition function, the corresponding code implementation, and various parameters. The unit test deliberately includes an incorrect test case (asserting 0 == 1), which triggers an error. The script uses several parameters including a prompt describing the desired functionality, an error message, a file for logging fixes, and LLM (Language Learning Model) parameters like strength and temperature. The function returns multiple values including updated versions of both the unit test and code, fixed versions of both, the total cost of the operation, and the name of the model used. The results are printed using Rich library's pretty printing functionality. The script is structured as a demonstration tool, showing how the error-fixing functionality can be used in practice.",2024-12-07T01:53:08.893156+00:00
"context/fix_main_example.py","The provided Python script defines a command-line tool using the Click library to fix errors in code and unit tests. The script imports necessary modules, including Click for command-line interface creation and Rich for formatted output. The main function, 'fix_command', is decorated with Click options to accept various file paths (e.g., prompt file, code file, unit test file, error log file) and additional parameters like maximum attempts, budget, and auto-submit flag. The function calls 'fix_main', a utility function, passing all the collected parameters to attempt fixing errors in the provided code and tests. The results of the fixing process, including success status, number of attempts, cost, and model used, are displayed using Rich's formatted print. If the fixing process fails, an error message is shown. The script also includes a '__main__' block that sets up a default Click context and simulates command-line arguments to demonstrate the tool's usage. This script is designed to automate the process of identifying and fixing issues in code and unit tests, with options for iterative fixing and verification.",2025-01-01T07:18:07.394325+00:00
"context/generate_output_paths_example.py","The file is a Python script that demonstrates the usage of a function called `generate_output_paths` from the `pdd.generate_output_paths` module. The script sets up example inputs, including a command, output locations, a basename, a programming language, and a file extension. It then calls the `generate_output_paths` function to generate output paths based on these inputs. The script includes examples for different scenarios: a basic 'generate' command, a case where an environment variable (`PDD_GENERATE_OUTPUT_PATH`) is used to specify the output path, and a 'fix' command that generates multiple output paths for test and code files. The results of these function calls are printed to the console, showcasing the generated output paths for each scenario. The script is structured to demonstrate the flexibility and functionality of the `generate_output_paths` function in handling various input configurations and commands.",2024-12-13T21:07:19.225256+00:00
"context/generate_test_example.py","The file contains a Python script that demonstrates the generation of a unit test for a given function using the `generate_test` function from the `pdd.generate_test` module. The script begins by importing necessary modules, including `os` and `rich` for environment variable handling and formatted output, respectively. It defines a prompt for creating a factorial function, along with the corresponding Python code implementation. Additional parameters such as `strength`, `temperature`, and `language` are specified to guide the test generation process. The script attempts to call the `generate_test` function with these inputs and captures the resulting unit test, total cost, and model name. If successful, it prints the generated unit test, cost, and model details using formatted output. In case of an error, it catches the exception and displays an error message. The script also includes commented-out code for setting an environment variable (`PDD_PATH`) if needed.",2024-12-21T03:10:34.209936+00:00
"context/get_comment_example.py","This file provides documentation and usage examples for the `get_comment` function from the `get_comment.py` module. The function retrieves comment characters for different programming languages. The documentation includes a practical example showing how to import and use the function, with sample calls for languages like Python (#), Java (//), and JavaScript (// or /* */). The function takes a single case-insensitive string parameter 'language' and returns the corresponding comment character(s) as a string. If there's an error (missing environment variable PDD_PATH, unknown language, or other issues), it returns 'del'. The function relies on a CSV file (language_format.csv) that should be located in a directory specified by the PDD_PATH environment variable. This CSV file must contain at least two columns: 'language' and 'comment' that map programming languages to their respective comment characters.",2024-12-07T01:53:08.992494+00:00
"context/get_extension_example.py","The file contains a Python script that imports a function named 'get_extension' from the 'pdd.get_extension' module. The script demonstrates example calls to the 'get_extension' function, which appears to return the file extension or identifier associated with a given programming language or file type. Examples include returning '.py' for 'Python', 'Makefile' for 'Makefile', and '.js' for 'JavaScript'.",2024-12-21T03:10:37.702819+00:00
"context/get_language_example.py","This Python script demonstrates the usage of a 'get_language' function that determines the programming language associated with a file extension. The code consists of a main function that serves as an example implementation. Inside the main function, it defines a sample file extension '.py' and attempts to get the corresponding programming language using the imported get_language function. The code includes error handling through a try-except block to catch any potential exceptions during execution. If a language is successfully identified, it prints the file extension and its associated programming language. If no language is found, it displays a message indicating that no programming language was found for the given extension. The script uses type hints for better code readability and follows Python's standard if __name__ == '__main__' pattern for script execution. The code is well-structured and includes proper documentation through docstrings.",2024-12-07T01:53:09.050133+00:00
"context/git_update_example.py","The provided Python script demonstrates the usage of a function called `git_update` from the `pdd.git_update` module. The script is designed to update a code prompt and save the modified version. It begins by reading an input prompt from a file located in the 'prompts' directory. The script specifies parameters such as the strength and temperature for the language model (LLM) used in the `git_update` function. It then calls the `git_update` function with these parameters, along with the input prompt and the path to the modified code file. If the function executes successfully, it returns the modified prompt, the total cost of the operation, and the model name, which are then printed to the console. The modified prompt is also saved back to the original file. The script includes error handling for `ValueError` and general exceptions, printing appropriate error messages in case of failures. The `main` function is executed when the script is run directly.",2025-01-01T07:18:10.460330+00:00
"context/increase_tests_example.py","This Python file demonstrates the usage of an 'increase_tests' function, which is designed to generate additional unit tests for existing code. The file contains an 'example_usage' function that showcases both basic and advanced implementations of the test generation process. The example uses a simple 'calculate_average' function as the target code, along with an existing basic unit test and a coverage report showing 60% code coverage. The function demonstrates two ways to call 'increase_tests': a basic usage with default parameters and an advanced usage with custom parameters including language specification, strength, temperature, and verbosity settings. The code includes proper error handling using try-except blocks to catch both ValueError for input validation errors and general exceptions. The example is structured to run when the script is executed directly. The code serves as a practical demonstration of how to use the test generation functionality while handling potential errors and displaying relevant output information such as the generated tests, associated costs, and the model used for generation.",2024-12-07T01:53:09.111951+00:00
"context/insert_includes_example.py","This Python file demonstrates the usage of an insert_includes module for processing dependencies. The main function, example_usage(), showcases how to set up and execute dependency processing with error handling. The code initializes a Rich console for formatted output and defines input parameters including a prompt for a Python function that handles CSV files, a directory path for example files, and a CSV filename for dependency information. The insert_includes function is called with specific parameters like strength and temperature to control the output. The code then displays the results including the original prompt, modified prompt with dependencies, CSV output, model name, and total cost. The processed CSV output is saved to a file. Error handling is implemented for common issues like missing files or processing errors. The code uses the Rich library for enhanced console output formatting and the pathlib library for file operations. This appears to be part of a larger system for managing and processing project dependencies with AI model integration.",2024-12-07T22:11:27.202537+00:00
"context/langchain_lcel_example.py","This Python code demonstrates various implementations and uses of Language Models (LLMs) using the LangChain framework. The file includes examples of working with different LLM providers including Google's Gemini, OpenAI, Azure, Fireworks, Anthropic's Claude, Groq, Together.ai, and Ollama. It showcases several key features:

1. Setting up a SQLite cache for LLM responses
2. Implementation of a custom CompletionStatusHandler for tracking completion status and token usage
3. Various prompt templates and chain configurations
4. Different output parsing methods including JSON and Pydantic models
5. Structured output handling with a custom Joke class
6. Examples of fallback configurations and model alternatives
7. Integration with multiple API endpoints and services
8. Usage of both chat and completion models
9. Implementation of prompt templates with variable substitution

The code serves as a comprehensive example of how to integrate and use different LLM providers within the LangChain ecosystem, including proper error handling, response parsing, and output formatting. It also demonstrates modern best practices like using invoke() instead of deprecated run() methods.",2024-12-07T01:53:09.175039+00:00
"context/llm_invoke_example.py","The provided Python script demonstrates the use of the `llm_invoke` function to generate jokes using a language model. It employs the Pydantic library to define a structured output model (`Joke`) with fields for the joke's setup and punchline. The script iterates through different 'strength' values (ranging from 0.0 to 1.0) to test the model's performance and tracks the strength ranges for each model used.

Two examples are showcased:
1. Unstructured output: The script generates a joke based on a given topic (e.g., programmers) and prints the result, cost, and model used.
2. Structured output: The script generates a joke in a predefined JSON format using the `Joke` Pydantic model, ensuring structured data output. The setup and punchline are accessed and printed.

The script also tracks and prints the strength ranges for each model used during the iterations. It concludes by summarizing the strength ranges for all models. The main function serves as the entry point, and the script is designed to be run as a standalone program.",2024-12-13T21:07:21.656593+00:00
"context/llm_selector_example.py","This Python code demonstrates the usage of a language model selector function (llm_selector) through a main function. The code iteratively tests different strength values ranging from 0.5 to 1.1 in increments of 0.05, while maintaining a fixed temperature of 1.0. For each iteration, it calls the llm_selector function, which returns various parameters including the selected LLM model, token counter function, input and output costs per million tokens, and the model name. The code includes error handling for FileNotFoundError and ValueError exceptions. Additionally, it demonstrates the usage of the token counter function with a sample text. The main function is structured to showcase how to interact with different language models based on varying strength parameters, making it useful for testing and comparing different LLM configurations. The code is organized with type hints and proper error handling, following good programming practices.",2024-12-07T01:53:09.245457+00:00
"context/llm_token_counter_example.py","This Python script demonstrates different methods of counting tokens in text using various tokenization libraries. The code includes three main approaches: tiktoken, anthropic, and autotokenizer. The main function contains three try-except blocks, each showcasing a different tokenization method. For tiktoken, it uses the 'o200k_base' encoding. For anthropic, it utilizes the 'claude-3-sonnet-20240229' model. For autotokenizer, it employs the 'deepseek-ai/deepseek-coder-7b-instruct-v1.5' model. Each method processes a sample text string and prints the token count. The script imports necessary libraries including tiktoken, anthropic, and transformers.AutoTokenizer, and uses a custom token counter function 'llm_token_counter' from a module called 'pdd'. The code is structured with error handling to gracefully manage potential issues with each tokenization method. The script is designed to be run as a standalone program, with the main function being called only if the script is the primary program being executed.",2024-12-07T01:53:09.273521+00:00
"context/load_prompt_template_example.py","This Python script demonstrates a simple program that loads and displays a prompt template. The code imports two functions: 'load_prompt_template' from a custom module 'pdd.load_prompt_template' and 'print' from the 'rich' library, which provides rich text formatting capabilities. The main function defines a variable 'prompt_name' set to 'generate_test_LLM' (presumably the name of a prompt template file without its extension), loads the template using the imported function, and then displays it with blue-colored formatting if the template is successfully loaded. The script uses a standard Python idiom with the '__name__ == __main__' check to ensure the main function only runs when the script is executed directly. This appears to be part of a larger system dealing with prompt templates, possibly for working with Large Language Models (LLMs).",2024-12-07T01:53:09.302428+00:00
"context/postprocess_0_example.py","This file provides a comprehensive example of how to use the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The example demonstrates processing output from a language model (LLM) that contains multiple code sections in different programming languages. The main example shows how to extract and process Python code specifically, though the function can work with other languages. The code includes a sample LLM output containing Python and Java code blocks, along with regular text. The example demonstrates how to use the function to process this mixed content, focusing on extracting Python code while commenting out other content. The documentation section clearly outlines the input parameters (`llm_output` and `language`) and explains that the function returns a string with only the largest section of the specified language's code uncommented, while other content is commented out. The file concludes with a note about ensuring proper implementation of supporting functions (`get_comment`, `comment_line`, and `find_section`) for the example to work correctly.",2024-12-07T01:53:09.330350+00:00
"context/postprocess_example.py","This Python script demonstrates the usage of a postprocessing function designed to extract code from mixed text and code output typically generated by Large Language Models (LLMs). The main function contains an example LLM output that includes a factorial function implementation along with usage examples, all embedded within markdown-style code blocks. The script uses the 'postprocess' function from a 'pdd.postprocess' module to extract the pure code from this mixed output. The script also utilizes the 'rich' library for enhanced console output formatting. The postprocessing function takes several parameters including the target programming language (python), model strength (0.7), and temperature (0.2). The script outputs the extracted code, the total cost of the operation, and the model name used. The output is formatted with color coding using the rich console library's formatting capabilities. This appears to be a demonstration or utility script that might be part of a larger toolkit for handling LLM-generated code outputs.",2024-12-07T01:53:09.359769+00:00
"context/preprocess_example.py","The file contains Python code that utilizes a preprocessing module to process various prompts and write the processed outputs to files. It imports the 'preprocess' function from the 'pdd.preprocess' module and uses the 'rich.console.Console' for formatted console output. The script processes multiple prompts, including a multiline string prompt, a file named 'change_LLM.prompt', and another file named 'example_generator_LLM.prompt'. The preprocessing involves options like recursion, handling double curly brackets, and excluding specific keys (e.g., 'test2'). The processed outputs are displayed in the console and written to corresponding files in the 'tests' directory. Additionally, the script includes a mock database example as a prompt, which is also processed and displayed. The code demonstrates the use of file reading, writing, and string manipulation for prompt processing tasks.",2025-01-01T07:18:13.611338+00:00
"context/preprocess_main_example.py","The provided Python script defines a command-line interface (CLI) using the Click library for preprocessing prompt files. The main function, `cli`, includes several options for customization, such as specifying the input prompt file (`--prompt-file`), output file path (`--output`), enabling XML tagging (`--xml`), recursive processing (`--recursive`), doubling curly brackets (`--double`), excluding specific keys from doubling (`--exclude`), and enabling verbose logging (`--verbose`). The script utilizes the `preprocess_main` function from the `pdd.preprocess_main` module to handle the preprocessing logic. It processes the input prompt file based on the provided options, applies transformations like XML tagging or bracket doubling, and outputs the results. The script also includes error handling to catch and display exceptions during execution. Additionally, it provides feedback to the user, such as the processed prompt, total cost, and model name. The CLI is designed to be flexible and user-friendly, allowing users to customize the preprocessing workflow for prompt files efficiently. The script is executed as a standalone program when run directly.",2025-01-01T07:18:16.216462+00:00
"context/process_csv_change_example.py","This Python code demonstrates the usage of a function called 'process_csv_change' from a module named 'pdd.process_csv_change'. The script is designed to process changes in code files based on CSV input. It initializes several parameters including the CSV file path, strength (0.8), temperature (0), code directory path, programming language (Python), file extension (.py), and a maximum budget of $10.00. The main function call is wrapped in a try-except block for error handling. If successful, it prints the total cost of changes, the AI model used, and displays modified prompts for each affected file. The code includes error handling for file not found scenarios and unexpected errors. The function appears to be part of a larger system that automates code modifications, possibly using AI models, while maintaining budget constraints and tracking costs.",2024-12-07T01:53:09.415833+00:00
"context/split_example.py","The provided Python script demonstrates the usage of the `split` function from the `pdd.split` module. It begins by setting up a Rich console for formatted output and defines a `main` function. The script sets an environment variable `PDD_PATH` to the parent directory of the current file. It then prepares input parameters, including a prompt to create a Python function for calculating the factorial of a number, a sample factorial function, example usage code, and configuration values for `strength` and `temperature`. The `split` function is called with these inputs, and its outputs`sub_prompt`, `modified_prompt`, and `total_cost`are printed to the console using Rich's formatted output. The script also includes error handling to catch and display any exceptions that occur during execution. The `main` function is executed when the script is run directly.",2025-01-01T07:18:19.890125+00:00
"context/split_main_example.py","The provided Python script, 'example_split_usage.py,' is a command-line interface (CLI) tool built using the Click library. It demonstrates how to use the 'split_main' function from the 'pdd.split_main' module. The script defines a CLI command named 'split-cli' with several options, including paths for input prompt files, input code files, example code files, and optional output files for sub-prompts and modified prompts. Additional flags allow users to force overwriting files and suppress console output. The 'split-cli' command processes these inputs and calls the 'split_main' function, passing the parameters and some custom settings (e.g., 'strength' and 'temperature'). The function returns a sub-prompt, a modified prompt, and the total cost of the operation, which are optionally displayed unless the 'quiet' flag is set. The script also includes error handling to catch and report exceptions. A top-level CLI group is defined to organize commands, and the 'split-cli' command is added to this group. The script is designed to be executed as a standalone program, providing a structured way to interact with the 'split_main' function for splitting and modifying prompts.",2025-01-01T07:18:22.495182+00:00
"context/summarize_directory_example.py","This Python script demonstrates the usage of the 'summarize_directory' module from the 'pdd' package. The main function showcases how to summarize Python files in a directory while handling existing summaries. It includes an example of existing CSV content and demonstrates calling the summarize_directory function with various parameters including directory path (with wildcard), strength (model capability), temperature (output determinism), verbosity, and existing CSV content. The script processes the files, generates summaries, and outputs the results including the generated CSV content, total cost in USD, and the model used. The results are both printed to console and saved to 'output.csv'. The code includes error handling and is structured as a standalone executable script with a main() function guard.",2024-12-07T01:53:09.471711+00:00
"context/tiktoken_example.py","This code snippet demonstrates the use of the tiktoken library for token counting in text processing. The code first imports the tiktoken library, then creates an encoding object using the 'cl100k_base' encoding scheme (which is commonly used with newer GPT models). Finally, it counts the number of tokens in a variable called 'preprocessed_prompt' by encoding the text and measuring the length of the resulting encoded sequence. The code is particularly useful for applications that need to track token usage when working with language models, as it provides a way to count tokens before sending text to an API. The 'cl100k_base' encoding is specifically mentioned, though the comment indicates other encoding options are available. This is a common pattern when working with OpenAI's GPT models, where understanding token count is important for both cost management and staying within model context limits.",2024-12-07T01:53:09.500027+00:00
"context/trace_example.py","The provided Python script demonstrates the use of the `trace` function from the `pdd.trace` module. It includes a `main` function that sets up an environment, defines example inputs, and calls the `trace` function to analyze a specific line of code within a given code snippet. The script uses the `rich.console` module for formatted console output. Example inputs include a Python code snippet, a prompt file with instructions, and parameters like `strength` and `temperature` for the LLM model. The `trace` function is called with these inputs, and its results, such as the corresponding prompt line, total cost, and model name, are displayed in the console. The script also includes error handling for `FileNotFoundError`, `ValueError`, and general exceptions, providing informative messages for each. The script is designed to be executed as a standalone program, with the `main` function serving as the entry point.",2025-01-01T07:18:25.658558+00:00
"context/trace_main_example.py","The file contains a Python script that utilizes the Click library to create a command-line interface (CLI) for tracing a specific line of code in a Python file. The script includes example content for a prompt file and a code file. The prompt file contains instructions for writing a factorial function with error handling for negative numbers, while the code file implements the factorial function and tests it with an example. The CLI accepts various options, such as specifying the prompt file, code file, line number to trace, output file path, and parameters for controlling the behavior of a language model (LLM), including strength and temperature. The main function, `cli`, calls a `trace_main` function to perform the tracing operation and outputs the results, including the traced line, total cost, and model name, unless the quiet mode is enabled. The script also includes options to overwrite existing output files and suppress console output. The CLI is designed to facilitate debugging and analysis of Python code using LLMs.",2025-01-01T07:18:28.839278+00:00
"context/track_cost_example.py","This Python file implements a Command-Line Interface (CLI) tool called PDD using the Click library. The tool is designed for processing prompts and generating outputs with cost tracking functionality. The code consists of two main components: a base CLI group and a 'generate' command. The CLI accepts an optional '--output-cost' parameter to track costs and save usage details to a CSV file. The 'generate' command requires a prompt file input and has an optional output file parameter. It's decorated with a 'track_cost' decorator for cost monitoring. The command reads a prompt from the input file, processes it (currently using placeholder logic), and either writes the output to a specified file or prints it to the console. The function returns a tuple containing the generated output, the cost of execution (simulated at $0.05 per million tokens), and the model name (set to 'gpt-4'). The file includes proper type hints, documentation, and error handling through Click's built-in functionality. The main execution block demonstrates example usage of the CLI with specific command-line arguments.",2024-12-07T01:53:09.597187+00:00
"context/unfinished_prompt_example.py","This Python file demonstrates the usage of an 'unfinished_prompt' function from a 'pdd' module. The code consists of a main function that analyzes the completeness of a given text prompt using a Language Learning Model (LLM). The example uses a simple fairy tale beginning ('Once upon a time...') as the test prompt. The function takes several parameters including the prompt text, model strength (0.5), and temperature (0.0). The unfinished_prompt function returns four values: reasoning, whether the prompt is finished, the total cost of the analysis, and the model name used. The code includes error handling through a try-except block and is structured to run the main function only when the file is executed directly. This appears to be a demonstration or testing script for a larger prompt analysis system, likely used to determine if a given text prompt is complete or needs additional content.",2024-12-07T01:53:09.626009+00:00
"context/update_main_example.py","The provided Python script demonstrates the usage of a CLI (Command Line Interface) tool built using the Click library. The script defines a command named 'update' that facilitates updating a prompt based on modified code. It accepts various options such as paths to input prompt files, modified code files, and optionally original code files. Users can also specify parameters like 'strength' (to control how strongly the model incorporates changes), 'temperature' (for randomness in text generation), verbosity, and whether to use Git history instead of an input code file. The script integrates with a function named 'update_main' from the 'pdd.update_main' module, which performs the core update operation. The results, including the updated prompt snippet, total cost, and model name, are displayed unless suppressed by the 'quiet' flag. Additional options include overwriting files ('force') and enabling verbose logging. The script also supports grouping commands using Click's 'group' functionality, allowing for future extensibility. Overall, the script serves as an example of how to build a flexible and user-friendly CLI for managing and updating text prompts based on code changes.",2025-01-01T07:18:31.507257+00:00
"context/update_prompt_example.py","The provided Python script demonstrates the usage of the `update_prompt` function from the `pdd.update_prompt` module. The script defines a `main` function that sets up input parameters, including an input prompt, input code, modified code, and parameters like strength and temperature for a language model. It then calls the `update_prompt` function with these parameters. The function is expected to return a modified prompt, the total cost of the operation, and the model name. If successful, the script prints these results; otherwise, it handles exceptions and prints an error message. The script is designed to be executed as a standalone program, with the `main` function being called when the script is run directly.",2024-12-21T03:10:40.913389+00:00
"context/xml_tagger_example.py","This Python script demonstrates the usage of an XML tagging functionality using a custom module 'pdd.xml_tagger'. The code imports the 'xml_tagger' function and the 'rich' library for enhanced console printing. The script takes a raw text prompt ('Write a story about a magical forest') and processes it with two parameters: 'strength' (set to 0.5, representing base model performance) and 'temperature' (set to 0.7, controlling output randomness). The code is wrapped in a try-except block for error handling. When successful, it outputs the XML-tagged version of the prompt, the total cost of processing, and the name of the model used. The output is formatted using rich's printing capabilities with color coding (green for success, red for errors) and bold text formatting. This appears to be a demonstration or example implementation of an AI-powered text processing tool that adds XML markup to input text.",2024-12-07T01:53:09.681130+00:00
