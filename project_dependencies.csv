full_path,file_summary,content_hash
context/cli_example.py,"This Python script serves as a comprehensive usage example and demonstration for the PDD CLI module (`pdd.core.cli`). It illustrates how to programmatically interact with the main CLI entry point and its various global options using the `click.testing.CliRunner`. The script defines several functions, each targeting specific features of the CLI, such as displaying help and version information, listing available configuration contexts, and invoking the tool with global flags like `--strength`, `--temperature`, and `--verbose`. 

Additionally, the file demonstrates advanced features including quiet mode execution, cost tracking setup via CSV output, context overriding, and the generation of core dumps for debugging purposes. It also highlights the custom `PDDCLI` class, which extends `click.Group` to provide enhanced help formatting and centralized error handling. The `main` function orchestrates the execution of these examples, printing the results to the console to verify the functionality of the CLI's argument parsing and option handling mechanisms.",04e5e7b7e841716190a924f327fe35c2e920202e6dd4176f6a0686141dd3a5ae
context/xml_tagger_example.py,"This file demonstrates the usage of the `xml_tagger` function from the `pdd.xml_tagger` module. It begins by importing the necessary function and the `rich` library for formatted printing. The script sets up example parameters, including a raw text prompt (""Write a story about a magical forest""), a `strength` value of 0.5 (indicating the base model tier), and a `temperature` setting of 0.7 for generation variability. Inside a try-except block, it calls `xml_tagger` with these parameters, capturing the returned XML-tagged string, the total cost of the operation, and the name of the model used. Finally, it prints these results to the console using bold and colored formatting to display the success message, the tagged prompt, the cost, and the model name, while handling any potential exceptions by printing an error message in red.",3b4606a0ac11cb728a5aa657d457312c83f2d710f53f4ac94d35675f7e911f60
context/insert_includes_example.py,"This Python script serves as a demonstration of how to use the `insert_includes` function from the `pdd.insert_includes` module. The script defines a main function, `example_usage`, which sets up a scenario for processing project dependencies. It initializes a `rich.console.Console` for formatted terminal output and defines key input parameters, including a sample prompt requesting a Python function for CSV processing, a glob pattern for locating relevant context files, and a target CSV filename for dependency information.

The core logic involves calling `insert_includes` with specific configuration arguments such as `strength` (set to 0.93) and `temperature` (set to 0) to control the processing behavior. The script wraps this call in a try-except block to handle potential errors like `FileNotFoundError` or generic exceptions. Upon successful execution, it prints the original prompt, the modified prompt enriched with dependencies, the resulting CSV output, the model name used, and the total cost to the console using color-coded formatting. Finally, it saves the generated CSV output to a file named `project_dependencies.csv`.",eed5d9f7c51e9c8531a0cbd524f0e4df710136ca4cc1b0b7555258d5786116ea
context/unrunnable_raw_llm_output.py,"The provided text outlines the implementation of a Python function named `context_generator`. This function is designed to automate the creation of usage examples for Python modules. Its workflow involves reading a specified Python source file, preprocessing its content, and constructing a prompt for an AI model (specifically GPT-4) to generate a concise usage example. The code snippet includes error handling for file operations, specifically checking if the input file exists. Although the provided code block is incomplete—cutting off after defining the prompt string and returning `True` without showing the model invocation or file writing steps—the accompanying explanation describes the intended full functionality, including invoking the model and saving the output. The text also provides a usage example demonstrating how to call the function with input and output filenames.",435a9e40a86fa84e438a492350e839e0e1cbcd09c3b420b8dc83aa98bcd2db22
context/code_generator_main_example.py,"This Python script serves as a standalone example and test harness for the `code_generator_main` function from the `pdd` package. It demonstrates how to programmatically invoke the PDD CLI's code generation logic without relying on actual Large Language Model (LLM) API calls. The script achieves this by using `unittest.mock.patch` to replace the internal generator functions (`local_code_generator_func` and `incremental_code_generator_func`) with mock implementations that return predictable, hardcoded strings.

The `main_example` function orchestrates three primary scenarios: full code generation using local execution, incremental code generation (updating existing code based on new prompts), and a forced incremental generation scenario. It also attempts a cloud generation scenario that falls back to local execution due to mocked credentials. The script manages file I/O by creating a temporary output directory, writing prompt files, and verifying the generated output. It utilizes a `MockContext` class to simulate the `click.Context` object usually provided by the CLI framework. Ultimately, this file illustrates the usage patterns, parameter configurations, and fallback behaviors of the PDD code generation tool in a controlled, cost-free environment.",4f81dbeae92ff120fdcdf23040cabc80152384bce754ed84b65cb0ec81a77dee
context/agentic_bug_example.py,"This file serves as an example script demonstrating the usage of the `run_agentic_bug` function for investigating bugs based on GitHub issues. The script sets up the environment by adding the project root to the system path and imports the necessary module from `pdd.agentic_bug`. To avoid making actual API calls or executing complex logic during the demonstration, it employs `unittest.mock.patch` to mock the `run_agentic_bug_orchestrator`. The mock is configured to simulate a successful eight-step workflow, returning a predefined success status, completion message, cost, model provider, and a list of changed files. The `main` function executes the `run_agentic_bug` function with a dummy GitHub issue URL and verbose settings, then prints a structured summary of the results, including success status, model used, cost, changed files, and the final message.",96e35df4e8425a9173ff6a30281892c2bd8b81361cff72b732e488172ddf52f5
context/simple_math.py,"The provided file content is extremely minimal, consisting of a single line of text. This line is a comment indicating that the file has been intentionally left blank, likely as a placeholder for a process involving 'pdd generate'. There is no actual code, data, or narrative content to summarize beyond this placeholder notice. The file appears to be a template or a temporary stub used in a software development or documentation generation workflow, waiting to be populated by an automated tool.",23798297717fdc449fb01b557103f0e3b7aa46b535e91953cbca5fcf2aae0ecb
context/sync_orchestration_example.py,"This Python script serves as a demonstration and usage example for the `pdd` (Prompt-Driven Development) library's synchronization orchestration module. It primarily illustrates how to programmatically invoke the `sync_orchestration` function to manage the lifecycle of a project's code generation process.

The script begins by defining a helper function, `setup_example_project`, which creates a mock directory structure (prompts, src, examples, tests) and initializes a dummy prompt file for a calculator application. In the main execution block, the script first sets up this environment and then performs two key operations. First, it runs a full synchronization process for a ""calculator"" project, simulating the generation of code, examples, and tests based on the prompt, while enforcing a budget. Second, it demonstrates a ""dry-run"" execution, which analyzes the current state of the project and reports what actions would be taken without actually modifying any files. The output of these operations is printed as formatted JSON to show the resulting status and logs.",cc1313a948946026d418c85a07e5bf0eedeb5915c44d630440e489979755e1f7
context/python_env_detector_example.py,"This file is a Python script designed to demonstrate the functionality of the `pdd.python_env_detector` module. It serves as an example usage guide, showcasing how to import and utilize key functions for inspecting the current Python runtime environment. The script defines a `main` function that sequentially calls four primary functions from the imported module: `detect_host_python_executable` to find the host Python path, `is_in_virtual_environment` to check for active virtual environments, `get_environment_type` to classify the environment, and `get_environment_info` to retrieve a detailed dictionary of environment metadata. The output of each function call is printed to the console, providing a clear, step-by-step verification of the module's capabilities. The script concludes with a standard `if __name__ == ""__main__"":` block to ensure the `main` function runs when the script is executed directly.",65619b3e8bad0509f1ccf54d81b5c3760f423b6fc1925c932fc7968da78a172e
context/agentic_architecture_orchestrator_example.py,"This Python script serves as an example and simulation for the `agentic_architecture_orchestrator` module. It demonstrates how to invoke the `run_agentic_architecture_orchestrator` function to generate a software architecture from a simulated GitHub issue without making actual calls to Large Language Models (LLMs). 

The script sets up a mock environment by patching internal dependencies such as `run_agentic_task` and `load_prompt_template`. It defines mock functions that simulate the outputs of an 8-step architectural generation workflow, including PRD analysis, module boundary identification, research, and dependency graph creation. 

In the `main` function, the script creates a temporary directory and defines dummy issue data representing a request for an e-commerce platform. It then executes the orchestrator within a patched context, printing the simulation results—including success status, total simulated cost, and output files—to the console. This allows developers to verify the orchestration logic and workflow state management without incurring API costs or requiring a live GitHub repository.",baa4e221c6f4d96766479b34fa6e5db52b51941cdb3304beea7e4f797fe76acc
context/update_model_costs_example.py,"This Python script, `example_update_model_costs.py`, serves as an end-to-end demonstration for using the `update_model_costs` helper utility within the `pdd` package. It illustrates how to automatically update an `llm_model.csv` file with current pricing and capability data (such as structured output support) for various Large Language Models (LLMs). The script begins by programmatically creating a minimal sample CSV file containing basic model identifiers for OpenAI's GPT-4o-mini and Anthropic's Claude-3-haiku, leaving cost and feature fields blank. It then imports and executes the `update_model_data` function from the `pdd` library, which modifies the CSV file in place by fetching and filling in the missing data using LiteLLM. Finally, the script prints the contents of the CSV before and after the update to verify the changes. It also includes commented-out code demonstrating how to achieve the same result using the command-line interface (CLI) via `subprocess`.",90c9e0bab64d7495f55ba5876f8beab75ef40037508b5e9800ebc6afb5b55f5e
context/autotokenizer_example.py,"This file contains a Python script designed to count the number of tokens in a given text string using the Hugging Face `transformers` library. It defines a function named `count_tokens` which accepts a text string and an optional model name (defaulting to ""deepseek-ai/deepseek-coder-7b-instruct-v1.5""). Inside the function, an `AutoTokenizer` is instantiated for the specified model with remote code trust enabled. The text is then tokenized, and the length of the resulting `input_ids` list is returned as the token count. The script concludes with an example usage block where the function is called with a sample string (""Write a quick sort algorithm in Python.""), and the resulting token count is printed to the console.",5e37a08f8ffede0d105cf1d5096f895f0a1a87663010e7b29c942d160c24883d
context/bug_to_unit_test_failure_example.py,"The provided file content consists of a short Python function definition named 'add'. This function takes two arguments, 'x' and 'y'. However, despite the function name suggesting an addition operation, the implementation actually performs subtraction, returning the result of 'x - y'. This indicates a logical error or a deliberate misnaming within the code, as the behavior contradicts the semantic expectation set by the function's identifier.",278742918a1e8806c703370429db53a4c178825af16be11405374a161e1d0198
context/sync_order_example.py,"This Python script serves as a demonstration and usage example for the `pdd.sync_order` module. It illustrates the workflow for managing dependencies between prompt files, specifically showing how to synchronize them in the correct order. The script begins by creating a temporary directory structure populated with mock prompt files (`base_utils`, `logger`, `database`, and `api`) that contain explicit `<include>` dependencies on one another. 

The main execution flow proceeds through five key steps: building a dependency graph from the mock files, performing a topological sort to determine the correct execution sequence, and identifying which modules are affected by changes to a base dependency (simulating a modification to `base_utils`). Finally, it generates a shell script (`sync_all.sh`) designed to execute synchronization commands in the calculated order. The script utilizes the `rich` library for formatted console output and includes cleanup logic to remove temporary directories after execution.",d33f05562f47811283ef8ebb0a799d3bbde1580c31e5a23f8292562faad5301e
context/anthropic_thinking_test.py,"This Python script demonstrates how to configure and execute a call to the Anthropic API using the `litellm` library, specifically testing the ""thinking"" or reasoning capabilities of newer Claude models. The script begins by setting up the project environment, locating the root directory, and loading API keys from a `.env` file, with error handling for missing keys. It defines specific model identifiers, such as `claude-3-7-sonnet-20250219`, and sets parameters for the API call, including a user prompt about recursion and a specific configuration for the `thinking` parameter (enabled with a token budget). The core functionality involves executing `litellm.completion` with these parameters. The script includes robust error handling to catch `UnsupportedParamsError` or general exceptions, which is useful for debugging experimental features. Finally, it attempts to parse and display the response, specifically looking for and printing any ""reasoning content"" returned by the model, verifying whether the thinking feature functioned as expected.",a442745e51983bb051292e5f260efdadc57d046504e50a7194af8e1872ce0c84
context/trace_example.py,"This Python script serves as a demonstration and entry point for using the `trace` function from the `pdd.trace` module. It imports necessary components, including `Console` from the `rich` library for enhanced terminal output and `DEFAULT_STRENGTH` from the `pdd` package. The core of the script is the `main` function, which sets up a simulated environment for tracing code execution back to a prompt. It defines example inputs, including a multi-line string representing a Python code snippet (`code_file`), a specific line number to trace (`code_line`), and a corresponding prompt file content (`prompt_file`). The script then attempts to call the `trace` function with these inputs, along with specified strength and temperature parameters for an underlying LLM model. It handles the output by printing the resulting prompt line number, the total cost of the operation, and the name of the model used, formatted with `rich` console styles. Additionally, the script includes robust error handling to catch and display `FileNotFoundError`, `ValueError`, and generic exceptions, ensuring that any issues during the tracing process are reported clearly to the user.",de4d5d1ca347ea7ef9acd11dc80e86188917f61355e68db9c397ca6fd5108f99
context/sync_animation_example.py,"This Python script serves as a demonstration and test harness for the `sync_animation` module, which visualizes the workflow of a hypothetical application (PDD). It illustrates how to run a terminal-based animation in a separate thread while a main application logic runs concurrently. The script sets up shared state variables—such as the current function name, accumulated cost, file paths, and UI colors—using mutable lists to allow real-time updates between threads. A `mock_pdd_main_workflow` function simulates various stages of a process (e.g., ""generate"", ""test"", ""fix""), updating these shared variables and sleeping to mimic processing time. The script initializes the animation thread with these shared references and a stop event, runs the mock workflow, and finally handles thread cleanup and exit signaling. It effectively shows how to synchronize a background UI visualization with a foreground process.",5b6a8af940fe44241889a279c1a8d3c978fe0c6ec3afc648510d8e33ffa7b18c
context/pytest_isolation_example.py,"This file serves as a reference guide for implementing proper test isolation in Python using `pytest`. It provides concrete code examples of correct patterns to prevent test pollution, ensuring that tests remain independent and do not interfere with one another. Key patterns demonstrated include using `monkeypatch` for safe environment variable handling and function mocking, utilizing `tmp_path` for isolated file system operations, and creating fixtures with guaranteed cleanup steps. The file also covers advanced techniques such as module-level mocking for import-time dependencies, safe restoration of `sys.stdout`/`sys.stderr` streams, and the correct usage of context managers for exception testing. Additionally, it explicitly warns against common anti-patterns, such as failing to stop patchers or leaving module-level mocks active, which are frequent causes of test suite instability.",c5b726f330c9d105978d51affffd38849f9685cde739816beded2bd34a1aba39
context/get_comment_example.py,"The provided file offers documentation and usage examples for the `get_comment` function, which is part of the `pdd.get_comment` module. This utility is designed to retrieve the specific comment syntax characters for various programming languages. The example code demonstrates how to import the function and call it with arguments like 'Python', 'Java', and 'JavaScript', returning their respective comment markers (e.g., '#' or '//'). It also shows that querying an unknown language returns 'del'. The documentation details the input parameter as a case-insensitive string representing the language name. The output is defined as the comment character string or 'del' if errors occur, such as the language being missing or the `PDD_PATH` environment variable not being set. Finally, the notes emphasize the necessity of setting the `PDD_PATH` environment variable to point to a directory containing a `data/language_format.csv` file, which serves as the data source mapping languages to their comment styles.",3293b4631db7c783d87745abf36a79996d5c182b74e8bee885a2e522550db851
context/job_manager_example.py,"This Python file implements an asynchronous job queue manager designed for a PDD (presumably a specific domain application) server. It provides a robust framework for submitting, executing, and tracking background tasks using Python's `asyncio` library. Key components include a `Job` dataclass that models task attributes (ID, command, status, result, cost, timestamps) and a `JobManager` class that handles concurrency via semaphores, allowing for limits on simultaneous executions. The manager supports job submission, status retrieval, cancellation, and cleanup of old records. Additionally, the file includes a `JobCallbacks` class to facilitate event-driven integration (e.g., for WebSocket streaming) by triggering hooks on job start, output, progress, and completion. The script concludes with a `main` function demonstrating usage by submitting simulated ""sync"" and ""generate"" commands and monitoring their execution states.",e421698fdf16c5dfebf97eb3f2cb7210878efa1869804f871100489e35102543
context/conflicts_in_prompts_example.py,"This Python script serves as a demonstration and entry point for a utility designed to detect conflicts between two different prompts. The script defines a `main` function that sets up two detailed example prompts: one for creating an authentication helper module (`auth_helpers.py`) using Firebase, and another for defining a `User` data model class (`user.py`) with specific fields and methods. 

The script configures parameters for the conflict detection process, including a `strength` setting of 0.89, a `temperature` of 0, and a verbose flag. It then calls the imported function `conflicts_in_prompts` with these inputs. Finally, the script uses the `rich` library to print the results of the analysis to the console, displaying the model used, the total cost of the operation, and any suggested changes or conflict resolutions if issues are detected between the two provided prompts.",d286f38ca6e43216d62197230f6c53ec665734db782d965e5448f8e9f8bbdfb2
context/agentic_fix_example.py,"This Python script serves as a demonstration and test harness for an ""agentic fix"" workflow, likely part of a larger system named `pdd`. The script illustrates how to automatically repair buggy code using Large Language Model (LLM) agents. It begins by defining a scenario with intentionally incorrect code (a calculator `add` function that subtracts instead), a corresponding unit test that fails, and a prompt describing the issue.

The core functionality is encapsulated in the `main` function, which orchestrates the workflow. It first checks for available LLM agents (such as Claude, Gemini, or OpenAI) via `check_prerequisites`. Then, it creates a temporary directory to simulate a project environment. Inside this sandbox, it writes the buggy code, test files, and prompt, and generates an initial error log by running `pytest`.

The script then calls `run_agentic_fix`, passing the paths to these files. This function invokes the underlying agentic logic to analyze the error and attempt a repair. Finally, the script reports the outcome—success or failure, the model used, estimated cost, and changed files—and prints the corrected code to verify the fix before cleaning up the temporary directory.",01ddb1127fbb8bbbdc6774635abace80cef43ccacb8aa525a8546da1f0654f5d
context/fastapi_example.py,"This Python file provides a comprehensive example of a FastAPI application designed for a ""PDD Server."" It implements a modular architecture using the app factory pattern (`create_app`) and manages application state via dependency injection. The code defines several Pydantic models (`ServerStatus`, `FileMetadata`, `FileContent`, `CommandRequest`) to structure request and response data strictly. 

The application includes three main routers: a status router for server health and version information, a files router for safely reading file content and metadata (incorporating security checks against path traversal), and a commands router that mocks job execution and status tracking. It also configures CORS middleware to allow cross-origin requests from local frontend development servers. Finally, the script includes a `main` function to demonstrate how to bootstrap and run the application using the Uvicorn ASGI server.",4b3d64175b8f46b48a4387e9cf98ed4024fd74a57413bd2f8a55c5d9c3dd0108
context/agentic_e2e_fix_orchestrator_example.py,"This Python script provides an example usage and simulation of the `agentic_e2e_fix_orchestrator` module. It demonstrates how to invoke the `run_agentic_e2e_fix_orchestrator` function by mocking internal dependencies like `run_agentic_task` and `load_prompt_template`. The script sets up a scenario where end-to-end tests fail due to a bug in a payment processing module. It defines mock functions to simulate the outputs of an LLM agent across a 9-step workflow, including unit test execution, root cause analysis, and code fixes. The `main` function initializes dummy issue data, patches the dependencies, runs the orchestrator, and prints the simulation results, including success status, final messages, costs, and changed files. This allows developers to test the orchestrator's logic without making actual API calls.",e5e9f717c1241b192b4594ad98233fd79f17af654f02e49cf80de7a8017e5161
context/increase_tests_example.py,"This file provides a usage example for the `increase_tests` function from the `pdd` library. It defines a function named `example_usage` that demonstrates how to programmatically generate additional unit tests for existing code. The script sets up mock data, including a simple Python function for calculating averages, an existing basic unit test, a coverage report string, and the original prompt used to generate the code. It then calls `increase_tests` twice: first with basic parameters to show default behavior, and second with advanced parameters like custom temperature, language specification, and verbose mode enabled. The script includes error handling to catch `ValueError` for input validation issues and general exceptions, printing the generated tests, cost, and model information to the console. Finally, it executes the `example_usage` function if the script is run as the main module.",f53be429e9a9e6a523c770ed6d0d4498e2201b22d5a8e4ef28ac64e8ff172c91
context/test.prompt,"This document outlines critical guidelines and best practices for writing isolated, non-polluting tests using Pytest, specifically within a project structure involving a `pdd` module and a `tests` directory. It emphasizes that generated tests must reside in `tests`, reference absolute paths, and output to an `output` directory without overwriting existing data files in `pdd/data`. 

The core of the document details twelve strict rules to prevent test pollution. Key mandates include using `monkeypatch` for environment variables instead of `os.environ`, employing context managers or `monkeypatch` for mocking external dependencies, and ensuring all fixtures use `yield` for proper cleanup. It strictly warns against direct manipulation of `sys.modules` or `sys.stdout/stderr` without immediate restoration mechanisms. 

Furthermore, the guide advises using the `tmp_path` fixture for file operations and warns against dangerous patterns like starting patchers at the module level without stopping them. It distinguishes between top-level and deferred imports, explaining how to patch bound names correctly. Finally, it highlights the necessity of in-place restoration for mutable containers (like dictionaries or lists) to ensure that shared references across modules are correctly reset, thereby preventing flaky tests and state leakage.",3198b780ac51f45c121fc1171f85862a4437ab1450eed39829b5c5b78d4a4828
context/sync_main_example.py,"This Python script serves as a demonstration and test harness for the `sync_main` function within the `pdd` (Project Driven Development) framework. It illustrates how to programmatically invoke the `sync_main` command, which is typically called via the Click CLI framework, in a controlled environment. 

The script defines a `setup_mock_project` function that creates a temporary directory structure containing prompt files for multiple languages (Python and JavaScript) to simulate a real project context. The `main` function orchestrates the demonstration by initializing a mock `click.Context` object with default configuration parameters. It then uses `unittest.mock.patch` to replace internal dependencies—specifically `construct_paths` and `sync_orchestration`—with mock implementations. These mocks simulate file path resolution and the outcomes of code generation/testing processes (success for Python, failure for JavaScript) without making actual LLM calls. Finally, the script executes `sync_main` and prints the aggregated results, including success status, total cost, and detailed JSON output, to the console using the `rich` library for formatting.",3eb17c0e79c0402489782f7e758d7063dfbd820428c5e5b695415290ebf2a7f8
context/command_logging.prompt,"This document outlines the guidelines for operation logging within the PDD CLI system. It mandates the use of the `@log_operation` decorator from `pdd.operation_log` for any commands that modify PDD state, ensuring unified tracking for both manual and sync-initiated actions. The guide specifies the correct decorator stacking order, noting that `@log_operation` should be applied before `@track_cost` because decorators execute bottom-up. It also details key parameters for the decorator: `clears_run_report` for operations invalidating test results, `updates_fingerprint` for code or example changes, and `updates_run_report` for operations producing test results. Finally, it explains that the decorator attempts to infer module identity (basename and language) from the `prompt_file` argument, silently skipping logging if this identity cannot be determined.",8f0a0935821fa5fe835fb105251a72eb4198faaf4bcd1cb5fbda5f578b13d8e6
context/preprocess_example.py,"This Python script demonstrates the usage of a `preprocess` function from the `pdd.preprocess` module. It begins by importing necessary libraries, including `rich.console` for formatted output. A multi-line string variable named `prompt` is defined, containing XML-like tags (e.g., `<shell>`, `<pdd>`, `<web>`) and placeholders like `{test}` and `{test2}`. The script sets configuration variables: `recursive` is set to `False`, `double_curly_brackets` is set to `True`, and `exclude_keys` is defined as a list containing ""test2"". It prints debug information about the excluded keys using the rich console. Finally, the script calls the `preprocess` function with the defined prompt and configuration settings, and then prints the resulting processed prompt to the console in bold white text. The code essentially serves as a test case or example for how the preprocessing logic handles specific tags, placeholders, and exclusion rules.",19151ac1cae6b41c190a9f6dd48b9f9c06ea8e9f7a87bf751909711fd6391298
context/render_mermaid_example.py,"This Python script serves as a demonstration and usage guide for the `render_mermaid` module, specifically illustrating how to convert `architecture.json` files into interactive HTML Mermaid diagrams. It defines a sample architecture structure containing backend (FastAPI, SQLAlchemy) and frontend (React) components to simulate real-world data. The script includes four key example functions: `example_basic_usage` which outlines command-line execution; `example_programmatic_usage` which generates a diagram from the sample data using imported functions; `example_with_real_architecture_file` which attempts to locate and process an existing JSON file on disk; and `example_customization` which highlights features like automatic categorization and color-coding. A `main` function orchestrates the execution of these examples, providing a comprehensive overview of the tool's capabilities for visualizing software architecture.",193a57bc4191595f252881c016ba7168c27c27c01ff192128d98cf76ce5f842b
context/construct_paths_example.py,"This file, `demo_construct_paths.py`, serves as a concise, end-to-end demonstration of the `construct_paths` function from the `pdd` library. It illustrates how the library processes input arguments typically supplied by a Command Line Interface (CLI). The script begins by creating a temporary prompt file named `Makefile_makefile.prompt` containing a simple task description to simulate a real-world input scenario. It then sets up the necessary arguments—such as input file paths, force flags, quiet mode settings, and the specific command type (in this case, ""generate"")—to mimic a CLI invocation. The core of the script calls `construct_paths` with these parameters and captures the returned values: the resolved configuration, the content of the input strings, the calculated output file paths, and the detected programming language. Finally, the script prints these returned structures to the console to demonstrate the function's output format and cleans up by deleting the temporary prompt file, ensuring no artifacts are left behind.",9fb88745a2a0e2834f9da30c2128a122fef4d5a74212ceacc5f9c3d9ddb9a8ca
context/update_prompt_example.py,"This file contains a Python script designed to demonstrate the functionality of the `update_prompt` function from the `pdd` package. The script defines a `main` function that sets up example inputs, including an initial prompt (""Please add two numbers and return the sum""), the corresponding original code, and a modified version of that code (changing addition to multiplication). It then calls `update_prompt` with these parameters, along with a default strength and temperature setting. The script includes error handling to catch exceptions during execution and prints the resulting modified prompt, the total cost of the operation, and the model name used if the update is successful. It serves as a usage example for developers integrating the prompt update utility.",56b9d8c7aa823c2996a3dc5741a47844e223bfd088366d74e48b6a099b5a11c4
context/unfinished_prompt.txt,"This file contains a Python implementation for a command-line interface (CLI) tool called PDD (Prompt-Driven Development). Built using the `click` and `rich` libraries, the script establishes a framework for AI-assisted coding workflows. It defines a main command group `cli` that accepts global configuration options such as force overwrite, AI model strength, temperature settings, and verbosity levels. Additionally, it includes functionality for tracking API costs via a CSV log. The script sets up a `generate` command, which takes a prompt file as input and uses imported helper functions (like `construct_paths` and `code_generator`) to produce runnable code, demonstrating how the tool orchestrates file handling and AI generation tasks.",620cc099421924160d5ee9f62467dfa37877e969ec536d1df0ba91cfc8f45682
context/agentic_crash_example.py,"This Python script serves as a test harness or demonstration for the `pdd.agentic_crash` module, specifically the `run_agentic_crash` function. It sets up a simulated crash scenario by creating dummy files: a specification prompt (`factorial_spec.md`), a buggy implementation (`math_lib.py` raising `NotImplementedError`), a runner script (`run_factorial.py`), and a crash log. The script then mocks external dependencies—such as the agentic task runner and subprocess execution—to simulate an AI agent successfully fixing the bug without incurring real API costs. It uses `unittest.mock.patch` to intercept calls, providing a mock response that claims success and a side effect that actually overwrites the buggy code with a correct factorial implementation. Finally, it executes `run_agentic_crash` with these mocks in place and prints a summary of the results, including success status, cost, changed files, and the content of the fixed file.",4a79b152db522063ccf099eca0419b419a656f168839d197f85781ae4fe1b5f6
context/unfinished_prompt_example.py,"This Python script serves as a usage example for the `unfinished_prompt` function from the `pdd` package. It demonstrates how to programmatically analyze a text prompt to determine if it is semantically complete or unfinished using an LLM. The script outlines necessary pre-requisites, such as ensuring the `pdd` package is in the Python path and configuring API keys for LLM access. It then walks through a concrete example: defining an intentionally incomplete string about baking sourdough bread, calling the `unfinished_prompt` function with specific parameters (strength, temperature, and verbosity), and using the `rich` library to print the structured results. The output includes the LLM's reasoning, a boolean completion flag, the cost of the operation, and the model name used. Additionally, commented-out code is provided to show how to invoke the function with default settings.",5d4d74064e5b845391f035fb8631903ec2f39a54a90713f9c13dc24d2eb73c4f
context/llm_invoke_example.py,"This Python script serves as a demonstration suite for the `llm_invoke` function from the `pdd` package, showcasing various ways to interact with Large Language Models (LLMs). It utilizes the `rich` library for formatted console output and `pydantic` for data validation. The script defines four distinct examples: first, a simple text generation task explaining a concept to a specific audience using a base-level model; second, a structured output example that enforces a Pydantic schema (`MovieReview`) for generating fictional movie reviews with a higher-performance model; third, a batch processing example that queries capital cities for multiple countries simultaneously using a cost-effective model; and fourth, a reasoning example that solves a riddle while requesting explicit ""thinking time"" from high-capability models. The main execution block runs these examples sequentially, handling potential errors and reminding users to configure necessary API keys in their environment.",88e1071cf088837c1adb6738f0a1f8e5134efd4a44370758dd19d0719475ac48
context/get_extension_example.py,"The provided file content is a Python script that demonstrates the usage of a function named `get_extension`. It begins by importing this function from a module located at `pdd.get_extension`. Following the import statement, the script includes three example calls to the `get_extension` function, printing the results to the console. These examples illustrate how the function handles different input strings representing programming languages or file types. Specifically, it shows that passing ""Python"" returns the file extension "".py"", passing ""Makefile"" returns the filename ""Makefile"" itself (indicating it likely handles specific filenames that lack standard extensions), and passing ""JavaScript"" returns the extension "".js"". The code serves as a simple usage demonstration or test case for the imported utility function.",def9c34c1416259a0a031a996b96aeff5e73a7012adff6af1cc557e74ba97cc8
context/fix_errors_from_unit_tests_example.py,"This file serves as a demonstration script for the `fix_errors_from_unit_tests` function, which is imported from the `pdd` package. The script defines a `main` function that sets up a mock scenario involving a buggy unit test, a piece of Python code with a deprecated NumPy function, a prompt, and a simulated error log containing an `AssertionError` and a `DeprecationWarning`. It then calls `fix_errors_from_unit_tests` with these inputs, along with parameters for LLM strength and temperature. Finally, the script uses the `rich` library to pretty-print the results returned by the function, including the updated unit test, fixed code, analysis results, total cost, and the model name used.",df6c93b3ae585af13827cf3edb398e7c0da75648b34ff0477044b659df85a117
context/cloud_example.py,"This Python script serves as a demonstration and usage example for the `pdd.core.cloud` module within the PDD CLI project. It illustrates how to interact with the centralized cloud configuration system. The script includes functions to demonstrate four key capabilities: retrieving base and specific endpoint URLs (including handling environment variable overrides for local testing), checking if cloud features are enabled based on the presence of API keys, simulating authentication flows (both environment variable injection and mocked device flow), and listing all available cloud endpoints. It uses `unittest.mock` to simulate various environment states and the `rich` library to provide formatted console output.",92418beb7a1e7f25682bf2abce5b19013b95e7eed95256c930b83c1dd6dbfb0a
context/create_gcp_credential.py,"This Python script demonstrates the process of retrieving and decoding a Google Cloud Platform (GCP) service account key stored within an Azure Key Vault. The code outlines the necessary steps to fetch a specific secret named ""GCP-VERTEXAI-SERVICE-ACC,"" which is expected to be a base64-encoded JSON string representing the service account credentials. It includes logic to decode this base64 string back into a usable JSON object, which can then be utilized to authenticate GCP services, such as Vertex AI. The file contains placeholder classes and commented-out sections for the `AzureKeyVaultService`, suggesting it serves as a template or example snippet rather than a fully functional standalone module. It also includes error handling for JSON decoding and general exceptions.",22babc43e3ee7f5bdef36d1e4830b4ad65e4e3658f874f6914f88f6b7e0df2e0
context/cli_python_preprocessed.prompt,"The provided file describes ""pdd"" (Prompt-Driven Development), a Python command-line interface (CLI) tool designed to streamline software development using AI. PDD leverages prompt files—named with a specific `<basename>_<language>.prompt` convention—to automate various coding tasks. Its core functionality includes generating runnable code from prompts, creating usage examples, and producing unit tests. 

The tool offers a suite of commands such as `generate`, `example`, `test`, `preprocess`, `fix` (for iterative error correction), `split` (for breaking down large prompts), `change`, and `update`. It supports multi-command chaining for complex workflows and includes features like cost tracking via CSV output, customizable AI model strength/temperature, and flexible output location management through environment variables. The CLI is built using the Python Click library and utilizes the Rich library for pretty-printed console output. The documentation includes detailed usage examples, directory structure, and Python code snippets demonstrating how to implement the CLI commands using internal functions.",bfc5835c208c47e02891a93a87064948059cca62a4a9b295b2b09f667ed75d4f
context/postprocess_example.py,"This Python script demonstrates the usage of the `postprocess` function from the `pdd.postprocess` module, which is designed to extract code blocks from text generated by Large Language Models (LLMs). The example illustrates two distinct extraction scenarios: a simple, zero-cost method using basic string manipulation (strength=0) to parse triple backticks, and an advanced method (strength>0) that leverages an LLM for more robust extraction. To run without external dependencies or costs, the script utilizes `unittest.mock` to simulate internal functions like `load_prompt_template` and `llm_invoke`. It uses the `rich` library for formatted console output, guiding the user through the inputs, simulated processing steps, and outputs for both scenarios, while verifying the mocked interactions.",b98362e6ffb84ae7826dad27d0854e30706f3993266314439859986b32ecd0bd
context/anthropic_tool_example.py,"The provided file contains a Python script demonstrating how to utilize the Anthropic API to interact with the Claude 3.7 Sonnet model. The script begins by importing the `anthropic` library and initializing a client instance. It then constructs a request to the `client.messages.create` method, specifying the model version `claude-3-7-sonnet-20250219` and setting a maximum token limit of 1024. A key feature of this request is the inclusion of a specific tool definition: a text editor tool named `str_replace_editor` with the type `text_editor_20250124`. The script sends a user message asking for assistance with a syntax error in a file named `patent.md`. Finally, the script captures the API's response in a variable and prints the output to the console, illustrating a basic workflow for programmatic interaction with Claude's tool-use capabilities.",6afb1637b9c794cad39f12d72969c50029b9e5cac80175cb3cc65666ce89e9e1
context/range_validator_example.py,"This Python script serves as a demonstration and usage example for a custom module named `range_validator`. It begins by dynamically adjusting the system path to locate and import the `range_validator` module from a relative directory structure, specifically targeting an experiment folder named `cloud_vs_local_fewshot_v2`. The script defines a `main` function that systematically showcases four key functionalities of the imported module. First, it demonstrates `is_in_range`, checking if values fall within specific bounds using both inclusive and exclusive logic, while also handling potential errors for invalid ranges. Second, it illustrates `normalize_to_range`, performing linear mapping examples such as converting sensor data to percentages and Celsius to Fahrenheit. Third, it exhibits `wrap_to_range`, applying modular arithmetic to constrain values like angles within a 0-360 degree circle. Finally, it presents `quantize`, showing how to snap floating-point numbers to a specific grid step (e.g., 0.25) using rounding, flooring, and ceiling modes. The script concludes by executing the `main` function when run directly.",ff9ea438b40abece583b8182d77340ccb0e45318a50374b187a4e2dbdaaf929c
context/postprocessed_runnable_llm_output.py,"The provided file contains a Python implementation and documentation for a function named `context_generator`. This function is designed to automate the creation of usage examples for Python modules. The implementation outlines a process that begins with reading a specified Python source file. It includes error handling for missing files and a placeholder for a preprocessing step (referencing a `processed_content` variable not fully defined in the snippet). The core functionality involves constructing a prompt for an AI model (specifically mentioned as GPT-4) to act as an expert Python engineer and generate a concise usage example based on the code provided. The file also includes markdown-style explanations detailing the steps taken—file reading and preprocessing—and provides a usage example showing how to call the function with input and output filenames. The text appears to be part of a larger tutorial or code explanation, as indicated by the repetitive introductory text at the beginning and end.",ca0e49021f9ba3ff162c1a8a98ef4639e49b1d4a0c22d143e2dc808e385325f2
context/git_update_example.py,"This file is a Python script designed to demonstrate the usage of the `git_update` function from the `pdd` package. The script defines a `main` function that sets up necessary parameters, including the prompt file path (`prompts/fix_error_loop_python.prompt`), the target code file (`pdd/fix_error_loop.py`), and configuration settings like strength and temperature. It reads the initial prompt content and then calls `git_update` with specific flags: `simple=False` to enable potential AI-powered agentic updates, and `verbose=True` for detailed output. The script handles the result by printing the modified prompt, the total cost of the operation, and the model used. It also includes logic to save the modified prompt back to the file system if a result is returned, while providing error handling for value errors and unexpected exceptions. The script is executable directly via a standard `if __name__ == ""__main__"":` block.",811304cee2cb890879ee68a816aed49dff2e7b50cfdca3125e16e0ae99819574
context/logo_animation_example.py,"This file serves as a demonstration script for the PDD branding animation functionality. It imports the `start_logo_animation` and `stop_logo_animation` functions from the `pdd.logo_animation` module. The `main` function initiates the animation, which runs in a background thread and takes over the terminal screen. The script then simulates a main program workload by sleeping for 7 seconds, allowing the animation sequence—comprising formation, hold, and expansion phases—to complete and display its final state. Finally, the script calls `stop_logo_animation` to terminate the animation thread and restore standard terminal control, printing confirmation messages to indicate the process has finished.",f32a5cc330235c3b61fb16ac62d3ae7531c7c6dbc11af077fc79054736f0944d
context/auto_deps_main_example.py,"This file defines a command-line interface (CLI) tool named `auto-deps` using the Python `click` library. The script serves as an entry point for a dependency analysis utility. It accepts several command-line options, including flags for forcing overwrites (`--force`), suppressing output (`--quiet`), and forcing a directory rescan (`--force-scan`). It also takes parameters for configuration, such as `strength`, `temperature`, paths for input prompts (`--prompt-file`), directory contexts (`--directory-path`), dependency CSVs (`--auto-deps-csv-path`), and output locations. The `main` function initializes the click context with these parameters and invokes the core logic function `auto_deps_main` from the `pdd` package. Upon successful execution, it prints the modified prompt, the total cost of the operation, and the model name used. If an exception occurs, it prints the error and aborts the execution.",099fb138baed69b975f9cc571ad4855905e13f9ef5a855ac223815fc0de95c1c
context/get_jwt_token_example.py,"This Python script serves as a command-line interface (CLI) utility for authenticating a user via GitHub Device Flow to obtain a Firebase ID token. It is designed to support multiple environments (local, staging, production) by dynamically loading the appropriate Firebase API key and GitHub Client ID based on the `PDD_ENV` environment variable. The script defines a helper function, `_load_firebase_api_key`, which attempts to fetch the API key from environment variables or specific `.env` files located in the project structure.

The core functionality is encapsulated in an asynchronous `main` function. It validates the presence of the API key and calls `get_jwt_token` to perform the authentication handshake. The script includes robust error handling for various scenarios such as user cancellation, network issues, token errors, and rate limiting. Upon successful authentication, it updates a local `.env` file with the retrieved JWT token, saving it under both a generic `JWT_TOKEN` key and an environment-specific key (e.g., `JWT_TOKEN_STAGING`). This allows subsequent applications or scripts to utilize the cached credentials for authenticated requests against the Firebase backend.",ae8e98e0b795bd1358ffc6a3e05da6112ae03dc9d4b58e4eb86e6926cbadff81
context/incremental_code_generator_example.py,"This Python script serves as an example demonstration of the `incremental_code_generator` function from the `pdd` package. It illustrates how to programmatically update existing code based on a revised prompt without necessarily rewriting the entire codebase from scratch. The script defines a `main` function that sets up specific input parameters, including an original prompt (calculating a factorial), a new prompt (adding input validation), the existing Python code, and configuration settings like `strength`, `temperature`, and `time`. It then calls the `incremental_code_generator` with these parameters to attempt an intelligent code update. Finally, the script uses the `rich` library to display the results in a formatted console output, indicating whether an incremental patch was successfully applied or if a full regeneration was recommended, along with the updated code, the total cost incurred, and the model used.",f6cd628ddc2c10853a15c449b32451a1290d2457942d642989678720bc0cea82
context/context_generator_example.py,"This file is a Python script designed to demonstrate and test the functionality of the `context_generator` function from the `pdd.context_generator` module. It begins by verifying that the required environment variable `PDD_PATH` is set, raising an error if it is missing. The script then initializes several input parameters, including a sample code module (a simple addition function), a corresponding prompt, the programming language (Python), and configuration settings for `strength` and `temperature`. It subsequently calls the `context_generator` function with these parameters and the `verbose` flag enabled. Finally, the script uses the `rich` library to print the outputs returned by the function, specifically displaying the generated example code, the total cost of the operation, and the name of the model used.",277b54ee71ffc836d9db9ad4d4fc0b3fee365ce6f5691222acad11b58461923b
context/process_csv_change_example.py,"This file, `run_demo.py`, serves as a concise usage example for the `process_csv_change` function from the `pdd` library. It demonstrates how to programmatically set up a workspace and execute a batch processing task driven by a CSV file. 

The script first establishes a temporary directory structure containing a sample Python source file (`factorial.py`) and a corresponding prompt file (`factorial_python.prompt`). It then generates a CSV file (`tasks.csv`) that lists specific instructions for modifying the prompt file. 

The core of the script invokes `process_csv_change` with various configuration parameters, including the path to the CSV, the target code directory, the desired modification strength, temperature settings for the LLM, and a budget constraint. Finally, the script prints the results of the operation to the console, displaying the success status, total cost, model name used, and the resulting JSON output, which shows the actual modifications applied to the prompt file based on the CSV instructions.",c7cf31cd55d5312b5398d3d518eddd33354fa184d0e92c6cf926a3c71d018227
context/bug_to_unit_test_example.py,"This file serves as a demonstration script for the `bug_to_unit_test` functionality within the `pdd` (Prompt Driven Development) library. It defines a `main` function that sets up a specific test scenario to generate a unit test based on a discrepancy between current and desired outputs. 

The script initializes hardcoded strings representing a `current_output` (which shows a failure to find a matching prompt line) and a `desired_output` (which shows a successful match). It then loads necessary context from external files: a prompt file (`trace_python.prompt`), the source code under test (`pdd/trace.py`), and an example driver program (`context/trace_example.py`). 

Using these inputs, along with configuration parameters like `strength`, `temperature`, and `language`, the script calls the `bug_to_unit_test` function. This function is designed to analyze the inputs and automatically generate a unit test that reproduces the bug or verifies the fix. Finally, the script uses the `rich` library to print the generated unit test code, the total cost of the LLM operation, and the model name used to the console, handling any potential exceptions gracefully.",ab1d665923937f6599f78c0cac82f5d7c04a7afa092e7767d67e417e067a8817
context/llm_selector_example.py,"This file contains a Python script designed to demonstrate the functionality of an `llm_selector` module. The script defines a `main` function that iterates through a range of `strength` values, starting at 0.5 and incrementing by 0.05 until it exceeds 1.1. Inside this loop, the script calls the imported `llm_selector` function with the current `strength` and a fixed `temperature` of 1.0. For each iteration, it retrieves and prints details about the selected Large Language Model (LLM), including the model name, input cost per million tokens, and output cost per million tokens. Additionally, the script demonstrates the usage of a returned token counter function by calculating and printing the token count for a sample text string. The execution is wrapped in a try-except block to handle potential `FileNotFoundError` or `ValueError` exceptions, ensuring robust error reporting.",0a8259580406d3c7ba68ee57230cacc49a5446e488433bf85c44958ab0b07541
context/crash_main_example.py,"This Python script demonstrates how to use the `crash_main` function from the `pdd` package to automatically fix a crashed program. The script sets up a dummy environment by creating several files: a prompt file describing the desired functionality, a buggy code file (`calculator.py`) that fails to handle string inputs correctly, a main program file (`main_app.py`) that triggers the bug, and an error log file simulating a captured traceback. It then constructs a mock Click context with configuration settings like verbosity and local execution mode. The script calls `crash_main` with these file paths and parameters to attempt a single-pass fix using a deterministic model setting. Finally, it checks the success of the operation and prints the results, including the fixed code, the cost of the operation, and the model used, using the `rich` library for formatted output.",5c9b909107f74f773ccf7013d556a5812643d2a3b086e79fe07da7ea7877458c
context/gemini_may_pro_example.py,"The provided file contains a concise Python code snippet demonstrating how to utilize the `litellm` library to interact with a specific Vertex AI model. The script begins by importing the `completion` function from the `litellm` package. It then executes a call to this function, specifying the model identifier as ""vertex_ai/gemini-2.5-pro-preview-05-06"". The request includes a standard message structure with a user role and a placeholder prompt content. Finally, the script captures the output of the completion call in a variable named `response` and prints this result to the console, illustrating a basic implementation for generating AI responses using the Gemini 2.5 Pro Preview model via the LiteLLM abstraction layer.",50c14eac723918c8a752ccc66d12eac4220d5ac62620bbf0c5e21bd821f08734
context/click_example.py,"This Python script implements a command-line image processing tool named `imagepipe` that leverages the `click` library to chain commands in a Unix-pipe style. The tool allows users to define a sequence of operations where the output of one command becomes the input for the next.

The core architecture relies on a `cli` group with `chain=True` and a result callback that orchestrates a stream of image objects using Python generators. Custom decorators, `processor` and `generator`, are used to wrap functions so they can participate in this streaming pipeline.

The script provides a variety of subcommands for image manipulation using the Pillow (PIL) library. Users can start a pipeline with the `open` command to load images. Subsequent processing commands include `resize`, `crop`, `transpose` (for rotation and flipping), `blur`, `smoothen`, `emboss`, and `sharpen`. There is also a `paste` command for overlaying images. Finally, the pipeline typically ends with output commands like `save` to write files to disk or `display` to view them. This structure enables complex image workflows, such as opening a file, resizing it, blurring it, and saving the result, all within a single terminal command.",07653c30b63c213102cf00f7c3380a5005330014e00a26342921cd85d20273c1
context/comment_line_example.py,"The provided file documents the usage and implementation of a Python function named `comment_line`, designed to programmatically comment out lines of code based on specified syntax rules. The function accepts two parameters: `code_line`, the string of code to be modified, and `comment_characters`, which dictates the commenting style. The logic handles three scenarios: deleting the line entirely if the character is 'del', wrapping the code with start and end markers if a space is present (e.g., for HTML comments like `<!-- -->`), or prepending a single character (e.g., `#` for Python). The documentation includes the source code for the function, three practical usage examples demonstrating Python, HTML, and deletion scenarios, and a clear explanation of the input parameters and return values.",debc424d539b65aef1267888bb2eb77cf4de16745407bb5df5a18b8e2f231095
context/agentic_langtest_example.py,"The provided file contents include a Python module, `pdd/agentic_langtest.py`, and an accompanying example script, `example.py`. The module is designed to facilitate automated testing across different programming languages by generating appropriate shell commands to run unit tests. It includes utility functions like `_which` to check for system tools and `_find_project_root` to locate project boundaries based on marker files (e.g., `package.json`, `pom.xml`).

The core function, `default_verify_cmd_for`, accepts a language (Python, JavaScript/TypeScript, or Java) and a test file path, returning a shell command string to execute the tests (e.g., using `pytest`, `npm test`, `mvn test`, or `gradle test`). Another function, `missing_tool_hints`, inspects the environment for required dependencies (like `npm`, `javac`, or `g++`) and provides installation instructions for macOS and Ubuntu if tools are missing.

The `example.py` script demonstrates these capabilities by creating temporary mock projects for JavaScript and Python. It utilizes the `rich` library to print formatted output, showing how the module generates verification commands and detects missing tools in a simulated environment.",dbc5c87dc1b75d3299dbfb67b17df1838f9ba2c9bdb457097df0832cea112c3f
context/generate_output_paths_example.py,"This Python script serves as a comprehensive demonstration and test suite for a function named `generate_output_paths`, which is imported from a module located in a parent directory (`pdd.generate_output_paths`). The script illustrates how the function resolves output file paths based on various inputs, including user-specified locations, environment variables, and default naming conventions.

The script executes eight distinct scenarios to validate the function's behavior. These scenarios cover: using default settings for the 'generate' command; specifying custom output filenames; specifying custom output directories; utilizing environment variables to override defaults for the 'fix' command; mixing user inputs with defaults; handling commands with fixed file extensions (like 'preprocess'); and managing edge cases such as unknown commands or missing basenames. For each scenario, the script prints the inputs, the actual result returned by the function, and the expected absolute path, allowing for verification of correct path resolution logic.",0f8a6384ebfe9dc3ea448a4df7b4f5ebf9aa9b714f67a4322437638700a4307f
context/execute_bug_to_unit_test_failure.py,"The provided file content is a very brief Python script that demonstrates a simple usage of an imported function. Specifically, it imports a function named 'add' from a module located at 'context.bug_to_unit_test_failure_example'. After importing the function, the script executes a print statement that calls 'add' with the integer arguments 2 and 3. The purpose of this script appears to be a basic test or demonstration to verify that the 'add' function works as expected, likely outputting the result '5' to the console. The file structure suggests it might be part of a larger project related to debugging or unit testing, given the specific naming convention of the source module.",cdc899979df1de48bba42fd026d5f980dc7381fdf224f8ab4db76554b69ee91e
context/config_example.py,"This file contains a brief Python script segment focused on the initialization of a configuration system. It begins with a comment emphasizing the priority of this operation, stating ""Initialize configuration FIRST."" The code then imports a function named `init_config` from a module located at `utils.config`. Finally, it immediately executes this `init_config()` function. The primary purpose of this snippet is to ensure that necessary configuration settings or environment variables are loaded and established before any other part of the application runs, likely to prevent errors related to missing or uninitialized parameters.",9440de90fa1ba1ed48f0bf1a080357ef15080d4406bc6f90c794f1eb539c8c40
context/detect_change_main_example.py,"This Python script serves as a wrapper or test harness for executing the `detect_change_main` function, simulating a Command Line Interface (CLI) environment without actual user input. It begins by importing necessary modules, including `click` for context management and `os` for file system operations. The core `main` function initializes a `click.Context` object, populating it with default model parameters such as strength, temperature, and flags for force and quiet modes. It defines a list of specific prompt files (e.g., `python_preamble.prompt`, `change_python.prompt`) and specifies a path for a change description file, which it then creates and populates with a specific instruction regarding prompt compactness. The script also ensures the existence of an output directory for results. It then invokes `detect_change_main` with these prepared arguments, capturing the returned list of changes, total cost, and model name. Finally, it prints these results to the console, detailing the model used, the calculated cost, and the specific change instructions for each prompt, while including error handling to catch and display any exceptions that occur during execution.",ddc494709d127398004f53de58bede388692b2839a97b26668c55575369d9edd
context/few_shot_hack_example.py,"This Python script serves as an example and simulation of the `agentic_bug_orchestrator` module within a project likely named `pdd`. It demonstrates how to invoke the `run_agentic_bug_orchestrator` function without requiring live LLM calls or a real GitHub repository. The script sets up a mock environment by patching internal dependencies such as `run_agentic_task`, `load_prompt_template`, and various git-related utilities using `unittest.mock`. 

The simulation walks through a hypothetical scenario where a user reports a ""ZeroDivisionError"" in a calculator application. The mocked `run_agentic_task` function provides predefined responses for an 11-step bug investigation workflow, covering stages like duplicate checking, documentation review, reproduction, root cause analysis, test generation, and PR creation. The script initializes dummy issue data, executes the orchestrator within a patched context, and prints the final results, including success status, total simulated cost, and a list of changed files. This allows developers to verify the orchestrator's logic flow and state management in a controlled, cost-free environment.",4f4ea852cdcd01c5f3b84e80aee551a87d13e81894648ec42104d9fe700c09a0
context/pdd_discussiion.txt,"This transcript records a collaborative interview between a developer and an AI to refine the concept of ""Prompt-Driven Development"" (PDD). The core philosophy of PDD is shifting the primary software artifact from source code to the prompt itself, analogous to the transition in chip design from netlists to high-level description languages like Verilog. The developer argues that traditional coding results in high maintenance costs and messy ""patched"" codebases, whereas PDD allows for cleaner systems by modifying prompts and regenerating code from scratch.

The conversation details the PDD workflow, which treats the prompt, generated code, usage examples, and unit tests as a single, synchronized unit. The developer contrasts PDD with interactive ""agentic"" tools like Cursor or Cloud Code, describing PDD as a more deterministic, token-efficient, and batch-oriented process that minimizes the need for constant human supervision. Key components of the proposed ecosystem include a PDD-CLI, an MCP server for tool integration, and a ""PDD Cloud"" for sharing few-shot examples. The discussion also highlights PDD's benefits for collaboration, as readable prompts bridge the gap between technical and business stakeholders, and emphasizes the importance of ""back-propagating"" learnings to keep documentation and specifications in sync.",9a627a93e31f408e5d5611413b72957f1137f9efc1b2fbcfeae16cca9197874c
context/split_main_example.py,"The file `example_split_usage.py` serves as a demonstration script for utilizing the `split_main` function from the `pdd` module via a command-line interface (CLI). Built using the `click` library, the script defines a CLI command named `split-cli` that accepts various arguments, including paths for input prompts, generated code, and example usage files, as well as optional output paths and flags for forcing overwrites or suppressing output.

Inside the `split_cli` function, a context object is initialized with custom settings like `strength` and `temperature`. The script then calls `split_main`, passing the provided arguments to process the prompt splitting logic. Upon successful execution, it retrieves a tuple containing result data, total cost, and the model name. If not in quiet mode, the script prints the content of the generated sub-prompt and modified prompt, their save locations, the model used, and the operation's cost to the console. The file concludes by setting up a top-level CLI group and invoking it, making the script directly executable.",ced192fcbfbfda15b35bd2dad5bbf4b4888acb091084cf58326832d224a77661
context/regression_example.sh,"This file is a Bash shell script designed to execute a comprehensive regression test suite for a software tool named ""pdd"". The script begins by defining various environment variables for paths, filenames, and logging configurations, including staging directories, prompt files, and context paths. It includes helper functions for verbose logging to the console and timestamped logging to a file (`regression.log`).

The core functionality involves running a series of `pdd` commands to test different features of the tool. These features include generating code from prompts, creating examples, running tests, preprocessing prompts (including XML output), updating prompts, and handling code changes. The script also tests the tool's ability to fix code based on test failures (using `pytest` output) and loop-based fixing with budget constraints. Furthermore, it verifies functionality for splitting prompts, detecting patterns, analyzing conflicts, and handling program crashes.

Throughout the execution, the script tracks the cost of operations in a CSV file. Finally, it calculates and displays the total cost of all operations performed during the regression test, ensuring that the tool's performance and resource usage are monitored alongside its functionality.",e4df58d36270ab7c005dfa8a111159c78bf149378510ae3423fb83efeb8f622a
context/get_run_command_example.py,"This Python script serves as a comprehensive demonstration of the `get_run_command` module, specifically illustrating how to retrieve execution commands for various programming languages based on file extensions. The script relies on a CSV configuration file (located via the `$PDD_PATH` environment variable) that maps extensions to command templates. Through six distinct examples, the `main` function showcases the module's capabilities: retrieving templates for standard extensions like `.py`, handling input normalization (such as missing dots or uppercase letters), and generating complete execution strings for specific file paths using `get_run_command_for_file`. Additionally, the script demonstrates error handling and edge cases, such as processing unknown extensions or files without extensions (e.g., Makefiles), which return empty strings. Finally, it iterates through a list of common extensions to verify configuration status. The code is structured to provide clear, printable output for each scenario, making it an effective usage guide for developers integrating this functionality.",4d187e844adb255c2738274556f4828728d0d73d84ebf8bda885d51d4be9414f
context/change_main_example.py,"This Python script serves as a demonstration and test harness for the `change_main` function from the `pdd` command-line program. It illustrates how to programmatically invoke the `change` command functionality in two distinct modes: single-change mode and CSV batch-change mode. The script sets up a mock Click context with configuration options such as LLM strength, temperature, and budget. It then creates necessary sample directories and files (prompts and Python code) to simulate real inputs. In the single-change example, it modifies a specific prompt based on a change instruction and an input code file. In the batch-change example, it generates a CSV file containing multiple change instructions mapped to specific prompt files and processes them in bulk. Finally, the script prints the results, including the modified prompts, total costs, and model names used, to the console using the `rich` library for formatting.",990b2dfc2b9ef2f1871cb4fe1b7a01d5d6f4f4f54f9a6a95fc3661c679802d42
context/architecture_sync_example.py,"This Python script provides comprehensive usage examples for the `architecture_sync` module, designed to manage bidirectional synchronization between a central `architecture.json` file and individual prompt files. The system utilizes PDD (Prompt Driven Development) metadata tags—such as `<pdd-reason>`, `<pdd-interface>`, and `<pdd-dependency>`—to embed architectural details directly within prompt content.

The file outlines nine distinct examples demonstrating core capabilities:
1.  **Parsing and Extraction:** Reading XML-style tags from prompt strings to extract metadata.
2.  **Synchronization:** Updating the central architecture registry from a single prompt or performing a bulk sync from all prompts.
3.  **Validation:** Verifying that declared dependencies exist and that interface JSON objects adhere to the required schema.
4.  **Tag Generation:** The reverse process of creating XML tag strings from architecture entries.
5.  **Workflow Integration:** A complete workflow showing how to detect missing tags in a prompt and inject them based on architecture data.

The script includes a main execution block that runs a subset of these functions, serving as both a tutorial for developers and a verification of the module's functionality.",95624e3241bf1d837528b897cf8014c0946c8b65bca2c174f3bd5cc54c7ee3ff
context/dict_utils_example.py,"This Python script serves as a demonstration and usage guide for a custom utility module named `dict_utils`. The script begins by dynamically adjusting the system path to locate and import the `dict_utils` module, handling potential import errors gracefully. It defines helper functions for printing headers and formatting JSON output to ensure readability.

The core of the script is the `main` function, which showcases five key functionalities of the library through practical examples. First, it demonstrates `deep_merge`, showing how to combine nested configuration dictionaries where overrides update specific values without erasing the entire structure. Second, it illustrates `flatten_dict`, converting nested data into single-level dictionaries using dot notation or custom separators. Third, it performs the reverse operation using `unflatten_dict` to reconstruct nested structures from flat keys. Fourth, the script exhibits `filter_keys`, allowing users to selectively include or exclude specific fields from a dictionary, useful for API response sanitization. Finally, it demonstrates `get_nested`, a safe method for retrieving values from deep within complex data structures using path strings, with support for default values when keys are missing.",4c123047dc113ec65a2fccf858e047a7379382ac66d09029037dd7489b6f113f
context/fix_code_module_errors_example.py,"This file is a Python script designed to demonstrate the usage of the `fix_code_module_errors` function from the `pdd.fix_code_module_errors` module. The script sets up a test scenario involving a deliberately buggy Python program that attempts to calculate the sum of a string instead of a list of numbers, which triggers a `TypeError`. It defines the original erroneous program, the prompt used to generate it, the specific code module containing the logic, and the resulting error message. The script then calls `fix_code_module_errors` with these inputs, along with configuration parameters for model strength, temperature, and verbosity. Finally, it prints the results returned by the function, including whether updates were needed, the fixed versions of the program and code module, the raw fix output from the LLM, the total API cost, and the name of the model used.",f35244d6e920711044fee92ade984f3e820a0fbfe766fd3d49718cbcfe793e60
context/get_test_command_example.py,"This Python script serves as a comprehensive demonstration of the `get_test_command` module, specifically illustrating how to use the `get_test_command_for_file()` function. The script showcases the module's layered resolution strategy for determining the appropriate test command for a given file, which prioritizes CSV-defined commands, falls back to smart detection, and finally returns `None` to signal the need for agentic intervention.

The code includes several distinct functions to demonstrate different use cases: `demonstrate_python_test` and `demonstrate_javascript_test` show basic usage for standard file extensions; `demonstrate_language_override` illustrates how to manually specify a language to bypass extension detection; and `demonstrate_multiple_files` presents a batch processing example using a table layout. Additionally, `demonstrate_handling_none_result` explicitly shows how to handle scenarios where no command is found, emphasizing the intended workflow for agentic fallbacks. The script utilizes the `rich` library to provide formatted, color-coded console output, making the examples easy to read and understand. A `main` function orchestrates the execution of all these examples sequentially.",e41c5ad25fb7e9ab01baf376e17226508365ab4269808f935de4d69b0bbdd4eb
context/edit_file_example.py,"This Python script serves as a test harness and usage example for the `run_edit_in_subprocess` function imported from the `pdd.edit_file` module. Its primary purpose is to verify that the file editing functionality works as expected by performing a controlled test on a dummy file.

The script begins by configuring the Python path to locate the necessary modules and validating the presence of the `PDD_PATH` environment variable, which defines the working directory. It includes a helper function, `create_example_files`, which generates a sample text file (`example_file_to_edit.txt`) containing specific initial lines of text within an output directory.

The core logic is encapsulated in the `run_edit_file_test` function. This function takes a file path and a set of natural language editing instructions (e.g., replacing lines, adding text). It invokes the `run_edit_in_subprocess` function to apply these changes. Post-execution, the script reads the modified file and performs detailed verification checks to ensure specific string manipulations—such as changing ""original"" to ""UPDATED"" and appending new lines—were successfully applied. Finally, the `run_example` function orchestrates the entire process, setting up the file, running the test, and reporting the success or failure of the operation.",64c4d8db9ecbb99f14083e10c9e5788e4b04fc6f9dd0942ab571ffea7a6dd1ca
context/update_main_example.py,"This Python script serves as an example implementation of a Command Line Interface (CLI) using the `click` library to interact with the `update_main` function from the `pdd` package. The script defines a primary command named `update` which facilitates updating a prompt based on code modifications. It accepts various arguments including paths for input prompts, modified code, and original code, as well as configuration flags for Git integration, output paths, model strength, temperature, verbosity, and a simplified execution mode. The script demonstrates how to pass these CLI arguments into a context object and invoke the core logic. Finally, it captures and displays the results of the operation, such as the updated prompt snippet, total cost, and the model name used, utilizing the `rich` library for formatted console output.",b7817f30fab788c21b4b0fb42846cbb5fe82b91e8dea265cd742252e036407e2
context/prompt_caching.ipynb,"This Jupyter Notebook serves as a cookbook demonstrating the implementation and benefits of prompt caching using the Anthropic API (specifically with the Claude 3.5 Sonnet model). The tutorial begins by explaining that prompt caching allows users to store and reuse context, significantly reducing latency and costs for repetitive tasks involving large amounts of data. 

The notebook is structured into two main examples. First, it sets up the environment by installing necessary libraries and fetching the full text of Jane Austen's ""Pride and Prejudice"" (approx. 187,000 tokens) from Project Gutenberg to serve as a large context window. 

Example 1 compares a single-turn API call without caching against one with caching enabled. It demonstrates a dramatic reduction in processing time (from ~21 seconds to ~3 seconds) when the large book content is cached. 

Example 2 illustrates a multi-turn conversation. It defines a `ConversationHistory` class to manage message turns and strategically places cache breakpoints (using `cache_control: {""type"": ""ephemeral""}`) on the system message and recent user turns. The simulation shows that subsequent turns in the conversation achieve near-instant processing of input tokens (reading from the cache), maintaining low latency even as the conversation history grows.",c9d723779341cfa01e6f78abd1128aae8ecdf66ba2c4cada31880df0b3668ff4
context/split_example.py,"This Python script serves as a demonstration and test harness for the `split` function from the `pdd.split` module. It begins by importing necessary libraries, including `os` for environment management and `rich.console` for formatted output. The core logic is encapsulated within a `main` function. Inside `main`, the script first ensures the `PDD_PATH` environment variable is correctly set relative to the script's location. It then defines several input parameters required for the `split` function: a natural language prompt requesting a factorial function, the corresponding Python implementation code, an example usage snippet, and numerical parameters for `strength` and `temperature`. The script calls the `split` function with these inputs and `verbose` mode enabled. It expects a return value consisting of a result tuple (containing a sub-prompt and a modified prompt), the total cost of the operation, and the model name used. Finally, it unpacks these results and uses the Rich console to print the sub-prompt, modified prompt, model name, and total cost in a styled format. Error handling is implemented to catch and display any exceptions that occur during execution.",c36a6c3e5ddac660ef95d72dabc89f5453a3ebcd47f5f82ddc5fec77806154d9
context/agentic_change_orchestrator_example.py,"This Python script serves as an example and simulation for the `agentic_change_orchestrator` module within a project likely named `pdd`. It demonstrates how to invoke the `run_agentic_change_orchestrator` function without performing actual Large Language Model (LLM) calls or interacting with a live GitHub repository. 

The script sets up a mock environment by patching internal dependencies such as `load_prompt_template` and `run_agentic_task`. It defines a `mock_run_agentic_task` function that simulates the outputs for a 13-step workflow, guiding the orchestrator through a scenario where a user requests an email validation feature. The mock responses cover steps ranging from duplicate issue checking and documentation review to identifying development units, modifying prompts, and creating a pull request. 

The `main` function initializes a temporary directory and dummy issue data, then executes the orchestrator within a context where git operations and state management are also mocked. Finally, it prints the simulation results, including success status, total simulated cost, and a list of changed files, illustrating the expected behavior of the orchestration tool.",26a09cdbb0b010106128bf51f6eca9e59fa8d15692a73dea94e8b6d77496f6aa
context/agentic_verify_example.py,"This Python script serves as a demonstration and test harness for the `run_agentic_verify` function from the `pdd.agentic_verify` module. It is designed to simulate an automated code verification and repair workflow without requiring actual external API calls or LLM access. The script first sets up the environment by ensuring the `pdd` package is importable and creating a temporary directory populated with dummy files: a specification file (`spec.md`), a buggy implementation (`calculator.py` containing a subtraction error), a driver test script (`driver.py`), and a verification log.

The core functionality relies on mocking internal dependencies using `unittest.mock.patch`. Specifically, it mocks `load_prompt_template` and `run_agentic_task`. The `mock_agent_behavior` function simulates an AI agent identifying the bug, rewriting the `calculator.py` file to fix the logic error (changing subtraction to addition), and returning a successful JSON response. The `main` function orchestrates this process, invoking `run_agentic_verify` with the dummy files, and subsequently printing the execution results—including success status, cost, model usage, and the corrected file content—to verify that the simulated repair was successful.",24022f00007d707a593280b1dd2151e05c33680a65a67aaf034243cac5a9bf6c
context/cmd_test_main_example.py,"This Python script serves as a demonstration and usage example for the `cmd_test_main` function from the `pdd` package. The script defines a `run_example` function that simulates a workflow for generating unit tests for a simple calculator module. It begins by setting up an output directory and creating dummy input files: a prompt file describing the task and a corresponding Python code file containing basic arithmetic functions. The script then mocks a `click.Context` object to simulate command-line execution parameters such as verbosity and local execution flags. It subsequently calls `cmd_test_main` with these parameters, specifying the input files, output destination, and model settings like strength and temperature. Finally, the script executes the test generation process, prints the results—including the model used, estimated cost, and file location—and displays a preview of the generated test code. The main execution block ensures necessary environment variables are set before running the example.",906f51b918543e5d7a9a2d30e33096dd7ff81df3c4673e0d20053155fbf4d841
context/remote_session_example.py,"This Python script serves as a comprehensive usage example for the `pdd.remote_session` module, specifically demonstrating the functionality of the `RemoteSessionManager` class. The script sets up a mock environment using `unittest.mock` to simulate cloud interactions without requiring actual network endpoints or credentials. It walks through the lifecycle of a remote development session in five key steps: initializing the manager with a JWT token and project path, registering a new session to obtain a cloud URL, starting a background heartbeat task to maintain session activity, listing all active sessions associated with the user, and finally deregistering the session to perform cleanup. The code illustrates how to handle asynchronous operations using `asyncio` and demonstrates proper error handling for `RemoteSessionError`. Additionally, it shows how to manage the global session state using `set_active_session_manager`. By patching `httpx.AsyncClient` and `CloudConfig`, the script provides a self-contained, runnable demonstration of the API's expected behavior and response structures.",83e17416100132467ee86c4e574cb2c52721d5c4aaf096d149fdd6f3df984faf
context/DSPy_example.py,"The provided file content is a Python code snippet demonstrating the initial setup and module definition for a DSPy application. The stated goal of the script is to use DSPy to select an optimal example. The code begins by importing a local context module and configuring the DSPy settings to use the OpenAI 'gpt-3.5-turbo-instruct' model with a specific token limit. Following the setup, it defines a custom class named `chainofthought` that inherits from `dspy.Module`. Inside this class, the `__init__` method initializes a standard `dspy.ChainOfThought` program designed to map questions to answers. The `forward` method is defined to execute this program when the module is called with a question. The snippet ends abruptly with a comment about compiling and optimizing, indicating that the code is incomplete and likely part of a larger tutorial or script.",403910f924da848e2016f2a19cd9283759b6eac67f969f4d0bd5c573e39784f7
context/agentic_update_example.py,"This Python script serves as a demonstration and utility for using the `run_agentic_update` function from the `pdd` package. Its primary purpose is to showcase how an AI agent can be employed to automatically update a prompt file to match the current state of a codebase. 

The script begins by setting up the environment, ensuring the project root is in the system path. It then creates a simulated development scenario within a local `./output` directory. This scenario involves generating three files: an outdated prompt file (`math_module.prompt`) that only specifies an addition function, an updated code file (`math_module.py`) that includes both addition and subtraction functions, and a corresponding test file (`test_math_module.py`). 

Once the environment is prepared, the script executes `run_agentic_update`. This function orchestrates an AI agent to analyze the code and tests, and subsequently rewrite the prompt file to reflect the new subtraction feature. Finally, the script prints the results of the operation, including success status, cost, the model used, and the content of the updated prompt file to verify the changes.",019bc86f2c262b279dd0c7ad89223fbbc3ffd9d5229ae06ccbd07ef1c954d26c
context/websocket_example.py,"This Python file provides a comprehensive example of implementing WebSocket connections for a PDD server using FastAPI and Starlette. It defines a robust protocol for bidirectional communication, enabling features such as connection management for multiple clients, real-time streaming of standard output and error logs from subprocesses, and interactive client input handling. 

The code establishes a structured message protocol using Pydantic models and Enums to define various message types like `STDOUT`, `PROGRESS`, `INPUT_REQUEST`, and `COMPLETE`. A `ConnectionManager` class handles the lifecycle of WebSocket connections, including broadcasting messages to specific job IDs. Additionally, an `OutputStreamer` class is provided to capture and stream subprocess output while stripping ANSI codes, and a `WebSocketInputHandler` manages interactive prompts. The file concludes with a `main` function that sets up a FastAPI application with these WebSocket routes and includes a simulation endpoint to demonstrate how a job streams progress and completion status to connected clients.",6f50399fd41177e47da3f170a1cc84380f8f266036e8f510f7807a916265ab0f
context/get_language_example.py,"This file contains a Python script designed to demonstrate the functionality of the `get_language` function imported from the `pdd.get_language` module. The script defines a `main()` function which serves as the entry point. Inside this function, a sample file extension (specifically `.py`) is defined. The script then attempts to retrieve the programming language associated with this extension by calling `get_language`. It includes error handling via a try-except block to catch potential exceptions during execution. If a language is successfully identified, it prints the result to the console; otherwise, it informs the user that no language was found. Finally, the script uses the standard `if __name__ == ""__main__"":` idiom to execute the `main()` function when the file is run directly.",8b09af25b96fa49e2dcf5ff307bbeb0bc83e84b64618971d393397772a2fafab
context/detect_change_example.py,"This Python script demonstrates how to use the `detect_change` function from the `pdd` package to analyze a set of prompt files for necessary modifications based on a specific change description. The script begins by importing necessary modules, including `Console` from `rich` for formatted output. It defines a list of specific prompt files to be examined, such as `python_preamble.prompt` and `change_python.prompt`, although a commented-out section suggests an alternative method to dynamically list all prompt files in specific directories. 

The core task is defined by the `change_description` variable, which instructs the system to incorporate `context/python_preamble.prompt` to make prompts more compact. The script sets configuration parameters for the LLM, including `strength` (set to 1 for a strong model) and `temperature` (set to 0 for deterministic output). It then calls `detect_change` inside a try-except block to handle potential errors. Upon successful execution, the script prints a list of detected changes, including the prompt name and specific instructions for each, followed by the total cost of the operation and the name of the model used. If an error occurs, it is caught and displayed in red.",44d0c9172db01bd645d48a11a7afa527a7e509f5aac911fa41ed091d3f982096
context/trace_main_example.py,"This Python script demonstrates the usage of the `trace_main` function from the `pdd.trace_main` module, likely part of a larger tool for tracing relationships between prompts and code using an LLM. The script begins by creating two example files in an `output` directory: `calculator.prompt`, which contains a natural language description of a simple addition task, and `calculator.py`, which contains the corresponding Python implementation. It then initializes a `click.Context` object to simulate a command-line environment, configuring parameters such as `quiet` mode, `force` overwrite, analysis `strength`, and LLM `temperature`. The core of the script invokes `trace_main` with these configurations, passing the paths to the generated prompt and code files. It specifically targets line 2 of the code file (the function definition) to find its corresponding origin in the prompt file. Finally, the script prints the results of the analysis, including the identified line number in the prompt file, the total cost of the LLM operation, and the model name used, while handling any potential exceptions during execution.",c1f43ebf55c05af801640f5f8331ee7a674aef9e3f383b0e5ffee7abc7246fe1
context/find_section_example.py,"This file provides a practical example and documentation for using the `find_section` function from the `pdd.find_section` module. The primary purpose of this function is to parse text input, specifically identifying and extracting code blocks delimited by Markdown-style triple backticks. The example demonstrates how to import the function, prepare a multi-line string containing mixed text and code (simulating LLM output), and process it by splitting the string into lines. It then shows how to iterate through the returned `sections` to print the programming language, start line index, and end line index for each detected block. Additionally, the file includes a second usage case that reads from a local file named `unrunnable_raw_llm_output.py`. The documentation section details the input parameters—`lines`, `start_index`, and `sub_section`—and describes the output format as a list of tuples containing the language identifier and line boundaries.",681043403aa02b9eb0d265a95560fce565507d0656d1693f79650f8eb5f0683a
context/fix_verification_errors_loop_example.py,"This Python script serves as a demonstration for a function named `fix_verification_errors_loop`, which is part of a larger package called `pdd`. The script sets up a temporary demo environment to showcase how an automated agent can fix buggy code based on verification failures.

The `setup_demo_environment` function creates a directory containing three files: a buggy Python script (`calculator.py`) that incorrectly implements addition as subtraction, a verification script (`verify_calc.py`) that asserts the correct behavior, and a prompt file describing the intended functionality. 

The `main` function initializes this environment and then invokes `fix_verification_errors_loop` with specific parameters such as budget, temperature, and retry limits. It attempts to automatically repair the buggy calculator code using an LLM-based loop. Finally, the script reports the success or failure of the operation to the console using the `rich` library for formatted output, displaying the final fixed code and execution statistics if successful.",0212e6010a7c163c476c3df095f6ca99414829f73df34c60651afae83633d307
context/path_resolution_example.py,"This file contains a Python script that demonstrates the usage of the `pdd.path_resolution` module, specifically the `get_default_resolver` function. The script defines a `main` function which initializes a default path resolver. It then uses this resolver to locate and print the absolute paths for three specific resources: an include file named ""context/python_preamble.prompt"", a prompt template named ""load_prompt_template_python"", and the project root directory. Additionally, the script attempts to resolve a data file path (""data/language_format.csv"") within a try-except block, handling a potential `ValueError` if the `PDD_PATH` environment variable is not set. The script concludes by executing the `main` function if run as the primary module.",15ba84fd11c61af8fa12dcb19d63d42f0cb09b747542bf20e69650b8ff4ac137
context/thinking_tokens.md,"This document outlines the maximum internal ""reasoning"" token limits for various AI models, distinguishing between standard models and those with dedicated chain-of-thought (CoT) capabilities. Standard models like GPT-4.1-nano, Claude 3.5 Haiku, DeepSeek Coder, and GPT-4.1 do not utilize hidden reasoning tokens, relying instead on single-pass generation within their context windows. Conversely, specialized reasoning models feature significant internal token budgets to process complex queries before outputting a final answer. 

DeepSeek's R1 series (including distilled variants and the main ""DeepSeek Reasoner"") generally supports a CoT budget of approximately 32,000 tokens. Anthropic's Claude 3.7 Sonnet offers a higher capacity, allowing up to 64,000 tokens for extended thinking. Google's Gemini 2.5 Flash and Pro models are estimated to use tens of thousands of tokens (likely ~65k) for internal reasoning. OpenAI's o4-mini and o3 models currently lead in capacity, supporting up to 100,000 reasoning tokens, enabling deep, multi-step problem solving. Grok 3 Beta also supports extended reasoning, potentially utilizing thousands of tokens, though specific limits remain unpublished.",47d0635d14672ad9b6aa133166058fa6eaea36029ea3f332bc9f647a01a25f6c
context/tiktoken_example.py,"The provided file content is a brief Python code snippet demonstrating how to count tokens using the `tiktoken` library, which is commonly used with OpenAI models. The code first imports the `tiktoken` module. It then initializes an encoding object by calling `tiktoken.get_encoding` with the specific encoding scheme ""cl100k_base"", which is the tokenizer used by models like GPT-4 and GPT-3.5 Turbo. Finally, the script calculates the number of tokens in a variable named `preprocessed_prompt` by encoding the text and measuring the length of the resulting list using the `len()` function, storing the result in the variable `token_count`.",0cc5e74006d0d30b399158127d9258105ce16be1815c012a08bafd3435f78f36
context/resolve_effective_config_example.py,"This file provides code examples and documentation for using the `resolve_effective_config` function from the `pdd.config_resolution` module. It explains that this function is designed to be called after `construct_paths` to determine the final configuration values for parameters such as `strength`, `temperature`, and `time`. The documentation highlights the priority order for resolving these settings: Command Line Interface (CLI) arguments take precedence, followed by values in the `.pddrc` configuration file, and finally default values. Two usage examples are provided: one demonstrating a basic call using the Click context (`ctx`) and the resolved configuration dictionary, and another showing how to pass a `param_overrides` dictionary to manually override specific settings like strength or temperature when they are provided as command parameters.",8bb73c48821e07f533909f368d5cc1e029b12690830ceb9918c4f1bd7611a293
context/summarize_directory_example.py,"This Python script demonstrates the usage of the `summarize_directory` module from the `pdd` package. The `main` function illustrates a workflow that includes creating a sample CSV string representing existing file summaries (used for cache invalidation via content hashes) and then invoking the `summarize_directory` function. This function is called with parameters to target specific Python files (`context/c*.py`), set the model strength and temperature, and enable verbose logging. The script captures the resulting CSV output, total cost, and model name, printing these details to the console. Finally, it ensures an `output` directory exists and saves the generated CSV content to a file named `output.csv` within that directory, handling any potential exceptions during the process.",a1fb936c45b2dc3b9995d22aed05e75af20b171d8990c4a483b1cf8a54dc6df3
context/__init__example.py,"This Python script serves as an example demonstration for initializing a command-line interface (CLI) using the `click` library and a custom command registration module. It defines a main CLI group named `main_cli` and imports a `register_commands` function from the `pdd.commands` package. The core functionality is encapsulated in the `run_example` function, which first initializes the CLI group and then calls `register_commands` to attach subcommands dynamically. To verify that the registration process works correctly, the script simulates running the CLI with the `--help` argument, printing the resulting help message to the console. This output confirms that all commands from the module have been successfully aggregated into the main interface. The script is designed to be run directly, showcasing the typical setup for an application's entry point.",84208a445d03a336e465709ec0687eb969c8d865e6739bf8b79f2c8a8aad351a
context/track_cost_example.py,"This Python script defines a command-line interface (CLI) tool named PDD, built using the `click` library, designed for processing prompts and generating outputs with integrated cost tracking capabilities. The core functionality is encapsulated in a `cli` command group which accepts a global option `--output-cost` to specify a CSV file for logging usage details. The primary command within this group is `generate`, which requires an input prompt file path and optionally accepts an output file path. The `generate` function is decorated with `@track_cost`, indicating that its execution metrics are monitored. Inside the function, the script reads the provided prompt file, simulates a generation process (currently a placeholder that prepends text to the input), and calculates a simulated cost and model name (e.g., ""gpt-4""). If an output path is provided, the result is written to that file; otherwise, it is printed to the console using the `rich` library. The script concludes with a standard `if __name__ == '__main__':` block that demonstrates how to invoke the CLI programmatically with sample arguments.",30c33d5c1c945e0e8f0c0f9861f8b952d1eb4f22e6cbc8c05089b160b1a4c8eb
context/auto_update_example.py,"This file provides example usage scenarios for the `auto_update` function from the `pdd` package. It defines a `main` function that demonstrates three distinct ways to invoke the utility: a basic call to check for updates to the `pdd` package itself, a specific check for the `requests` library, and a version comparison against a manually specified version string for the `pandas` library. The accompanying docstring explains the function's internal logic, which involves verifying the installed version, comparing it against the latest available version (either fetched from PyPI or provided as an argument), and prompting the user to upgrade via pip if a newer version is detected.",4fc8ab0da96c0e8abe793d195dbc64b6b9771631be818eecde059769e5914fc9
context/addition_of_time_param.prompt,"This file contains a system prompt designed for the `pdd detect` command. Its primary purpose is to instruct an AI agent on how to analyze existing prompt files within the PDD CLI ecosystem to determine if they require updates due to a new feature: a global `--time` parameter. The prompt details the mechanics of this new parameter, which accepts a float between 0.0 and 1.0 (default 0.25) to control the ""thinking effort"" or reasoning allocation of the underlying Large Language Model (LLM). It explains how this value translates into specific behaviors in the `llm_invoke` Python function, such as calculating token budgets or mapping to qualitative effort levels (low, medium, high). The agent is tasked with reviewing a target prompt to identify potential conflicts, such as hardcoded instructions about reasoning depth that might contradict the user-controlled `--time` setting, or outdated documentation. The goal is to decide if the target prompt needs modification to align with this new global configuration option.",77ea69875b7c21bd5b6989620c5dd67a0a6e4b740fba1b726c4aa6df93d6cb27
context/cloud_function_call.py,"This file contains a Python script demonstrating how to invoke a Google Cloud Function securely using an HTTP GET request. It imports the `requests` library to handle the HTTP communication. The core functionality is encapsulated in the `call_cloud_function` function, which accepts a Firebase authentication token as an argument. Inside this function, a specific Cloud Function URL is defined, and an authorization header is constructed using the provided Bearer token. The script then sends a GET request to the Cloud Function endpoint with this header and returns the JSON response. Finally, the script includes a usage example where a placeholder Firebase token is defined, the function is called, and the resulting output from the Cloud Function is printed to the console.",62ac7724ce2e3eddbba2b743bd7aa5790dbed34eae01e77e0252c4cc6358c95e
context/install_completion_example.py,"This Python script serves as an example usage guide for the PDD shell completion installation module. It demonstrates how to utilize two primary functions from the `pdd` package: `get_local_pdd_path()`, which retrieves the absolute path to the PDD directory, and `install_completion()`, which automates the setup of shell completion for the PDD CLI. The script includes a `setup_example_environment()` function that safely creates a dummy environment—including a temporary home directory, a mock PDD path, and dummy configuration files (like `.bashrc` and a completion script)—to prevent modifying the user's actual system configuration. By forcing the shell environment to `/bin/bash`, it simulates the installation process, showing how the module detects the shell, locates the correct RC file, and appends the necessary source command. The `main()` function orchestrates this workflow, providing feedback via the `rich` library to visualize the installation steps.",999be17c0ce476845d7048a1a909ad425e60c298dbc7c74f51b596ef134a8aec
context/auto_include_example.py,"This Python script serves as an example usage demonstration for the `auto_include` function from the `pdd` library. The script defines a `main` function that orchestrates the process of generating project dependencies based on a specific prompt. It begins by reading an existing CSV file named `project_dependencies.csv`. It then constructs a detailed input prompt designed to instruct an LLM to act as an expert Python Software Engineer and generate a unit test function (`generate_test`). This prompt outlines specific inputs, outputs, and a multi-step Langchain-based workflow involving preprocessing, model selection, execution, and post-processing. The script sets parameters such as directory paths (targeting Python files in a context folder), model strength, and temperature. Finally, it calls `auto_include` with these parameters and prints the resulting dependencies, the updated CSV output, the total cost of the operation, and the name of the model used.",2bae7ee830c9bcf7f5c45ce639a6b49337c0d55294fd91b51cd0717dcde11c78
context/bug_main_example.py,"This Python script serves as a demonstration of how to utilize the `bug_main` function from the `pdd` package to automatically generate unit tests. The script sets up a mock environment by creating several example files in an `output` directory, including a prompt file describing a desired function, a Python source file with an implementation, a main program file, and text files representing both current (buggy) and desired program outputs. It configures a Click context object with specific parameters such as model strength and temperature. The core of the script invokes `bug_main`, passing in paths to these generated files to simulate a scenario where a developer wants to create a test case based on a discrepancy between observed and expected behavior. Finally, the script prints the generated unit test code, the associated cost of the operation, and the name of the model used to the console using the `rich` library.",662ecde39d05a5345533c4ae74c6b1031f016916ec2ff4764d34a26f7ec26172
context/anthropic_counter_example.py,"The provided file content is a Python script snippet, primarily consisting of commented-out code, that demonstrates how to use the Anthropic Python client library to count tokens for a given text string. It begins by importing the `anthropic` module and initializing an `Anthropic` client instance. The script defines a sample variable named `text` containing the string ""Sample text"". It then attempts to calculate the token count for this text using the `client.count_tokens(text)` method. A specific comment warns that this method of token counting is not accurate for Anthropic models version 3.0 and above. Finally, the script includes a print statement intended to display the total number of tokens calculated. The entire block of functional code is currently commented out, meaning it would not execute if run directly.",50d2554102428b7961b67ea6a5161d7c1099a9f4f5090e9eb124181a6e5d90ff
context/litellm_bedrock_sonnet.py,"This file contains a Python script demonstrating how to use the `litellm` library to interact with an Anthropic Claude 3.7 Sonnet model hosted on Amazon Bedrock. The script begins by importing the `completion` function from the `litellm` package. It includes commented-out lines for setting necessary AWS environment variables, such as the access key ID, secret access key, and region name, which are required for authentication. The core functionality is executed through a call to the `completion` function, specifying the model identifier `bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0`. The request includes a simple user message asking for the capital of France and sets a `reasoning_effort` parameter to ""low"". Finally, the script prints the response received from the model to the console.",3a0bb8269d3a765b728a09be9fca29f7b932f2860111dde2a79f370192eefea4
context/load_prompt_template_example.py,"This Python script demonstrates the usage of a custom utility function for loading prompt templates. It imports the `load_prompt_template` function from the `pdd.load_prompt_template` module and the `print` function from the `rich` library for enhanced terminal output. The script defines a `main` function where it specifies a target prompt name, ""generate_test_LLM"". It then attempts to load this template using the imported utility. If the prompt is successfully loaded, the script prints a styled header followed by the content of the loaded prompt template to the console. The script includes a standard entry point check to ensure the `main` function runs only when the file is executed directly.",a1cd6619182c6c951f5856dda4070e202875a5884bbfab9cc191d24de2f4951f
context/postprocess_0_example.py,"This file provides a concise usage example and documentation for the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The example demonstrates how to import and utilize the function within a Python script (`example_usage.py`). It sets up a mock `llm_output` string containing mixed content—including text, multiple Python code blocks, and a Java code block—to simulate a typical Large Language Model response. The script then calls `postprocess_0` with the target language set to ""python"" and prints the result. The accompanying documentation details the function's input parameters: `llm_output` (the raw string from the LLM) and `language` (the target programming language). It explains that the function's output is a string where only the largest code section matching the specified language remains active, while all other text and code blocks are commented out. Finally, a note advises that dependency functions like `get_comment`, `comment_line`, and `find_section` must be implemented for the example to execute correctly.",2ff3f65a972fbd46519fefbb1467687c4d9f2b269e14e3810eb1a8427b1b396e
context/fix_main_example.py,"This Python script serves as a demonstration and test harness for the `fix_main` function from the `pdd` package. It sets up a mock environment by creating a directory named `output` and populating it with several files: a buggy Python script (`calculator.py` where addition is implemented as subtraction), a failing unit test, a prompt file describing the desired functionality, a verification script, and a sample error log. The script then initializes a dummy Click context with configuration settings like verbosity and LLM parameters. It proceeds to execute `fix_main` in two distinct modes: first, an iterative ""Loop Mode"" that attempts to fix the code by running tests and verifying results up to a maximum number of attempts; and second, a ""Single-Pass Mode"" that attempts a one-time fix based on an existing error log. The script uses the `rich` library to print status updates, success metrics, and the resulting fixed code to the console.",29f88d8bfcc2256ebebe6efcb611f9487eb5ca6d8781286a6dd98689f92bca7a
context/fix_verification_main_example.py,"This Python script serves as a demonstration and test harness for a module named `fix_verification_main`, which appears to be part of a larger package (likely `pdd`). The script sets up a mock environment by creating dummy files in a directory called `example_verification_env`. These files include a prompt file requesting a calculator module, a Python code file containing an intentional bug (subtraction instead of addition), and a verification program designed to test the code and fail. The script then mocks a `click.Context` object to simulate command-line arguments and configuration settings. Finally, it executes the `fix_verification_main` function with these inputs to demonstrate how the system attempts to verify and potentially fix the code against the provided prompt and test program. The execution results, including success status, attempts made, and cost, are printed to the console using the `rich` library for formatting.",6490f398aca6df47b1a95e46a4ecfc0aabe299b5022de5c31ebc436e1bc3e459
context/generate_test_example.py,"This file is a Python script designed to demonstrate the usage of the `generate_test` function from the `pdd` library. It begins by importing necessary modules, including `os`, `generate_test`, and `rich` for formatted printing. The script defines several input parameters required for test generation: a natural language prompt requesting a factorial function, the corresponding Python source code for that factorial function, and configuration settings such as `strength`, `temperature`, and the target programming language. Inside a try-except block for error handling, the script calls `generate_test` with these parameters. Upon success, it prints the generated unit test code, the estimated cost of the operation, and the name of the model used, utilizing `rich` tags for colored output. If an exception occurs during the process, it catches the error and prints a formatted error message.",f73533a9cf63f2e2f4818ee923a8500e612d155e230017dbdb35aec945e41998
context/agentic_architecture_example.py,"This Python script serves as an example demonstration of how to utilize the `run_agentic_architecture` function within the `pdd` project. It is designed to simulate the workflow of generating a software architecture based on a GitHub issue without making actual API calls. The script begins by setting up the Python path to include the project root, ensuring that the `pdd` module can be imported correctly. The core of the script is the `main` function, which mocks the `run_agentic_architecture_orchestrator` using `unittest.mock.patch`. This mocking setup simulates a successful execution of an 8-step agentic workflow, returning predefined values for success status, a completion message, total cost, the model provider used (Anthropic), and a list of generated output files (such as `architecture.json` and `architecture_diagram.html`). The script then calls the actual `run_agentic_architecture` function with a dummy GitHub issue URL and verbose settings. Finally, it prints a structured summary of the results to the console, including the success status, cost, output files, and a message indicating the next steps for code generation.",dd66c9f68f5798dcb8f9f684da1bb88a6b2399e4e3b54b39e92a7d59576cc6be
context/continue_generation_example.py,"This Python script serves as a demonstration and entry point for utilizing the `continue_generation` function from the `pdd.continue_generation` module. The script defines a `main()` function that orchestrates the process of extending text generation using a language model. It begins by reading necessary input data from local files: a preprocessed prompt is loaded from `context/cli_python_preprocessed.prompt`, and an initial fragment of LLM output is read from `context/llm_output_fragment.txt`. 

The script sets specific configuration parameters for the generation process, including a `strength` of 0.915 and a `temperature` of 0. It then invokes the `continue_generation` function with these inputs and parameters, enabling verbose mode. Upon successful execution, the script captures the completed text, the total cost of the operation, and the model name used. It prints the cost and model name to the console and saves the final generated output to a file named `context/final_llm_output.py`. The code includes error handling to manage potential `FileNotFoundError` or other general exceptions that might occur during file operations or the generation process.",29c33a967bcdf4fa0037e1c7b0ec699c00027606dbe9d93fa4d1434f9c16c19e
context/langchain_lcel_example.py,"This Python script demonstrates the implementation of various Large Language Models (LLMs) and chat models using the LangChain framework. It showcases how to integrate and invoke models from multiple providers, including OpenAI (GPT-4o, o1, o4-mini), Google (Gemini via VertexAI and GenerativeAI), Anthropic (Claude), DeepSeek, Groq, Together AI, Ollama, AWS Bedrock, and MLX (Apple Silicon). 

The code emphasizes structured output generation using Pydantic models (`Joke` class) and parsers like `JsonOutputParser` and `PydanticOutputParser` to enforce specific data schemas (setup and punchline). It also illustrates the use of `PromptTemplate` and `ChatPromptTemplate` for constructing queries, as well as `RunnablePassthrough` and `ConfigurableField` for creating flexible chains. 

Additionally, the script includes a custom `CompletionStatusHandler` callback to track token usage and completion reasons. It demonstrates caching strategies using `SQLiteCache` to optimize performance. The examples cover diverse use cases, such as generating jokes, explaining technical concepts, and performing mathematical reasoning, while highlighting the transition from deprecated `LLMChain` methods to the modern `invoke` syntax and LCEL (LangChain Expression Language) pipelines.",cffe5f77babeb7ac27dc42383a99a03f5a9f55ae1d6f2472e5f9863df0a84daf
context/ctx_obj_params.prompt,"This file documents the standard parameters available within the `ctx.obj` dictionary for PDD CLI commands. It details the keys populated by the main CLI group, which include configuration settings such as `verbose` for debugging, `force` for overwriting files, `quiet` for suppressing output, and `local` for execution mode. Additionally, it specifies parameters for controlling Large Language Model (LLM) behavior, specifically `strength`, `temperature`, and `time` (relative thinking time). The document also outlines the resolution logic for `strength` and `temperature` parameters, explaining that explicit function arguments take precedence over the values stored in the CLI context (`ctx.obj`), allowing orchestrators to override global settings when necessary.",829e2ab327e37b75afcd0dcf3893dcaec9012a89a1f6e5198ec7cfc0395d2782
context/example.prompt,"This file outlines specific guidelines and constraints for generating code examples, particularly for a module located within a 'pdd' directory. It emphasizes that the environment is pre-configured with necessary variables and packages, so the focus should remain strictly on usage demonstration. Key instructions include using absolute imports (e.g., 'from pdd.module_name import module_name') because the example script runs from a different directory than the module itself. The guidelines specify that input and output units must be clearly documented, such as costs in dollars per million tokens. For command-line interface examples using Click, arguments must be passed directly to ensure non-interactive execution. Additionally, the instructions mandate that any file operations—whether reading required example content or writing outputs—must be directed to a specific './output' directory. Error handling via try/except blocks is explicitly discouraged to maintain clarity. Finally, it notes that the 'PDD_PATH' environment variable is pre-set, simplifying path resolution for the examples.",cf2eb7f018902c0ae9fbd70722cbd512236fd0de2aa15990b29cb65762fae321
context/python_preamble.prompt,"The provided text outlines a set of specific coding guidelines and requirements for an expert Python engineer. It mandates that all Python files begin with a future import for annotations and that every function must be fully type-hinted. For outputting text to the console, the instructions require the use of the `rich.console.Console` library instead of standard print statements. Regarding package structure, the guidelines specify the use of relative imports (e.g., single dot notation) for internal modules and reference a specific `__init__.py` file located in a `./pdd/` directory that houses global constants like `EXTRACTION_STRENGTH` and `DEFAULT_TIME`. Finally, the instructions emphasize robust error handling, requiring the code to manage edge cases such as missing inputs or model errors gracefully with clear, informative error messages.",f2d47592d2ca895fe3bc1071293dbbbfcb10dc922c2b762369f60ae827bc7e05
context/list_stats_example.py,"This Python script serves as a usage demonstration for a custom module named `list_stats`. It begins by dynamically adjusting the system path to locate and import the `list_stats` module from a specific relative directory structure. The core functionality is encapsulated within a `main` function, which defines a sample dataset of floating-point numbers and an empty list to showcase error handling. The script systematically executes and prints the results of four key statistical functions provided by the imported module: `safe_mean`, which calculates the average with a fallback for empty lists; `median`, which determines the middle value of the dataset; `variance`, demonstrating both population and sample variance calculations; and `percentile`, which computes specific quantiles such as the 90th and 25th percentiles. The code includes basic error handling for import failures and value errors during statistical computations.",46ae10331f07afe5a953030db2cb03cc02109518f4fa95b0b49dcfb784e85462
context/o4-mini-test.ipynb,"This file is a Jupyter Notebook containing a Python script that demonstrates how to interact with the Azure OpenAI API. The code initializes an `AzureOpenAI` client using specific credentials, including an endpoint URL, a deployment model name (""o4-mini""), an API version (""2024-12-01-preview""), and a subscription key. The script constructs a chat completion request with a sequence of messages simulating a conversation between a user and an assistant. The conversation context involves a user asking for travel recommendations in Paris, the assistant suggesting landmarks like the Eiffel Tower and the Louvre, and the user subsequently asking for more details about the Eiffel Tower specifically. The code then sends this request to the API and prints the content of the assistant's response. The notebook's output cell displays the generated response, which provides a detailed explanation of why the Eiffel Tower is significant, highlighting its architectural marvel, status as an iconic symbol, spectacular views, day-to-night experiences, and role as a cultural hub. The metadata indicates the notebook uses Python 3.12.9.",ce3de59ca52b7ca7623749f0a07887bd1dba25bc21da0b32a2a1c51c61b995f9
context/preprocess_main_example.py,"This Python script defines a command-line interface (CLI) tool using the `click` library to facilitate the preprocessing of prompt files. The script serves as an entry point for the `preprocess_main` function imported from the `pdd` package. It accepts a required `--prompt-file` argument specifying the input file path and offers several optional flags to customize the processing behavior. Users can specify an output path, enable XML delimiter insertion for better structure, recursively process referenced files, and double curly brackets while excluding specific keys. Additionally, the tool supports injecting PDD metadata tags from an architecture configuration and enabling verbose logging. The main `cli` function initializes the context object with default settings for strength and temperature, invokes the `preprocess_main` logic with the provided arguments, and prints the resulting processed prompt, total cost, and model name to the console. It includes error handling to catch and display exceptions that occur during execution.",dc4cf0483361ef94467d8936ceef30b54b62e61d658c916663407a8f142ec851
context/agentic_common_example.py,"This Python script serves as a demonstration for using the `agentic_common` module within the `pdd` package. It illustrates a workflow for orchestrating headless AI agents to perform file system tasks. The script begins by setting up the environment and verifying available agent providers (such as Claude, Gemini, or Codex) using `get_available_agents()`. It then defines a natural language instruction—specifically, asking an agent to create a Python script that calculates factorials—and executes this task via the `run_agentic_task` function. The script handles the execution results, printing success status, the specific provider used, estimated costs, and the agent's output. Furthermore, it includes a verification step to check if the requested file (`generated_math.py`) was successfully created in the specified output directory. Finally, the code references `STEP_TIMEOUTS` to demonstrate how to manage execution times for different stages of complex, multi-step workflows.",5c2a5da7c0281a5277e775bda0e64e1de379f546b59eef359179052fafe7dea9
context/llm_output_fragment.txt,"This file, `pdd.py`, implements a command-line interface (CLI) tool called ""pdd"" (Prompt-Driven Development) using the Python `click` library. It serves as the main entry point for a suite of AI-assisted development tools. The CLI defines a group of commands including `generate` (to create code from prompts), `example` (to generate example files), `test` (to create unit tests), `preprocess` (to handle prompt formatting and XML tagging), `fix` (to iteratively repair code and tests based on error logs), and `split` (to break down complex prompts). The tool supports global options for controlling AI model parameters like strength and temperature, output verbosity, and cost tracking. It utilizes the `rich` library for formatted console output and delegates specific logic to imported modules within the `pdd` package.",dc07a5094cc7e87b5536a2e3dc7e7e688c3f03b00c8b48dc422f97e28d7f9a59
context/fix_error_loop_example.py,"This Python script serves as a demonstration of the `pdd.fix_error_loop` module, illustrating how to automate the repair of buggy code using Large Language Models (LLMs) and unit tests. The script begins by setting up a temporary environment and creating mock files: a buggy Python file (`calculator.py`) that incorrectly implements an addition function, a failing unit test (`test_calculator.py`) that asserts correct addition logic, a prompt file describing the desired functionality, and a verification script to check for syntax errors.

The core of the script invokes the `fix_error_loop` function, configuring parameters such as LLM creativity (strength), temperature, maximum attempts, and a cost budget. This function orchestrates an iterative process: running the tests, detecting failures, sending the code and error logs to an LLM for analysis, applying the suggested fixes, and re-verifying. Finally, the script prints the results to the console, including whether the fix was successful, the number of attempts used, the total cost incurred, and the final corrected code. This example effectively showcases the module's ability to autonomously debug and repair code based on test feedback.",09aeef3c02612c54ba643feca7e2e692adaa1cec9fa1217a61507fec031929cc
context/conflicts_main_example.py,"This Python script serves as a demonstration or test harness for the `conflicts_main` function from the `pdd.conflicts_main` module. It begins by creating two sample prompt files, `prompt1_LLM.prompt` and `prompt2_LLM.prompt`, each containing slightly different instructions for an AI assistant. To simulate the command-line environment in which `conflicts_main` typically operates, the script defines a `MockContext` class that mimics a Click context object, although it initializes an empty configuration dictionary. The core of the script executes `conflicts_main` using this mock context, the two generated prompt files, and a specified output path for a CSV file. Finally, it prints the results of the execution to the console, including the detected conflicts, the total cost of the operation, and the name of the model used. The script notes that the detailed results are saved to `outputconflicts_output.csv`.",36e9f73fd888af26dc73f35ee765715e99e4325881bc0ffdc5d70106eda7e239
context/fix_code_loop_example.py,"This Python script serves as a demonstration and test harness for a module named `fix_code_loop`. It sets up a controlled environment to verify the functionality of an automated code repair system. The script defines a `create_dummy_files` function that generates a buggy Python file (`calculator.py`), a corresponding verification script (`verify_calculator.py`) that is designed to fail initially, and a prompt file. The main execution flow initializes these files in an `example_output` directory and configures a dictionary of parameters including file paths, LLM settings (temperature, strength), budget constraints, and retry limits. It then invokes the `fix_code_loop` function to attempt an automated repair of the buggy code. Finally, the script reports the results to the console using the `rich` library, displaying success status, total attempts, cost, and the final fixed code content.",c512f0f317474550a9ed74b5211ae0eab62c3f04115db14af67a88033aacfa87
context/fix_verification_errors_example.py,"This file is a Python example script demonstrating the usage of the `fix_verification_errors` function from the `pdd` package. The script illustrates an automated workflow for identifying and repairing bugs in code modules using Large Language Models (LLMs). It sets up a simulated scenario involving a buggy calculator module (which subtracts instead of adds) and a verification program that detects the failure. The script defines necessary inputs such as the program code, the original generation prompt, the buggy module code, and the captured verification output logs. It then calls `fix_verification_errors` with specific parameters for LLM strength and temperature. Finally, the script parses and displays the results returned by the function, including whether issues were found, if the code or program was updated, the specific fixes applied, the LLM model used, and the associated cost.",087d89544db864fe4c8025124ce2fd3fdf9f621c988bd415ba9a0ba942d2f497
context/operation_log_example.py,"This Python script serves as a comprehensive demonstration of the `operation_log` module, which provides logging infrastructure for tracking PDD (Prompt Driven Development) operations. The file illustrates how to manage operation logs, infer module identities, and handle state management within a development workflow.

The script defines three primary demonstration functions: `demonstrate_log_operations`, which shows how to load, create, update, and append log entries for both synchronous and manual CLI invocations; `demonstrate_module_identity`, which exhibits how to parse module names and languages from file paths; and `demonstrate_state_management`, which covers saving fingerprints, recording run reports, and clearing stale data.

Additionally, the file includes examples of integrating the logging system with the `click` CLI framework using the `@log_operation` decorator. This decorator automates the logging lifecycle—including initialization, execution tracking, and state updates—for commands like `generate` and `fix`. The script concludes with a main execution block that runs these demonstrations sequentially to verify functionality.",6cd7962ab255d6080aeb97897175250f987ce936cf7333e7601f10834405a940
context/code_generator_example.py,"This file is a Python script designed to demonstrate the usage of a `code_generator` function imported from the `pdd.code_generator` module. The script defines a `main` function that orchestrates the code generation process. It begins by reading a specific prompt from a file located at `prompts/generate_test_python.prompt`. It then configures several parameters for the generation process, including the target programming language (set to Python), the model strength, the temperature for randomness, and a verbosity flag. The script executes the `code_generator` function within a try-except block to handle potential errors gracefully. Upon successful execution, it prints the generated runnable code, the total cost of the operation, and the name of the model used to the console. The script follows standard Python practices by using the `if __name__ == ""__main__"":` block to ensure the `main` function runs only when the script is executed directly.",4e66ed9866b05a2de791376d4881ae5d1d52ac4a6ea99176dce8febdf9a14355
context/agentic_change_example.py,"This Python script serves as an example demonstration of how to utilize the `run_agentic_change` function within the `pdd` project. The script sets up the necessary environment by adding the project root to the system path and imports the core function. The `main` function simulates a workflow triggered by a GitHub issue URL. To avoid making actual API calls or executing complex logic during this demonstration, the script uses `unittest.mock.patch` to mock the internal `run_agentic_change_orchestrator`. It configures the mock to return a successful result, including a completion message, a simulated cost, the model provider used (Anthropic), and a list of modified files. Finally, the script executes `run_agentic_change` with verbose logging enabled and prints a structured summary of the results—including success status, cost, and changed files—to the console, advising the user on the next step of running 'pdd sync'.",be4da4499b5007ee7e8e7b464dc9f0cb5c9a04aaf5de192ae9cbb0133db06d85
context/pytest_example.py,"This Python script defines a custom test runner utility designed to execute `pytest` programmatically while capturing detailed execution metrics and logs. The core component is the `TestResultCollector` class, which acts as a pytest plugin. It hooks into the test execution lifecycle via `pytest_runtest_logreport` to count failures and errors during the setup, call, and teardown phases. Additionally, it implements `pytest_sessionfinish` to tally warnings reported by the terminal reporter. The class also includes methods to redirect `sys.stdout` and `sys.stderr` to an in-memory string buffer, allowing the script to capture all console output generated during the test run.

The script includes a `run_pytest` function that instantiates the collector, initiates log capturing, and runs `pytest.main` specifically targeting `tests/test_get_extension.py`. It ensures standard output streams are reset in a `finally` block to prevent interference with the main process. The script's entry point prints the current working directory and executes the test runner, subsequently printing the final counts of failures, errors, and warnings, along with the full captured logs, making it useful for CI/CD pipelines or debugging scenarios where programmatic access to test results is required.",f13c02d911d386ac5bab8e3d4aae64a768e4a7e960803c80fcac14e9ba5975cb
context/pytest_output_example.py,"This Python script serves as a comprehensive demonstration and test harness for a custom module named `pdd.pytest_output`. The script, designed to be run directly, showcases various ways to utilize the module's functionality for running Pytest suites and capturing their results. It begins by defining a helper function to generate dummy test files containing passing, failing, error-raising, and warning-triggering test cases. The `main_example` function then orchestrates six distinct examples. These examples illustrate: invoking the module's main entry point via simulated command-line arguments; programmatically running tests and saving the captured output to a JSON file; handling edge cases such as non-existent files or non-Python files; processing test files that contain no tests; and an advanced usage scenario where the `TestResultCollector` class is instantiated directly to manually capture logs and test outcomes. The script uses the `rich` library to format console output, providing clear visual separation between the different usage scenarios.",6f0cd68e3c4440081267c716651caec4fbdd7d9c4516f967517f02ae2a853920
context/change_example.py,"This Python script serves as a demonstration and entry point for utilizing the `change` function from the `pdd.change` module. It is designed to showcase how to programmatically modify code snippets using a Large Language Model (LLM) based on specific prompts and parameters. The script begins by importing necessary libraries, including `os`, the `change` function itself, and `Console` from the `rich` library for enhanced terminal output.

The core functionality is encapsulated within the `main` function. It sets up example inputs, including an initial prompt asking for a factorial function, the corresponding Python code implementation of that factorial function, and a subsequent `change_prompt` requesting a modification to calculate the square root of the factorial's result. It also defines configuration parameters such as `strength` and `temperature` to control the behavior of the underlying LLM.

The script executes the `change` function within a try-except block to handle potential errors gracefully. Upon successful execution, it uses the `rich` console to print the modified prompt generated by the model, the total cost of the API call, and the name of the model used. This file effectively acts as a test harness or usage example for developers integrating the `pdd` library.",cc5f850c8e41a8c5b82c94b6667975d1811dff1f51888bb8caf4ff69815f8352
context/final_llm_output.py,"This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` library. It serves as the main entry point for the application, orchestrating various AI-driven coding tasks. The CLI supports a range of commands including `generate` (creating code from prompts), `example` (generating examples from code/prompts), `test` (creating unit tests), `preprocess` (formatting prompts), `fix` (iteratively repairing code based on test errors), `split` (breaking down complex prompts), `change` (modifying prompts based on change requests), and `update` (syncing prompts with modified code). It also includes utility commands for version checking and installing shell completion. The script manages global configuration options such as AI model strength, temperature, and verbosity, and integrates with backend modules to handle file I/O, cost tracking, and the execution of specific AI logic for each command.",69322ebbb5b23b8d0f7c29f9f25fe28aff81a55cbed4fbd8be30652aa894cf47
context/agentic_e2e_fix_example.py,"This Python script serves as an example usage guide for the `agentic_e2e_fix` module within the `pdd` package. It demonstrates how to invoke the `run_agentic_e2e_fix` function, which orchestrates a 9-step automated workflow to fix GitHub issues. The script highlights cross-machine capabilities, noting that it can automatically locate worktrees on the same machine or detect branch mismatches when run on a different machine, providing safety checks unless overridden by a force flag. The `main` function sets up a sample call with parameters such as `issue_url`, `timeout_adder`, `max_cycles`, and `resume`, while also handling the return values to display success status, total cost, the model used, and any files changed.",9d78fd50a5bd3510f4a13caf19214e3d0e07f6149e59e666d35e6c2d0e0f4b1d
context/vertex_ai_litellm.py,"This Python script demonstrates how to utilize the `litellm` library to interface with Google Vertex AI's Gemini models. The code begins by importing necessary modules, including `completion` from `litellm`, `json`, and `os`. It enables debug logging for the library by setting the `LITELLM_LOG` environment variable. The script then retrieves the path to a Vertex AI credentials file from the `VERTEX_CREDENTIALS` environment variable, reads the JSON content from that file, and converts it into a JSON string format required for authentication. Finally, it executes a completion request using the `vertex_ai/gemini-2.5-pro-preview-05-06` model. This request includes a system and user message, passes the loaded credentials string, specifies the Google Cloud project ID (`meta-plateau-401521`), and sets the location to `us-central1`. The response from the model is then printed to the console.",ffbbe9dcdec01683ffe96cb1f027dbf1eb095f4f93158ffc2a47a443f2bbe699
context/auth_service_example.py,"This Python module, intended for use within the `pdd.auth_service` package, provides a suite of helper functions for managing authentication in a CLI environment. It handles the storage and retrieval of authentication tokens, specifically focusing on JWT caching via local files and refresh token management using the system keyring. Key functionalities include checking the validity of cached JWTs (`get_jwt_cache_info`, `get_cached_jwt`), verifying the existence of refresh tokens (`has_refresh_token`, `get_refresh_token`), and clearing credentials for logout purposes (`clear_jwt_cache`, `clear_refresh_token`, `logout`). Additionally, it offers high-level status checks (`get_auth_status`) and an asynchronous deep verification method (`verify_auth`) that can attempt token refreshes. The file concludes with extensive usage examples demonstrating how to integrate these functions into CLI commands for status checks, logging out, and making authenticated API calls.",9dc49bee224f626f94777a252ef3571d6d313feaf1262fae3a795b7a5d0d2c7c
context/sync_determine_operation_example.py,"This Python script serves as a demonstration and test harness for the `sync_determine_operation` function within a module named `pdd`. It simulates a project environment by creating a temporary directory structure and generating various file artifacts (prompts, code, tests, and metadata) to mimic different development states. The script walks through four distinct scenarios to verify how the system decides on the next operation: detecting a new unit (Scenario 1), handling test failures (Scenario 2), identifying manual code changes via hash mismatches (Scenario 3), and confirming a fully synchronized state where all hashes match and tests pass (Scenario 4). For each scenario, it sets up the necessary preconditions, calls `sync_determine_operation`, and prints the resulting decision and reasoning to the console.",851e0759a0d08e1a567879a98c9e6fd14b310c324a8c11e03bdfdaf562a543e7
context/gcs_hmac_test.py,"This Python script demonstrates how to upload a file to Google Cloud Storage (GCS) using the Amazon S3 interoperability API via the `boto3` library. The script begins by loading environment variables to retrieve necessary configuration details, specifically the GCS HMAC access key ID, secret access key, and the target bucket name. It configures logging to track the execution flow and validates that all required credentials are present before proceeding.

The core functionality involves creating an S3 client configured to point to the Google Cloud Storage endpoint (`https://storage.googleapis.com`) rather than AWS. It includes commented-out configuration options for newer `boto3` versions regarding checksums and addressing styles. The script then attempts to upload a simple text string as a file named `test-hmac-upload.txt` to the specified bucket. It includes robust error handling to catch and log specific `ClientError` exceptions, such as missing buckets or authentication failures, as well as general exceptions, ensuring the user is informed of the operation's success or failure.",965d3bb183e7e3898690b121a76ae0c57eb96425e7b670cecc50b829ea003bab
context/device_flow.txt,"This document outlines the GitHub Device Flow, an authorization method designed for headless applications such as CLI tools or the Git Credential Manager. To utilize this flow, it must first be enabled in the application's settings. The process involves three primary steps: requesting verification codes, user authorization, and polling for an access token.

First, the app sends a POST request to GitHub to obtain a device code, a user code, and a verification URL. The response includes a polling interval and an expiration time (default 15 minutes). Second, the user is prompted to enter the user code at the provided URL in a browser. Third, the application polls GitHub's token endpoint to check if the user has authorized the device. This polling must adhere to the specified minimum interval to avoid rate limits. If the app polls too frequently, it receives a 'slow_down' error, adding 5 seconds to the wait time. Upon successful authorization, the app receives an access token. The document also details various error codes, such as 'authorization_pending' and 'expired_token,' and notes a limit of 50 submissions per hour per application.",ae0cab6a414e4d7189c2025ef677fb7e6d1b800325f62bbb6de68a1505a5cdac
context/click_executor_example.py,"This Python script provides an example implementation for programmatically executing Click commands within a server environment, specifically tailored for a ""PDD Server."" It defines utilities to isolate command execution, capture standard output and error streams, and handle exceptions gracefully. Key components include an `OutputCapture` context manager that redirects streams to buffers (with optional real-time callback support), a `create_isolated_context` function to mock the Click CLI environment with shared state objects, and a `ClickCommandExecutor` class that orchestrates the execution process. The script also includes a mock command factory (`get_pdd_command`) and a `main` function demonstrating usage scenarios, such as streaming output, handling errors, and injecting parameters into commands.",322d322bcbe2672e9adb1d52744018f38ae6ae3209711441816f923727760b8e
context/no_include_conflicts_in_prompts_python.prompt,"This file outlines the specifications for an AI assistant to generate a Python function named ""conflicts_in_prompts"". The purpose of this function is to analyze two input prompts, identify any conflicting instructions or logic between them, and provide suggestions for resolution. The function takes two prompt strings, a model strength parameter (defaulting to 0.5), and a temperature setting (defaulting to 0) as inputs. It is required to utilize Langchain and a custom `llm_selector` module to process the prompts in a multi-step workflow. This workflow involves loading specific prompt templates from a project path, running an initial LLM pass to detect conflicts, and then performing a second extraction pass to format the output into a structured JSON list. The final output must include a list of conflict dictionaries—each detailing the conflict description, explanation, and resolution suggestions for both prompts—along with the total cost of the operation and the name of the model used.",09137b6ae364af16795a3066a06d28d6ebfdc7d42d83fad4c068e75bfbad77ab
context/whitepaper.md,"This white paper introduces Prompt-Driven Development (PDD), a transformative software engineering methodology that elevates high-level prompts to the central artifact of the development process. Arguing that traditional and even AI-assisted coding methods are struggling with increasing complexity and technical debt, the paper proposes PDD as a solution to align software more closely with business objectives. In PDD, developers focus on crafting detailed prompts describing intent and requirements, while advanced AI models generate the actual code, tests, and documentation. The document outlines the technical workflow, compares PDD favorably against traditional methods regarding productivity and scalability, and details specific strategies for adoption. Crucially, it addresses potential challenges—such as debugging AI-generated code and handling manual code tweaks—by describing a suite of PDD-specific CLI commands (e.g., `pdd trace`, `pdd update`) that maintain synchronization between prompts and code. Ultimately, the paper presents PDD not merely as a tool, but as a necessary evolution in software engineering that fosters better collaboration, higher abstraction, and future-proof development practices.",e8bd279d08e21590bf548de02f5cc9486c868b534efbba4051ed3f324bcff975
context/firecrawl_example.py,"This file contains a Python code snippet demonstrating the basic usage of the `firecrawl-py` library for web scraping. It begins with a comment instructing the user to install the package via pip. The script imports the `FirecrawlApp` class and the `os` module. It then initializes an instance of `FirecrawlApp`, attempting to retrieve the API key from an environment variable named `FIRECRAWL_API_KEY`, with a fallback placeholder provided. Finally, the code uses the `scrape_url` method to scrape content from 'https://www.google.com' in markdown format and prints the resulting markdown output to the console.",9dbfc798c1670a244e5c2d8822e065ce2800c2d861dcd4e4e2a02874e9de854f
context/agentic_bug_orchestrator_example.py,"This Python script serves as an example and simulation for the `agentic_bug_orchestrator` module. It demonstrates how to invoke the `run_agentic_bug_orchestrator` function without requiring live LLM calls or a real GitHub repository. The script sets up a mock environment by patching internal dependencies—specifically `load_prompt_template` and `run_agentic_task`—using the `unittest.mock` library. 

The simulation creates a scenario involving a ""ZeroDivisionError"" in a calculator application. It defines a `mock_run_agentic_task` function that returns pre-determined success responses for an 11-step bug investigation workflow, covering steps such as duplicate checking, reproduction, root cause analysis, test generation, and PR creation. The `main` function initializes dummy issue data (e.g., issue URL, content, and repository details) and executes the orchestrator within the patched context. Finally, it prints the results of the simulation, including the success status, final message, total simulated cost, model used, and a list of changed files, verifying that the orchestrator logic flows correctly.",ad61cef410a3fe8b20e083d10cebc6ffdc13a2e83e9484e7ca919993c46b24d5
context/context_generator_main_example.py,"This Python script serves as a demonstration and test harness for the `context_generator_main` function within the `pdd` package. It defines a `run_example` function that simulates a typical workflow for generating example code based on a prompt and existing source code. The script first sets up a local output directory and creates dummy input files: a prompt file (`math_utils_python.prompt`) describing a math utility task, and a corresponding source code file (`math_utils.py`). It then mocks a `click.Context` object to simulate command-line arguments and global configuration settings such as model strength, temperature, and verbosity. The core logic executes `context_generator_main` using these inputs to generate a new example file (`math_utils_example.py`). Finally, the script prints the generation results, including the model used, the calculated cost, and a snippet of the generated code. The main execution block also ensures necessary environment variables are mocked if missing.",77c3de786f31392540ec609d4b3942e98e996ae14c4014f5018cf2b05a560df4
context/split/9/cli_python.prompt,"The provided file is a detailed prompt designed for an LLM to generate a Python command-line interface (CLI) tool named `pdd`. The prompt instructs the AI to act as an expert Python engineer and utilize the `click` library for the CLI structure. It outlines the program's directory structure and references external files for the program description and specific implementation details. The core of the prompt consists of instructions and examples for implementing various commands, including `change`, `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `update`, `detect`, `conflicts`, `crash`, `trace`, `bug`, and `auto-deps`. It provides specific guidance on using internal modules like `auto_update`, `track_cost`, and `construct_paths`. Additionally, it includes instructions for an `install_completion` command to handle shell completion scripts robustly and mandates consistency across all commands.",e587e1338016f4702249d76538ccab4778670edde1a3000a983cc9fcfbed3909
context/split/9/install_completion.py,"The provided file content is a brief code snippet defining a Python function named `install_completion`. This function is decorated with `@cli.command(name=""install_completion"")`, indicating that it is likely part of a command-line interface (CLI) application, possibly built using a library like `click` or `typer`. The decorator registers the function as a command named ""install_completion"" within the CLI structure. The purpose of this command, based on its name, is presumably to install shell completion scripts, which allow users to use tab-completion for the CLI tool in their terminal environment (e.g., Bash, Zsh, or Fish). The snippet ends at the function definition line, so the actual implementation logic for installing these scripts is not visible.",1eac80184769c28eccb35e1baba1eeded8b464cd01de7129c386d9a28ca50cd1
context/split/9/sub_cli.prompt,"This file contains a detailed prompt for an AI coding assistant to generate a Python function named `install_completion` for the `pdd` command-line program. The prompt instructs the AI to act as an expert Python engineer and outlines the specific logic required for the function, which handles the installation of shell completion scripts. Key steps include detecting the user's current shell (using `ps`, `$0`, or environment variables), locating the appropriate completion script, and updating the user's shell configuration file (e.g., `.bashrc`, `.zshrc`) to source the script. The prompt specifies error handling requirements, such as using `click.Abort()` and `rprint` for colored output, and lists necessary helper functions like `get_current_shell`, `get_shell_rc_path`, and `get_local_pdd_path`. It also includes placeholders for context files like a Python preamble and usage examples for the Click library.",278c01b32695237e99ecac7be5d17556c458277bea94722c2a3c6f231f512d70
context/split/9/cli.py,"This file serves as the main entry point for the PDD (Prompt-Driven Development) Command Line Interface. It defines a `click`-based CLI application that orchestrates various development tasks driven by AI prompts. The script initializes the CLI context with global options like AI model strength, temperature, and verbosity. It then defines and exposes multiple sub-commands including `generate` (creating code from prompts), `test` (generating unit tests), `fix` (repairing code based on errors), `crash` (fixing crashing programs), `update` (syncing prompts with code changes), and `auto_deps` (managing project dependencies). Additionally, it includes utility functions for runtime path detection, shell completion installation, and automatic self-updates. The file acts as a central dispatcher, importing logic from specific sub-modules to execute the requested development workflows.",d4f1f5bb29900f56ef676ca2cf14f17f69603696455ed1a692e25540a3e778b8
context/split/9/modified_cli.prompt,"This file serves as a comprehensive prompt or specification for an AI to generate the source code for a Python command-line interface (CLI) tool named `pdd`. The tool is to be built using the Python `Click` library and is structured around a main `cli` function. The document outlines the program's directory structure and includes references to external files (via `<include>` tags) for the README, Python preamble, and specific implementation examples.

The core of the file provides detailed instructions on implementing various sub-commands by referencing internal modules and example usage patterns. These sub-commands include `change`, `generate` (code generation), `example` (context generation), `test` (unit test generation), `preprocess`, `fix` (error correction), `split`, `update`, `detect` (change detection), `conflicts`, `crash` (crash analysis), `trace`, `bug` (bug-to-test conversion), `auto-deps`, and `install_completion`. It also specifies utility functions like `auto_update`, `track_cost` (a decorator), and `construct_paths`. Finally, it instructs on handling environment variables like `PDD_PATH` and `PDD_AUTO_UPDATE`.",8b8c7363455b4cb630ef90f7faf248227f5a7a2e5ef74d03f8c4e35814bf3086
context/split/7/modified_cli_python.prompt,"This document provides a comprehensive prompt for an AI to generate a Python command-line interface (CLI) tool named `pdd`. The tool is to be built using the `click` library and follows a specific directory structure. The prompt details the implementation of numerous commands, including `generate` (for code generation), `example` (for context generation), `test` (for unit test creation), `preprocess` (for prompt processing), `fix` (for error correction), `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`. It provides specific instructions on importing internal modules to avoid naming conflicts, using the `@track_cost` decorator for cost tracking, and handling file paths via a `construct_paths` helper. Additionally, it includes instructions for an `install_completion` command to manage shell completion scripts.",4bd93aa7ba1298dfaab089c32e39fe5082d1c3e4166456c54692b6f16e74a71e
context/split/7/cli_python.prompt,"This file contains a detailed prompt for an AI to generate a Python command-line interface (CLI) tool named `pdd`. The tool is to be built using the `click` library and serves as a developer utility with various commands for code generation, testing, and maintenance. The prompt outlines the directory structure, provides examples for using `click`, and details specific instructions for implementing numerous commands (e.g., `generate`, `test`, `fix`, `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, `bug`). It explicitly lists internal modules and functions to be used for each command, often requiring specific import aliasing to avoid naming conflicts. The prompt also includes placeholders for context files (like `python_preamble.prompt` and `README.md`) and usage examples for internal functions like `track_cost`, `construct_paths`, and `code_generator`.",198890ab03025d5e8585b77375cd396d29d6961c758bfd96627c94c83c5aec07
context/split/7/trace_main_python.prompt,"This file contains a detailed specification for creating a Python function named `trace_main`, which serves as the core logic for the `trace` command within the `pdd` command-line interface. The function is designed to link a specific line of generated code back to its corresponding line in a prompt file. The specification outlines the required inputs, including the Click context, file paths for the prompt and code, the target code line number, and an optional output path. It defines the expected output as a tuple containing the prompt line number, operation cost, and model name. The document provides a step-by-step implementation guide, instructing the developer to construct file paths, load file contents, perform the trace analysis using an internal `trace` function, save the results, and provide user feedback via the Rich library. It also emphasizes robust error handling and adherence to existing coding styles, including type hinting and docstrings. Examples for using the Click library and internal modules like `construct_paths` are referenced to aid implementation.",dfd02a104fd1e4f96aeb0feacd9bf17091e54924544d748b09ce03ce3039fee5
context/split/7/cli.py,"This file defines the command-line interface (CLI) for a tool called PDD (Prompt-Driven Development). It utilizes the `click` library to structure the CLI and `rich` for enhanced console output. The main entry point is the `cli` group, which sets global options like AI model strength, temperature, and verbosity. The file implements numerous commands that orchestrate various AI-driven development tasks, including `generate` (creating code from prompts), `example` (generating examples from code), `test` (creating unit tests), `preprocess` (formatting prompts), `fix` (iteratively repairing code based on errors), `split` (breaking down complex prompts), `change` (modifying prompts), `update` (syncing prompts with code changes), `detect` (identifying necessary prompt changes), `conflicts` (resolving prompt conflicts), `crash` (fixing crashing modules), `bug` (generating tests from bug reports), and `trace` (linking code lines back to prompt lines). Each command typically handles file I/O, invokes specific backend logic imported from other modules, tracks costs, and outputs results to the user.",d62918ef5903ffb5fb0b194a9321cf8742f2d51ace5e2ee3bec9e9e0e8d266bd
context/split/7/split_trace_main.py,"The provided code snippet defines a command-line interface (CLI) command named `trace` using the `click` library. This command is designed to identify the relationship between a specific line in a prompt file and a corresponding line in a generated code file. It accepts three required arguments: `prompt_file` (a path to an existing prompt file), `code_file` (a path to an existing code file), and `code_line` (an integer specifying the line number in the code). Additionally, it supports an optional `--output` flag to specify a destination for saving trace analysis results. The function is decorated with `@track_cost` and `@click.pass_context`. The implementation of the command delegates its core logic to a separate function called `trace_main`, returning a tuple containing a string, a float, and another string.",d9a7d733a1fd74b5d3d0e25c298bef38b6951880f0f86f645cfe07cde092905f
context/split/6/modified_cli_python.prompt,"This document outlines the specifications for a Python command-line tool named `pdd`, built using the `click` library. It serves as a prompt-driven development utility with a variety of commands for managing code generation, testing, and prompt manipulation. Key features include generating code from prompts (`generate`), creating examples from code (`example`), generating unit tests (`test`), and preprocessing prompts (`preprocess`), with specific sub-commands for XML tagging. It also includes functionality for fixing errors (`fix`), splitting prompts (`split`), changing prompts (`change`), and updating prompts based on code changes (`update`), including Git integration. Advanced features cover conflict detection between prompts (`conflicts`), crash analysis (`crash`), execution tracing (`trace`), and converting bugs to unit tests (`bug`). The document provides instructions on internal module usage, emphasizing the need to alias imported functions (e.g., `preprocess_func`) to avoid naming conflicts with CLI commands, and details directory structure and shell completion installation.",11221fcb359e67d91ba305ba8c1c428def99b81ba9422d7fe5954171d2962e85
context/split/6/cli_python.prompt,"This file contains a detailed prompt for an LLM to generate a Python command-line interface (CLI) tool named `pdd`. The tool is to be built using the `click` library and serves as a developer utility for managing prompts, generating code, running tests, and fixing errors. The prompt outlines the directory structure, naming conventions to avoid conflicts between CLI commands and internal functions, and provides specific instructions for implementing various commands such as `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`. It includes references to numerous internal modules and example files demonstrating how to use specific functions like `track_cost`, `construct_paths`, and `code_generator`.",af6414f4512dc36571632bacff6d084783df93bd88319774470b5abaeda55870
context/split/6/split_conflicts.py,"The provided code snippet defines a command-line interface (CLI) command named `conflicts` using the `click` library. This command is designed to analyze two input prompt files, identified by the arguments `prompt1` and `prompt2`, to detect potential conflicts between them and suggest resolutions. It also accepts an optional `--output` argument to specify a destination for saving the analysis results as a CSV file. The function is decorated with `@track_cost`, implying it tracks resource usage or cost associated with the operation. The implementation of the command is minimal, serving as a wrapper that delegates the actual processing logic to a separate function called `conflicts_main`, passing along the context and arguments. The return type hint indicates that the function returns a tuple containing a list of dictionaries, a float, and a string.",ed275ba6d2c4742df3bab8eb78f6cb38b855677364244b0fc205625d73c579ee
context/split/6/conflicts_main_python.prompt,"This file contains a prompt specification for an expert Python engineer to generate a function named `conflicts_main`. This function is intended to be a core component of a command-line interface (CLI) tool called `pdd`, specifically handling the logic for a `conflicts` command. The prompt outlines the function's inputs (Click context, two prompt file paths, and an optional output path) and expected outputs (a tuple containing analysis results, cost, and model name). It provides detailed implementation steps, including constructing file paths using internal utilities, loading file contents, analyzing conflicts via a `conflicts_in_prompts` function, saving the results to a CSV file, and providing user feedback using the Rich library. The prompt also includes references to context files for Python preambles, Click library usage, and internal module examples to guide the code generation process.",3a99cdbb40bf5d080247c2417133cb5d811219d3283f5d7c816149c1ed69388a
context/split/6/cli.py,"This file defines the Command Line Interface (CLI) for a Prompt-Driven Development (PDD) tool. It utilizes the `click` library to structure the CLI and `rich` for formatted console output. The script exposes a main command group `cli` with global options for controlling AI model parameters (strength, temperature), verbosity, and cost tracking. It implements numerous subcommands that orchestrate various AI-assisted development tasks, including `generate` (creating code from prompts), `example` (generating examples from code), `test` (creating unit tests), `preprocess` (formatting prompts), `fix` (iteratively repairing code based on errors), `split` (breaking down complex prompts), `change` (modifying prompts), `update` (syncing prompts with code changes), `detect` (identifying necessary changes), `conflicts` (resolving prompt conflicts), `crash` (fixing crashing modules), `bug` (generating tests from bug reports), and `trace` (linking code lines back to prompt lines). Each command typically handles file I/O, invokes specific backend logic functions, and tracks the cost of AI model usage.",aeaaceab84a98e9cb777d691f1a9de4aa3538569ce0fa0b732228c8dfbae51cf
context/split/1/sub_pdd_python.prompt,"The file outlines a programming task for an expert Python engineer to create a function named ""get_extension"". This function is designed to accept a programming language name (e.g., Bash, Python) as a string input and return the corresponding file extension. The implementation steps specified include converting the input string to lowercase to ensure case-insensitive comparison, performing a lookup to find the correct extension associated with the language, and finally returning that extension as a string.",45146709fd9f1032380de677025bc37b7cdfb5c6e9a4269ff7ddaf0d0e6be562
context/split/1/split_get_extension.py,"The provided file content consists of a single line of code, likely written in Python or a similar high-level programming language. This line performs a variable assignment operation. Specifically, it calls a function named 'get_extension' and passes a variable or object named 'language' as an argument to it. The return value of this function call is then stored in a variable named 'file_extension'. The purpose of this code snippet appears to be utility-focused, aiming to determine the appropriate file extension associated with a specific programming language. For example, if the 'language' variable held the value 'Python', the 'get_extension' function would likely return '.py'. This snippet is likely part of a larger script or module responsible for file management, code generation, or input processing where handling different file types based on language context is necessary.",9e3fc213b536b755722c8c4c407d92e88d22e64ab0d491b0c5e1d22b746f2500
context/split/1/final_pdd_python.prompt,"The file outlines the specifications for a Python command-line tool named ""pdd"" designed to compile prompts into code files or generate example code from existing code files. The program utilizes the Python ""rich"" library for console output and handles input files typically formatted as ""basename_language.prompt"". 

The tool supports flexible output configurations, allowing users to specify filenames, paths, or rely on defaults derived from the input basename. Key features include a ""--force"" flag for overwriting existing files without confirmation, and specific flags (""-o"" and ""-oe"") to control the destination of runnable code and generated examples, respectively. 

The workflow involves reading the input filename, determining the language and extension, and calculating output paths based on user arguments. If the input is a prompt file, ""pdd"" generates runnable code using a ""code_generator"". If the input is a code file or if the example flag is set, it generates example code using a ""context_generator"". The specification includes detailed logic for handling file extensions, directory resolution, and step-by-step execution instructions for the resulting program.",4337784a5517d6e80fac11df769749c916c003261b27b36d6d5e6c7b580af78c
context/split/1/pdd.py,"The provided file contains the implementation of a command-line program named ""pdd,"" written in Python. This tool is designed to facilitate a workflow involving prompt files and code generation. It utilizes the `argparse` library to handle command-line arguments and the `rich` library for enhanced console output. The core functionality includes compiling "".prompt"" files into runnable code (supporting languages like Python, Bash, and Makefile) and generating example usage code from existing source files. The script defines helper functions for determining file paths and extensions, confirming file overwrites, and orchestrating the generation process via imported `code_generator` and `context_generator` modules. The file also includes comments explaining the code structure, usage instructions, and necessary dependencies.",e748fdf06a3f0bfed6c18bdcb1d7c8a7a78c18256dfb51adfd99086c3a14f2cc
context/split/1/initial_pdd_python.prompt,"The file outlines the specifications for a Python command-line tool named ""pdd"" designed to compile prompts into code files or generate example code from existing code files. The tool utilizes the Python `rich` library for console output and handles input files typically formatted as 'basename_language.prompt'. 

The program supports flexible output configurations, allowing users to specify filenames, paths, or rely on defaults derived from the input basename. Key features include a `--force` flag for overwriting existing files, an `-o` option for specifying the runnable code output path, and an `-oe` option for generating and directing example code. 

The workflow involves reading the input filename, determining the language and extensions, and calculating output paths based on user arguments. If the input is a prompt file, `pdd` generates runnable code using a `code_generator`. If the input is a code file or if the `-oe` flag is used, it generates example code using a `context_generator`. The tool intelligently handles file extensions (e.g., .py, .sh, makefile) based on the extracted language context.",0900e3d8d148d17b913e323855ca7cf60e70d2c1bf9600183c5f3ae2e3dc1dba
context/split/8/cli_python.prompt,"The provided file is a detailed prompt designed to instruct an LLM to generate a Python command-line interface (CLI) tool named `pdd`. The tool is built using the Python `Click` library and serves as a developer utility for managing prompt-driven development workflows. The prompt outlines the directory structure, imports, and specific functionality for various commands including `generate` (creating code from prompts), `test` (generating unit tests), `preprocess` (handling prompt files), `fix` (debugging code), `split` (dividing prompts), `change` (modifying prompts), and `trace` (tracing execution). 

It provides explicit instructions on how to structure the `cli.py` file, emphasizing the need to alias imported functions to avoid naming conflicts with Click commands. The prompt includes references to numerous internal examples (e.g., `track_cost`, `construct_paths`, `code_generator`) to guide the implementation of specific features. Additionally, it details requirements for sub-commands like `update --git` and `fix --loop`, and includes instructions for an `install_completion` command to handle shell integration. The overall goal is to produce a robust, modular CLI entry point that orchestrates various underlying development tools.",f2f50720c4bef8c93dad2b7d42677eeaffae49f17052f40a0232f621296179bf
context/split/8/change_main_python.prompt,"This file contains a detailed prompt for an AI coding assistant to generate a Python function named `change_main`. This function is intended to be part of a command-line tool called `pdd` and specifically handles the logic for a `change` command. The prompt outlines the function's inputs (including Click context and file paths for prompts, code, and CSVs) and expected outputs (modified prompt string, cost, and model name). It provides specific implementation steps: parsing arguments, constructing file paths using helper functions, performing prompt modifications (either singly or in batch via CSV), saving results, and providing user feedback via `rprint`. The prompt also includes references to context files for Python preambles and examples of internal module usage (`construct_paths`, `change_func`, `process_csv_change`), emphasizing adherence to existing code styles and error handling practices.",fb4558a1149c68efd12212af23e22c3d1a93e744acc07f7c4b7699e71f5146cf
context/split/8/cli.py,"This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` library. It serves as the main entry point for the application, orchestrating various AI-driven development tasks. The CLI exposes a suite of commands including `generate` (creating code from prompts), `example` (generating examples from code), `test` (creating unit tests), `preprocess` (formatting prompts), `fix` (iteratively repairing code based on errors), `split` (breaking down complex prompts), `change` (modifying prompts based on instructions), `update` (syncing prompts with code changes), `detect` (analyzing necessary changes), `conflicts` (resolving prompt conflicts), `crash` (fixing crashing modules), `bug` (generating tests from bug reports), and `trace` (linking code lines to prompt sections).

The script handles global configuration options such as AI model strength, temperature, verbosity, and cost tracking. It utilizes the `rich` library for formatted console output. Each command typically reads input files, invokes specific backend logic (imported from other modules), tracks API costs via a decorator, and writes the resulting artifacts (code, tests, or analysis) to specified output paths.",8950b0a13caf9936d281dde770fb572bec99060800e2c0b1d68a5ca7fa631f3a
context/split/8/change_main.py,"The provided file content defines a command-line interface (CLI) command named `change` using the `click` library in Python. This command is designed to modify an input prompt file based on a specified change prompt and corresponding input code. The function accepts several arguments: optional file paths for an input prompt, input code, and a change prompt, as well as options for specifying an output destination and using a CSV file for change prompts. The command is decorated with `@cli.command()`, argument and option parsers, context passing, and a cost tracking decorator `@track_cost`. The implementation of the command delegates its core logic to a separate function called `change_main`, returning a tuple containing a string, a float, and another string.",549e436acc4baf223c73decdf493d11dd9ab063bb938dea05b001efac351a13a
context/split/8/modified_cli.prompt,"This file contains a comprehensive prompt for an AI to generate a Python command-line interface (CLI) tool named `pdd`. The tool is designed to assist with software development tasks, specifically focusing on prompt engineering and code generation workflows. The prompt outlines the directory structure, imports, and specific instructions for implementing various commands using the Python `click` library. Key commands include `generate` (creating code from prompts), `test` (generating unit tests), `fix` (debugging code), `preprocess` (handling prompt files), `update` (modifying prompts based on code changes), and `trace` (tracing execution). It provides detailed examples of internal module usage for each command, such as `track_cost` for monitoring usage and specific logic for handling git integration, error loops, and shell completion installation. The instructions emphasize using specific internal functions for core logic to ensure modularity and consistency.",34a0b2ec2995bf377ad36ee35316bd38828b4147eefd7100da401d7c81149e5e
context/split/simple_math/split.prompt,"The provided file content consists of a concise directive aimed at refactoring code. Specifically, it instructs the developer to isolate the existing input validation logic and move it into a dedicated helper function. This change is likely intended to improve code modularity, readability, and reusability by separating the validation concerns from the main execution flow.",db775c89b5841e554d0e476e211514248525612ada2d78fd1f9ab2329c864944
context/split/simple_math/split_script.py,"The provided file content is a brief Python script snippet. It begins with a comment indicating that it serves as a placeholder for a split command script, suggesting it might be part of a larger codebase or a template for future development. The core functionality present in the file is a single function definition named 'add'. This function takes two arguments, 'a' and 'b'. Before performing any operation, the function implements type checking to ensure that both inputs are either integers or floating-point numbers. If either argument fails this check, the function raises a TypeError with a descriptive message. If the type checks pass, the function returns the sum of the two numbers. The code demonstrates basic defensive programming practices by validating inputs before execution.",74e08259f6febb7c93e191d0687e3fb5c30358b920e108d6798146fb9141c0e3
context/split/simple_math/split_example.py,"This file contains a Python script designed to demonstrate and test the functionality of an imported module named `split_script`. The script primarily focuses on invoking an `add` function from this module within error-handling blocks. It first attempts a valid operation by passing two integers (10 and 20) to the `add` function and printing the resulting sum. Subsequently, it attempts an invalid operation by passing an integer (5) and a string (""x"") to the same function, anticipating a `TypeError`. Both operations are wrapped in `try...except` structures to gracefully catch and print any type-related errors that occur during execution. The code serves as a basic usage example or test case for the `split_script` module.",68347c89811c07ed8503dd519f44e6ddd88e0c937829b865180c94664dbcd701
context/split/4/construct_paths.py,"This Python script provides utility functions for handling file paths and I/O operations within a command-line interface (CLI) application. The core functionality is encapsulated in two main functions: `generate_output_filename` and `construct_paths`. The `generate_output_filename` function dynamically creates filenames based on the specific command being executed (e.g., 'generate', 'test', 'fix'), the base filename, and the programming language extension. The `construct_paths` function orchestrates the file handling workflow: it validates and standardizes input file paths, reads their contents into memory, and determines the appropriate output paths. It also attempts to infer the programming language and file extension from the input filename. Additionally, the script includes logic to handle user interaction, such as printing status messages (unless silenced by a 'quiet' flag) and prompting the user for confirmation before overwriting existing output files, ensuring safe file operations.",c227dc44f90465b64f94000958c11eb8885854466becf403a7f19e61519b50d2
context/split/4/final_construct_paths_python.prompt,"The file outlines the requirements for creating a Python function named `construct_paths`, intended for use within a CLI tool called `pdd`. This function is responsible for managing file I/O operations, specifically generating and validating input and output file paths, and loading input file contents. It takes inputs such as a dictionary of file paths, a force overwrite flag, a quiet mode flag, the specific command being run, and command options. The function must handle file extensions intelligently (adding defaults like '.prompt' if missing), extract metadata like language and basename from filenames, and utilize helper functions like `get_extension` and `generate_output_filename`. It is required to use the `rich` library for pretty-printed console output and `click` for CLI interactions. The process involves four main steps: constructing input paths, loading file content into a dictionary while handling errors, constructing output paths according to specific rules, and checking for existing files to prompt for overwrite confirmation unless forced. Finally, it returns the loaded input strings, the determined output paths, and the detected language.",4035ae675b3bb8b6620db80e43c284abf0036ceb8df132741038424897aa9338
context/split/4/sub_construct_paths_python.prompt,"The file outlines a task for an expert Python software engineer to implement a function named `generate_output_filename`. This function is designed to construct specific output filenames based on the context of a command being executed within a Python program. It requires five input parameters: the command name (e.g., 'generate', 'test'), a dictionary key, the file's basename, the programming language, and the file extension. The core logic involves a set of rules for formatting the filename string depending on the `command` argument. For instance, 'generate' simply appends the extension to the basename, 'test' prefixes the basename with 'test_', and 'preprocess' appends language information and a '.prompt' extension. The 'fix' command has conditional logic based on the `key` parameter. A default fallback format is provided for unrecognized commands. The implementation must be robust, handling edge cases efficiently.",5cfc774bcf11370f270d03d4c7336e41ea519f9915c60c76a47ce8b4bd25ceee
context/split/4/initial_construct_paths_python.prompt,"The provided text outlines the requirements for a Python function named `construct_paths`, designed for a software tool called `pdd`. Acting as an expert Python engineer, the function's primary role is to manage file I/O operations, specifically generating and validating file paths and loading input content. It utilizes the `rich` library for pretty-printed console output and `Click` for CLI interactions. The function accepts inputs such as a dictionary of input file paths, boolean flags for `force` (overwrite) and `quiet` (suppress output), the specific `pdd` command run, and its options. It returns a dictionary of loaded input strings, a dictionary of constructed output file paths, and the detected programming language. Key logic includes appending appropriate extensions (e.g., `.prompt`) if missing, extracting language and basenames for the `generate` command, and handling file naming conventions. The process involves four main steps: constructing input paths, loading files with error handling, constructing output paths via a sub-function according to specific rules, and checking for existing files to prompt the user for overwrite permission unless forced. Console feedback is provided throughout unless silenced.",5c0e63adb81aa695a26d555aa6bedc5eb632bcaca2dd5fa8d701d7ed8bcab88f
context/split/4/split_construct_paths_generate_output_filename.py,"The provided code snippet demonstrates a function call to `generate_output_filename`, which is designed to construct a filename based on several parameters. The function accepts five arguments: `command` (representing a specific operation or command type), `key` (identifying a specific output category or dictionary key), `basename` (the core name of the file), `language` (specifying the programming language or locale), and `file_extension` (indicating the file type). Each argument is accompanied by an inline comment explaining its purpose, suggesting this snippet serves as documentation or an example of how to properly invoke the filename generation utility within a larger codebase.",2c39060071cc48ca5baab6467c4c4cbcfa5e7cb8eb9e1d05d2b0a892f4f5185a
context/split/3/split_postprocess_find_section.py,"The provided file content consists of a Python code snippet demonstrating a function call to `find_section`. This function appears to be designed for parsing or processing text data, specifically structured around identifying sections within a list of text lines. The function takes three arguments: `lines`, which is described via a comment as an array of text strings (splitlines); `start_index`, an integer defaulting to 0 that specifies the starting row for the search; and `sub_section`, a boolean flag defaulting to `False` that indicates whether the search is for a nested or sub-section. The code assigns the result of this operation to a variable named `sections`. The snippet serves as an example of how to invoke this specific text-processing utility, likely part of a larger parser or document analysis tool.",31bd47832cd2fb2ad1787a9d7b4064af1d57dc3dfba26b64dbd7d15a78cc571f
context/split/3/postprocess.py,"This file defines a post-processing utility for handling output from a Large Language Model (LLM), specifically focusing on extracting and isolating code blocks of a target programming language. It imports helper functions `get_comment` and `comment_line` from the `pdd` package to handle language-specific commenting syntax.

The core logic is contained in two functions. The `find_section` function parses a list of strings to identify markdown-style code blocks (delimited by triple backticks), returning a list of tuples containing the language identifier and the start/end line indices. The main function, `postprocess`, takes the raw LLM output and a target language as input. It first retrieves the appropriate comment characters for that language. Then, it scans the output for code blocks matching the target language, identifying the largest such block. Finally, it reconstructs the output string by preserving the content of the largest matching code block while commenting out all other lines—including text outside code blocks and code blocks in other languages. If no matching block is found, the entire output is commented out. This ensures the result is valid, executable code in the desired language.",7bd1d2cf74436558a0fa14db3d7fc31abf6fcced57faa1d875a70a5566be23ae
context/split/3/initial_postprocess_python.prompt,"The file outlines a specification for a Python function named `postprocess` designed to sanitize Large Language Model (LLM) output into executable code. The function takes an `llm_output` string (containing mixed text and code blocks) and a target `language` (e.g., python, bash) as inputs. Its primary goal is to return a string where all conversational text and irrelevant code blocks are commented out, leaving only the main executable code block active.

The process involves several steps: first, identifying the correct comment syntax for the target language; second, parsing the string to locate top-level code sections using a recursive `find_section` logic that handles nested blocks; third, identifying the largest code block matching the target language; and finally, commenting out everything else—including the explanatory text and the markdown backticks surrounding the code—while leaving the primary code block uncommented. The file includes examples of input strings, expected output strings showing proper commenting, and references to helper functions for comment syntax lookup.",5d4baddf9f4ee69a26f8db6480e32fdedc5959289f44e6eedc81e5cdec57f3a8
context/split/3/sub_postprocess_python.prompt,"The file outlines a task for an expert Python engineer to create a function named `find_section`. This function is designed to parse a list of strings representing LLM output and identify top-level code sections, specifically ignoring nested code blocks. The function takes a list of lines, a starting index, and a recursion flag as inputs, and returns a list of tuples containing the code language, start line, and end line for each identified section. The document provides detailed logic for the function, including iterating through lines to detect triple backticks, handling recursive calls for nested blocks, and correctly recording the boundaries of top-level code segments. An example of LLM output containing Python code and explanations is included to illustrate the data structure the function must process.",58b5cc265fdc219c18e64ef1fa8d5dea44b0ce126592ce26a0384f2fe8d1f8a3
context/split/3/consolidated_xml_filled_in.prompt,"The file contains a prompt engineering task designed for an expert LLM Prompt Engineer. The primary goal is to split a complex `input_prompt` into two distinct components: a `sub_prompt` (a smaller, self-contained task extracted from the original) and a `modified_prompt` (the original prompt adjusted to utilize the new sub-prompt). 

The file provides two detailed examples (`example_1` and `example_2`) demonstrating this process. Each example includes an initial prompt describing a Python command-line tool (specifically a program called ""pdd""), the resulting code, and how to extract specific functionality (like `get_extension` or `construct_output_paths`) into a sub-prompt while updating the main prompt to reference it. 

The file concludes with the actual task: the user must process a specific `input_prompt` that describes a Python function called `postprocess`. This function is intended to parse LLM output strings, identify code blocks, and comment out non-code text. The user is instructed to generate the corresponding `sub_prompt` and `modified_prompt` based on the provided input code and logic, following the pattern established in the examples.",e8a713f21a088b0afa73e7ab777af562a95fd787e4aed99e13b2d0200ac189df
context/split/3/final_postprocess_python.prompt,"The file outlines the requirements for creating a Python function named 'postprocess'. This function is designed to sanitize the output of a Large Language Model (LLM) so that it can be executed as a script. The function takes two inputs: the raw string output from an LLM (containing a mix of natural language text and code blocks) and a target programming language (e.g., Python, Bash). Its primary task is to identify the largest code block matching the target language within the LLM's response. It then comments out all surrounding text, explanations, and other code blocks using the appropriate comment syntax for that language, leaving only the main code section executable. The prompt includes references to helper functions for finding code sections, determining comment characters, and commenting lines, along with detailed examples of the expected input and output formats.",b27cde9185b170d15761b830ad784af4ecd2694c2aa3f3660d58264015855051
context/split/2/consolidated.prompt,"The file contains a prompt engineering task designed to demonstrate how to split a complex input prompt into a smaller `sub_prompt` and a `modified_prompt` without losing functionality. The specific scenario involves a Python command-line tool called ""pdd"" (Prompt Driven Development). The original prompt describes the requirements for ""pdd"", which compiles prompt files into runnable code or generates example code from existing code files using the `rich` library for output. The task illustrates extracting a specific logic component—constructing output file paths based on various command-line arguments and file extensions—into a separate function `construct_output_paths`. The file includes the full original prompt, the generated Python code for the ""pdd"" tool, and an example of how the extracted logic would be called in the modified code structure.",7c9ea2e83d38c5df64e79fa279b2df03daf4033150914b3812c2e8bc320a0a7d
context/split/2/sub_pdd_python.prompt,"The file outlines the requirements for a Python function named ""construct_output_paths,"" designed for a command-line tool called ""pdd."" This function is responsible for generating file paths for two types of outputs: a runnable file and an example output file. It takes four inputs: the file's basename, the file extension, and optional paths provided via command-line flags ('-o' and '-oe'). The function must handle four scenarios for path specification: filename only, filename with path, path only, and no user input (default). The logic involves defining a helper function to construct paths based on these scenarios and then applying it to determine both the runnable and example output paths. The final output is a tuple containing these two path strings. Examples provided illustrate how the tool handles different command-line arguments to generate files in specific directories or with default naming conventions.",bbc23abd6c529f9d0e2240973d74c08d91f24132321447a1bbcf96c0a4736e8a
context/split/2/split_pdd_construct_output_path.py,"The provided code snippet demonstrates a function call to `construct_output_paths`, which is designed to generate file paths for outputting data. This function takes four arguments: `basename` (the file's base name), `file_extension` (determined by the programming language), `argv_output_path` (derived from a command-line `-o` flag), and `argv_example_output_path` (derived from a command-line `-oe` flag). The function returns two values, `runnable_output_path` and `example_output_path`, which are assigned to variables of the same names. This logic suggests a utility for handling file system paths within a command-line interface application.",0f2ab7543853ba418ad7d35b3fd113a5312e5ecc6117ad906aa77931e6190215
context/split/2/final_pdd_python.prompt,"The provided text outlines the specifications for a Python command-line utility named ""pdd"". This tool is designed to streamline development workflows by compiling prompt files into runnable code or generating example usage files from existing code. The program accepts input files following specific naming conventions (e.g., 'basename_language.prompt') and supports various command-line arguments. Key options include '--force' for overwriting existing files without confirmation, '-o' to specify the output path for runnable code, and '-oe' to trigger and direct the generation of example code.

The utility operates through a structured process: it first parses the input filename to determine the basename and language, then constructs appropriate output paths. If the input is a prompt file, `pdd` generates the corresponding runnable code. Subsequently, if an example is requested via flags or if the input was a raw code file, it generates an example file using a context generator. The output is formatted using the Python `rich` library for enhanced console readability. The text also references specific helper modules like `code_generator`, `context_generator`, and `get_extension` to handle the core logic of code synthesis and file management.",59a3a06a611946dd14657809e4e0ea6b7afb0930441ec5206186690340abd267
context/split/2/pdd.py,"The provided file contains the implementation of a Python command-line tool named `pdd`, designed to compile prompt files into runnable code or generate example code from existing source files. The script utilizes the `argparse` library to manage command-line arguments, including input files, output paths (`-o`), example output paths (`-oe`), and a force overwrite flag (`--force`). It relies on external modules `code_generator`, `context_generator`, and `get_extension` to perform its core logic, alongside the `rich` library for formatted console output.

The program's workflow involves several steps: verifying the existence of the input file, appending a `.prompt` extension if missing, and determining the target programming language based on the filename. It then constructs appropriate output paths. If the input is a prompt file, `pdd` generates runnable code using `code_generator`. Subsequently, if the input is a code file or if an example output is requested, it attempts to create example code using `context_generator`. The script includes a helper function `write_to_file` that handles file creation and prompts the user for confirmation before overwriting existing files unless the force flag is used.",1021df55a642878527ad41794379eb1ba835c739a1e6f13b0896f2a0605ff8c2
context/split/2/initial_pdd_python.prompt,"The file outlines the specifications for a Python command-line tool named ""pdd"" designed to compile prompts into code files or generate example code from existing code files. The program utilizes the Python ""rich"" library for console output and handles input files typically formatted as ""basename_language.prompt"". 

The tool supports flexible output configurations, allowing users to specify filenames, paths, or rely on defaults derived from the input basename. Key features include a ""--force"" flag for overwriting existing files without confirmation, and specific flags (""-o"" and ""-oe"") to control the destination of runnable code and generated examples, respectively. 

The workflow involves reading the input filename, determining the language and extension, and calculating output paths based on user arguments. If the input is a prompt file, ""pdd"" generates runnable code using a ""code_generator"". If the input is a code file or if the example flag is set, it generates example code using a ""context_generator"". The specification includes detailed logic for handling file extensions, directory resolution, and step-by-step execution instructions for the resulting program.",4337784a5517d6e80fac11df769749c916c003261b27b36d6d5e6c7b580af78c
context/split/5/modified_cli_python.prompt,"The provided text is a detailed specification prompt for generating a Python command-line interface (CLI) tool named ""pdd"". The tool is to be constructed using the `click` library and serves as a comprehensive utility for AI-assisted code generation, testing, and prompt management.

The specification outlines the project structure and defines a central `cli` entry point. It lists a wide array of required commands, including `generate` (code creation), `test` (unit testing), `preprocess` (prompt handling), and `fix` (automated error correction). Other commands address workflows such as splitting prompts, detecting changes, resolving conflicts, tracing execution, and converting bugs into unit tests.

The prompt provides specific architectural instructions, such as aliasing imported functions (e.g., importing `preprocess` as `preprocess_func`) to prevent naming conflicts with CLI commands. It references numerous external context files and examples to guide the implementation of specific features like cost tracking, file path construction, and git integration. Additionally, it includes a requirement for an `install_completion` command to handle shell configuration.",05b105bbb2992d5fd110c1fe13558c710ee84bb05d56e15595a832f654a39144
context/split/5/cli_python.prompt,"The provided text is a comprehensive prompt designed to instruct an AI model to generate a Python command-line interface (CLI) tool named ""pdd"". Built using the `click` library, this tool is intended to assist developers with tasks related to code generation, prompt engineering, and testing. The prompt specifies the project's directory structure and enforces naming conventions to prevent conflicts between CLI commands and internal function imports.

It details a wide array of required commands, including functionality for generating code and unit tests (`generate`, `test`), preprocessing and splitting prompts (`preprocess`, `split`), fixing errors (`fix`), and managing updates or changes to prompts (`change`, `update`, `detect`). Advanced debugging features like `trace`, `crash`, and `bug` are also requested. Additionally, the prompt outlines requirements for installing shell completions and implementing a cost-tracking feature that logs usage details to a CSV file. The instructions rely on numerous referenced context files to provide specific implementation examples for each module.",f4ebb644bcad3cade98d856a268cb99231f7532f623bcfcfd4ec6d1f3cdb8213
context/split/5/track_cost_python.prompt,"The file outlines the requirements for creating a Python decorator named `track_cost` designed for the ""pdd"" command-line program, which utilizes the Click library. The primary function of this decorator is to monitor and log the cost associated with executing specific commands. It operates by wrapping a command function to record the start and end times of execution. The decorator must access the current Click context to retrieve command details and check for an output cost path, either specified directly or via an environment variable (`PDD_OUTPUT_COST_PATH`). It is responsible for extracting cost and model information from the command's return value, as well as identifying input and output file paths from the arguments. This data—including timestamp, model, command name, cost, and file paths—is then appended to a CSV file, creating the file with a header if it doesn't already exist. The implementation must be robust, using `functools.wraps` to preserve metadata and handling any logging exceptions gracefully with Rich's `rprint` to ensure the main command execution remains uninterrupted.",d9191e985846507bdfa6b7ccf7f8b3de2fc59d251aab381b303f031d7d42f98f
context/split/5/split_track_cost.py,"The provided file content is extremely minimal, consisting solely of the text ""@track_cost"". This syntax is characteristic of a Python decorator, which is a design pattern used to modify or enhance the behavior of functions or methods without permanently changing their source code. In this specific context, the identifier ""track_cost"" suggests that the decorator is intended to monitor, calculate, or log the computational cost, resource usage, or financial expense associated with the execution of the function it decorates. 

However, because the file lacks any surrounding code—such as the definition of the decorator function itself, the necessary import statements, or the function being decorated—it is impossible to determine the specific implementation details or the exact logic used for tracking costs. It serves merely as a snippet or a placeholder indicating where such a monitoring mechanism would be applied in a larger Python script or application.",92e4ef534572378e08209f913e43d248e18cd10e70de8f2fa15528cbc41200e1
context/split/5/cli.py,"This file defines a command-line interface (CLI) for a Prompt-Driven Development (PDD) tool, utilizing the `click` and `rich` libraries. The CLI provides a suite of commands to facilitate AI-assisted coding workflows, including generating code from prompts (`generate`), creating examples (`example`), generating unit tests (`test`), and preprocessing prompts (`preprocess`). It features robust error handling and cost tracking capabilities via a decorator that logs usage metrics to a CSV file. Advanced functionalities include an iterative error-fixing loop (`fix`), splitting complex prompts (`split`), updating prompts based on code changes (`update`, `change`), detecting necessary changes (`detect`), and resolving prompt conflicts (`conflicts`). Additionally, the tool supports debugging workflows through commands like `crash` for fixing module errors, `bug` for generating tests based on output discrepancies, and `trace` for mapping code lines back to prompt lines. The CLI allows configuration of AI model parameters such as strength and temperature, and supports shell completion installation.",9eea12ec7e9e8f98c0075207b0d88e60c3e01bc1128e25e0c80362028ae1d217
context/change/20/generate_test.py,"This file defines a Python function named `generate_test` designed to automatically generate unit tests for a given piece of code using Large Language Models (LLMs) via the Langchain library. The function orchestrates a multi-step process that begins with validating input parameters such as model strength, temperature, and the target programming language. It loads a specific prompt template from a file path defined by an environment variable (`PDD_PATH`) and preprocesses it. The script then selects an appropriate LLM using a helper function `llm_selector`, estimates token usage and costs, and executes the generation chain. Following the initial generation, the code checks for incompleteness using an `unfinished_prompt` utility; if the output is cut off, it triggers a continuation process. Otherwise, it runs a post-processing step to refine the result. Throughout execution, the script utilizes the `rich` library to display formatted logs, token counts, and cost estimates to the console. Finally, it returns the generated unit test code, the total calculated cost, and the name of the model used, while handling potential errors like missing files or invalid values.",7088603cf835c09d0466679a8d08109bdaebfd64d5e69a3e3f0369f549b8f7b7
context/change/20/change.prompt,"The provided file outlines a refactoring requirement where the methods `llm_invoke` and `load_prompt_template` have superseded a previous implementation. Consequently, the `input_prompt` must be updated to reflect this change. The document includes a series of examples demonstrating the transformation, contrasting the state of files before and after the update. These examples reference specific prompt files, such as `unfinished_prompt_python.prompt`, `context_generator_python.prompt`, `code_generator_python.prompt`, and `generate_test_python.prompt`, along with their updated counterparts located in a `context/change/` directory structure.",bf518ed2b6ee2ac5fef90b4f4f41180d0780e28ed6061096731936357c6be375
context/change/20/generate_test_python.prompt,"This file outlines the specifications for a Python function named ""generate_test,"" designed to automatically create unit tests from a given code file using a Large Language Model (LLM). The function takes inputs such as the original prompt, the source code, model strength, temperature, and the target programming language. It outputs the generated unit test code, the total cost of the operation, and the name of the model used.

The process involves several distinct steps orchestrated via LangChain Expression Language (LCEL). First, it loads a specific prompt template from an environment path and preprocesses it. It then selects an appropriate LLM using an internal `llm_selector` module, calculating initial token usage and cost. The function executes the model to generate the test, subsequently checking for incomplete output using an `unfinished_prompt` utility. If the output is incomplete, it triggers a continuation; otherwise, it post-processes the result. Finally, the function calculates and prints the total cost (including input, output, and processing overhead) before returning the final unit test, cost, and model name. The file also references several internal example modules for preprocessing, token counting, and post-processing.",f4154f38a0e4ee9fc9d090806c5864298d1b0f179d0e08f76418db8160a7906e
context/change/20/updated_generate_test_python.prompt,"This file outlines the specifications for creating a Python function named ""generate_test"" designed to automatically generate unit tests from a given code file using a Large Language Model (LLM). The function takes several inputs, including the original prompt, the source code, model strength, temperature, target language, and a verbose flag. It returns a tuple containing the generated unit test code, the total cost of the operation, and the name of the model used.

The process involves a multi-step workflow: loading a specific prompt template ('generate_test_LLM'), preprocessing inputs to handle formatting, and invoking the LLM via an internal `llm_invoke` module. The workflow includes logic to handle incomplete generations by detecting unfinished outputs and continuing generation if necessary. It also incorporates post-processing of the results. The specification emphasizes error handling and provides detailed instructions for verbose logging, including token counts and cost calculations at various stages. Internal modules for prompt loading, preprocessing, and postprocessing are referenced to guide the implementation.",e3e6d61f17deaf125fcf4bc8c28ccc2e3d5f475b291b2455a563e41831e6a2ab
context/change/18/updated_unfinished_prompt_python.prompt,"This file outlines the specifications for an AI agent to generate a Python function named `unfinished_prompt`. The primary purpose of this function is to analyze a given text string (`prompt_text`) and determine whether it represents a complete thought or requires continuation. The function accepts inputs including the prompt text, model strength, temperature, and a verbosity flag. It is expected to return a tuple containing a reasoning string, a boolean indicating completion status (`is_finished`), and optional metadata like total cost and model name. The instructions detail a three-step process: loading a specific prompt template (`unfinished_prompt_LLM`), invoking an LLM using internal modules for prompt execution, and parsing the Pydantic-structured output to return the final assessment. The file also references included context files for Python preambles and examples of how to load templates and invoke the LLM.",69a573fc153e4f4f86817317054b0854bb3d0fd93b2d1238a8d78c62e76357a3
context/change/18/unfinished_prompt_python.prompt,"This file outlines the requirements for an expert Python engineer to create a function named `unfinished_prompt`. The primary goal of this function is to analyze a given text string (`prompt_text`) and determine whether it represents a complete thought or requires continuation. The function accepts inputs for the prompt text, model strength, and temperature. It is expected to output a boolean indicating completion status (`is_finished`), a reasoning string, and optional metrics like total cost and model name. The implementation steps involve loading a specific prompt file from the project path, creating a LangChain LCEL template that enforces JSON output, and utilizing an internal `llm_selector` module to choose the model and calculate costs. The process also requires pretty-printing status updates and results using the `rich` library, specifically detailing token counts and costs before returning the final analysis.",4fe9f4f136ebb461f65832e3675afe6d31a8e27e2ec324c0980fd4eeb6e3651e
context/change/18/change.prompt,"The provided file content outlines a specific directive for updating code or documentation related to prompt engineering methods. It explicitly states that the functions `llm_invoke` and `load_prompt_template` have superseded a previous method, necessitating a corresponding update to the prompt structure. To illustrate the required modification, the file includes a comparison section featuring a ""before"" example, referenced as `unfinished_prompt_python.prompt`, and an ""after"" example, referenced as `updated_unfinished_prompt_python.prompt`. These examples are embedded via include tags pointing to specific file paths within a context directory (`context/change/17/`). Essentially, this snippet serves as a change log or instruction set for developers or automated systems to transition from an obsolete prompting mechanism to the new standard involving `llm_invoke` and `load_prompt_template`.",fb1bc585a05b415040d556f2c0c63edecd7672dc6f150a251b6879427c138f51
context/change/18/code_generator_python.prompt,"This file outlines the specifications for a Python function named ""code_generator,"" designed to compile a raw text prompt into a runnable code file using a Large Language Model (LLM). The function requires inputs for the prompt string, target programming language, model strength, and temperature. It returns the generated runnable code, the total cost of the operation, and the name of the model used.

The process involves a multi-step workflow utilizing LangChain Expression Language (LCEL). Key steps include preprocessing the raw prompt, creating an LCEL template, and selecting an appropriate LLM via an `llm_selector` module. The workflow executes the prompt, calculates costs based on token usage, and displays the output with Markdown formatting. Crucially, the system includes a check for incomplete generation using an `unfinished_prompt` function; if the output is cut off, it triggers a `continue_generation` function. Finally, the result undergoes post-processing before the function returns the final code, total cost, and model name. The file also references several external context files and examples for preprocessing, LLM selection, and handling unfinished prompts.",5b5b11fad7d22d99712ca4597b5a9a1e3a7d52eb402e2ab804458be11b6b6e56
context/change/18/updated_code_generator_python.prompt,"This file outlines the specifications for creating a Python function named ""code_generator"" designed to compile a raw text prompt into a runnable code file. The function requires inputs such as the raw prompt string, the target programming language (e.g., Python, Bash), model strength (0-1), temperature, and a verbose flag. It is expected to return the resulting runnable code, the total cost of the operation, and the name of the LLM model used.

The process involves a multi-step workflow using Langchain. First, the raw prompt is preprocessed and loaded into a template. The prompt is then executed via `llm_invoke`. If the verbose flag is set, the function must print detailed cost and token usage metrics. Crucially, the function includes logic to detect incomplete outputs using an `unfinished_prompt` module; if incomplete, it triggers a `continue_generation` process. Otherwise, the output is refined using a `postprocess` module. The file provides references to internal example modules for handling tasks like loading templates, invoking the LLM, and managing unfinished prompts, ensuring the final implementation handles errors gracefully and tracks total costs accurately.",0bd90df38d9390dd23b7fa9f119314f15dbcaf365d16d28abfde6f5728c5d53f
context/change/18/code_generator.py,"This file defines a Python function named `code_generator` designed to generate code using a Large Language Model (LLM) based on a user-provided prompt. The function orchestrates a multi-step pipeline that begins by preprocessing the raw prompt to handle recursive formatting and double curly brackets. It then utilizes a helper module, `llm_selector`, to choose an appropriate LLM based on a specified strength and temperature, calculating estimated input costs based on token counts.

The core logic involves creating a LangChain prompt template and invoking the selected model. The output is displayed using Markdown formatting via the `rich` library. A critical feature of this pipeline is its ability to handle incomplete generations; it checks the last portion of the output using an `unfinished_prompt` module. If the generation is deemed incomplete, it triggers a `continue_generation` process; otherwise, it proceeds to `postprocess` the result for the target programming language.

Throughout execution, the function tracks and prints detailed cost estimates for input, output, and any subsequent processing steps. It handles potential errors gracefully, returning the final runnable code, the total calculated cost, and the name of the model used.",c2df2d5c6852d547b62ac13ba50e6cd63cfce7e86431ee0e73c678660f9ce9e7
context/change/9/initial_fix_error_loop_python.prompt,"The provided text outlines the requirements for creating a Python function named ""fix_error_loop,"" designed to iteratively repair errors in unit tests and their corresponding code files. Acting as an expert Python Software Engineer, the function takes inputs including file paths for the unit test, the code being tested, and a verification program, along with parameters for LLM strength, temperature, and a maximum attempt limit. The process involves a loop that runs pytest, captures output to an error log, and analyzes failures. If tests fail, the function backs up current files, calls a helper function ""fix_errors_from_unit_tests"" to generate fixes using an LLM, and verifies the new code using the verification program. If verification fails, backups are restored. The loop continues until tests pass or the maximum attempts are reached. The function must handle errors gracefully, use the ""rich"" library for pretty-printed console output, and return the success status, final file contents, and total attempt count. It emphasizes resource management via context managers and proper integration of the temperature parameter into the LLM selection process.",dcb17e57a63d38312a12e92a7bf7f662adfb5939a145a942c7ce97ea65072656
context/change/9/initial_fix_error_loop.py,"This file defines a Python function named `fix_error_loop` designed to iteratively repair errors in code and unit tests. The function takes paths to a unit test file, a code file, and a verification program, along with parameters for error-fixing strength and temperature. It operates in a loop up to a specified maximum number of attempts. In each iteration, it runs the unit tests using `pytest` and captures the output. If the tests fail, it reads the error log, creates backups of the current files, and calls an external function `fix_errors_from_unit_tests` to generate fixes. If updates are suggested, it applies them and runs a separate verification program to ensure the changes are valid; if verification fails, it restores the backups. The process repeats until the tests pass or the maximum attempts are reached. Finally, it performs one last test run and returns the success status, the final content of the files, and the total attempt count.",c09034f0ac25551111af9568b2a7d57ab640ee2b56497cfb771af0af7d7ea258
context/change/9/final_fix_error_loop_python.prompt,"The provided text outlines the requirements for a Python function named `fix_error_loop`, designed to iteratively repair errors in a unit test and its associated code file. Acting as an automated debugging loop, the function takes inputs such as file paths for the unit test, code, and a verification program, along with constraints like `max_attempts`, `budget`, and LLM parameters (`strength`, `temperature`). The process involves running `pytest`, capturing errors, and utilizing a helper function `fix_errors_from_unit_tests` to generate fixes. It manages file backups, tracks costs, and verifies that changes do not break the core functionality using a verification program. The loop continues until the tests pass, the budget is exceeded, or the maximum attempts are reached. Finally, the function ensures the best-performing iteration is restored if the final attempt fails, returning the success status, final file contents, and total resource usage. All console output is formatted using the `rich` library.",e2a611a318013d20bb6910092141ae2fe67c6330cee00a0828c4b64cf5d51639
context/change/9/change.prompt,"The file contains instructions for updating a prompt related to a Python function named `fix_errors_from_unit_tests`. It provides a specific code example demonstrating how to call this function with inputs such as unit test code, code under test, an error message, strength, and temperature. The example also shows how to handle the output, which includes updated unit tests, updated code, and the total cost. Additionally, the file requests two functional enhancements: first, to track the total cost of all runs and implement a 'budget' input to halt iterations if the cost is exceeded; and second, to implement logic that selects and returns the 'best' iteration of the fixed code and unit tests. The criteria for the best iteration prioritize the lowest number of errors first, followed by the lowest number of failures, ensuring the function saves the most successful attempt even if the final run was not the best one.",6634e986dc230c7880aeb4b976f545350d3a72c0dcc94078a2164e74a561ec6e
context/change/11/initial_split_python.prompt,"This file outlines the requirements for developing a Python function named ""split,"" designed to decompose a single input prompt into a ""sub_prompt"" and a ""modified_prompt"" without losing functionality. The function targets an expert Python Software Engineer and mandates the use of the Python Rich library for pretty-printing console output. 

The function takes several inputs, including the original prompt, generated code, an example usage code snippet, and LLM parameters like strength and temperature. It outputs the two resulting prompts and the total execution cost. The implementation steps involve loading specific prompt files, preprocessing them (including doubling curly brackets), and utilizing Langchain LCEL templates. The process requires a two-step LLM invocation: first to generate a raw split based on the inputs, and second to extract the specific JSON components (""sub_prompt"" and ""modified_prompt"") from that output. 

Crucially, the function must integrate an ""llm_selector"" for model choice and token counting to calculate and display costs at each step. The final output must be displayed using Rich Markdown, and the code must handle edge cases and errors robustly.",b023c7200fa40dd4e4629ce84264033380c8b5b3563b4a3636aec7d574e40ad3
context/change/11/initial_code_generator.py,"This Python script defines a `code_generator` function designed to automate the creation of code snippets using Large Language Models (LLMs) via the LangChain framework. The process begins by preprocessing a raw prompt, which handles specific formatting requirements like double curly brackets. It then utilizes a custom `llm_selector` to choose an appropriate model based on a specified strength and temperature, calculating the associated token costs for the input. The script constructs a LangChain pipeline combining the prompt template, the selected LLM, and a string output parser to generate a response. After invoking the model, it calculates the output token costs and displays the raw result using rich text formatting. The generated content then undergoes a post-processing step to extract or refine runnable code for a specific target language. Finally, the function returns the clean, runnable code along with the total calculated cost of the operation. The script includes an execution block demonstrating how to call the generator with a sample prompt requesting a Python factorial function.",415b0161c9a487eec0da91cfc582cb3eeae46bee8d5c163d07eba6cc3fb5721e
context/change/11/change.prompt,"The file outlines specifications for creating a Python function named ""change"". This function is designed to process a specific prompt file located at 'prompts/change_LLM.prompt'. The core functionality involves running this prompt through LangChain Expression Language (LCEL) and applying post-processing steps similar to the current context. The primary objective of the ""change"" function is to output a modified version of a given prompt. The file details the expected inputs for the underlying 'change_LLM' prompt, which include: 'input_prompt' (the original prompt text), 'input_code' (code generated from that prompt), and 'change_prompt' (instructions on how to modify the original prompt). The final output of the system is defined as 'modified_prompt', representing the updated prompt text derived from the provided change instructions.",55472b854c77c56f47ed8e83fcc4588f14a6eacf0b23b693f84357c276058978
context/change/11/initial_code_generator_python.prompt,"The document outlines the requirements for an expert Python engineer to create a function named ""code_generator."" This function is designed to compile a raw text prompt into a runnable code file, utilizing the Langchain library and LCEL (LangChain Expression Language). The function accepts inputs including the raw prompt, target programming language, model strength, and temperature. It returns the resulting runnable code and the total cost of the operation. The process involves several distinct steps: preprocessing the prompt, creating a Langchain template, selecting an appropriate LLM based on strength, executing the model while tracking token usage and cost, and pretty-printing the output using the Python `rich` library. Finally, the output is post-processed to extract executable code from mixed text/code responses. The document references external context files for examples of LCEL usage, preprocessing, LLM selection, and postprocessing logic.",460661d899c5324a82e4172d05852aa94e00225c1f4dc9e6abd1ffdb581649ef
context/change/7/final_fix_errors_python.prompt,"The provided text outlines the requirements for creating a Python function named ""fix_error_loop,"" designed to iteratively repair errors in unit tests and their corresponding code files. Acting as an expert Python Software Engineer, the function takes inputs including file paths for the unit test, the code being tested, and a verification program, along with parameters for LLM strength, temperature, and a maximum attempt limit. The process involves a loop that runs pytest, captures output to an error log, and analyzes failures. If tests fail, the function backs up current files, calls a helper function ""fix_errors_from_unit_tests"" to generate fixes using an LLM, and verifies the new code using the verification program. If verification fails, backups are restored. The loop continues until tests pass or the maximum attempts are reached. The function must handle errors gracefully, use the ""rich"" library for pretty-printed console output, and return the success status, final file contents, and total attempt count. It emphasizes resource management via context managers and proper integration of the temperature parameter into the LLM selection process.",dcb17e57a63d38312a12e92a7bf7f662adfb5939a145a942c7ce97ea65072656
context/change/7/initial_fix_errors_python.prompt,"The file outlines a specification for an expert Python software engineer to create a script named ""fix_errors.py"". This script is designed to automate the debugging process using a CLI tool called ""pdd"" and the ""rich"" library for pretty-printed console output. The script accepts arguments for a unit test file, a code file, a verification program, a strength parameter, and a retry limit. Its workflow involves an iterative loop: first, it removes old logs and runs pytest, piping output to an error log. If tests fail, it analyzes the error count, backs up the current files with versioned names based on failure counts, and invokes ""pdd"" to attempt a fix on the source files. It then verifies that the code still runs; if successful, the loop repeats with the updated files. If the fix breaks the program, it reverts to the original files and retries the fix. Finally, it runs pytest one last time to report the final status.",647488c8bba7eeaabcf4f78c17a8bfbdf486fd26971d7f8f89533266a6fb5e3b
context/change/7/change.prompt,"The provided text outlines the usage and specifications for a Python code module designed to automatically fix errors arising from unit tests. It presents a usage example where the function `fix_errors_from_unit_tests` is imported and executed with four key inputs: a failing `unit_test` string, the corresponding source `code`, the specific `error` message encountered, and a `strength` parameter (0.0 to 1.0) to guide the underlying Large Language Model (LLM). The documentation details the input parameters and the four return values: booleans indicating if the test or code was updated, and the actual fixed strings for both the unit test and the code. Additionally, a specific instruction notes that this functionality replaces a CLI program named 'pdd' and will be implemented as a Python function named `fix_error_loop`, which will also accept a `temperature` argument.",09f6d7298550a6ad36ef5c1175fa9e0386469eb5e9bc17476524033b9d067529
context/change/7/initial_fix_errors.py,"The provided file contains the source code and documentation for a Python script named `fix_errors.py`. This script is designed to automate the debugging and repair process for software development workflows. It operates by iteratively running unit tests using `pytest`, capturing the output, and utilizing an external tool referred to as ""PDD"" (likely a programmatic debugging or patching tool) to attempt fixes on the code based on the error logs.

The script's workflow involves several key steps: executing tests and logging results to `error.log`, parsing the logs to count failures and errors, creating backup copies of the current code and test files stamped with iteration metrics, and applying fixes via the PDD tool. Crucially, it includes a verification step where a separate program checks if the applied fixes broke the build; if verification fails, the script reverts to the backup files. This loop continues until all tests pass or a maximum iteration count is reached. The file concludes with a usage guide, explaining how to invoke the script with arguments for the test file, code file, verification program, fix strength, and iteration limit.",ba61c10f4ecce85b97dfdc751830d1d15a6d8b26bd3649928da0ec843867ff6e
context/change/16/fix_errors_from_unit_tests_example.py,"This file is a Python script designed to demonstrate the functionality of the `fix_errors_from_unit_tests` module. It defines a `main` function that sets up a hypothetical scenario involving a simple addition function and a corresponding unit test with an intentional error. The script initializes several parameters, including the source code, the unit test code, a prompt description, a simulated error message, and configuration settings for an LLM (Large Language Model) such as strength and temperature. It then calls the `fix_errors_from_unit_tests` function with these inputs to attempt an automated repair of the code or test. Finally, the script uses the `rich` library to print the results of this operation to the console, displaying whether the unit test or code was updated, the fixed versions of the content, the cost of the operation, and the model name used.",65ffaca611e08ebc0c56469a3be2fbb0abf8d17b6b91a2b263233c99743b5694
context/change/16/fix_code_module_errors_python.prompt,"This document outlines the specifications for a Python function named `fix_code_module_errors`, designed to resolve errors in code modules that cause program crashes. Acting as an expert Python Software Engineer, the function takes inputs such as the crashing program code, the original prompt, the problematic module, error logs, and LLM configuration parameters (strength and temperature). It outputs corrected code, flags indicating if updates are needed, the total cost, and the model name used. The implementation plan involves a multi-step process using LangChain Expression Language (LCEL). It requires loading specific prompt templates from a project directory, selecting an LLM via an internal `llm_selector` module, and executing two distinct LLM passes. The first pass generates a fix based on the error context, while the second pass extracts structured JSON data (updated code and boolean flags) from the initial fix. The process also mandates pretty-printing status updates, token counts, and costs using the `rich` library.",6db86946ec70cfdaaaa9e4b36790cf24a35b38c2ef7b32e33f3e2f2c3c47accc
context/change/16/fix_error_loop_python.prompt,"This document outlines the specifications for a Python function named ""fix_error_loop,"" designed to iteratively repair errors in a unit test and its associated code file. The function takes inputs such as file paths for the unit test, code, and a verification program, along with parameters for LLM strength, temperature, maximum attempts, and budget. It operates by initializing counters and removing old error logs, then entering a loop that runs until success, budget exhaustion, or maximum attempts are reached. Inside the loop, it executes pytest, captures output, and if failures occur, backs up current files and calls a helper function, ""fix_errors_from_unit_tests,"" to generate fixes. The process includes a verification step for code changes; if verification fails, the system restores the previous working state. The function tracks the ""best iteration"" based on the lowest number of errors and failures. Finally, it returns a status report including success boolean, final file contents, total attempts, and accumulated cost, ensuring the best version of the code is restored if the final attempt was not the most successful.",e8305dac166fd8799877e1de5a9f577e70a241b564fd7d80ca13fa15f54bcf00
context/change/16/modified_fix_error_loop_python.prompt,"This file provides a detailed specification for creating a Python function named `fix_code_loop`. The function's primary purpose is to iteratively attempt to fix errors in a given code module using an LLM until the code passes a verification program or resource limits are reached. 

The specification outlines the required inputs, which include file paths for the code and verification program, the original prompt, LLM parameters (strength, temperature), and constraints like `max_attempts` and `budget`. The expected outputs include success status, final file contents, total attempts, cost, and the model name used.

The core logic is described in a five-step process. It involves initializing counters and removing old logs, followed by a `while` loop. Inside the loop, the function runs the verification program, captures errors, and calls a helper function `fix_code_module_errors` to generate fixes if errors occur. It handles file backups, updates code files based on LLM suggestions, tracks costs, and manages version restoration if a fix fails. Finally, the function returns the results of the debugging session.",b8d999aca35c02331fb444c7db8e89f1fc3561ca11e097205741d65c508db8c9
context/change/16/fix_code_module_errors_example.py,"This file is a Python script designed to demonstrate the functionality of the `fix_code_module_errors` function from the `pdd.fix_code_module_errors` module. The script defines a `main` function that simulates a scenario where a piece of generated code contains a runtime error. Specifically, it sets up a `program_with_error` string that attempts to sum a string of digits instead of a list of numbers, causing a `TypeError`. It also defines the original prompt used to generate the code, the isolated code module, and the specific error message produced by the failure.

The core of the script involves calling `fix_code_module_errors` with these inputs, along with parameters for model strength and temperature. This function is expected to analyze the error and return corrected versions of both the full program and the specific code module, along with metadata like the cost of the operation and the model used. Finally, the script prints the results, including boolean flags indicating if updates were necessary, the fixed code strings, the total API cost, and the name of the AI model employed for the fix.",e04c30498a2bb7de25d0df307457e6f7b156baaadfa28e0c9a11f33fb8f49594
context/change/16/change.prompt,"The provided file contents outline a set of specific instructions for modifying a code generation or prompt engineering workflow. It directs the replacement of an existing prompt, identified as `fix_error_loop`, with a new version located at `context/change/16/fix_error_loop_python.prompt`. Furthermore, the instructions mandate a change in the function call logic: instead of invoking `fix_errors_from_units_tests` (referenced with an example file), the system should now call `fix_code_module_errors`. This new function is associated with an example implementation file (`fix_code_module_errors_example.py`) and a specific prompt file (`fix_code_module_errors_python.prompt`). Essentially, the file serves as a configuration or directive script to update the error-handling mechanism within a larger software context, switching from a unit-test-based fix approach to a module-level error fix approach.",98a3af97a4e02ef8594decf9fda9e92e64a755e8ae1766198e92777c3a029731
context/change/16/fix_error_loop.py,"This Python script implements an automated loop for fixing errors in code and unit tests. It defines a `fix_error_loop` function that iteratively runs `pytest` on a specified unit test file, analyzes the output for failures and errors, and attempts to repair the code or tests using an external `fix_errors_from_unit_tests` module (likely LLM-based). The process involves creating backups of files before modification, tracking iteration costs against a budget, and verifying code changes with a separate verification program. It maintains an `IterationResult` dataclass to track performance metrics (fails/errors) across attempts. If the final iteration is not the most successful, the script restores the files from the best-performing iteration. The output includes the success status, final file contents, attempt count, total cost, and the model name used.",01872cfcc6c3f5095551acd109e06b35ece96c15ca75dcf40ac395c3ed867e40
context/change/6/initial_xml_tagger_python.prompt,"This document outlines the specifications for creating a Python function named ""xml_tagger"" designed to enhance LLM prompts by adding XML tags for better structure and readability. The function takes a raw prompt string, along with strength and temperature parameters for the LLM, and returns an XML-tagged version of the prompt. The implementation requires using the Langchain library, specifically its LCEL (LangChain Expression Language) syntax, and the Python ""rich"" library for pretty-printing console output.

The process involves a multi-step workflow: loading specific prompt templates from a project path defined by an environment variable, selecting an LLM model via a helper function, and calculating token usage and costs using tiktoken. The workflow executes in two main phases: first, converting the raw prompt into an XML-structured analysis using an ""xml_convertor"" prompt; and second, extracting the clean XML content using an ""extract_xml"" prompt that returns JSON. The function must handle errors gracefully and provide detailed console feedback regarding token counts and costs at each step before returning the final ""xml_tagged"" string.",a0bf28aba89354b28131e03d81f319567db18c9dd8caa0051a4d7ba4393b0cec
context/change/6/final_xml_tagger_python.prompt,"The file outlines the requirements for creating a Python function named ""xml_tagger"" designed to enhance LLM prompts by adding XML tags for better structure and readability. The function takes a raw prompt string, along with strength and temperature parameters, as input. It utilizes the Langchain library and LCEL (LangChain Expression Language) to process the prompt in two main steps: first, converting the raw prompt into an XML-tagged format using a specific prompt template, and second, extracting the clean XML content using a JSON-output template. The process involves loading prompt files from a project path defined by an environment variable, selecting an LLM model, and calculating token usage and costs. The function is required to pretty-print status updates and the final output using the Python `rich` library, handle errors gracefully, and return both the final XML-tagged string and the total execution cost.",9ead2043eef028452b68a623678b10ff6e54597642fa161bde840f44fb7f3a2c
context/change/6/initial_xml_tagger.py,"This Python script defines a function named `xml_tagger` designed to process raw text prompts by applying XML tagging through a multi-step Large Language Model (LLM) workflow using LangChain. The script initializes an SQLite cache for LLM responses and defines a Pydantic model, `XMLTaggedOutput`, to structure the final JSON output. The core function first retrieves prompt templates from file paths specified by environment variables. It then utilizes a custom `llm_selector` to choose an appropriate model based on provided strength and temperature parameters. The workflow consists of two main chains: the first converts the raw prompt into an XML-analyzed format, and the second extracts the specific XML-tagged content from that analysis. Throughout the process, the script calculates and prints token counts and estimated costs using `tiktoken`, and utilizes the `rich` library for formatted console output. Finally, it returns the processed XML-tagged string.",57efa1ce5b19060faa671ae23b6c64c631bb9fdea304d3fc38d7079719987b74
context/change/6/change.prompt,"The provided text outlines specific technical requirements for updating a software component, likely within a Large Language Model (LLM) application context. It specifies two primary changes. First, the system should switch its method for counting tokens in prompts; instead of using the 'tiktoken' library directly, it must utilize a 'token_counter' function imported from a module named 'llm_selector'. Second, the 'xml tagger' component needs to be enhanced to calculate and return the total financial cost associated with executing LangChain Expression Language (LCEL) chains. These instructions aim to standardize token counting via a specific selector and introduce cost tracking capabilities to the tagging functionality.",0973aaea3cef9e6549d2ee870b42ca7ca13f2ff6bdc1ee151e3a303849f55cd2
context/change/17/updated_unfinished_prompt_python.prompt,"This file outlines the specifications for an AI agent to generate a Python function named `unfinished_prompt`. The primary purpose of this function is to analyze a given text string (`prompt_text`) and determine whether it represents a complete thought or requires continuation. The function accepts inputs including the prompt text, model strength, temperature, and a verbosity flag. It is expected to return a tuple containing a reasoning string, a boolean indicating completion status (`is_finished`), and optional metadata like total cost and model name. The instructions detail a three-step process: loading a specific prompt template (`unfinished_prompt_LLM`), invoking an LLM using internal modules for prompt execution, and parsing the Pydantic-structured output to return the final assessment. The file also references included context files for Python preambles and examples of how to load templates and invoke the LLM.",69a573fc153e4f4f86817317054b0854bb3d0fd93b2d1238a8d78c62e76357a3
context/change/17/continue_generation.py,"This Python file defines a module for managing and continuing the generation of text using Large Language Models (LLMs), specifically leveraging the LangChain framework. The core functionality is encapsulated in the `continue_generation` function, which orchestrates a multi-step process to extend incomplete LLM outputs. The process involves loading and preprocessing specific prompt templates from an environment-defined path, selecting appropriate LLMs based on desired strength and temperature, and executing a loop that generates text until completion is detected. It utilizes helper functions like `extract_text_from_response` to parse JSON outputs and `unfinished_prompt` to determine if generation should cease. The module also tracks token usage and calculates costs associated with the generation process, providing detailed console output via the `rich` library. Pydantic models are defined to structure the expected output for trimming operations, ensuring clean code block extraction and concatenation.",6d48786ab0ae7ac29cb5852dfca0de6af5363657967f82e99d009ae0a042d97a
context/change/17/updated_continue_generation_python.prompt,"This file contains a detailed prompt specification for an AI agent acting as an expert Python Software Engineer. The objective is to generate a Python function named ""continue_generation"" designed to complete unfinished outputs from a Large Language Model (LLM). The prompt outlines the function's inputs (including the initial prompt, partial LLM output, and model parameters like strength and temperature) and outputs (the final completed string, total cost, and model name). It provides specific implementation steps: loading and preprocessing prompt templates ('continue_generation_LLM', 'trim_results_start_LLM', 'trim_results_LLM'), trimming the initial output, and entering a loop where the model continues generation until completion is detected via an 'unfinished_prompt' check. The instructions also detail how to use internal modules for tasks like invoking the LLM and handling prompt templates, while emphasizing error handling and verbose logging options.",cc1e0b15c386c41512b665d7fb540d0384ccd0a17b3d6cad8423f984b0a26e78
context/change/17/unfinished_prompt_python.prompt,"This file outlines the requirements for an expert Python engineer to create a function named `unfinished_prompt`. The primary goal of this function is to analyze a given text string (`prompt_text`) and determine whether it represents a complete thought or requires continuation. The function accepts inputs for the prompt text, model strength, and temperature. It is expected to output a boolean indicating completion status (`is_finished`), a reasoning string, and optional metrics like total cost and model name. The implementation steps involve loading a specific prompt file from the project path, creating a LangChain LCEL template that enforces JSON output, and utilizing an internal `llm_selector` module to choose the model and calculate costs. The process also requires pretty-printing status updates and results using the `rich` library, specifically detailing token counts and costs before returning the final analysis.",4fe9f4f136ebb461f65832e3675afe6d31a8e27e2ec324c0980fd4eeb6e3651e
context/change/17/change.prompt,"The provided file content outlines a specific directive for updating code or documentation related to prompt engineering methods. It explicitly states that the functions `llm_invoke` and `load_prompt_template` have superseded a previous method, necessitating a corresponding update to the prompt structure. To illustrate the required modification, the file includes a comparison section featuring a ""before"" example, referenced as `unfinished_prompt_python.prompt`, and an ""after"" example, referenced as `updated_unfinished_prompt_python.prompt`. These examples are embedded via include tags pointing to specific file paths within a context directory (`context/change/17/`). Essentially, this snippet serves as a change log or instruction set for developers or automated systems to transition from an obsolete prompting mechanism to the new standard involving `llm_invoke` and `load_prompt_template`.",fb1bc585a05b415040d556f2c0c63edecd7672dc6f150a251b6879427c138f51
context/change/17/continue_generation_python.prompt,"This document outlines the requirements for a Python function named ""continue_generation,"" designed to complete unfinished outputs from a Large Language Model (LLM). The function takes a formatted input prompt, the current partial LLM output, and model parameters (strength and temperature) as inputs. It returns the completed text, the total cost of execution, and the model name used. The process involves several steps: loading and preprocessing specific prompt files from a directory, setting up Langchain LCEL templates, and selecting an appropriate LLM model. The core logic iterates through a loop where it detects if the generation is incomplete using a helper function; if so, it continues generating text until completion. It also employs specific trimming prompts to manage the output format. Throughout the execution, the function must track token usage and calculate costs for all model invocations. Finally, it pretty-prints the result using Rich Markdown and returns the final output string along with cost metrics.",0bad36512ca7aa74cb49aa6b4a53c0d0c0764264897c1c336b9a488f31c12e66
context/change/1/initial_code_generator.py,"This file defines a Python function named `code_generator` designed to generate programming code based on a user-provided prompt. The function orchestrates a multi-step pipeline involving preprocessing, model selection, execution, and postprocessing. It utilizes the `langchain` library for managing prompt templates and LLM interactions, and `tiktoken` for calculating token usage and associated costs. The process begins by preprocessing the raw prompt and selecting an appropriate Large Language Model (LLM) via a custom `llm_selector` module based on specified strength and temperature parameters. It also implements SQLite caching for LLM responses. During execution, the script calculates and displays the estimated costs for input and output tokens using the `rich` library for formatted console output. After generating the initial result, the function invokes a `postprocess` module to refine the output into runnable code. Finally, it returns the executable code string and the total calculated cost of the operation, while including error handling to catch and report exceptions during the process.",d9a74c307cfa97c080110b0867bb6cd0e653f0819fa843ba269f20f2b281e244
context/change/1/change.prompt,"The file contents describe a specific code refactoring or dependency change within a software project. The primary modification involves switching the mechanism used for counting tokens in a prompt. Previously, the system utilized the `tiktoken` library directly for this purpose. The update replaces this direct usage with a `token_counter` utility imported from a module named `llm_selector`. This change suggests an effort to abstract or centralize token counting logic, likely to support multiple Large Language Model (LLM) backends or to standardize how tokens are calculated across the application, rather than relying on a specific tokenizer implementation like `tiktoken` in this particular context.",c020c75bbee108543aa8569af733d5aee35329ca7c4947d59aa3ed9f3f91c86f
context/change/1/final_code_generator_python.prompt,"The document outlines the requirements for an expert Python engineer to create a function named ""code_generator."" This function is designed to compile a raw text prompt into a runnable code file, utilizing the Langchain library and LCEL (LangChain Expression Language). The function accepts inputs including the raw prompt, target programming language, model strength, and temperature. It returns the resulting runnable code and the total cost of the operation. The process involves several distinct steps: preprocessing the prompt, creating a Langchain template, selecting an appropriate LLM based on strength, executing the model while tracking token usage and cost, and pretty-printing the output using the Python `rich` library. Finally, the output is post-processed to extract executable code from mixed text/code responses. The document references external context files for examples of LCEL usage, preprocessing, LLM selection, and postprocessing logic.",460661d899c5324a82e4172d05852aa94e00225c1f4dc9e6abd1ffdb581649ef
context/change/1/initial_code_generator_python.prompt,"The provided text outlines a specification for an expert Python engineer to develop a function named ""code_generator."" This function is designed to compile a raw text prompt into a runnable code file, utilizing the Langchain library and its LCEL (LangChain Expression Language) framework. The function accepts four inputs: the raw prompt string, the target programming language (e.g., Python, Bash), a strength parameter (float 0-1) to determine the LLM model capability, and a temperature setting for generation variability. 

The process involves a seven-step workflow: preprocessing the prompt, creating an LCEL template, selecting the appropriate LLM based on the strength parameter, and executing the model run. Crucially, the function must provide real-time feedback to the console using the ""rich"" Python library, displaying pretty-printed messages, Markdown formatting, token counts (calculated via tiktoken), and cost estimates for both input and output. Finally, the raw model output—which mixes text and code blocks—must be post-processed to extract clean, runnable code. The function returns this runnable code string along with the total calculated cost of the operation.",c29bf72464415b60b85b670a2a85d06139818ebe9c821cf72c07bdfb877d76a5
context/change/10/initial_fix_errors_from_unit_tests_python.prompt,"The provided text outlines the requirements for creating a Python function named `fix_errors_from_unit_tests`. This function is designed to automatically resolve errors encountered during unit testing by leveraging Langchain LCEL (LangChain Expression Language) and LLM models. The function takes inputs such as the unit test code, the code under test, the error message, and model parameters like strength and temperature. It operates in a multi-step process: first, it loads specific prompt templates from a project directory and uses an LLM to generate a fix based on the provided errors. Second, it employs a secondary LLM call to parse the generated fix into structured JSON outputs, specifically extracting boolean flags for updates and the corrected code strings. The process emphasizes using the `rich` library for pretty-printing console output, tracking token usage and costs for each LLM interaction, and handling potential errors gracefully. The final output includes the fixed code, fixed unit test, update flags, and the total calculated cost of the operations.",e996814013526e08ecd4258f33dc84ba9c3594d3662496c081332567bce8e5a3
context/change/10/change.prompt,"The provided text outlines a specific modification required for the `fix_errors_from_unit_tests` function. The primary change involves adding a new input parameter named `error_file` to this function. The purpose of this file is to store logs; specifically, the output from the initial LCEL (LangChain Expression Language) run will be appended to the existing content of this file. Additionally, the instructions specify that a clear separator must be inserted between the pre-existing content in the `error_file` and the newly appended LCEL output. This formatting requirement is intended to ensure readability, making it easy for developers to distinguish which parts of the log originated from which specific function execution or run.",545412589a4de78e8428fb1e82973fa61366869013f81bf2ffde503cf66a1153
context/change/10/final_fix_errors_from_unit_tests_python.prompt,"This document outlines the requirements for developing a Python function named ""fix_errors_from_unit_tests"". The function is designed to automatically resolve errors in code and unit tests using Large Language Models (LLMs) via the Langchain library. It takes inputs including the unit test code, the code under test, error messages, a log file path, and LLM configuration parameters (strength and temperature). The process involves a multi-step workflow: first, it loads specific prompt templates from a project directory defined by an environment variable. It then executes an initial Langchain LCEL run to generate a fix based on the provided errors, logging the process and costs to the console using the ""rich"" library and appending results to a specified error file. A second LCEL run parses the initial output into structured JSON data to extract the corrected code and boolean flags indicating updates. Finally, the function returns the updated unit test, updated code, update flags, and the total calculated cost of the LLM operations, while ensuring robust error handling for file I/O and model interactions throughout the execution.",afdcebec86210c8c94977ec521b9f57743078ff5613e8050d960f449f804d7ad
context/change/10/initial_fix_errors_from_unit_tests.py,"This Python script defines a function named `fix_errors_from_unit_tests` designed to automatically resolve errors in unit tests and their associated code using Large Language Models (LLMs). The script leverages the LangChain library for orchestrating LLM interactions and the `rich` library for formatted console output. It begins by setting up an SQLite cache to optimize performance and reduce costs.

The core function takes the failing unit test, the source code, the error message, and LLM configuration parameters (strength and temperature) as inputs. It operates in a two-step process. First, it loads a prompt template to analyze the error and generate a fix, invoking an LLM selected via a custom `llm_selector`. The output of this step is displayed as Markdown. Second, it uses another prompt template to extract the specific code and unit test modifications from the previous analysis into a structured JSON format. The script calculates and prints the token usage and financial cost for each step. Finally, it returns a tuple containing flags indicating whether the unit test or code was updated, the actual fixed code strings, and the total cost of the operation, while handling potential exceptions gracefully.",7a3eb969bcaa76dc85d59c4013a41a7da5bb7472daf97df2b2cad0475642d2c7
context/change/19/updated_unfinished_prompt_python.prompt,"This file outlines the specifications for an AI agent to generate a Python function named `unfinished_prompt`. The primary purpose of this function is to analyze a given text string (`prompt_text`) and determine whether it represents a complete thought or requires continuation. The function accepts inputs including the prompt text, model strength, temperature, and a verbosity flag. It is expected to return a tuple containing a reasoning string, a boolean indicating completion status (`is_finished`), and optional metadata like total cost and model name. The instructions detail a three-step process: loading a specific prompt template (`unfinished_prompt_LLM`), invoking an LLM using internal modules for prompt execution, and parsing the Pydantic-structured output to return the final assessment. The file also references included context files for Python preambles and examples of how to load templates and invoke the LLM.",69a573fc153e4f4f86817317054b0854bb3d0fd93b2d1238a8d78c62e76357a3
context/change/19/updated_context_generator_python.prompt,"The file outlines a prompt specification for an AI agent acting as an expert Python engineer. The objective is to create a Python function named `context_generator` that produces a concise usage example for a given code module. The function takes inputs such as the code module string, the original prompt, language (defaulting to Python), and model parameters like strength and temperature. It returns a tuple containing the generated example code, the total cost, and the model name used. The prompt details specific internal modules to utilize for tasks like loading templates, invoking the LLM, handling unfinished prompts, continuing generation, and post-processing results. The execution flow involves four steps: loading a specific prompt template (`example_generator_LLM`), invoking the model with the code module and processed prompt, checking for incomplete output (triggering a continuation if necessary), and finally post-processing the result before returning the required outputs.",fa70f2c3b7026a9c4b6d13b1a54692de580c0e3783cf2417c84899a51f2f6176
context/change/19/unfinished_prompt_python.prompt,"This file outlines the requirements for an expert Python engineer to create a function named `unfinished_prompt`. The primary goal of this function is to analyze a given text string (`prompt_text`) and determine whether it represents a complete thought or requires continuation. The function accepts inputs for the prompt text, model strength, and temperature. It is expected to output a boolean indicating completion status (`is_finished`), a reasoning string, and optional metrics like total cost and model name. The implementation steps involve loading a specific prompt file from the project path, creating a LangChain LCEL template that enforces JSON output, and utilizing an internal `llm_selector` module to choose the model and calculate costs. The process also requires pretty-printing status updates and results using the `rich` library, specifically detailing token counts and costs before returning the final analysis.",4fe9f4f136ebb461f65832e3675afe6d31a8e27e2ec324c0980fd4eeb6e3651e
context/change/19/change.prompt,"The provided file content serves as an instruction for updating a prompt or code snippet to reflect a change in methodology. Specifically, it mandates replacing current methods with `llm_invoke` and `load_prompt_template`. The file includes an example structure demonstrating the transformation, featuring a `<before_example>` section referencing an unfinished Python prompt and an `<after_example>` section referencing the updated version. This suggests the file is likely part of a migration guide, a refactoring task description, or a template for an automated code modification tool.",0ff732468c287e399268934d058fac0d96a4fdbc42d6bb339bbd00ae52a07e2b
context/change/19/context_generator_python.prompt,"The file outlines a prompt for an expert Python engineer to create a function named ""context_generator."" This function is designed to generate concise usage examples for a given code module. The prompt specifies the function's inputs (code module, original prompt, language, strength, and temperature) and outputs (the generated example code, total cost, and model name). It details a nine-step process using LangChain Expression Language (LCEL), which involves loading a specific prompt file, preprocessing it, selecting an LLM via an internal `llm_selector`, and invoking the model. The process also includes logic for handling incomplete generations using an `unfinished_prompt` check and a `continue_generation` function, as well as post-processing the final output. The file references several internal context examples for tasks like preprocessing, token counting, and post-processing to guide the implementation.",1a59e3e1f7d3aa598a9a92552c6300b827c1d0791903757150d7fbbee41ff889
context/change/19/context_generator.py,"This Python file defines a function named `context_generator` designed to generate concise usage examples for a given code module using a Large Language Model (LLM). The function orchestrates a multi-step process that begins by validating the `PDD_PATH` environment variable and loading a specific prompt template (`example_generator_LLM.prompt`). It utilizes helper functions to preprocess inputs, select an appropriate LLM based on desired strength and temperature, and construct a Langchain LCEL chain for execution. The script includes logic to handle token counting and cost estimation for both input and output. Crucially, it implements a mechanism to detect incomplete LLM responses and automatically continue generation if necessary. Finally, the output undergoes post-processing before the function returns the generated example code, the total calculated cost, and the name of the model used. The code makes extensive use of the `rich` library for console logging and debugging output throughout the execution flow.",046c50404d58d06586c6acb481acbd2e20042de7155b5f7eaffab5ebebceda0a
context/change/8/initial_fix_errors_from_unit_tests_python.prompt,"This document outlines the specifications for creating a Python function named ""fix_errors_from_unit_tests"" designed to resolve errors in code and unit tests using an LLM. The function takes the unit test code, the code under test, error messages, and a model strength parameter as inputs. It utilizes the Langchain library, specifically LCEL (LangChain Expression Language), to process these inputs through a multi-step workflow. The process involves loading specific prompt files from a project directory, selecting an LLM based on the provided strength, and calculating token usage and costs using tiktoken. The workflow first generates a fix explanation and then extracts the structured data (updated code and tests) using a second LLM call with a JSON parser. The function ultimately returns boolean flags indicating updates and the fixed code strings, while providing rich console output detailing the process and costs.",d7a9a11b7642bf867eeae01da392ae21070135994493a5ca37adb1ef2d426127
context/change/8/change.prompt,"The file outlines specific refactoring and feature enhancement requirements for a software project. The primary directive is to replace the current token counting mechanism, 'tiktoken', with a custom 'token_counter' imported from the 'llm_selector' module. Additionally, the 'fix_errors_from_unit_tests' function requires modification to accept and utilize a 'temperature' parameter, likely to adjust the variability of the language model's output. Finally, the system needs to be updated to calculate and output the total financial cost associated with the LangChain Expression Language (LCEL) execution runs.",d199bc8789406397849d0c5ab6389d9db19cff85a3724c2a0333ba7b8c455ba1
context/change/8/final_fix_errors_from_unit_tests_python.prompt,"This document outlines the requirements for creating a Python function named ""fix_errors_from_unit_tests"" designed to resolve unit test failures using Large Language Models (LLMs) and LangChain. The function takes inputs including the failing unit test, the code under test, the error message, and LLM configuration parameters (strength and temperature). It outputs boolean flags indicating if updates are needed, the fixed code strings, and the total operational cost.

The process involves a multi-step workflow using LangChain's LCEL (LangChain Expression Language). First, it loads specific prompt templates from a project directory defined by the $PDD_PATH environment variable. It then executes an initial LLM run to generate a fix based on the provided error context, utilizing a helper `llm_selector` to manage model selection and cost estimation. A second LLM run parses this output into structured JSON to extract the specific code modifications. Throughout the execution, the function is required to use the Python `rich` library for pretty-printing console output, including markdown formatting, token counts, and cost calculations. Finally, it aggregates the costs from both steps and returns the corrected code components and total expense.",9846fde278e0a34b15bcc506160c161df3fc3bdb53004615f524770c460bfae5
context/change/8/initial_fix_errors_from_unit_tests.py,"This Python script defines a function, `fix_errors_from_unit_tests`, designed to automatically correct coding errors identified during unit testing using Large Language Models (LLMs). The process involves a two-step chain. First, it loads prompt templates from external files and uses an LLM (selected via a custom `llm_selector` based on a strength parameter) to analyze the provided unit test, source code, and error message. This step generates a textual explanation and proposed fix. Second, the script employs another LLM call to parse that textual explanation into a structured format defined by the `FixResult` Pydantic model, extracting the specific boolean flags for updates and the corrected code strings. The script utilizes `LangChain` for orchestration, `SQLiteCache` for caching LLM responses to save costs, and `tiktoken` for estimating token usage and costs, which are printed to the console using `rich`. The file includes an example usage block demonstrating how to invoke the function with sample inputs.",0093a6e6dba15c0e14d9df90de11940a49782e1bd0b4bdf22db995718900adb8
context/change/21/change.prompt,"The file contains instructions for creating a wrapper prompt, specifically named with a '_main' suffix, to simplify the main CLI program's execution logic. The goal is to encapsulate complexity so the CLI can invoke the prompt in a single line of code. The instructions specify that only the wrapper prompt should be created, as the core input prompt resides in a separate file. The content includes an example structure showing a 'before' state with a prompt and code, and an 'after' state with the resulting '_main' wrapper prompt. It also references a README file for further details on how the CLI command operates.",df1f9c1accc768b7a9a42e859d92151926fcdc26230020e9ef480483c8864d2e
context/change/21/auto_deps_main_python.prompt,"The provided file is a prompt template designed for an LLM to generate a Python function named `auto_deps_main`. This function is intended to be part of a command-line interface (CLI) tool called `pdd` and handles the logic for an `auto-deps` command. The prompt outlines the specific inputs required for the function, including a Click context object, file paths for prompts and directories, a CSV path for dependency tracking, an output path, and a force scan flag. It also defines the expected outputs: a tuple containing the modified prompt string, the operation cost, and the model name. The prompt includes references to external context files, such as Python preambles, Click library examples, and internal module usage examples (`construct_paths`, `insert_includes`), as well as a README for the CLI command to guide the implementation.",7a2db061d10da159b9656f76988e86fe480df50db534f946fcc7b1b79eac4a91
context/change/21/auto_include_python.prompt,"This document outlines the specifications for creating a Python function named ""auto_include,"" designed to automatically identify and generate necessary dependencies for a given prompt. Acting as an expert Python Software Engineer, the developer is tasked with implementing this function, which takes inputs such as an input prompt, a directory path for dependencies, a CSV file string, and LLM configuration parameters (strength, temperature, verbose). The function's expected output is a tuple containing the generated dependencies string, an updated CSV string, the total generation cost, and the model name used.

The implementation process involves several key steps: loading specific prompt templates ('auto_include_LLM' and 'extract_auto_include_LLM'), summarizing directory contents to create a list of available includes, and invoking an LLM to select relevant dependencies based on the input prompt. A second LLM invocation extracts the final string of includes. The document also provides references to internal module examples for loading templates, invoking LLMs, and summarizing directories, ensuring the developer follows established patterns for these operations.",218dc5ec63e4f861af8ae1edcda9f958dd07f7700e6d46a5476f42d9eec41786
context/change/21/auto_include.py,"This Python script defines a function named `auto_include` designed to automatically identify and insert relevant file dependencies into a given input prompt. The process begins by validating inputs and loading specific prompt templates (`auto_include_LLM` and `extract_auto_include_LLM`). It then utilizes the `summarize_directory` function to generate summaries of files within a specified directory path, potentially using an existing CSV file as a cache or starting point. These file summaries are parsed into a readable format and fed into an LLM via `llm_invoke` to determine which files are relevant to the user's `input_prompt`. A second LLM invocation extracts the specific string of dependencies from the previous analysis using a Pydantic model (`AutoIncludeOutput`) for structured output. The script includes robust error handling and verbose logging using the `rich` library to display progress steps, costs, and model usage. A `main` function is provided to demonstrate example usage, showing how to call `auto_include` with sample parameters and print the final results, including the identified dependencies and total operation cost.",ad932af885a1834d387f2789811b3f369e0d134de0271df965e3d7a065dd7f1c
context/change/simple_math/change.prompt,"The provided file content appears to be a brief instruction or context prompt intended for a software development task, specifically related to a regression test for a module named ""simple_math"". The primary directive within the file is a request to modify code by adding type hints. Specifically, it instructs the developer to annotate the function signature and the return value with appropriate data types. This suggests a refactoring or code quality improvement task aimed at enhancing the readability and static analysis capabilities of the ""simple_math"" codebase.",95fb98651b0fd99b0bb8144f7a6ef78d1f3558f74ceb980d7d44fc2713a539f4
context/change/4/initial_postprocess.py,"This Python script defines a `postprocess` function designed to extract code from raw LLM output, utilizing a secondary LLM call if necessary. The function takes the raw output, the target programming language, a `strength` parameter, and a `temperature` setting as inputs. If the `strength` is set to 0, it defaults to a simpler extraction method via `postprocess_0`. Otherwise, it proceeds to a more robust extraction process. This involves loading a specific prompt template (`extract_code_LLM.prompt`) from a path defined by the `PDD_PATH` environment variable. The script then selects an appropriate LLM model using `llm_selector` based on the provided strength and temperature. It constructs a LangChain LCEL chain combining the prompt, the selected model, and a JSON output parser. Before execution, it calculates and prints the estimated cost based on token counts using `tiktoken`. The chain is then invoked to process the input, and the resulting extracted code is retrieved from the JSON response. Finally, the script prints the extracted code as Markdown, displays the final token usage and cost metrics, and returns both the clean code string and the total operation cost.",6f98012fc0b9ad94acba1c8ec2358bd2e5b695e79ef6f0f868e43d2705abe0e3
context/change/4/initial_postprocess_python.prompt,"The file outlines the requirements for creating a Python function named `postprocess`, designed to extract and clean executable code from a raw string output generated by a Large Language Model (LLM). The function accepts inputs including the raw `llm_output`, the target programming `language`, and model parameters like `strength` and `temperature`. Its primary goal is to return a properly formatted string of code (`extracted_code`) and the associated computational cost. The logic involves a conditional workflow: if the `strength` is 0, it defaults to a zero-cost extraction method (`postprocess_0`). Otherwise, it employs a more complex process using Langchain LCEL. This involves loading a specific prompt template from an environment path, selecting an LLM via a helper function, and invoking the model to parse the output into JSON. The function must also handle token counting (via tiktoken), calculate costs, strip markdown backticks, and pretty-print the results using the `rich` library before returning the final code and cost.",05495463cb8cde9e403585cea916616aeea9a41d3c928b38955c6710db56c5d2
context/change/4/change.prompt,"The file contents describe a specific code refactoring or dependency change within a software project. The primary modification involves switching the mechanism used for counting tokens in a prompt. Previously, the system utilized the `tiktoken` library directly for this purpose. The update replaces this direct usage with a `token_counter` utility imported from a module named `llm_selector`. This change suggests an effort to abstract or centralize token counting logic, likely to support multiple Large Language Model (LLM) backends or to standardize how tokens are calculated across the application, rather than relying on a specific tokenizer implementation like `tiktoken` in this particular context.",c020c75bbee108543aa8569af733d5aee35329ca7c4947d59aa3ed9f3f91c86f
context/change/4/final_postprocess_python.prompt,"The file outlines the requirements for creating a Python function named `postprocess` designed to extract and clean executable code from raw Large Language Model (LLM) string outputs. The function accepts inputs including the raw `llm_output`, the target programming `language`, and parameters for model `strength` and `temperature`. Its primary goal is to return a properly formatted `extracted_code` string and the `total_cost` of the operation. The logic dictates a two-path approach: if the strength is 0, it defaults to a zero-cost `postprocess_0` method. Otherwise, it employs a more complex workflow using Langchain LCEL. This involves loading a specific prompt template from an environment path, selecting an LLM via a helper function, and invoking the model to parse the output into JSON. The process includes steps for token counting, cost calculation, removing markdown backticks, and pretty-printing the results using the `rich` library. The function must also handle errors gracefully and ensure the final output is ready for execution.",a01ccd6084923b345905f2a253f1dd8e40d26e68c5fb944341daf53c4a6b50ae
context/change/15/initial_cli.py,"This file defines the command-line interface (CLI) for a tool called PDD (Prompt-Driven Development). Built using the `click` and `rich` libraries, the CLI provides a suite of commands to facilitate AI-assisted coding workflows. Key functionalities include generating code from prompt files (`generate`), creating example files (`example`), generating unit tests (`test`), preprocessing prompts (`preprocess`), and iteratively fixing code errors based on test failures (`fix`). It also includes commands for splitting complex prompts (`split`), modifying prompts based on changes (`change`), and updating prompts to reflect code modifications (`update`). The tool supports cost tracking via CSV logging, caching with SQLite, and shell completion installation for Bash, Zsh, and Fish.",b607c3e67702ad3bc5e61cc40ce85571e4ff09015de96016c8d7d56eb78b8c74
context/change/15/README.md,"PDD (Prompt-Driven Development) is a command-line interface tool (version 0.1.0) designed to streamline software development by leveraging AI models. It supports multiple programming languages, including Python, JavaScript, and C++, using a specific `.prompt` file naming convention. The tool provides a comprehensive suite of commands to generate runnable code, create unit tests, and produce usage examples directly from prompts. Advanced capabilities include the `fix` command for iterative error correction with budget controls, `split` for managing complex prompts, and `update` to synchronize prompts with modified code.

Users can customize AI behavior through strength and temperature settings and monitor usage via cost tracking CSV outputs. PDD emphasizes workflow efficiency, supporting multi-command chaining to automate complex tasks and allowing flexible output locations via environment variables. Additional features include conflict analysis, crash resolution, and prompt preprocessing. PDD is designed to integrate into various development workflows, including CI/CD pipelines, while providing tools for maintaining code quality and security.",013091190282df4fc6944ba7f0aa93ee1785953e3815a2b2aee2c9245ff0894d
context/change/15/change.prompt,"The file contains instructions to update a prompt by incorporating new commands. Specifically, it notes that new commands have been documented in a referenced README file (located at context/change/15/README.md) and explicitly requests that these additions, such as the 'detect' command, be integrated into the current prompt structure.",e84c7f0f10bcd76b0ef282e5d97d67532efa99bdb3e468c2fa02d2e0e5435a4b
context/change/15/initial_cli_python.prompt,"This file serves as a comprehensive prompt or specification for an AI agent acting as an expert Python engineer to build a command-line interface (CLI) tool named ""pdd"". The tool is designed to be built using the Python `Click` library for CLI management and the `rich` library for pretty-printed console output. The document outlines the project's directory structure (including folders for prompts, context, and data) and specifies several core commands that the tool must support. These commands include 'generate' for creating runnable code from prompts, 'example' for generating context examples, 'test' for creating unit tests, 'preprocess' for handling prompt files (with an XML sub-command), 'fix' for resolving code errors (with a loop sub-command), 'split' for dividing prompt files, 'change' for modifying prompts, and 'update' for refreshing prompt content. Additionally, it includes instructions for an 'install_completion' command to handle shell auto-completion. The file references external context files (e.g., `<./context/click_example.py>`) to provide specific implementation examples for each function, ensuring the AI follows established patterns for file loading, path construction, and module imports.",f82bd3b41440348f58b8d2a98606b1edc4db6ff8eb20922551e7fd4f79c9003b
context/change/15/modified_initial_cli.prompt,"The provided text serves as a detailed specification prompt for an AI to generate a Python command-line interface (CLI) tool named ""pdd"". The tool is designed to be built using the `click` library for command handling and the `rich` library for formatted console output. The prompt outlines the project's directory structure and lists specific commands the CLI must support, including `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `change`, `update`, `detect`, `conflicts`, `crash`, and `install_completion`. For each command, the prompt references specific example files (e.g., `<./context/code_generator_example.py>`) that demonstrate the intended functionality, such as generating code from prompts, creating unit tests, preprocessing files, and fixing errors based on test outputs. It also includes instructions on handling imports to avoid naming conflicts and managing file paths.",5415a463b9a2048fb889ced2d0c4f76a727cbe7b8e2d0d95ade7500ce2d35495
context/change/3/initial_test_generator_python.prompt,"The file outlines the requirements for a Python function named `test_generator`, designed to automatically create unit tests from a given code file using a Large Language Model (LLM). The function takes inputs such as the original prompt, the source code, model strength, temperature, and the target language. It returns the generated unit test code and the total cost of the operation. The process involves several steps: loading a specific prompt template from an environment path, preprocessing the prompt, creating a LangChain LCEL template, selecting an appropriate LLM based on the provided parameters, and invoking the model. The function is also required to utilize the `rich` library for pretty-printing console output, including token counts calculated via `tiktoken` and cost estimates. Finally, the output must be post-processed to extract runnable code from the model's response before returning the final unit test and total cost.",2cbcf0515c1fb80cecd158b4311b995eee532f589f056ded1da75cfecd26989f
context/change/3/intial_test_generator.py,"This Python script defines a function named `test_generator` designed to automate the creation of unit tests for a given piece of code using a Large Language Model (LLM). The function takes inputs such as the original prompt, the source code, model strength, temperature, and the programming language. It operates through a multi-step process: first, it loads and preprocesses a specific prompt template from a file path defined in the environment variables. Next, it utilizes a custom `llm_selector` to choose an appropriate LLM based on the provided parameters. The script then constructs a LangChain processing chain to generate the test code. Throughout execution, it calculates and displays token usage and estimated costs for both input and output using `tiktoken`. Finally, the generated output undergoes a post-processing step to refine the unit test code before returning both the final test code and the total calculated cost. The script also includes error handling to catch and report exceptions during the generation process.",461bc71259c7cc21645e4109483b1e84b90338709bddc6904b9d336e8157a014
context/change/3/final_test_generator_python.prompt,"The file outlines the requirements for a Python function named `test_generator`, designed to automatically create unit tests from a given code file. Acting as an expert Python Software Engineer, the function takes inputs such as the original prompt, the source code, model strength, temperature, and the target language. It utilizes the Langchain library and LCEL (LangChain Expression Language) to process these inputs. The workflow involves loading a specific prompt template from an environment path, preprocessing it, selecting an appropriate LLM via `llm_selector`, and invoking the model while tracking token usage and cost. The output is then post-processed to extract runnable unit test code from the model's response, separating code from text. Finally, the function returns the generated unit test code and the total calculated cost, while utilizing the Python `rich` library for pretty-printing console output throughout the process.",9b81f2b3c56affef9963b6ef595d45329651b99f9f33bf60c5798b5114650f77
context/change/3/change.prompt,"The file contents describe a specific code refactoring or dependency change within a software project. The primary modification involves switching the mechanism used for counting tokens in a prompt. Previously, the system utilized the `tiktoken` library directly for this purpose. The update replaces this direct usage with a `token_counter` utility imported from a module named `llm_selector`. This change suggests an effort to abstract or centralize token counting logic, likely to support multiple Large Language Model (LLM) backends or to standardize how tokens are calculated across the application, rather than relying on a specific tokenizer implementation like `tiktoken` in this particular context.",c020c75bbee108543aa8569af733d5aee35329ca7c4947d59aa3ed9f3f91c86f
context/change/12/final_unfinished_prompt_python.prompt,"This document outlines the requirements for creating a Python function named 'unfinished_prompt', designed to evaluate whether a given text prompt is complete or requires continuation. The function takes 'prompt_text', 'strength', and 'temperature' as inputs and returns a reasoning string, a boolean status ('is_finished'), and an optional total cost. The implementation process involves five steps: loading a specific prompt file using the '$PDD_PATH' environment variable, creating a Langchain LCEL template that outputs JSON, selecting an LLM model via a helper function, executing the analysis while calculating and printing token usage and costs, and finally returning the results. The instructions emphasize using relative imports for dependencies like 'llm_selector', handling errors gracefully, and utilizing the 'rich' library for pretty-printing status updates and results.",49363e12ed87470d174ca3735c6f0484e9acbced0ada19fc3dc965b9e808722e
context/change/12/initial_postprocess.py,"This Python file defines a `postprocess` function designed to extract and format code snippets from the raw text output of a Large Language Model (LLM). The function takes the LLM's raw output, the target programming language, and optional parameters for model strength and temperature as inputs. It first checks if the `strength` parameter is zero; if so, it defaults to a simpler `postprocess_0` function. Otherwise, it proceeds with a more complex workflow using LangChain components. This involves loading a specific prompt template from a file path defined by the `PDD_PATH` environment variable and initializing an LLM via an `llm_selector` utility. The function constructs a processing chain that feeds the raw output into the selected model to isolate the code. It calculates and prints token usage and estimated costs for both input and output using the `rich` library for console formatting. Finally, it cleans the extracted code by removing markdown code block delimiters (triple backticks) and returns a tuple containing the clean code string and the total estimated cost. Error handling is included to catch and report exceptions during the process.",845d5ada84654630518eda1f06e372bd2af87a230077ce8078efc543b9286714
context/change/12/initial_postprocess_python.prompt,"The file outlines the requirements for creating a Python function named `postprocess` designed to refine raw string output from a Large Language Model (LLM) into executable code. The function accepts the LLM's output, the target programming language, and parameters for model strength and temperature. Its primary goal is to extract code blocks while properly commenting out non-code text. The logic involves a conditional workflow: if the strength parameter is zero, it defaults to a zero-cost post-processing method (`postprocess_0`). Otherwise, it employs a more complex Langchain LCEL pipeline. This pipeline involves loading a specific prompt template from an environment path, selecting an LLM model via a helper function, and invoking the model to generate a JSON response containing the cleaned code. The function must also handle token counting and cost estimation, pretty-print status updates and the final code using the `rich` library, and strip markdown code fences (triple backticks) from the result. Error handling and relative imports for dependencies are also required.",5fe8c783e322fab102ae0ea87817e30a24571dfc745e235568a8b61216f88478
context/change/12/change.prompt,"The file contains a single, specific instruction intended for a Large Language Model (LLM). This instruction directs the model to generate a new prompt. The goal of this generated prompt is to create a function named 'unfinished_prompt'. This function is specifically designed to handle the execution of another prompt identified as 'unfinished_prompt_LLM'. The file includes an XML-like tag structure referencing an external file path ('prompts/unfinished_prompt_LLM.prompt'), suggesting a system where prompts are modular and can include other prompt files.",af205548a344020b63fb246f92c6b1ef95390f76693c4efee4e3d03e86423306
context/change/2/final_context_generator_python.prompt,"The provided text outlines a specification for an expert Python engineer to create a function named ""context_generator."" This function is designed to automatically generate concise usage examples for a given code module. The function takes several inputs: the code module itself, the original prompt used to create it, the programming language (defaulting to Python), and parameters for the LLM model such as strength and temperature. The expected outputs are the generated example code string and the total computational cost.

The implementation plan involves a multi-step process using LangChain LCEL (LangChain Expression Language). It requires loading a specific prompt template from a project directory defined by an environment variable ($PDD_PATH). The workflow includes selecting an appropriate LLM, preprocessing the input prompt, and invoking the model with specific parameters. Crucially, the function must calculate and display token usage and costs. Finally, the raw output from the LLM must be post-processed to extract runnable code, separating it from explanatory text, before returning the final example and the total cost.",385d6ba50da55b5e361831090410ad30050c67b09f87c78814698504bb5d9894
context/change/2/change.prompt,"The file contents describe a specific code refactoring or dependency change within a software project. The primary modification involves switching the mechanism used for counting tokens in a prompt. Previously, the system utilized the `tiktoken` library directly for this purpose. The update replaces this direct usage with a `token_counter` utility imported from a module named `llm_selector`. This change suggests an effort to abstract or centralize token counting logic, likely to support multiple Large Language Model (LLM) backends or to standardize how tokens are calculated across the application, rather than relying on a specific tokenizer implementation like `tiktoken` in this particular context.",c020c75bbee108543aa8569af733d5aee35329ca7c4947d59aa3ed9f3f91c86f
context/change/2/initial_context_generator_python.prompt,"The provided file outlines a specification for an expert Python engineer to create a function named ""context_generator."" This function is designed to automatically generate concise usage examples for a given code module. It takes several inputs: the code module itself, the original prompt used to create it, the programming language (defaulting to Python), and parameters for the LLM model such as strength and temperature. The function is expected to return the generated example code and the total cost of the operation.

The implementation details require using LangChain's LCEL (LangChain Expression Language) framework. The process involves seven specific steps: loading a prompt template from a specific file path defined by an environment variable, creating an LCEL template, selecting an appropriate LLM model, preprocessing the input prompt, and invoking the model with specific parameters. Additionally, the function must calculate and pretty-print token usage and costs using tiktoken, post-process the model's output to extract runnable code, and finally return the clean example code along with the total financial cost.",9cd2c95487a87d5477be43fe8b75e9d7e18c1a665b1dc88bbe634a3c8c6a2501
context/change/2/initial_context_generator.py,"This Python script defines a function named `context_generator` designed to generate example code snippets using a Large Language Model (LLM). The function takes a code module, a user prompt, a target programming language, and model parameters (strength and temperature) as inputs. It orchestrates a multi-step process involving loading a specific prompt template from a file path defined by an environment variable (`PDD_PATH`), selecting an appropriate LLM via a custom `llm_selector`, and preprocessing the user input. The script utilizes LangChain's LCEL syntax to create a processing chain that pipes the formatted prompt through the selected model and parses the string output. It also incorporates `tiktoken` to calculate token usage and estimates the financial cost of the API calls, displaying these metrics to the console using the `rich` library. Finally, the raw output undergoes a post-processing step to refine the code before the function returns both the generated example code and the total calculated cost of the operation.",41caae210b6ef96935fdc2aa17ce7d2d6f9ac8af5c120fff001e27e0d2dd7709
context/change/13/modified_initial_split.prompt,"This document outlines the requirements for creating a Python function named ""continue_generation"" designed to ensure the completeness of Large Language Model (LLM) outputs. The function, intended for a Python package using relative imports and the Rich library for console output, takes a formatted input prompt and the current LLM output as inputs. Its primary goal is to return a fully completed string, ""final_llm_output,"" along with the total cost of generation. The process involves loading a specific prompt file, preprocessing it, and creating a Langchain LCEL template. The function must utilize an ""llm_selector"" for model selection and token counting. It executes a loop where it checks if the generation is incomplete using an ""unfinished_prompt"" helper; if so, it continues generating content until finished. Throughout the process, the function is required to pretty-print outputs, token counts, and estimated costs to the console, while also handling edge cases and errors robustly.",75c138c60dc6b19011ebf541a8f550551a25d47f7b7fd773aa6bb8f3011e3b5f
context/change/13/initial_split_python.prompt,"The file outlines the requirements for creating a Python function named ""split"" designed to decompose a single input prompt into a ""sub_prompt"" and a ""modified_prompt"" without losing functionality. Intended for a Python package using relative imports, the function must utilize the Python Rich library for pretty-printing console output. The process involves loading specific prompt files, preprocessing them (handling curly brackets appropriately), and employing Langchain LCEL templates to interact with an LLM. The function takes inputs including the original prompt, generated code, example usage code, and model parameters (strength, temperature). It executes a multi-step workflow: running the initial prompt through an LLM, calculating token usage and costs via an `llm_selector`, and then using a second extraction prompt to parse the LLM's output into JSON to retrieve the split prompts. Finally, it returns the two resulting prompts and the total operational cost, while ensuring robust error handling for edge cases.",6757971fa9db7569eae50474c720e7b049baeb7b804256a02441be3660ab5414
context/change/13/initial_split.py,"This Python script defines a function named `split` designed to process and split input code using a Large Language Model (LLM). The script leverages the `langchain` library for LLM interactions and prompt management, and the `rich` library for formatted console output. The `split` function takes an input prompt, code, example code, and model parameters (strength and temperature) as arguments. It loads specific prompt templates from files, preprocesses them, and uses a custom `llm_selector` to choose an appropriate model. The function executes a two-step LLM chain: first, it generates a split of the code based on the inputs, calculating token usage and estimated costs; second, it parses the LLM's output into a structured JSON format to extract a 'sub_prompt' and a 'modified_prompt'. The script also includes caching via SQLite to optimize performance and cost. An example usage block at the end demonstrates how to call the function and print the results.",61247194419b17b8cd986efdbbbb6782a34e89b3cfe6e895ce24419490b33578
context/change/13/change.prompt,"The file describes a Python function named 'continue_generation' designed to complete unfinished outputs from a Large Language Model (LLM). This function takes a formatted input prompt and the initial, potentially incomplete, LLM output as arguments. It utilizes a specific prompt template located at 'prompts/continue_generation_LLM.prompt' to generate the missing content. The process involves a loop that checks if the generation is incomplete using a helper function called 'unfinished_prompt'. If the output is detected as unfinished, the function triggers the continuation prompt and appends the new text to the existing LLM output, repeating this cycle until the generation is fully complete.",662ac3b1a52fdde909657242e63a887aaf0b17272d79301a184b51c27049479e
context/change/5/initial_split_python.prompt,"This document outlines the requirements for a Python function named ""split,"" designed to decompose a single input prompt into a ""sub_prompt"" and a ""modified_prompt"" without losing functionality. The function targets an expert Python Software Engineer and mandates the use of the Python Rich library for pretty-printing console output. Inputs include the original prompt, generated code, example usage code, and LLM parameters like strength and temperature. The process involves loading specific prompt files, preprocessing them, and utilizing Langchain LCEL templates to interact with an LLM selected via an `llm_selector`. The workflow requires a two-step LLM interaction: first to generate a split suggestion based on the inputs, and second to extract the specific sub-prompt and modified prompt as JSON. Throughout execution, the function must calculate and display token counts and estimated costs using `tiktoken`. Finally, it returns the two resulting prompts and the total cost, while ensuring robust error handling for edge cases.",6a653c48fcdcf33dcc269eeea3282660082a42c71c64bd1397ec10234fed7570
context/change/5/final_split_python.prompt,"The file outlines the requirements for a Python function named ""split,"" designed to decompose a single input prompt into a ""sub_prompt"" and a ""modified_prompt"" without losing functionality. The function leverages the Langchain LCEL framework and requires inputs such as the original prompt, generated code, example usage code, and LLM parameters like strength and temperature. The process involves loading specific prompt templates, preprocessing them, and executing a two-step LLM interaction: first to generate a split strategy and second to extract the specific prompt components in JSON format. The function must also utilize a custom ""llm_selector"" for model selection and cost estimation, pretty-print all console outputs using the Rich library (including token counts and costs), and handle edge cases robustly. The final output includes the two resulting prompt strings and the total calculated cost of the operation.",29740d1d81ac51d191ef18e748e8ab5a62e41df41a9623e12f4b3b620ba49e7c
context/change/5/initial_split.py,"This file implements a Python function named `split` designed to process prompts and code using Large Language Models (LLMs) via the Langchain library. The function orchestrates a multi-step workflow: first, it loads specific prompt templates from file paths defined by an environment variable (`PDD_PATH`). It then utilizes a custom `llm_selector` to choose an appropriate model based on desired strength and temperature. The core logic involves two main LLM invocations: one to process the initial input (prompt, code, and examples) and a second to extract structured JSON output containing a 'sub_prompt' and a 'modified_prompt'. Throughout the execution, the script uses the `tiktoken` library to calculate token usage and estimates the financial cost of the API calls. Finally, it employs the `rich` library to pretty-print status updates, costs, and the resulting prompts in Markdown format before returning the extracted prompts and the total calculated cost.",90fc7bff1e86307d7e11ef7f24eb8cd2dcb922a75d8a40083c3a22a2c9a47c28
context/change/5/change.prompt,"The file contents describe a specific code refactoring or dependency change within a software project. The primary modification involves switching the mechanism used for counting tokens in a prompt. Previously, the system utilized the `tiktoken` library directly for this purpose. The update replaces this direct usage with a `token_counter` utility imported from a module named `llm_selector`. This change suggests an effort to abstract or centralize token counting logic, likely to support multiple Large Language Model (LLM) backends or to standardize how tokens are calculated across the application, rather than relying on a specific tokenizer implementation like `tiktoken` in this particular context.",c020c75bbee108543aa8569af733d5aee35329ca7c4947d59aa3ed9f3f91c86f
context/change/14/initial_change_python.prompt,"This document outlines the requirements for creating a Python function named ""change"" designed to modify an existing prompt based on specific instructions. The function takes inputs including an initial prompt, generated code, modification instructions, and LLM parameters (strength and temperature). It outputs a modified prompt string, the total operational cost, and the model name used. The implementation must utilize the Python Rich library for console output and relative imports for modules. The process involves several steps: loading specific prompt files, preprocessing them (handling curly brackets), creating Langchain LCEL templates, and selecting an LLM model with token counting capabilities. The function executes a two-step LLM process: first, generating a raw modified prompt based on the inputs, and second, extracting the final structured ""modified_prompt"" from that output using a secondary extraction prompt. Throughout execution, the function must calculate and display token counts and estimated costs, handle edge cases, and return the final modified prompt along with cost metrics.",5e4d1b24cfaf48eed80704c64cc75d9c888914189ba700487e3d77c3b6a97f3b
context/change/14/modified_initial_change.prompt,"The file outlines the requirements for a Python function named `detect_change`, designed to analyze a list of prompt files against a change description to determine necessary modifications. The function takes inputs including a list of prompt filenames, a description of the changes, and LLM parameters like strength and temperature. It outputs a list of JSON objects detailing required changes, the total operational cost, and the model name used. The implementation involves several steps: loading specific prompt templates (`detect_change_LLM.prompt` and `extract_detect_change_LLM.prompt`), preprocessing them, and utilizing Langchain LCEL for execution. It requires selecting an LLM model via `llm_selector` to handle token counting and cost estimation. The process includes two main LLM invocations: first to analyze the prompts and description, and second to extract a structured JSON list of changes from the initial analysis. All console output must be formatted using the Python Rich library, and the code should handle edge cases and errors robustly.",6855a0031175a7ed777a9ed3460fe01a7e98cae3057615d2ca3e14e5df1674fa
context/change/14/initial_change.py,"This Python script defines a function named `change` designed to automate the modification of prompts using Large Language Models (LLMs). Leveraging the `langchain` framework for orchestration and the `rich` library for formatted console output, the function executes a multi-step workflow. It begins by loading specific prompt templates from a directory defined by an environment variable and selecting an appropriate LLM configuration via a `llm_selector` helper based on provided strength and temperature parameters. The core logic involves two sequential LangChain operations: first, a generation chain that takes an input prompt, associated code, and a change description to produce a raw modification result; and second, an extraction chain that parses this output to isolate the final `modified_prompt` using a JSON parser. Throughout the process, the code calculates and displays token usage and estimated financial costs for each step. It includes error handling for file access and JSON parsing issues, ultimately returning the modified prompt, the total cost, and the model name used.",fec3fba35a4710e02375bdb017bcf33882c2a3b690733fc9d5c6021c1602785c
context/change/14/change.prompt,"The provided file contents outline a prompt designed to generate Python code for a function named 'detect_change'. This function's primary objective is to create a list of necessary modifications to existing prompts based on a user-provided change description. The function accepts a list of prompt filenames (e.g., [""prompt1_filename"", ""prompt2_filename""]) and a description of the desired change as inputs. To accomplish its task, the function utilizes two specific embedded LLM prompts: 'detect_change_LLM', which is included from 'prompts/detect_change_LLM.prompt', and 'extract_detect_change_LLM', included from 'prompts/extract_detect_change_LLM.prompt'. The process concludes by extracting a structured list identifying which prompts require updates and detailing the specific changes needed for each.",b781292c1505d291af2765278aade539d947b51d6ddae09d042f1dbe7a1a2d64
context/change/22/preprocess.py,"This Python script implements a text preprocessing utility, likely designed for preparing prompts for Large Language Models (LLMs) or template rendering systems. It utilizes the `rich` library to provide styled console feedback during execution. The core functionality is encapsulated in the `preprocess` function, which orchestrates a series of text transformations. The script handles dynamic content insertion through two primary mechanisms: resolving file paths embedded within triple backticks and processing custom XML-like tags. Specifically, it supports an `<include>` tag for inserting external file content and a `<shell>` tag for executing system commands and embedding their standard output. Both inclusion methods support recursive processing. Additionally, the script features a robust mechanism to escape curly braces by doubling them (converting `{` to `{{`), which is crucial for preventing syntax errors in Python string formatting. This escaping logic is context-aware, distinguishing between general text and code blocks, and allows users to specify a list of keys to exclude from escaping, ensuring intended variables remain valid placeholders.",eff9a44440459568c8508c99b180659c2bc81ada5580f793a075296aafd91c2d
context/change/22/preprocess_main_python.prompt,"The provided file is a prompt template designed for an expert Python engineer AI. Its primary objective is to generate a Python function named `preprocess_main`, which serves as a Command Line Interface (CLI) wrapper for preprocessing prompt files. The prompt specifies that the function should utilize the `click` library for CLI interactions, accepting inputs such as the prompt file path, an optional output path, and a boolean flag for XML delimiter insertion. The expected output of the generated function is a tuple containing the preprocessed prompt string, the operation cost, and the model name. The prompt template includes references to external context files, such as Python preambles, examples of `click` usage, and internal module examples (`construct_paths`, `preprocess`, `xml_tagger`). It also references a README file to ensure the generated code aligns with existing documentation and supports global options like verbosity.",5d1fc2fd413af4318ba2f5fc32b59dbd17e313e368bb8b6422ca4a2d17dc2091
context/change/22/change.prompt,"The provided file contains instructions and examples for creating a specific type of wrapper prompt, denoted as `*_main`. The primary goal outlined in the file is to simplify the main Command Line Interface (CLI) program by encapsulating complexity within these wrapper prompts, allowing the CLI to execute tasks with a single line of code. The instructions specify that only the `_main` wrapper prompt should be generated, as the core input prompt resides in a separate file. The content includes XML-like tags defining examples (`<examples>`) that demonstrate the transformation from a `before_example` state (containing a prompt and associated code) to an `after_example` state (the resulting `*_main` prompt). Additionally, it references a README file (`<cli_command_readme>`) which presumably contains further details on the CLI command's operation.",b3adb705bb6357568444a9c46928e8183382cbf8db0a313c7e6e907d15933d3c
context/change/22/preprocess_python.prompt,"The file outlines the requirements for a Python function named `preprocess_prompt`, designed to prepare prompt strings for Large Language Models (LLMs). The function takes a prompt string, along with optional boolean flags for recursion and curly bracket doubling, and an optional list of keys to exclude from doubling. Its primary task is to parse and process specific XML-like tags within the prompt: `<include>` tags replace the tag with the contents of a referenced file, `<pdd>` tags act as comments and are removed entirely, and `<shell>` tags execute shell commands and replace the tag with the command's output. The function must handle nested includes recursively if the `recursive` flag is set, supporting both XML-style includes and file paths enclosed in angle brackets within triple backticks. Additionally, if `double_curly_brackets` is True, the function must escape single curly brackets by doubling them, unless the enclosed key is in the `exclude_keys` list, while handling nested brackets via recursion. Finally, the function should resolve file paths relative to the current directory and return the cleaned, preprocessed prompt string with whitespace trimmed.",eb8f9a57c8e111424fb259290b44663eea87d8d6941113f9f578d9dc084ee313
context/fix_errors_from_unit_tests/1/conflicts_in_prompts.py,"This Python file defines functionality for detecting and analyzing conflicts between two text prompts using Large Language Models (LLMs). It utilizes the LangChain library to orchestrate the interaction with LLMs and Pydantic for structured data validation. The core component is the `conflicts_in_prompts` function, which takes two prompt strings as input along with optional parameters for model strength and temperature. 

The process involves a multi-step workflow: first, it loads specific prompt templates from a directory specified by the `PDD_PATH` environment variable. It then uses an `llm_selector` utility to choose an appropriate model based on the requested strength. The function executes a primary LLM chain to generate a raw analysis of potential conflicts between the two inputs. Subsequently, a second extraction chain parses this raw output into a structured JSON format defined by the `ConflictOutput` and `Conflict` Pydantic models. This structure includes descriptions, explanations, and specific suggestions for resolving the conflicts in each prompt. Finally, the function returns a list of identified conflicts, the total estimated cost of the API calls, and the name of the model used.",e1a0cd705e318d7f93d847b383a646ec764d0decc95526ddc68d616783fa8e21
context/fix_errors_from_unit_tests/1/conflicts_in_prompts_python.prompt,"This file provides a detailed specification for an expert Python engineer to create a function named ""conflicts_in_prompts"". The purpose of this function is to analyze two input prompts, identify conflicts between them, and suggest resolutions. The function takes two prompt strings, a strength parameter (default 0.5), and a temperature parameter (default 0) as inputs. It returns a list of conflict dictionaries (containing descriptions, explanations, and suggestions for both prompts), the total cost of the operation, and the model name used.

The implementation instructions require using the LangChain Expression Language (LCEL). The process involves six specific steps: loading prompt templates from environment paths, creating an LCEL template for conflict detection, selecting an LLM using an internal `llm_selector` module, running the prompts through the model, and then using a second LCEL template to extract the results into a structured JSON format. The specification also emphasizes calculating and pretty-printing token counts and costs at various stages using the `llm_selector`'s token counter. It includes references to external context files for Python preambles, LCEL examples, and internal module usage.",c02afe41a32b1ddb8b4c9c7c6ad6a74581a8969f88d787b16b9e79a8ffbc1276
context/fix_errors_from_unit_tests/1/test_conflicts_in_prompts.py,"This file contains a suite of unit tests for the `conflicts_in_prompts` function, which appears to be part of a module named `pdd`. The tests are written using the `pytest` framework and rely heavily on `unittest.mock` to isolate the function from external dependencies like file systems, environment variables, and Large Language Model (LLM) calls. 

The test suite includes several fixtures to mock the environment (`PDD_PATH`), prompt files, the LLM selector mechanism, and the JSON output parser. The tests cover various scenarios: successful execution where conflicts are correctly identified and parsed; error handling for missing environment variables (`PDD_PATH`); handling of `FileNotFoundError` when prompt templates are missing; and graceful handling of unexpected exceptions during execution. Additionally, there is a test case verifying that custom parameters for `strength` and `temperature` are correctly passed to the underlying LLM selector.",1c49a90ac93a88b00811fb98b28661b6dd788cb9086b7778f232f5e692ada024
context/fix_errors_from_unit_tests/4/test_detect_change_1_0_1.py,"This file contains a suite of unit tests for the `detect_change` function within the `pdd` module, utilizing the `pytest` framework. The tests are designed to verify the behavior of the function under various scenarios, including successful execution and error handling. 

The file defines several fixtures to mock external dependencies and environment variables, such as `mock_environment` for setting the `PDD_PATH`, `mock_file_contents` for simulating file reads, `mock_llm_selector` for simulating Large Language Model interactions, and `mock_preprocess` for bypassing preprocessing logic. 

The test cases cover four main scenarios: `test_detect_change_success` verifies that the function correctly processes a prompt file and returns the expected change instructions and costs; `test_detect_change_file_not_found` ensures the function handles missing files gracefully by returning an empty list; `test_detect_change_json_decode_error` checks the robustness against malformed JSON outputs from the LLM; and `test_detect_change_unexpected_error` confirms that general exceptions are caught without crashing the application. Overall, the file ensures the reliability of the change detection logic by isolating it from external file systems and API calls.",ce21ab0c26b1dc32f6d00ad1699a191804913e3cdabce68b68cfb968241e90e5
context/fix_errors_from_unit_tests/4/detect_change_1_0_1.py,"This file defines a Python function named `detect_change` designed to analyze a list of prompt files and determine necessary modifications based on a provided change description. The function leverages Large Language Models (LLMs) via LangChain to perform this analysis. It begins by loading specific prompt templates (`detect_change_LLM.prompt` and `extract_detect_change_LLM.prompt`) from a directory specified by an environment variable. The process involves preprocessing these templates and the change description, selecting an appropriate LLM based on strength and temperature parameters, and then invoking the model to generate an analysis of which prompts require updates. A secondary extraction step parses the LLM's raw output into a structured JSON format containing a list of changes. The function calculates and reports the token usage and estimated cost for these operations using the `rich` library for console output. Finally, it returns a list of required changes, the total cost of the operation, and the name of the model used, while handling potential errors such as missing files or invalid JSON.",8f6abd862b4b6bfb75367f9e0e891e3587830f6616bae02e3b0bbd13bf571ade
context/fix_errors_from_unit_tests/4/test_detect_change.py,"This file contains a suite of unit tests for the `detect_change` function within the `pdd.detect_change` module, utilizing the `pytest` framework. The tests rely heavily on mocking external dependencies and internal components to isolate the function's logic. Key components mocked include the `PromptTemplate`, `LLM` (Large Language Model), `OutputParser`, file system operations (`builtins.open`), and environment variables (`PDD_PATH`).

The test suite covers several scenarios:
1.  **Success Case (`test_detect_change_success`)**: Verifies that the function correctly processes prompt files, invokes the LLM chain, parses the output, and returns the expected list of changes, total cost, and model name.
2.  **File Not Found (`test_detect_change_file_not_found`)**: Ensures the function handles missing files gracefully by returning an empty result and zero cost.
3.  **JSON Decode Error (`test_detect_change_json_decode_error`)**: Checks the function's resilience against malformed JSON output from the LLM, ensuring it fails safely without crashing.
4.  **Unexpected Error (`test_detect_change_unexpected_error`)**: Tests the general exception handling mechanism when an unforeseen error occurs during execution.

Overall, the file ensures the robustness of the change detection logic by simulating various success and failure conditions.",8323dc0c5697c60fb4d86275525538e8500805a1a2b2f8e9875f96f462184111
context/fix_errors_from_unit_tests/4/detect_change_python.prompt,"This file provides a detailed specification for an AI agent acting as an expert Python Software Engineer. The agent's primary task is to generate a Python function named ""detect_change"". This function is designed to analyze a list of prompt files against a provided change description to determine which prompts require modification. The specification outlines the function's inputs (prompt files, change description, strength, temperature) and outputs (a list of changes, total cost, and model name). It includes references to context files for Python preambles and LangChain examples. The core of the instruction is a step-by-step procedure for the function logic: loading specific prompt templates, preprocessing inputs, utilizing a LangChain LCEL pipeline with an LLM selector for model management and cost calculation, and executing a two-step LLM process (detection followed by extraction). Finally, the function must pretty-print the results using Rich Markdown and return the structured change list along with cost metrics.",70c81d3b3c7354a1213c362cac334741c95ef71917442103444a514aad84cc46
context/fix_errors_from_unit_tests/4/detect_change.py,"This Python file defines a function named `detect_change` designed to analyze a list of prompt files against a change description to identify necessary modifications. The process involves several steps: loading specific prompt templates (`detect_change_LLM.prompt` and `extract_detect_change_LLM.prompt`) from a directory specified by an environment variable, preprocessing these templates, and selecting an appropriate Large Language Model (LLM) based on strength and temperature parameters using a custom `llm_selector`. The core logic reads the content of the provided prompt files, constructs a prompt list, and executes a LangChain-based processing chain to generate an analysis of required changes. Subsequently, a second LLM call extracts a structured JSON list of changes from the initial analysis. The function calculates and reports the token usage and estimated cost for each step using the `rich` library for console output. Finally, it returns a list of changes (containing prompt names and instructions), the total estimated cost, and the name of the model used, while handling potential file or JSON errors gracefully.",43ad9e25e79a3f7c6e0dd30ee31c2d28a96602ee14c0a93e00407c511664e30b
context/fix_errors_from_unit_tests/3/test_context_generator.py,"This file contains a suite of unit tests for the `context_generator` function within the `pdd.context_generator` module, utilizing the `pytest` framework. The tests employ several fixtures (`mock_environment`, `mock_file_content`, `mock_llm_selector`, `mock_chain_run`) to simulate external dependencies like environment variables, file I/O, and Large Language Model (LLM) interactions. The test suite covers various scenarios: a successful execution path where the generator produces output correctly; a scenario handling unfinished prompts that require continued generation; a failure case where the required `PDD_PATH` environment variable is missing, triggering a `ValueError`; a `FileNotFoundError` scenario which asserts a safe fallback return value; and a general exception handling test where an error during preprocessing results in an empty return tuple. Extensive mocking is used for internal calls such as `preprocess`, `llm_selector`, `LLMChain`, `unfinished_prompt`, and `postprocess` to isolate the unit logic.",653b103f8921a11155192345c234a1856a80bc190b9baca47c5f19e4b23a9b3f
context/fix_errors_from_unit_tests/3/context_generator_python.prompt,"The file outlines a prompt for an expert Python engineer to create a function named ""context_generator."" This function is designed to generate concise usage examples for a given code module. The prompt specifies the function's inputs (code module, original prompt, language, strength, and temperature) and outputs (the generated example code, total cost, and model name). It details a nine-step process using LangChain Expression Language (LCEL), which involves loading a specific prompt file, preprocessing it, selecting an LLM via an internal `llm_selector`, and invoking the model. The process also includes logic for handling incomplete generations using an `unfinished_prompt` check and a `continue_generation` function, as well as post-processing the final output. The file references several internal context examples for tasks like preprocessing, token counting, and post-processing to guide the implementation.",1a59e3e1f7d3aa598a9a92552c6300b827c1d0791903757150d7fbbee41ff889
context/fix_errors_from_unit_tests/3/context_generator.py,"This file defines a Python function named `context_generator` designed to generate example code using a Large Language Model (LLM). The function orchestrates a multi-step process that begins by validating the `PDD_PATH` environment variable and loading a specific prompt template (`example_generator_LLM.prompt`). It utilizes a custom `llm_selector` to choose an appropriate model based on strength and temperature parameters. The workflow involves preprocessing both the template and the user's input prompt, creating a LangChain `LLMChain`, and executing the model to generate code. Crucially, the function includes logic to handle incomplete outputs by detecting unfinished generation and invoking a `continue_generation` routine. It also performs post-processing on the final output. Throughout the execution, the code calculates and prints the estimated token usage and financial cost using a token counter, returning the generated code, total cost, and the name of the model used.",13e2be9a3c3f6fc3e1bce5184358a9d3aa2c4eb161d9f75022b4354c0fe3453d
context/fix_errors_from_unit_tests/2/test_code_generator.py,"This file contains a suite of unit tests for the `code_generator` function within the `pdd.code_generator` module, utilizing the `pytest` framework and `unittest.mock` library. The tests verify the function's behavior across three primary scenarios: successful code generation, handling of incomplete generation requiring continuation, and exception handling. 

The first test case, `test_code_generator_success`, mocks dependencies like `preprocess`, `llm_selector`, and `postprocess` to simulate a standard workflow where code is generated, processed, and returned correctly with accurate cost calculations. The second test, `test_code_generator_incomplete_generation`, simulates a scenario where the initial prompt response is unfinished, verifying that the `continue_generation` logic is triggered and functions correctly. Finally, `test_code_generator_exception_handling` ensures that the function gracefully handles errors by returning empty strings and zero costs when an exception (e.g., during preprocessing) occurs. Throughout the file, mock data is defined for prompts, language settings, and model parameters to ensure consistent testing conditions.",f17f51a3df266a3912ce071c83a62889d2b2d3d4281dc84f578e8c5fce032a4c
context/fix_errors_from_unit_tests/2/code_generator_python.prompt,"This file outlines a prompt for an expert Python engineer to create a function named ""code_generator."" The function's purpose is to compile a raw text prompt into a runnable code file. The prompt specifies the function's inputs (prompt string, target language, model strength, and temperature) and outputs (runnable code, total cost, and model name). It includes references to external context files for Python preambles, LangChain Expression Language (LCEL) examples, and internal module usage (preprocessing, LLM selection, unfinished prompt detection, continuation, and postprocessing). The prompt details an 8-step workflow using LangChain: preprocessing the prompt, creating an LCEL template, selecting an LLM, running the model with cost/token tracking, displaying results with Markdown, detecting and handling incomplete generations, postprocessing the output, and finally returning the code, cost, and model name.",d901b1f56706f97efddd1f30075456f5441419f5276191220e4bed7d09204a77
context/fix_errors_from_unit_tests/2/code_generator.py,"This file defines a `code_generator` function designed to generate programming code based on a user prompt using a Large Language Model (LLM). The process begins by preprocessing the raw prompt to handle recursive elements and formatting. It then selects an appropriate LLM based on a specified strength and temperature using an `llm_selector` module. The prompt is executed through a LangChain pipeline, and the initial result is displayed and cost-calculated based on token usage. The function includes logic to detect if the LLM's output is incomplete; if so, it triggers a continuation process. If the output is complete, it undergoes post-processing to refine the code for the target language. Throughout the execution, the script tracks and prints the estimated costs for input, output, and any additional processing steps, returning the final runnable code, the total cost, and the name of the model used. Error handling is included to manage value errors and unexpected exceptions.",112974499903ccf96fb523e49dea795f2928cceab035a1c6fe554073efec8a0e
context/fix_errors_from_unit_tests/5/error_log.txt,"The provided file content captures the output of a failed `pytest` session executed on a macOS system using Python 3.12.4. The test session, initiated in the `/Users/gregtanaka/Documents/PDD` directory, utilized `pytest` version 8.3.2 along with plugins for coverage (`cov-5.0.0`) and asynchronous I/O (`anyio-4.4.0`). The execution attempted to collect tests but ultimately found zero items. The process concluded almost instantly (0.00s) without running any tests. The primary reason for this outcome is explicitly stated as an error: the test runner could not locate a specific file or directory, identified as `/path/to/non/existent/file.py`. This indicates that the command was likely invoked with an incorrect file path argument, preventing the test suite from launching successfully.",f0eac5f3c31ac6f7f53f16ad78e018d2f05c4d7982d67d9f041f948b4f54a260
context/fix_errors_from_unit_tests/5/continue_generation.py,"This Python script defines a module for continuing and refining text generation using Large Language Models (LLMs) via the LangChain framework. The core functionality is encapsulated in the `continue_generation` function, which orchestrates a multi-step process to extend an initial LLM output. The process involves loading and preprocessing specific prompt templates from a directory specified by an environment variable. It utilizes an `llm_selector` to choose appropriate models based on desired strength and temperature settings. The script iteratively generates text, checking for completion using a helper function `unfinished_prompt`. It employs auxiliary chains to trim and format the results, ensuring the output is clean code blocks or text. Throughout the execution, the script tracks and prints token usage and associated costs using the `rich` library for console output. Pydantic models are defined to structure the expected JSON outputs from the LLMs for trimming tasks.",60870002c908d1af45df2bb36cccf3c0ffac431e52f8a52881982a97dea7b7e0
context/fix_errors_from_unit_tests/5/continue_generation_2_0_1.py,"This Python script defines a function named `continue_generation` designed to extend and complete code generation tasks using Large Language Models (LLMs) via the LangChain framework. The process begins by loading and preprocessing specific prompt templates from a directory specified by an environment variable. It utilizes Pydantic models (`TrimResultsOutput` and `TrimResultsContinuedOutput`) to structure the parsing of LLM outputs. The core logic involves an iterative loop where the script first extracts an initial code block from previous output, then repeatedly invokes an LLM to continue generating code based on the formatted input and the current code block. Inside the loop, the script checks if the generation is complete using a helper function `unfinished_prompt`. If unfinished, it appends the new content and continues; if finished, it trims the final output for cleanliness and breaks the loop. Throughout the execution, the script tracks token usage and calculates costs associated with different LLM calls (generation, trimming, and completion checking), displaying progress and costs via the `rich` library. Finally, it returns the fully generated code block, the total cost, and the model name used.",c1c92431563dde04a966bfc7d11408f82ac7a350c872b2615d9deacaf920db6e
context/fix_errors_from_unit_tests/5/test_continue_generation.py,"This file contains a suite of unit tests for the `continue_generation` function within the `pdd` module, utilizing the `pytest` framework. The tests verify the functionality of continuing text generation from a Large Language Model (LLM) output. Key components include fixtures for mocking the environment (`mock_environment`), prompt templates (`mock_prompts`), the LLM selection logic (`mock_llm_selector`), and the logic for determining if a prompt is unfinished (`mock_unfinished_prompt`). The test cases cover various scenarios: successful generation with mocked dependencies, error handling when the `PDD_PATH` environment variable is missing, handling of `FileNotFoundError` when prompt files are inaccessible, verifying behavior over multiple generation iterations, and ensuring accurate cost calculation. The tests heavily rely on `unittest.mock` to isolate the function from external file systems and actual LLM API calls.",08fc2afe93e17da1d224e31652ea38b3a383a1a4672d2b8f90bfbbdd4f17184f
context/fix_errors_from_unit_tests/5/continue_generation_python.prompt,"This document outlines the requirements for a Python function named ""continue_generation,"" designed to complete unfinished outputs from a Large Language Model (LLM). The function takes a formatted input prompt, the current partial LLM output, and model parameters (strength and temperature) as inputs. It returns the completed text, the total cost of execution, and the model name used. The process involves several steps: loading and preprocessing specific prompt files from a directory, setting up Langchain LCEL templates, and selecting an appropriate LLM model. The core logic iterates through a loop where it detects if the generation is incomplete using a helper function; if so, it continues generating text until completion. It also employs specific trimming prompts to manage the output format. Throughout the execution, the function must track token usage and calculate costs for all model invocations. Finally, it pretty-prints the result using Rich Markdown and returns the final output string along with cost metrics.",0bad36512ca7aa74cb49aa6b4a53c0d0c0764264897c1c336b9a488f31c12e66
context/fix_errors_from_unit_tests/5/test_continue_generation_2_0_1.py,"This file contains a suite of unit tests for the `continue_generation` function within the `pdd` module, utilizing the `pytest` framework. The tests verify the functionality of continuing text generation processes, likely involving Large Language Models (LLMs). Key components include fixtures for mocking environment variables (`PDD_PATH`), prompt templates, LLM selection logic, and unfinished prompt detection. The test cases cover various scenarios: successful generation completion, handling missing environment variables, managing file not found errors, and verifying behavior over multiple generation iterations. Additionally, there is a specific test to ensure the accurate calculation of total costs associated with the generation process. The tests make extensive use of `unittest.mock` to simulate external dependencies like file operations, LLM responses, and console output.",967abdff9f87433d54f94bcb50397587a8330cd3fc854da619f2bdcf616dd39a
context/core/cli_example.py,"This Python script serves as a comprehensive demonstration and test suite for the PDD (Prompt Driven Development) CLI module. It illustrates how to programmatically interact with the main CLI entry point (`cli`) and its custom Click Group class (`PDDCLI`). The script defines several functions to showcase various global options and features, including displaying help and version information, listing available contexts from configuration files, and invoking the CLI with parameters for AI model strength, temperature, and verbosity. It also demonstrates specific flags like `--quiet` for suppressing output, `--output-cost` for tracking usage costs in a CSV file, `--context` for overriding configuration contexts, and `--core-dump` for generating debug bundles. The `main` function orchestrates the execution of these examples, providing a structured overview of the CLI's capabilities, error handling, and configuration management.",48068766551cf90d9fdca612b4210b4d901441931e3578e23fbf72083e95e075
context/core/utils_example.py,"This Python script serves as a demonstration and documentation suite for the `pdd.core.utils` module, which provides helper functions for the PDD CLI tool. The file defines several example functions, each targeting a specific utility from the core module, such as `_first_pending_command` (for parsing Click context arguments), `_api_env_exists` (checking for API configuration files), `_completion_installed` (verifying shell tab completion), and `_project_has_local_configuration` (detecting local environment settings). Additionally, it illustrates the logic behind `_should_show_onboarding_reminder`, which determines if new users need setup guidance based on environment variables and existing configurations. The script includes a `main` entry point that executes these examples sequentially, printing the results to standard output to visualize how the utility functions behave under different scenarios (e.g., using `unittest.mock` to simulate CLI contexts).",efdc29724f5f2de035193da6f57f5354448c920c79182138a648369a75dd91c2
context/core/cloud_example.py,"This Python script serves as a comprehensive example and demonstration of the `CloudConfig` module within the PDD CLI project. It illustrates how to manage centralized cloud configuration settings, specifically focusing on retrieving endpoint URLs, handling authentication via JWT tokens, and managing environment variable overrides. The script includes several functions that simulate different usage scenarios: `example_url_configuration` shows how to switch between default production URLs and local emulator URLs; `example_cloud_enabled_check` verifies if cloud features are active based on API keys; `example_jwt_token_handling` demonstrates retrieving authentication tokens either from environment variables (for CI/CD) or via a mocked interactive device flow; and `example_cloud_timeout` shows how to configure request timeouts. Additionally, it includes error handling examples for missing configuration keys. The script uses `unittest.mock` to simulate environment variables and network responses, allowing it to run as a standalone demonstration without requiring actual cloud connectivity.",8a14ae9e69dbccfbbfffd9bde1ea11192334a10aaf9d5b5e6dba5ac23ba0dd90
context/core/errors_example.py,"This Python script serves as a comprehensive demonstration of the `pdd.core.errors` module, which provides centralized error handling for the PDD CLI. The script illustrates how to use the module's key components: a `Rich` console with custom themes for styled output, and functions for handling exceptions gracefully without crashing the application. 

The `main` function walks through several examples, including: printing messages with specific styles (info, warning, success, etc.); handling various exception types like `FileNotFoundError`, `ValueError`, and `IOError`; and demonstrating the difference between verbose and quiet error reporting modes. Additionally, the script shows how the module collects errors into a buffer for core dump analysis. It includes examples of retrieving these collected errors, writing them to a JSON file for debugging purposes, and clearing the error buffer. Finally, it confirms that the `handle_error` function captures exceptions without re-raising them, allowing the CLI to continue execution or exit cleanly.",b50dbf08aec028bd57a8fe74762bf92aee9a1fe953fb3d7db50cc686c72bfee3
context/core/dump_example.py,"This Python script serves as a comprehensive example and demonstration of the `pdd.core.dump` module, which handles core dump generation and debugging utilities for the PDD CLI tool. The script illustrates how to capture execution contexts, errors, and environment details to facilitate issue reproduction and diagnosis.

The file defines several example functions corresponding to key features of the dump module: `example_write_core_dump` mocks a Click context to simulate writing a JSON core dump file containing cost and model usage data; `example_github_config` checks for necessary environment variables (`PDD_GITHUB_TOKEN`, `PDD_GITHUB_REPO`) required for automated issue reporting; `example_write_replay_script` generates a shell script to reproduce a specific CLI command execution; and `example_build_issue_markdown` constructs a formatted GitHub issue body from a core dump payload. Finally, `example_post_issue_to_github` outlines the signature for posting issues via the GitHub API. The script includes a main execution block that runs these examples sequentially, outputting results to a local directory.",0416a935ac490b958ceedfc6e68c7fbd7e383966374ba13ccf7ebfeaf8ad0803
context/insert/1/prompt_to_update.prompt,"This file outlines the requirements for an expert Python Software Engineer to create a function named ""postprocess"". The primary goal of this function is to extract specific programming code from a raw string output generated by a Large Language Model (LLM), which typically contains a mix of conversational text and code blocks. The function accepts inputs such as the raw LLM output, the target programming language, and configuration parameters like model strength, temperature, and verbosity. It returns a tuple containing the cleaned code string, the total cost of the operation, and the model name used. The logic involves a conditional check where a strength of 0 triggers a simpler ""postprocess_0"" function. Otherwise, it utilizes an ""extract_code_LLM.prompt"" template via an ""llm_invoke"" call to parse the output. The process includes specific cleaning steps, such as stripping triple backticks and language identifiers from the code block boundaries. All console output is required to be formatted using the Python ""rich"" library.",098afefde3d09926ba42f4cd18881b7dd447003bb7084c9d8ad0d60fd0be8805
context/insert/1/updated_prompt.prompt,"This file outlines the requirements for a Python function named `postprocess`, designed to extract code blocks from a mixed-text string generated by a Large Language Model (LLM). Acting as an expert Python Software Engineer, the user is instructed to create a function that accepts inputs such as the raw LLM output, the target programming language, model strength, temperature, and a verbosity flag. The function returns a tuple containing the extracted code, the total cost of the operation, and the model name used. The process involves conditional logic: if the strength is 0, it delegates to a `postprocess_0` function. Otherwise, it loads a specific prompt template (`extract_code_LLM.prompt`) and utilizes an internal `llm_invoke` module to process the text. The instructions specify cleaning the output by removing markdown code fences (triple backticks) and language identifiers. The file also references internal modules for loading prompt templates and invoking LLMs, providing context via included example files.",1fcdbba25fca894689f6d83c6461a9b054b938e7c936b1d5035f5a2b0cea7756
context/insert/1/dependencies.prompt,"The provided file contents serve as a documentation snippet or usage guide for internal modules within a software project. It specifically outlines how to utilize two key functionalities: loading prompt templates and executing prompts using an `llm_invoke` function. The guide is structured using XML-like tags to delineate sections. Under the `<internal_modules>` tag, it provides two distinct examples. The first, `<load_prompt_template_example>`, demonstrates the method for importing or accessing prompt templates, pointing to a specific file path: `context/load_prompt_template_example.py`. The second, `<llm_invoke_example>`, illustrates how to run prompts using the `llm_invoke` mechanism, referencing the file `context/llm_invoke_example.py`. Essentially, this file acts as a quick reference for developers to locate the source code examples necessary for implementing prompt management and LLM invocation within the system.",ee4a1c42a3115dc65850843bc461fbe6488db13f9897eae96d4db7815902a92b
context/insert/2/prompt_to_update.prompt,"This file outlines the specifications for an AI agent acting as an expert Python engineer to generate a function named ""conflicts_in_prompts"". The purpose of this function is to analyze two input prompts, identify conflicts between them, and suggest resolutions. The function takes two prompt strings, a model strength float, and a temperature float as inputs. It is required to output a list of necessary changes in JSON format, the total cost of the operation, and the model name used. The implementation details specify using LangChain Expression Language (LCEL) and loading specific prompt templates from a project path defined by the $PDD_PATH environment variable. The process involves a multi-step workflow: first, running a conflict detection LLM to generate a markdown analysis; second, pretty-printing cost and token usage; and third, using a second extraction LLM to parse that analysis into a structured JSON list of changes. The file also references external context files for Python preambles and LCEL examples.",a738bbf964d351f4e4a728992c6dd3d1b7d8ce34ca50d52497c2c9178c2b8c74
context/insert/2/updated_prompt.prompt,"This file provides a detailed specification for creating a Python function named ""conflicts_in_prompts"". The function's purpose is to analyze two input prompts, identify conflicts between them, and suggest resolutions. It requires inputs for the two prompts, along with optional parameters for model strength and temperature. The expected output includes a list of necessary changes in JSON format, the total cost of the operation, and the name of the LLM model used.

The specification outlines a seven-step process using LangChain Expression Language (LCEL). Key steps include loading specific prompt templates from a project path defined by the $PDD_PATH environment variable, selecting an appropriate LLM using an internal `llm_selector` module, and calculating token usage and costs. The workflow involves two main LLM calls: first, to generate a markdown analysis of the conflicts, and second, to extract a structured JSON list of changes from that analysis. The file also references external context files for Python preambles, LCEL examples, and internal module usage examples to guide the implementation.",3be57216b5220b17d916ccac261216247bcd07ea280ea3dd92d171c48e2c21cf
context/insert/2/dependencies.prompt,"The provided file content serves as a documentation snippet or a template demonstrating how to include and reference internal code modules. Specifically, it showcases an example of how to use an internal module named `llm_selector`. The content includes an XML-like structure `<internal_example_modules>` which wraps a specific example case. This case illustrates selecting a LangChain Large Language Model (LLM) and counting tokens, referencing an external Python file located at `./context/llm_selector_example.py` via an `<include>` tag. The structure suggests this file is part of a larger system for managing or documenting code examples and internal tooling usage.",f75d15f0817d09a3aca235f8060ce3c16c82886609ec19b84ea72e71eaa0778f
context/detect_change/1/prompt_list.txt,"The provided input contains a collection of prompt definitions designed to instruct an LLM to generate specific Python functions for an automated coding workflow. The collection begins with a `python_preamble` that establishes coding standards, such as using relative imports, the Rich library for console output, and robust error handling.

The primary prompts define three distinct capabilities:
1. **Code Modification (`change_python`):** This prompt instructs the creation of a function that modifies existing code based on user instructions. It utilizes LangChain LCEL, calculates token costs, and employs specific sub-prompts to extract and apply changes.
2. **Automated Debugging (`fix_error_loop_python`):** This defines a complex iterative process to fix unit test errors. The function runs `pytest`, parses error logs, creates file backups, and repeatedly attempts to fix code within a specified budget and attempt limit, verifying fixes against a separate program.
3. **Code Generation (`code_generator_python`):** This prompt guides the creation of a function to compile raw prompts into runnable code files. It handles preprocessing, model selection, cost tracking, and logic to detect and complete unfinished model generations.

Collectively, these prompts outline a system for generating, modifying, and debugging Python code using LLMs and LangChain.",26ea374cd215a2eacd72f63eabca529bb0498a353de793434d5cc53943f5e199
context/detect_change/1/change_python.prompt,"This document outlines the requirements for creating a Python function named ""change"" designed to modify an existing prompt based on specific instructions. The function takes inputs including an initial prompt, generated code, modification instructions, and LLM parameters (strength and temperature). It outputs a modified prompt string, the total operational cost, and the model name used. The implementation must utilize the Python Rich library for console output and relative imports for modules. The process involves several steps: loading specific prompt files, preprocessing them (handling curly brackets), creating Langchain LCEL templates, and selecting an LLM model with token counting capabilities. The function executes a two-step LLM process: first, generating a raw modified prompt based on the inputs, and second, extracting the final structured ""modified_prompt"" from that output using a secondary extraction prompt. Throughout execution, the function must calculate and display token counts and estimated costs, handle edge cases, and return the final modified prompt along with cost metrics.",5e4d1b24cfaf48eed80704c64cc75d9c888914189ba700487e3d77c3b6a97f3b
context/detect_change/1/fix_error_loop_python.prompt,"This document outlines the specifications for a Python function named ""fix_error_loop,"" designed to iteratively repair errors in a unit test and its associated code file using an LLM. The function takes inputs including file paths for the unit test, code, and a verification program, as well as parameters for the LLM (strength, temperature), a maximum attempt limit, and a budget. 

The process involves a loop that runs until the tests pass, the maximum attempts are reached, or the budget is exceeded. In each iteration, the function executes the unit test using pytest, logging the output. If failures occur, it backs up the current files, analyzes the error logs, and calls a helper function, ""fix_errors_from_unit_tests,"" to generate fixes. The function tracks costs and validates code changes using the verification program. If verification fails, changes are reverted. The system also tracks the ""best iteration"" based on the lowest number of errors and failures. Upon completion, it ensures the best version of the code is restored if the final run wasn't the most successful, returning the final status, file contents, attempt count, and total cost. All console output is formatted using the Python ""rich"" library.",e5613bb6ce63aa396e54904fdbbeef99f854b20676c737a39f14256a765bc2ba
context/detect_change/1/code_generator_python.prompt,"This file outlines a prompt for an expert Python engineer to create a function named ""code_generator"". The primary purpose of this function is to compile a raw text prompt into a runnable code file in a specified language (e.g., Python, Bash). The function takes inputs including the raw prompt, target language, model strength, and temperature, and returns the runnable code, total cost, and the model name used.

The prompt details a specific workflow using LangChain Expression Language (LCEL). The process involves eight steps: preprocessing the raw prompt, creating an LCEL template, selecting an LLM using an internal `llm_selector` module, and executing the prompt while tracking token usage and cost. It includes logic to handle incomplete generations by detecting unfinished output and continuing generation if necessary. Finally, the output is post-processed, and the total cost is calculated. The file references several external context files and examples for internal modules like preprocessing, LLM selection, unfinished prompt detection, and post-processing.",2cb942ea670aa6742a3feff76ca34121806135bafcc7fe39bd7d5e64994e35cc
context/detect_change/1/python_preamble.prompt,"The provided text outlines specific requirements for developing a Python function within a package structure. It mandates the use of relative imports, specifically denoted by a single dot, to reference internal modules, ensuring the code integrates seamlessly into the larger package ecosystem. A key requirement is the implementation of the Python Rich library to handle all console output, guaranteeing that any information displayed to the user is aesthetically formatted and 'pretty printed.' Furthermore, the instructions emphasize robust error handling; the function must be designed to gracefully manage edge cases, such as missing input data or potential errors arising from model interactions. In these scenarios, the function is expected to provide clear, informative error messages to guide the user, rather than failing silently or crashing. Overall, the directive focuses on creating a modular, user-friendly, and visually polished component of a Python package.",afcfd4246bc7fa0387c45154a99e65c483e3e6513dad984b94e75ce36fff64b9
context/detect_change/1/detect_change_output.txt,"This document outlines a plan to standardize Python-related instructions across various prompt files by utilizing a shared `context/python_preamble.prompt`. The goal is to reduce redundancy and improve consistency by centralizing common requirements, such as using the Python Rich library for output and handling relative imports. The analysis considers three implementation strategies: direct inclusion, conditional inclusion, and reference-based inclusion. Plan C, reference-based inclusion, is selected as the optimal approach because it minimizes redundancy, facilitates easy updates, and aligns with existing system capabilities using XML include tags. The document details specific instructions for modifying two files: `prompts/change_python.prompt` and `prompts/fix_error_loop_python.prompt`. For each file, the plan prescribes adding an XML include tag for the preamble and removing specific redundant text that is now covered by the shared file. It notes that `context/python_preamble.prompt` and `prompts/code_generator_python.prompt` do not require changes.",2b81cf48819c551c13bcbf9483112c26e883ce9f466d132b2334992886b6b393
context/detect_change/2/change_python.prompt,"The file outlines the requirements for an expert Python Software Engineer to create a function named ""change"". This function is designed to modify an existing ""input_prompt"" based on a user-provided ""change_prompt"" and associated ""input_code"". The function must utilize the Python Rich library for pretty-printing console output and employ relative imports. It requires specific inputs including the prompts, code, and LLM parameters like strength and temperature. The process involves loading specific prompt files, preprocessing them, and executing a multi-step Langchain LCEL workflow. This workflow includes selecting an LLM model, counting tokens to estimate costs, and running two distinct LLM invocations: one to generate the modification logic and another to extract the final ""modified_prompt"" as JSON. The function must return the modified prompt string, the total cost of operations, and the model name used, while handling edge cases and errors robustly.",937a920d597a336286c7ae934dfd64482679bc3d20baeefd9cd02277a46c8ee9
context/detect_change/2/unfinished_prompt_python.prompt,"The provided text outlines the specifications for a Python function named `unfinished_prompt`, designed to determine whether a given text prompt is complete or requires continuation. The function takes `prompt_text`, `strength`, and `temperature` as inputs and returns a structured assessment including `reasoning`, a boolean `is_finished` status, and optional cost and model details. The implementation requires using the Langchain LCEL framework, loading a specific prompt template from an environment-variable-defined path, and utilizing a helper module `llm_selector` for model selection and token counting. The process involves five steps: loading the prompt, creating an LCEL template, selecting the LLM, executing the analysis while logging token usage and costs via the `rich` library, and finally returning the parsed results. Error handling and relative imports are emphasized requirements.",2fa8a55103f26a33a516d4cd2add38b10b2a4a6975e2eca3b14ddb297c8a1405
context/detect_change/2/change.prompt,"The provided file content appears to be a snippet from a prompt engineering template or configuration system. It instructs the user or system to utilize a specific file, `context/python_preamble.prompt`, to make prompts more compact, noting that some prompts may already include it. The content includes placeholders or directives for including external files, specifically referencing `<include>context/python_preamble.prompt</include>` within a `<preamble>` tag and `<include>prompts/code_generator_python.prompt</include>` within an `<example>` tag. Essentially, it serves as a meta-instruction for constructing prompts by reusing existing preamble and example components.",44c27e23c016ad4b255cbd1b844e482a87673fb9679f3c053ec2609377c34d9e
context/detect_change/2/preprocess_python.prompt,"The provided text outlines the requirements for a Python function named 'preprocess_prompt', designed to prepare prompt strings for Large Language Models (LLMs). The function takes a prompt string, a recursive boolean flag, a double_curly_brackets flag, and an optional list of keys to exclude from bracket doubling. Its primary goal is to process specific XML-like tags within the prompt using regular expressions. Key tags include 'include' for inserting file contents directly, 'pdd' for removing comments, and 'shell' for executing and capturing shell command output. The function must handle nested includes recursively if the recursive flag is set, supporting two inclusion methods: standard XML 'include' tags and file paths enclosed in angle brackets within triple backticks. Additionally, if enabled, the function doubles single curly brackets to escape them, unless the enclosed keys are in the exclusion list. File paths are resolved using a 'PDD_PATH' environment variable via a helper function 'get_file_path'. The implementation requires using the 'rich' library for pretty-printing console messages to track progress, and the final output is the cleaned, preprocessed prompt string with whitespace trimmed.",50f953ffe80221f6217e0a58f2a0a0ca2a3390402903b3b3a03e858a7e487997
context/detect_change/2/python_preamble.prompt,"The provided text outlines specific requirements for developing a Python function within a package structure. It mandates the use of relative imports, specifically denoted by a single dot, to reference internal modules, ensuring the code integrates seamlessly into the larger package ecosystem. A key requirement is the implementation of the Python Rich library to handle all console output, guaranteeing that any information displayed to the user is aesthetically formatted and 'pretty printed.' Furthermore, the instructions emphasize robust error handling; the function must be designed to gracefully manage edge cases, such as missing input data or potential errors arising from model interactions. In these scenarios, the function is expected to provide clear, informative error messages to guide the user, rather than failing silently or crashing. Overall, the directive focuses on creating a modular, user-friendly, and visually polished component of a Python package.",afcfd4246bc7fa0387c45154a99e65c483e3e6513dad984b94e75ce36fff64b9
context/detect_change/2/prompt_list.json,"The provided file contains a JSON array defining specifications for four Python functions designed to automate various aspects of LLM prompt engineering. 

1.  **change_python.prompt**: Describes a function named ""change"" that modifies an input prompt based on a specific ""change_prompt"" instruction. It utilizes Langchain LCEL, handles token counting, and calculates the cost of the operation.
2.  **preprocess_python.prompt**: Outlines a ""preprocess_prompt"" function responsible for parsing prompt strings. It handles specific XML-like tags (such as `include`, `pdd`, and `shell`), manages recursive file inclusions, and formats curly brackets for LLM compatibility.
3.  **unfinished_prompt_python.prompt**: Details a function called ""unfinished_prompt"" that analyzes text to determine if a prompt is complete. It returns a boolean status and reasoning, aiding in iterative prompt generation.
4.  **xml_tagger_python.prompt**: Specifies an ""xml_tagger"" function that enhances raw prompts by adding structural XML tags to improve clarity. It employs a multi-step LLM process to generate and extract the tagged content.

All functions are designed to integrate with a project environment (referencing `$PDD_PATH`), use the `rich` library for console output, and include mechanisms for error handling and cost estimation.",2126c50e4b33078edbfd15319b1f7e966afc6b6e495a8de3db68e2321348e266
context/detect_change/2/change_detect.csv,"The provided file content outlines a series of identical change instructions for three specific prompt files located within the `context/detect_change/2/` directory: `change_python.prompt`, `preprocess_python.prompt`, and `unfinished_prompt_python.prompt`. For each of these files, the instruction mandates the insertion of a standard preamble file, `./context/python_preamble.prompt`, immediately following the role and goal statements using XML `include` tags. Furthermore, the instructions require the removal of any redundant guidelines already covered by this new preamble, specifically mentioning tasks like pretty printing and edge case handling. The goal is to streamline the prompts by leveraging a shared context file while preserving the unique logic and flow specific to each individual prompt.",853a291e2b92085b3471b1dc9ee9d8487dc8a4262043256766f89031c1639fcd
context/detect_change/2/xml_tagger_python.prompt,"The provided file outlines the requirements for creating a Python function named ""xml_tagger,"" designed to enhance raw LLM prompts by adding structural XML tags. Acting as an expert Python engineer, the developer is tasked with implementing a multi-step process using LangChain Expression Language (LCEL). The function takes a raw prompt string, along with strength and temperature parameters, and returns the enhanced prompt, the total execution cost, and the model name used. The workflow involves loading specific prompt templates from a project path, utilizing an internal `llm_selector` module for model selection and token counting, and executing two distinct LCEL chains. The first chain generates an analysis with XML tags, while the second extracts the clean, tagged prompt from that analysis into a JSON format. The process requires pretty-printing status updates, token counts, and costs at each stage, culminating in a final output of the refined prompt and the aggregated cost of the operations.",e1872f733299b344649a9f6778c5aa3d82acde7d2c2690217483e51f8ee518c1
context/detect_change/2/detect_change_output.txt,"This document outlines a plan to standardize Python-related prompts by integrating a centralized preamble file (`context/python_preamble.prompt`). The goal is to reduce redundancy and improve maintainability by consolidating common instructions regarding relative imports, pretty printing with the Python Rich library, and edge case handling. The analysis evaluates three implementation strategies: direct inclusion, conditional inclusion, and a hybrid approach. Plan C, the hybrid approach, is selected as the optimal strategy because it offers the flexibility to include the preamble only where it adds value while retaining specific existing instructions. The document then applies this plan to specific prompt files. It identifies `change_python.prompt`, `preprocess_python.prompt`, and `unfinished_prompt_python.prompt` as requiring modification. For each of these files, specific instructions are provided to insert the preamble immediately after the role/goal statement using XML include tags and to remove any redundant text. The `xml_tagger_python.prompt` is noted as already compliant, requiring no changes. The overall outcome is a targeted refactoring of prompt files to ensure consistency and cleaner code generation instructions.",9af4d232b7d214e792627e60c15900857a58104cb9dbc0a31a1db5ad894d3ebf
context/update/1/modified_change.py,"This Python script defines a function named `change` designed to automate the modification of prompts using Large Language Models (LLMs). Leveraging the `langchain` framework for orchestration and the `rich` library for formatted console output, the function executes a multi-step workflow. It begins by loading specific prompt templates from a directory defined by an environment variable and selecting an appropriate LLM configuration via a `llm_selector` helper based on provided strength and temperature parameters. The core logic involves two sequential LangChain operations: first, a generation chain that takes an input prompt, associated code, and a change description to produce a raw modification result; and second, an extraction chain that parses this output to isolate the final `modified_prompt` using a JSON parser. Throughout the process, the code calculates and displays token usage and estimated financial costs for each step. It includes error handling for file access and JSON parsing issues, ultimately returning the modified prompt, the total cost, and the model name used.",fec3fba35a4710e02375bdb017bcf33882c2a3b690733fc9d5c6021c1602785c
context/update/1/initial_change_python.prompt,"This document outlines the requirements for a Python function named ""change,"" designed to modify an existing prompt based on specific instructions. Acting as an expert Python Software Engineer, the developer must implement this function to take inputs such as an initial prompt, generated code, modification instructions, and LLM parameters (strength and temperature). The function's primary goal is to produce a ""modified_prompt"" by processing these inputs through a Langchain LCEL pipeline. Key steps include loading specific prompt templates, preprocessing them, selecting an LLM model via a selector utility, and executing a two-step invocation process: first to generate a raw change output and second to extract the structured modified prompt as JSON. The implementation must utilize the Python Rich library for pretty-printing console output, handle relative imports, calculate and display token usage and costs, and manage edge cases robustly. The final output includes the modified prompt string, the total cost of operations, and the model name used.",e541f45a7505b59b512d1a42e03b8ab4066069874f074341ad5f781c77e8fa08
context/update/1/initial_change.py,"This Python file defines a function named `change` that orchestrates a multi-step process to modify prompts using a Large Language Model (LLM). The function takes an initial prompt, input code, a description of desired changes, and model parameters (strength and temperature) as inputs. It first loads specific prompt templates from external files (`change_LLM.prompt` and `extract_prompt_change_LLM.prompt`) and preprocesses them. Using a helper `llm_selector`, it configures an LLM and calculates token costs. The process involves two main LLM invocations: first, to generate a modification based on the inputs using a `StrOutputParser`, and second, to extract the final `modified_prompt` from the previous output using a `JsonOutputParser`. The script utilizes the `rich` library for formatted console output, displaying intermediate results, token counts, and estimated costs at each step. Finally, it returns the modified prompt, the total calculated cost, and the name of the model used, while including error handling for file operations and JSON parsing.",43fa387f9f1cb1f4a6d4b214e4da990fdbb39b71b0a8e6df52cbd9a6ce6a0f56
context/update/1/final_change_python.prompt,"This document outlines the requirements for creating a Python function named ""change"" designed to modify an existing prompt based on specific instructions. The function takes inputs including an initial prompt, generated code, modification instructions, and LLM parameters (strength and temperature). It outputs a modified prompt string, the total operational cost, and the model name used. The implementation must utilize the Python Rich library for console output and relative imports for modules. The process involves several steps: loading specific prompt files, preprocessing them (handling curly brackets), creating Langchain LCEL templates, and selecting an LLM model with token counting capabilities. The function executes a two-step LLM process: first, generating a raw modified prompt based on the inputs, and second, extracting the final structured ""modified_prompt"" from that output using a secondary extraction prompt. Throughout execution, the function must calculate and display token counts and estimated costs, handle edge cases, and return the final modified prompt along with cost metrics.",5e4d1b24cfaf48eed80704c64cc75d9c888914189ba700487e3d77c3b6a97f3b
context/update/2/initial_continue_generation.py,"This Python script defines a function named `continue_generation` designed to extend and refine text generation from a Large Language Model (LLM), specifically focusing on code blocks. The process begins by loading and preprocessing specific prompt templates from a directory specified by an environment variable. It utilizes LangChain components to create processing chains for continuing generation and trimming results, employing Pydantic models (`TrimResultsOutput` and `TrimResultsContinuedOutput`) to structure the output parsing.

The core logic involves an iterative loop where the script first extracts an initial code block from a provided LLM output. It then repeatedly invokes an LLM to continue generating content based on the original prompt and the current code block. Inside the loop, the script checks if the generation is complete using a helper function `unfinished_prompt`. If unfinished, the loop continues; if finished, the result is trimmed and appended to the final output. Throughout the execution, the script calculates and logs the cost of token usage for each step using an `llm_selector`. Finally, it pretty-prints the resulting code block to the console using Markdown formatting and returns the complete code, total cost, and model name.",e9cd8ad342ad7d0699595cab2854584162848e8c37719b0d3221a57b5fa03186
context/update/2/initial_continue_generation_python.prompt,"The provided text outlines the requirements for creating a Python function named `continue_generation`, designed to complete the output of a Large Language Model (LLM) prompt. Acting as part of a Python package, the function utilizes relative imports and the Rich library for console output. It takes a formatted input prompt, initial LLM output, strength, and temperature as inputs, and returns the final completed output, total cost, and model name. The process involves several steps: loading and preprocessing specific prompt files from an environmental path, setting up Langchain LCEL templates, and selecting an LLM model with cost tracking. The function then iteratively generates content by checking for incompleteness using a helper function (`unfinished_prompt`), appending new generations until the output is complete. It also employs specific prompts to trim results at the start and end of the process. Throughout execution, the function must calculate and print token costs for every model invocation.",03d2933ae91c1d02aa24778c8d1f3be290a8e234f62d7955c65ab65a0e0ee98e
context/update/2/modified_continue_generation.py,"This Python script defines a function named `continue_generation` designed to extend and complete code generation tasks using Large Language Models (LLMs) via the LangChain framework. The process begins by loading and preprocessing specific prompt templates from a directory specified by the `PDD_PATH` environment variable. It utilizes an `llm_selector` to choose appropriate models based on desired strength and temperature settings. The core logic involves an iterative loop where the script first extracts an initial code block from previous output, then repeatedly invokes an LLM to continue generating code based on the formatted input and the current code block. Inside the loop, it checks if the generation is complete using a helper function `unfinished_prompt`. If finished, it trims the final output for cleanliness; if not, it appends the new content and continues the loop. Throughout the execution, the script tracks and prints token usage and costs using the `rich` library for formatted console output. Finally, it returns the fully generated code block, the total cost incurred, and the name of the model used.",c5c930022897786dc668e13d807b29f7eef887ae731cdccee2a4415a1827260d
context/update/2/modified_initial_continue_generation.prompt,"The provided text outlines the requirements for creating a Python function named `continue_generation`, designed to complete the generation of a prompt using a Large Language Model (LLM). The function, intended for a Python package, must utilize relative imports and the Rich library for console output. It takes a formatted input prompt, existing LLM output, and model parameters (strength, temperature) as inputs, returning the final completed output, total cost, and model name. The implementation involves a multi-step process: loading and preprocessing specific prompt files from an environment path, setting up Langchain LCEL templates, and selecting an LLM model with cost tracking. The core logic requires trimming the initial output, running a continuation loop that checks for completeness using an `unfinished_prompt` helper, and iteratively appending generated text until finished. Finally, the function must trim the results, pretty-print the final output with token costs, and return the accumulated string and metrics.",5e4246af93242342aceea14974d072b9fb4216e286d6a27082f69c18ce7e51fe
context/output/sample_instructions.prompt,"The file contains a concise instruction for a coding task. It requests the generation of unit tests for a specific function named `compute_area`. The requirements specify that the function is expected to handle positive floating-point numbers correctly. Additionally, the tests must verify that the function raises a `ValueError` when provided with negative input values. This suggests the function is likely calculating the area of a geometric shape where negative dimensions are invalid.",c4430188fd88d20b9d11bd000117242b18f81e661cd73df5e4af69e751187976
context/output/sample_module.py,"The provided file content defines a single Python function named `compute_area`. This function is designed to calculate the area of a circle based on a given radius. It accepts one argument, `radius`, which is type-hinted as a floating-point number. The function includes input validation to ensure the radius is non-negative; if a negative value is passed, it raises a `ValueError` with a descriptive message. If the input is valid, the function returns the calculated area using the formula πr², approximating Pi as 3.14159. The return value is also type-hinted as a float.",3e6a202bf9445acf74ab3e62942d8bf3629db16e3b2b44ba6cb5843cd22a18fe
context/output/tests/test_sample_module.py,"This file contains a comprehensive test suite for a `compute_area` function, combining standard unit testing with formal verification techniques. It begins with a detailed test plan outlining functional analysis, edge cases, and verification methods. The implementation uses `pytest` for unit tests, covering scenarios such as zero radius, standard positive values, large values, and error handling for negative inputs (expecting a `ValueError`). Additionally, the file integrates the Z3 theorem prover to formally verify mathematical properties of the function. Specifically, it includes Z3-based tests to prove that non-negative inputs always yield non-negative outputs and to verify the monotonicity of the area calculation (i.e., larger radii result in larger areas). This hybrid approach ensures both specific instance correctness and broad mathematical validity of the logic.",22b13f91b52733148ddd0e59217b9fe354ef4a19aa26af5814d36a2f6fa35a3f
context/server/executor_example.py,"This Python script serves as a demonstration and test harness for a command execution module, specifically focusing on `ClickCommandExecutor` and `execute_pdd_command`. It begins by importing necessary libraries and attempting to import specific classes from a `pdd.server.executor` package, providing mock fallback classes (`CapturedOutput`, `ClickCommandExecutor`) if the import fails to ensure the script remains runnable in standalone contexts.

The script defines a sample Click command named `greet_command` which accepts arguments like `name`, `count`, and `loud`, and utilizes a context object to retrieve configuration values. It also defines a streaming callback function, `on_output`, to demonstrate real-time output handling.

Two main demonstration functions are provided: `demo_executor_class`, which instantiates the `ClickCommandExecutor` with a base context and callback to run the `greet_command`, and `demo_high_level_helper`, which shows how to use the `execute_pdd_command` helper function to run commands by name string. The script concludes with a main execution block that runs both demos, printing results such as exit codes, costs, and captured standard output/error to the console using the `rich` library for formatting.",fb5d8d40aeeb2bf401dc6a84a43bee6936ee1e734b2b00bd08e7fbc44219a9cf
context/server/security_example.py,"This Python script demonstrates the implementation of a secure FastAPI server using a custom security module (`pdd.server.security`). It initializes a FastAPI application and configures several security layers, including a `PathValidator` to prevent directory traversal attacks by restricting file access to a specific project root and blacklisting sensitive files like `.git` or `.env`. The script sets up middleware for security logging and configures CORS to allow requests only from specific local origins. Authentication is handled via a token dependency, requiring a bearer token for protected routes. The application defines three endpoints: a public status check, a secured data endpoint requiring authentication, and a file reading endpoint that combines authentication with path validation to safely handle user-provided file paths. Finally, it includes a main block to run the server using `uvicorn` on localhost, printing examples of valid and malicious access attempts to the console.",dc4f3dc06455ba6180f4536aa67ce4e9a5efd52e0e9cd7702c3f2ec6ef96aac9
context/server/token_counter_example.py,"This Python script serves as a demonstration and usage example for a custom module named `token_counter`. The script primarily illustrates how to utilize various functions within the `token_counter` module to analyze text for Large Language Model (LLM) usage. It begins by ensuring the module can be imported and defines a helper function, `create_dummy_pricing_csv`, to generate a temporary CSV file containing pricing data for models like GPT-4 and Claude 3 Opus. The `main` function orchestrates the workflow: it first sets up sample text and ensures the pricing data exists. It then proceeds to demonstrate four key capabilities: counting tokens in the sample text using `tiktoken`, retrieving context limits for specific model families (e.g., GPT-4 Turbo), estimating financial costs based on the generated pricing CSV, and generating comprehensive metrics that include usage percentages and costs for a specific model (Claude Sonnet). Finally, the script outputs these metrics in a JSON format to the console, providing a clear example of how to integrate token counting and cost estimation into a broader application.",08885532e1406cee06db0398232624d7d9a3db72a9843152fa81db38027cd8a3
context/server/models_example.py,"This Python script serves as a demonstration and usage guide for the Pydantic models defined in the `pdd.server.models` module. It defines a single function, `example_usage`, which systematically walks through creating instances of various data models including `FileMetadata`, `CommandRequest`, `JobHandle`, `WSMessage`, `StdoutMessage`, `WriteFileRequest`, and `ServerStatus`. The script highlights key features such as input validation (e.g., catching invalid file paths), serialization to JSON and dictionaries using Pydantic v2 methods like `model_dump_json`, and handling polymorphic WebSocket messages. It effectively acts as documentation by code, showing developers how to interact with the server's data structures for tasks like file handling, job management, and status reporting.",223a832718e72ca6640728e4bd700da58edb89667931085d7d949774d8fbc228
context/server/terminal_spawner_example.py,"This Python script serves as a demonstration and test utility for the `TerminalSpawner` class, which is part of a larger package (likely named `pdd`). The script begins by configuring the system path to ensure the `pdd.server.terminal_spawner` module can be imported correctly, handling potential import errors gracefully. The `main` function showcases three distinct use cases for spawning external terminal windows. First, it launches a simple terminal that prints a greeting and lists the current directory contents. Second, it demonstrates the ability to specify a working directory by spawning a terminal that opens in the user's home folder. Finally, it simulates a job execution scenario by passing a `job_id` and `server_port`, illustrating how the spawner constructs commands intended to callback to a server upon completion (though it notes the callback will fail without an active server). Throughout execution, the script prints status messages to the console indicating the success or failure of each spawn attempt, providing immediate feedback on the functionality of the `TerminalSpawner` across different configurations.",de89761e9228d058064a52ed3bf4d84f2ceba1991e5b8e0a45a4af8f0c477e19
context/server/__init___example.py,"This Python script serves as a comprehensive usage example and demonstration for the `pdd.server` package. It illustrates three core functionalities of the system: security validation, asynchronous job management, and API server interaction. First, the `demo_security_validation` function showcases the `PathValidator` component, proving its ability to accept valid file paths within a project root while correctly rejecting directory traversal attacks (e.g., accessing `../../etc/passwd`). Second, the `demo_job_manager` function utilizes `asyncio` to demonstrate the `JobManager` class, simulating the submission, execution, and status tracking of a background job using a mock executor. Finally, the `demo_server_interaction` function spins up the actual FastAPI server in a background thread using `run_server`. It then performs a live HTTP GET request to the server's status endpoint (`/api/v1/status`) to verify connectivity and retrieve version information. The script uses the `rich` library for formatted console output and includes fallback import logic to support both installed package usage and local development environments.",8817e0d29e93b3b67a7fe8fb08962ea8df850bbceda7390bbc12d42afd5070b8
context/server/jobs_example.py,"This Python script serves as a demonstration and usage example for a `JobManager` class within a project named `pdd`. It illustrates how to initialize the manager with concurrency limits and register lifecycle callbacks for job start and completion events. The script defines a `mock_worker` function to simulate asynchronous tasks with variable durations and potential failure states, replacing actual CLI command execution for testing purposes. In the `main` execution flow, the script submits a batch of five jobs with different parameters to demonstrate queuing behavior (where only two jobs run concurrently). It specifically showcases features such as job submission, real-time status logging via callbacks, cancelling a queued job, and handling simulated errors. Finally, the script waits for all tasks to complete, prints a summary table of job statuses and costs, performs a cleanup of old job records, and gracefully shuts down the manager.",7694bd486d012fd9d52543c5f79a289f61dac3ed0f8c94c2ea9fd25ed6333f12
context/server/app_example.py,"This Python script serves as a demonstration and testing utility for the `pdd` server application. It illustrates how to initialize, configure, and interact with the server programmatically. The script begins by setting up the necessary import paths to access the `pdd` package and defines a helper function, `start_server_in_thread`, to run the Uvicorn-based server in a separate daemon thread, allowing the main thread to act as a client simultaneously.

The `main` function orchestrates the workflow: it first creates a temporary dummy project directory containing a test file. After launching the server on localhost port 9999, the script performs a series of HTTP requests to verify functionality. These tests include checking the server's status endpoint (`/api/v1/status`) to retrieve version and uptime information, attempting a path traversal attack to confirm security measures correctly block unauthorized file access (returning a 403 Forbidden status), and verifying the accessibility of the OpenAPI documentation (`/docs`). Finally, the script cleans up by removing the temporary directory and exits, relying on the daemon thread to shut down the server automatically.",554bb0307f2f7a0dc79ef86f8a4fd42e5017225d0c56379ad99d2939f3c1087a
context/server/click_executor_example.py,"This Python script serves as a comprehensive usage example and test suite for a custom module named `ClickCommandExecutor`. It demonstrates how to programmatically execute `click`-based CLI commands within a Python application rather than invoking them from a shell. The script begins by defining sample `click` commands: a `greet` command that accepts arguments and options, and a `fail_command` designed to raise exceptions. It then implements four distinct demonstration functions: `demo_basic_execution`, which shows how to run a command and capture standard output; `demo_streaming_execution`, which illustrates real-time output handling via callbacks; `demo_error_handling`, which verifies the capture of standard error and exception details; and `demo_context_passing`, which proves the ability to inject shared state into the command context. The script includes fallback import logic to handle potential path issues and a main entry point that sequentially runs all demonstrations, printing the results to the console.",a61ee766f5b09bfa0de58533125b85d8855186f17f3109101887177a87d8d5c9
context/server/routes/session_example.py,"This Python module defines a FastAPI router for handling remote session status endpoints within the `pdd.server.routes.session` namespace. It establishes an API route prefixed with `/api/v1/session` and tagged as ""session"". The core component is the `RemoteSessionInfo` Pydantic model, which structures the response data for session queries, including fields for the session ID, cloud URL, registration boolean status, and a timestamp for when registration occurred. The module implements a single GET endpoint, `/info`, designed to return the current server's registration status with PDD Cloud. Although the implementation of the `get_session_info` function is elided with an ellipsis, the provided comments and example usage demonstrate how it is intended to interact with a session manager to retrieve active session details. If a session manager is active, it populates the response model with the relevant session ID and cloud URL; otherwise, it returns a status indicating the server is not registered.",7afec7e9141ead74f2ba74ac5a70a7f35793eb3409606539c81cf1e7b34ed1a9
context/server/routes/prompts_example.py,"This Python script serves as a standalone demonstration and testing harness for a FastAPI router module named `prompts_module`. Its primary purpose is to showcase the functionality of an API endpoint (`/api/v1/prompts/analyze`) that analyzes prompt files or raw strings for token usage and cost estimation. 

The script begins by mocking external dependencies (`pdd.security`, `pdd.token_counter`, `pdd.preprocess`) to isolate the environment and avoid requiring the full `pdd` package. It defines mock classes for token metrics and cost estimates, as well as a mock preprocessing function that expands variables. 

After setting up the mocks, the script initializes a FastAPI application, includes the router, and configures a mock `PathValidator` to simulate secure file access within a temporary directory. The `run_examples` function then executes three scenarios using `TestClient`: analyzing a file with preprocessing enabled, analyzing direct string content, and testing error handling for non-existent files. Finally, it cleans up the temporary directory, providing a complete, self-contained example of how to interact with the prompt analysis API.",0c143874f91ca038a84580bd829fccf6adbe8bedfd33517089cc2bee6b7a04e1
context/server/routes/config_example.py,"This Python module defines a FastAPI router dedicated to server configuration endpoints, specifically designed to help frontend applications synchronize with the backend environment. Located under the `/api/v1/config` prefix, the module establishes a route for retrieving the cloud functions URL. It utilizes Pydantic to define a `CloudUrlResponse` model, ensuring structured responses containing both the `cloud_url` and the current `environment`. The primary endpoint, `GET /cloud-url`, is intended to return this configuration data, preventing mismatches between staging and production environments by ensuring the frontend communicates with the correct cloud services. The file also includes commented example usage demonstrating how to register this router within a main FastAPI application and what a typical JSON response would look like.",58f2b27cdf85200a52fbe76324d02431dc62f339afd3896de37c0fe759f55cde
context/server/routes/websocket_example.py,"This Python script serves as a comprehensive demonstration and testing utility for the WebSocket routing module within the PDD Server architecture. It illustrates how to implement real-time communication features, specifically focusing on job output streaming, file system monitoring, and connection management. The script sets up a mock environment—including a mock FastAPI application, job manager, and WebSocket client—to simulate server behavior without requiring the full production stack. 

The code defines several key example functions: `example_job_streaming` shows how to broadcast job progress, standard output/error logs, and completion status to clients; `example_file_watching` simulates file system events (creation, modification, deletion) and broadcasts them; `example_connection_management` handles multiple client connections and subscriptions; and `example_error_handling` demonstrates robustness against invalid inputs or dropped connections. By running these asynchronous examples via `asyncio.run(main())`, developers can verify the WebSocket protocol's functionality, ensuring that messages are correctly formatted, routed, and handled for various server events.",a4c2313449f3174039edf4ab1f2030247b4da0eb31e6411b1008516be95eb3cf
context/server/routes/auth_example.py,"This Python script serves as a standalone test harness and example runner for an authentication module, specifically designed to simulate and verify the behavior of `auth_routes.py` without requiring the full production environment. It primarily focuses on mocking external dependencies to isolate the testing of authentication flows.

The script begins by setting up mock environment variables (e.g., GitHub Client ID) and patching internal modules like `pdd` and `pdd.get_jwt_token` using `unittest.mock.MagicMock`. This mocking strategy simulates the behavior of device flow authentication, including generating user codes and polling for tokens, ensuring the test can run independently. It carefully saves and restores original system modules to prevent side effects on other tests.

After configuring the mocks, the script initializes a FastAPI application and includes the authentication router. It defines a `run_example_flow` function that uses `TestClient` to execute a sequence of API calls: checking authentication status, initiating a login via the GitHub Device Flow, polling for login completion, and performing a logout. This flow demonstrates the expected interaction between a client and the backend authentication endpoints, validating the logic for login initiation, status polling, and session termination.",8d6c5e1e858f07440184c0ae1b7f98ffa53a57233031513fa6566131404ceab5
context/server/routes/files_example.py,"This Python script implements a standalone FastAPI application designed to simulate a file system management server. It defines a set of Pydantic models to represent file structures (FileTreeNode), content (FileContent), and operations (WriteFileRequest, WriteResult). The script includes a mock security layer with a `PathValidator` class to prevent directory traversal attacks by ensuring all file operations remain within a designated temporary project root. 

The core functionality is encapsulated in an `APIRouter` which exposes three main endpoints: `/tree` for retrieving a hierarchical view of the file system up to a specified depth, `/content` for reading file contents, and `/write` for creating or updating files. The script also includes helper functions for building file trees and detecting binary files. 

The `main` execution block sets up a temporary directory with dummy files (`hello.txt`, `src/main.py`) to serve as the sandbox environment, initializes the FastAPI app, registers the router, and launches the server using Uvicorn on localhost port 8000. It also handles cleanup by removing the temporary directory upon server shutdown.",9e5d8e77e1d0238394c331a872fcb31e4597100fc9427d8bba7ee3550df4323d
context/server/routes/commands_example.py,"This Python script serves as a self-contained demonstration and test harness for an asynchronous job execution API built with FastAPI. To function independently, the file explicitly defines mock dependencies, including Pydantic models for data validation and a `MockJobManager` class that simulates the lifecycle of background tasks (queuing, processing, and completion) using `asyncio` delays.

The script utilizes dynamic module patching via `sys.modules` to mimic the import structure of a larger project (`pdd.server`), allowing the router logic to run as if it were inside a full application context. The API defines two primary endpoints: one for submitting commands (restricted to specific allowed types like ""sync"" or ""generate"") and another for polling the status and results of those jobs.

Finally, the file includes a `run_example` function that acts as an integration test. Using `TestClient`, it spins up the application and executes a complete workflow: submitting a valid command, polling the status endpoint until the mock job completes, and verifying that invalid commands trigger appropriate HTTP errors. This allows developers to verify the API contract and logic in isolation.",2aee960b3f00528d1da6cd878806b82a9ea703827d66a8618d1899d6ea9076ea
context/server/routes/architecture_example.py,"This Python script serves as a demonstration and testing harness for an architecture validation module, likely part of a larger system named 'pdd'. It illustrates two primary methods of interacting with the validation logic: direct function calls and HTTP API requests via FastAPI. 

The script defines a `create_app` function to initialize a FastAPI application that includes an `architecture_router`. It then provides two main demonstration functions: `demonstrate_direct_validation` and `demonstrate_api_usage`. The direct validation demo tests the internal `_validate_architecture` function against three scenarios: a valid module structure, a structure with circular dependencies (causing errors), and a structure with missing dependencies and duplicates (triggering errors and warnings). 

The API usage demo utilizes `TestClient` to simulate a POST request to the `/api/v1/architecture/validate` endpoint with a sample payload, printing the resulting validation status. The script concludes with a `__main__` block that executes these demonstrations sequentially, handling potential import errors if the underlying architecture module is missing, and includes commented-out code for running the actual Uvicorn server.",6adddc3703c575ef7eaa12c58df0067e438c2fe328e5037fb69856595e862bf8
context/server/routes/__init___example.py,"This Python script serves as an illustrative example for utilizing the `pdd.server.routes` module within a FastAPI application. It demonstrates the architectural pattern of modularizing API routes by importing them from separate packages and integrating them into a central application instance. Although the script mocks the actual sub-modules (`files_router`, `commands_router`, and `websocket_router`) for demonstration purposes, it clearly outlines how these components would typically be structured in a production environment, such as in `pdd/server/main.py`. 

The code defines a `create_app` factory function that initializes a FastAPI application with specific metadata (title, description, version) and registers the mocked routers using `app.include_router`. It also includes a `main` entry point that instantiates the app, prints a list of registered routes (both HTTP and WebSocket) to the console for verification, and launches the server using `uvicorn` on localhost port 8000. This example effectively guides developers on setting up the PDD (Prompt Driven Development) Server's routing infrastructure.",a8bed7e09c6cd96bf68bff186993dc56d32f106fee86ca14f7324285d44f2267
context/xml/1/split_xml_LLM.prompt,"The provided file outlines a prompt engineering task designed to train an LLM to split a single complex prompt into two distinct components: a 'sub_prompt' and a 'modified_prompt'. The goal is to modularize the original prompt without losing any functionality. The file structure includes a section of examples, which reference external file paths for inputs (initial prompts and code) and expected outputs (split prompts and example usage code). It defines the context for the AI, establishing its role as an Expert LLM Prompt Engineer. The input definitions clarify that the model will receive an initial prompt, the code generated from it, and an example of how the split code should function. The output definitions specify the requirement to generate the two new prompts. Finally, the instructions guide the model to first analyze the difficulties of the split, propose solutions, and then generate the actual 'sub_prompt' and 'modified_prompt' based on the provided inputs.",0b203bd64cd6c92f8a1b47a6ed243f8061590a43ad627d8c695bdf0b24324f02
context/xml/1/split_LLM.prompt,"The provided file content is a prompt template designed for an expert LLM Prompt Engineer. Its primary objective is to instruct the LLM on how to refactor a single, large `input_prompt` into two distinct components: a `sub_prompt` and a `modified_prompt`, ensuring no loss of functionality. The template includes a section for few-shot learning, referencing four specific examples that demonstrate this splitting process using various input files (e.g., `initial_pdd_python.prompt`, `pdd.py`). It defines the expected inputs—the original prompt, the code generated from it, and an example of how the split code should function—and the required outputs. Finally, it outlines a step-by-step instruction set for the LLM to follow: identifying difficulties in the split, proposing solutions, and then generating the actual `sub_prompt` and `modified_prompt` text.",48ad8ab8f21f72b82b32484e54fc6fd3c7ea101fe3dec54cedfc5cf35f30c1b9
context/xml/2/xml_convertor_LLM.prompt,"This file contains a prompt designed for an expert Prompt Engineer whose task is to enhance the structure and readability of a raw input prompt by adding appropriate XML tags. The prompt instructs the model to analyze the input, identify components such as instructions, context, and examples, and then insert XML tags like `<instructions>`, `<context>`, `<examples>`, and `<formatting>` without altering the original content. It includes an example of how a raw prompt should be transformed and outlines a step-by-step process for the model to follow: analyzing the input, discussing appropriate tags, and finally generating the tagged prompt. The goal is to improve the organization of prompts, especially those containing placeholders or extensive text.",6b13729c7cb616406629f73a7b6f431659721b9ca0347d0fc62cbbabf3c9381a
context/xml/2/xml_convertor_xml_LLM.prompt,"The provided file outlines a prompt template designed for an AI agent acting as an expert Prompt Engineer. The primary objective of this agent is to take a raw input prompt (`input_raw_prompt`) and enhance its structure and readability by inserting appropriate XML tags. The instructions explicitly state that the agent should not add any new content, but rather organize existing text using tags such as `<instructions>`, `<context>`, `<examples>`, and `<formatting>`. The process involves a three-step workflow: analyzing the raw prompt to identify its components, determining suitable XML tags, and inserting those tags into the correct locations. The file includes examples of how raw prompts should be transformed into tagged versions and provides general guidelines, noting that XML tags are particularly useful for organizing large blocks of text or placeholders.",b5222a1465e870334d6e22d916dbe691527b1cc8ace21d7d681c85bfb5578fc7
context/generate/7/README.md,"PDD (Prompt-Driven Development) is a command-line interface tool designed to streamline software development by leveraging AI models to generate code, create unit tests, and manage prompt files. Installed via pip, it requires API keys from providers like OpenAI or Anthropic and supports various programming languages through a specific `.prompt` file naming convention.

The tool offers a comprehensive suite of commands, including `generate` for creating code, `test` for producing coverage-based unit tests, and `fix`, which utilizes an iterative loop to resolve code errors automatically. Advanced features include `update` for syncing prompts with modified code, `split` for managing complex prompts, and `crash` or `bug` for debugging runtime issues. PDD emphasizes workflow efficiency through multi-command chaining, detailed cost tracking via CSV outputs, and extensive configuration options using environment variables. It is designed for integration into CI/CD pipelines, offering features like auto-updates, dependency management, and security controls to facilitate a robust prompt-driven development lifecycle.",c6e6e0c2878f1753ad0669145ed552627a8bb7ddddcc89bab7338df1e92a75bd
context/generate/7/code_generator_main.py,"The provided code defines a main function, `code_generator_main`, designed to orchestrate the generation of code based on a prompt file. It utilizes the `click` library for command-line interface context and `rich` for formatted terminal output. The function takes a Click context, a path to a prompt file, and an optional output path as arguments. 

Internally, it first constructs necessary file paths and determines the target programming language using the helper function `construct_paths`. It then reads the content of the prompt file. The core logic involves calling `code_generator` with parameters such as strength and temperature (retrieved from the context), which returns the generated code, the cost of the operation, and the model name used. 

If an output path is specified, the function saves the generated code to that file. Finally, unless quiet mode is enabled, it prints a success message, the model name, the total cost, and the save location to the console. The function includes error handling that catches exceptions, prints an error message in red, and exits the program with a status code of 1 if a failure occurs.",319d4e3596f837183ff589638fe6ac87f180b95d7c0956bfa9f3c77e787ab0dd
context/generate/7/code_generator_main_python.prompt,"The provided file is a prompt template designed for an LLM to generate a Python function named `code_generator_main`. This function serves as a Command Line Interface (CLI) wrapper for a code generation tool. The prompt instructs the LLM to act as an expert Python engineer and defines the specific inputs and outputs for the target function. Inputs include a Click context object (`ctx`), a path to a prompt file, and an optional output path. The expected output is a tuple containing the generated code string, the operation cost, and the model name. The prompt template includes placeholders for various context files, such as a Python preamble, examples of using the `click` library, internal module usage examples (specifically `construct_paths` and `code_generator`), and a README file detailing the CLI command's behavior. Essentially, this file acts as a meta-prompt to guide an AI in writing the main logic for a code generation CLI tool.",7a42d9a6e9fe4ec1d066474a1f6d027a8888f93df743c35e174bb253d44e792f
context/generate/6/conflicts_main_python.prompt,"This file contains a prompt for an expert Python engineer to develop the `conflicts_main` function for the `pdd` command-line program. The function is designed to handle the core logic of the 'conflicts' command, which analyzes potential conflicts between two prompt files. The prompt specifies the function's inputs (Click context, file paths) and outputs (conflict results, cost, model name). It outlines a step-by-step implementation plan: constructing file paths using internal utilities, loading file contents, invoking the `conflicts_in_prompts` function for analysis, updating result data with actual file paths, saving the output to a CSV file, and providing user feedback via the Rich library. The prompt also includes references to context files for Python preambles, Click library usage, and internal module examples (`construct_paths`, `conflicts_in_prompts`) to guide the implementation.",b850d40459d986b3e5e8141fe3987896e9c98b14f6ca7cc3e012da011387fc71
context/generate/6/conflicts_main.py,"This file defines the `conflicts_main` function, which serves as the core logic for a command-line tool designed to analyze and resolve conflicts between two text prompts. The function takes two prompt file paths and an optional output path as arguments. It utilizes helper functions to construct file paths and read the content of the prompts. The core analysis is performed by calling `conflicts_in_prompts`, using parameters like strength and temperature derived from the Click context. The results, which identify conflicting instructions and suggest changes, are then mapped back to their original filenames and saved to a CSV file. The function also handles user feedback by printing the analysis status, the model used, the total cost, and a detailed list of detected conflicts to the console using the `rich` library. Error handling is included to catch exceptions and exit gracefully.",e7a05ba27d622574ff90a578304b063aa68970bdd270edf852c86f13d9470937
context/generate/1/fix_errors_from_unit_tests_python.prompt,"The provided text outlines the requirements for creating a Python function named `fix_errors_from_unit_tests`. This function is designed to automatically resolve errors encountered during unit testing by leveraging Langchain LCEL (LangChain Expression Language) and LLM models. The function takes inputs such as the unit test code, the code under test, the error message, and model parameters like strength and temperature. It operates in a multi-step process: first, it loads specific prompt templates from a project directory and uses an LLM to generate a fix based on the provided errors. Second, it employs a secondary LLM call to parse the generated fix into structured JSON outputs, specifically extracting boolean flags for updates and the corrected code strings. The process emphasizes using the `rich` library for pretty-printing console output, tracking token usage and costs for each LLM interaction, and handling potential errors gracefully. The final output includes the fixed code, fixed unit test, update flags, and the total calculated cost of the operations.",e996814013526e08ecd4258f33dc84ba9c3594d3662496c081332567bce8e5a3
context/generate/1/fix_errors_from_unit_tests.py,"This Python script defines a function named `fix_errors_from_unit_tests` designed to automatically resolve errors in unit tests and their associated code using Large Language Models (LLMs). The script leverages the LangChain library for orchestrating LLM interactions and the `rich` library for formatted console output. It begins by setting up an SQLite cache to optimize performance and reduce costs.

The core function takes the failing unit test, the source code, the error message, and LLM configuration parameters (strength and temperature) as inputs. It operates in a two-step process. First, it loads a prompt template to analyze the error and generate a fix, invoking an LLM selected via a custom `llm_selector`. The output of this step is displayed as Markdown. Second, it uses another prompt template to extract the specific code and unit test modifications from the previous analysis into a structured JSON format. The script calculates and prints the token usage and financial cost for each step. Finally, it returns a tuple containing flags indicating whether the unit test or code was updated, the actual fixed code strings, and the total cost of the operation, while handling potential exceptions gracefully.",7a3eb969bcaa76dc85d59c4013a41a7da5bb7472daf97df2b2cad0475642d2c7
context/generate/8/llm_invoke_python.prompt,"The provided text outlines the requirements for generating a Python function named `llm_invoke` within a file called `llm_invoke.py`. This function is designed to execute Large Language Model (LLM) prompts using the LangChain framework and its caching capabilities. The function accepts inputs such as a prompt string, input JSON, a `strength` parameter (0.0 to 1.0) for model selection, temperature, verbosity, and an optional Pydantic output schema. A core feature is the dynamic selection of the LLM model based on the `strength` value: values below 0.5 interpolate based on cost relative to a base model (defaulting to ""gpt-4o-mini""), while values above 0.5 interpolate based on ELO score up to the highest-rated model. The instructions specify handling different model providers (like Google and OpenAI) correctly regarding token limits and structured outputs, specifically mandating `.with_structured_output()` for Pydantic models. Additionally, the prompt details verbose logging requirements, including printing model costs, token usage, and results using `rich` printing where possible.",e938fc6ef160c8bf1b07147276a7c1dc9fd957cadfefb7c2d866f0f5b9caf45e
context/generate/8/llm_invoke.py,"The provided file, `llm_invoke.py`, serves as a flexible utility wrapper for invoking various Large Language Models (LLMs) using the LangChain framework. Its primary purpose is to abstract model selection and execution based on performance requirements and cost constraints. The script relies on an external CSV file (`llm_model.csv`) to load metadata for supported models, including providers (OpenAI, Anthropic, Google, etc.), pricing per million tokens, and ""Coding Arena ELO"" ratings.

A core feature is the `select_model` function, which dynamically chooses an LLM based on a `strength` parameter (0.0 to 1.0). A strength below 0.5 prioritizes cost savings by selecting models cheaper than a default base, while a strength above 0.5 prioritizes performance by selecting models with higher ELO ratings. The main function, `llm_invoke`, orchestrates the process by validating inputs, initializing the selected model with appropriate API keys, and constructing a processing chain. It supports both standard string outputs and structured data extraction via Pydantic models. Additionally, the script includes a custom callback handler to track token usage, allowing it to calculate and return the specific financial cost of each execution alongside the generation result.",3a0d5138cc2b1f6668a94dff40cf5d97376bde2581e83e2538c3ef012a94f1bf
context/generate/4/cli_python.prompt,"This file contains a detailed prompt for an AI to generate a Python command-line interface (CLI) tool named ""pdd"". The tool is designed to be built using the `click` library and serves as a developer utility for managing code generation, testing, and prompt engineering workflows. The prompt outlines the directory structure, import conventions to avoid naming conflicts, and specific instructions for implementing various commands. These commands include `generate` (creating code from prompts), `example` (generating examples from code), `test` (creating unit tests), `preprocess` (handling prompt files), `fix` (resolving code errors), `split` (dividing prompts), `change` (modifying prompts), `update` (refreshing prompts), `detect` (identifying necessary changes), `conflicts` (resolving prompt conflicts), `crash` (fixing crash errors), `trace` (execution tracing), and `bug` (converting bugs to tests). It provides specific internal module usage examples for each command.",e795460d8284f0a103eca1ffc797f2a0f834225ca513e520f2c5af19f7f704a4
context/generate/4/cli.py,"This file defines the Command Line Interface (CLI) for a Prompt-Driven Development (PDD) tool. It utilizes the `click` library to structure the CLI and `rich` for formatted console output. The script exposes a main command group `cli` with global options for controlling AI model parameters (strength, temperature), verbosity, and cost tracking. It implements numerous subcommands that orchestrate various AI-assisted development tasks, including `generate` (creating code from prompts), `example` (generating examples from code), `test` (creating unit tests), `preprocess` (formatting prompts), `fix` (iteratively repairing code based on errors), `split` (breaking down complex prompts), `change` (modifying prompts), `update` (syncing prompts with code changes), `detect` (identifying necessary changes), `conflicts` (resolving prompt conflicts), `crash` (fixing crashing modules), `bug` (generating tests from bug reports), and `trace` (linking code lines back to prompt lines). Each command typically handles file I/O, invokes specific backend logic functions, and tracks the cost of AI model usage.",aeaaceab84a98e9cb777d691f1a9de4aa3538569ce0fa0b732228c8dfbae51cf
context/generate/3/cli_python_preprocessed.prompt,"The provided file describes ""pdd"" (Prompt-Driven Development), a Python command-line interface (CLI) tool designed to streamline software development using AI models. PDD facilitates a workflow where developers define functionality in prompt files, which the tool then uses to generate code, unit tests, and usage examples. Key features include support for multiple programming languages (Python, JavaScript, C++, etc.), cost tracking for AI usage, and configurable model parameters like strength and temperature.

The CLI offers a comprehensive suite of commands: `generate` creates code from prompts; `test` produces unit tests; `example` generates usage examples; `fix` iteratively repairs code based on errors; and `preprocess` handles prompt formatting. Advanced commands like `split`, `change`, `update`, `detect`, `conflicts`, and `crash` allow for managing complex prompts, detecting necessary changes, resolving conflicts between prompts, and fixing runtime crashes. The tool supports multi-command chaining for complex workflows and integrates with environment variables for flexible output management. It emphasizes security and efficiency, aiming to automate repetitive coding tasks while maintaining high code quality through iterative testing and refinement.",19d93d524e9b050886a9ab599dab0d3ba2d637d9b31a0333e8285ec2a84c12a3
context/generate/3/cli.py,"This file implements the main entry point for a Prompt-Driven Development (PDD) Command Line Interface (CLI) tool, built using the Python `click` library. It orchestrates various AI-assisted software development tasks by importing logic from several internal modules. The script defines a global CLI group that accepts configuration parameters such as AI model `strength`, `temperature`, verbosity, and cost tracking options.

A key component is the `track_cost` decorator, which wraps commands to log execution time, API costs, and model usage to a CSV file. The CLI exposes a wide range of subcommands including `generate` (creating code from prompts), `test` (generating unit tests), and `fix` (iteratively resolving errors via a feedback loop). It also includes commands for prompt engineering workflows, such as `preprocess`, `split`, `change`, `update`, `detect`, and `conflicts`. Additionally, the tool features a `crash` command for fixing module errors and a utility to install shell auto-completion. The script utilizes the `rich` library for formatted console output and handles file path management for inputs and outputs.",43a3aaf18afc0af2bc214890f87e561f5f61e885452c4dc69daff01818e1eb4c
context/generate/2/cli_python.prompt,"This file serves as a comprehensive prompt or specification for an AI agent acting as an expert Python engineer to build a command-line interface (CLI) tool named ""pdd"". The tool is designed to be built using the Python `Click` library for CLI management and the `rich` library for pretty-printed console output. The document outlines the project's directory structure (including folders for prompts, context, and data) and specifies several core commands that the tool must support. These commands include 'generate' for creating runnable code from prompts, 'example' for generating context examples, 'test' for creating unit tests, 'preprocess' for handling prompt files (with an XML sub-command), 'fix' for resolving code errors (with a loop sub-command), 'split' for dividing prompt files, 'change' for modifying prompts, and 'update' for refreshing prompt content. Additionally, it includes instructions for an 'install_completion' command to handle shell auto-completion. The file references external context files (e.g., `<./context/click_example.py>`) to provide specific implementation examples for each function, ensuring the AI follows established patterns for file loading, path construction, and module imports.",f82bd3b41440348f58b8d2a98606b1edc4db6ff8eb20922551e7fd4f79c9003b
context/generate/2/cli.py,"This file defines the command-line interface (CLI) for a tool called PDD (Prompt-Driven Development). Built using the `click` and `rich` libraries, the CLI provides a suite of commands to facilitate AI-assisted coding workflows. Key functionalities include generating code from prompt files (`generate`), creating example files (`example`), generating unit tests (`test`), preprocessing prompts (`preprocess`), and iteratively fixing code errors based on test failures (`fix`). It also includes commands for splitting complex prompts (`split`), modifying prompts based on changes (`change`), and updating prompts to reflect code modifications (`update`). The tool supports cost tracking via CSV logging, caching with SQLite, and shell completion installation for Bash, Zsh, and Fish.",b607c3e67702ad3bc5e61cc40ce85571e4ff09015de96016c8d7d56eb78b8c74
context/generate/5/generate_output_paths_python.prompt,"This document outlines the requirements for implementing a Python function named `generate_output_paths` for the Prompt-Driven Development (PDD) CLI tool. The function is responsible for constructing appropriate output filenames based on the specific command executed (e.g., `generate`, `fix`, `split`, `test`), user-provided options, and environment variables. The document includes a comprehensive description of the PDD tool, detailing its version (0.1.0), supported languages, and prompt file naming conventions. It elaborates on global options, AI model configuration, and cost tracking features. Crucially, it lists specific commands and their default output naming behaviors, such as `generate` defaulting to `<basename>.<ext>` and `fix` producing multiple outputs like fixed code and test files. The text also specifies environment variables that override default paths (e.g., `PDD_GENERATE_OUTPUT_PATH`). The implementation task requires a multi-step approach: analyzing output construction methods, listing default conventions and environment variables, identifying error cases, and finally writing the code to handle these logic flows robustly, ensuring correct handling of file extensions and directory paths.",78d3b23bb07b5a646a123abe3fa266dd4ee6b98953c84e01ae93f598f5a9dcae
context/generate/5/generate_output_paths.py,"This file defines a Python utility function named `generate_output_paths` designed to determine file paths for various outputs based on a specified command. The function takes inputs such as the command type (e.g., 'generate', 'test', 'fix', 'split'), a dictionary of explicit output locations, a base filename, a language, and a file extension. It employs helper logic to construct default filenames tailored to each command—for instance, appending '_example' for example generation or '_fixed' for code repair tasks. Additionally, the function checks for environment variables (prefixed with `PDD_`) to override default paths if explicit locations are not provided. It handles logic for both single-file outputs and multi-file outputs (as seen in 'fix' and 'split' commands), ensuring that if a directory path is provided, the appropriate filename is appended to it. The final output is a dictionary mapping output keys to their resolved string file paths.",78a4d97bd4c80d164c19e59efdf0153d6d7eafb75faf4f077ed630e15602ad17
context/commands/generate_example.py,"This Python script serves as a comprehensive demonstration and documentation for the `pdd.commands.generate` module within the PDD CLI tool. It illustrates the usage of three primary Click commands: `generate` (for creating code from prompts), `example` (for generating usage examples), and `test` (for creating or enhancing unit tests). The script defines helper functions to set up a sample environment, including creating dummy prompt and code files for a calculator module. It then provides detailed functions (`example_generate_command`, `example_example_command`, `example_test_command`) that print out command signatures, arguments, options, and equivalent CLI invocations for each command. Additionally, it includes examples for programmatic invocation using Click's context management and demonstrates how to use templates with environment variable substitution. The script is designed to be run directly to display these educational examples to the user.",1fcc47190c0b830afb57a76a693215bc98ac719ed56824691e17a2623896e8b5
context/commands/connect_example.py,"This Python script serves as a usage example and test suite for the `pdd connect` command module within a Click-based CLI application. It demonstrates four key scenarios: integrating the command into a main CLI group, simulating server startup using mocked dependencies (such as `uvicorn` and `webbrowser`) to verify argument parsing without blocking execution, testing security warnings when binding to remote interfaces without authentication tokens, and overriding the default browser target with a custom frontend URL. The script utilizes `unittest.mock` to isolate the command logic from external side effects and `click.testing.CliRunner` to programmatically invoke and verify the command's behavior and output.",d2bcda4ae1e9de22d1b657c28045a0b122dc1ef22a77e4d670f41229779a6083
context/commands/analysis_example.py,"This Python script serves as a demonstration and test harness for programmatically invoking various analysis commands from the `pdd` (Prompt Driven Development) library. It specifically showcases how to use the `detect_change`, `conflicts`, `bug`, `crash`, and `trace` commands by calling their Click command callbacks directly within a Python script rather than via the command line.

The script sets up a Click `Context` object to simulate the runtime environment, injecting configuration parameters such as verbosity, model strength, and temperature. It defines a helper function, `clean_output`, to manage file cleanup and prevent overwrite prompts. The `main` function iterates through five distinct scenarios: detecting necessary prompt changes based on a description, identifying conflicts between two prompt files, generating a unit test to reproduce a bug, analyzing and fixing a crash caused by division by zero, and tracing execution flow back to a prompt. For each scenario, the script generates dummy input files (prompts, code, logs) in an output directory, executes the corresponding analysis callback, and prints the results.",01ff82910c50ec8f8cfd22f8e3b7c011f286a4b9f90e7dd5b2eb685c1f339e56
context/commands/fix_example.py,"This Python script serves as a comprehensive demonstration and documentation for the `pdd fix` command within the `pdd` package. The script illustrates how to automatically repair errors in code and unit tests using AI-driven analysis of error logs and original prompts. It begins by setting up a simulated environment, creating sample files including a calculator module with an intentional bug (missing division-by-zero handling), a corresponding unit test, and an error log.

The script defines a CLI wrapper using `click` and provides six distinct example functions showcasing different usage patterns: basic command invocation, iterative fixing with loop mode and verification programs, agentic fallback for complex errors using broader context, automatic submission of successful fixes to the PDD Cloud, cost tracking via CSV output, and programmatic usage within Python code. Each example demonstrates specific configuration options such as budget constraints, maximum attempts, model strength, and file output paths. The `main` function orchestrates the setup but leaves the actual execution of examples commented out to prevent accidental API costs, serving as a template for users to understand and implement automated code repair workflows.",b5230334f121f2257f6ee42f1ae038b3c9c441e513b082e9856576aa0eefdf83
context/commands/utility_example.py,"This Python script serves as a demonstration and documentation tool for the utility commands within the `pdd` package, specifically focusing on the `install_completion_cmd` and `verify` Click commands. It illustrates how to programmatically interact with these tools without executing them fully, thereby avoiding changes to the user's shell configuration or incurring API costs. The script sets up a local environment by creating a directory of dummy files—including a prompt, a buggy Python module (`calculator.py`), and a verification script—to simulate a real-world workflow for the `verify` command. It then prints detailed information to the console using the `rich` library, explaining the inputs, outputs, and command-line arguments for both utilities. The `install_completion_cmd` section explains how shell completion is installed, while the `verify` section details the iterative code fixing process using LLMs, including parameters like budget and max attempts. Additionally, the script documents the specific return value structure of the `verify` command, describing the tuple format containing results, costs, and model names. This file acts as both an executable example and a reference guide for developers integrating or using `pdd` utility commands.",fb7389c0ec837b8b6ee481e2eda04800f565fe247643f5bb9922dd1dfdab984a
context/commands/modify_example.py,"This Python script serves as a comprehensive demonstration and test suite for the `modify` module within the `pdd` CLI tool. It specifically showcases three core commands—`split`, `change`, and `update`—which are designed to manage and evolve LLM prompts and code. The script begins by establishing a local output directory structure and generating necessary sample artifacts, including prompt files (`.prompt`), source code (`.py`), and change instructions. It then executes four distinct scenarios using `Click`'s testing runner: `split`, which demonstrates breaking a large prompt into modular sub-prompts; `change` (manual mode), which modifies an existing prompt based on specific textual instructions (e.g., adding logging); `change` (batch mode), which illustrates processing multiple prompt modifications simultaneously using a CSV file; and `update`, which performs a reverse-synchronization where a prompt file is updated to reflect manual edits made to the corresponding source code. The script is self-contained, handling file creation, command execution, and output verification.",35b98245915a4e4fe341affaa67da6b9e4a61a5f1c7a9f0d1fba042a3c280dc1
context/commands/auth_example.py,"This Python script serves as an integration example for the 'auth' command group within a CLI tool named PDD. It sets up the execution environment by adding the project root to `sys.path` to ensure internal modules can be imported correctly. The script attempts to import `auth_group` from `pdd.commands.auth` and registers it under a main Click group named `cli`. 

Instead of executing a specific command logic directly, the script is designed to demonstrate the structure and availability of authentication commands. When run as the main program, it prints an introductory message and simulates running `pdd auth --help` to display the help output for the registered authentication commands. It concludes by printing several usage examples to the console, showing users how to perform actions such as interactive login, headless login for CI environments, checking authentication status, retrieving tokens, and logging out. The primary purpose is to verify that the authentication subcommands are correctly wired into the main CLI structure.",d1e77e2ab5a5b6f3c400a8a68a73bb633f6d45e39dd4a90306dcc18c729d9000
context/commands/maintenance_example.py,"This Python script serves as a comprehensive example demonstrating the usage of maintenance commands within the `pdd` package's CLI framework. It showcases three primary Click commands: `sync` (for synchronizing prompts with code and tests), `auto-deps` (for analyzing and injecting dependencies into prompt files), and `setup` (for running an interactive setup utility). The script includes helper functions to create a mock project environment with sample prompt and code files. It then defines separate functions (`example_sync_command`, `example_auto_deps_command`, `example_setup_command`) that utilize `unittest.mock` to simulate the execution of these commands without making actual LLM calls or system modifications. Additionally, it includes an example of how to invoke these commands programmatically, bypassing the CLI parsing while maintaining the Click context.",f347251582821c4347664f674fdd6d5549bb14ba1d0e8bc428cffb9abedbfed3
context/commands/templates_example.py,"This Python script serves as a demonstration and documentation for the `pdd.commands.templates` module, specifically showcasing the CLI commands used to manage PDD templates. It utilizes the `click.testing.CliRunner` to programmatically invoke and display the output of three primary subcommands: `list`, `show`, and `copy`. The script defines separate functions for each operation: `example_list_templates` illustrates how to list available templates with options for JSON formatting and tag filtering; `example_show_template` demonstrates how to retrieve detailed metadata, variable definitions, and usage examples for a specific template; and `example_copy_template` shows how to duplicate a template file to a local destination directory. The `main` function orchestrates these examples, providing context on the purpose of the templates command group, which is designed to facilitate the discovery, inspection, and reuse of prompt templates within the PDD framework.",017d7a9cbdec60dd67cee662f3b21a68638137cd086fb335ef92889e1d91b09a
context/commands/misc_example.py,"This Python script serves as a comprehensive example and test suite for the `pdd.commands.misc.preprocess` module, specifically demonstrating the functionality of the `preprocess` Click command. The script illustrates how to prepare prompt files for Large Language Models (LLMs) by handling directives such as file includes, comments, shell commands, and web scraping locally without API calls. It defines a setup function to generate sample prompt and text files, followed by five distinct example functions that simulate command-line execution using `CliRunner`. These examples cover basic preprocessing, XML delimiter insertion, curly bracket doubling (with exclusion logic for template variables), recursive file inclusion, and a combination of these options. The script concludes with a main execution block that runs all examples sequentially, printing the results and exit codes to the console, thereby verifying the preprocessor's capabilities in handling various formatting and content integration tasks.",7d7985d21dd7b962257856cb92d13e26c016bc09ce14aaac4c68baaead97dc4b
context/commands/__init___example.py,"This Python script serves as a demonstration and usage example for the `register_commands` module within a project named PDD (Python Development Driver). The file illustrates how to set up a Click-based Command Line Interface (CLI) by importing and utilizing the `register_commands` function to attach various subcommands to a main `click.Group`. It defines a primary CLI entry point (`cli`) and a secondary custom example (`custom_cli`), showing that the registration process works on any Click group. The script documents the specific categories of commands available, including Generation, Fixing, Modification, Maintenance, Analysis, Miscellaneous, Utility, and Templates. When executed directly, the script prints a formatted list of these registered commands organized by category, displaying their help text summaries, and provides usage examples for invoking the CLI tools. Essentially, it acts as both a functional test of the command registration logic and a documentation reference for the available CLI capabilities.",cc6e0de7d94b1dddda35d9c8ecb800710263d928191f1d4cb2400bae36c31cb4
context/commands/sessions_example.py,"This Python script serves as a standalone test runner and demonstration for the `pdd.commands.sessions` CLI module. It primarily focuses on mocking external dependencies to simulate the environment required for the CLI commands to run without a real backend. The script sets up `MagicMock` objects for various system modules like `pdd.core.cloud` and `pdd.remote_session`, defining dummy session data and asynchronous mock functions for listing and retrieving session details. It includes a `run_example` function that utilizes `click.testing.CliRunner` to programmatically invoke CLI commands such as `sessions list` (in both table and JSON formats) and `sessions info`. The script demonstrates how to handle successful command executions as well as error cases, like querying for a non-existent session ID, while using `rich.console` for formatted output.",d235c7a2fad154663ce3016baa1babd45a1b5fa5041ba130024dbe88ab791a28
