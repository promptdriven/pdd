full_path,file_summary,date
"context/DSPy_example.py","The Python script snippet demonstrates the initial setup and definition of a DSPy module. The primary goal, as stated in a comment, is to use DSPy for selecting an optimal example. The script begins by importing DSPy and setting up an OpenAI language model, specifically 'gpt-3.5-turbo-instruct', with a maximum token limit of 250. This model is then configured as the default LM for DSPy operations. Following the setup, a custom DSPy module named `chainofthought` is defined. This class inherits from `dspy.Module` and, in its constructor, initializes a `dspy.ChainOfThought` program. This program is structured to take a 'question' as input and produce an 'answer'. The `forward` method of the `chainofthought` module is defined to pass the input `question` to this `ChainOfThought` program. The script concludes with a comment indicating that the next steps would involve compiling and optimizing the defined module, a common practice in DSPy to enhance performance and tailor the language model's behavior.",2025-06-03T06:20:02.428546+00:00
"context/anthropic_counter_example.py","The provided Python script demonstrates how to use the Anthropic library to count the number of tokens in a given string. It begins by importing the `anthropic` library. Although commented out, the script shows the initialization of an `Anthropic` client. It then defines a sample text string. The core functionality, also commented out, involves calling the `client.count_tokens()` method with the sample text to determine the token count. A crucial comment within the script highlights that this `count_tokens` method is 'not accurate for 3.0 and above models'. Finally, the script, if uncommented and executed, would print the calculated number of tokens to the console. The script serves as a basic example of token counting with the Anthropic library, while also cautioning about its accuracy limitations with newer model versions.",2025-06-03T06:20:12.681564+00:00
"context/anthropic_tool_example.py","The provided file is a Python script that utilizes the Anthropic API to interact with an AI model, specifically claude-3-7-sonnet-20250219. The script imports the anthropic library, initializes a client, and sends a message to the AI model. The message, sent as a user role, asks for help in fixing a syntax error in a file named patent.md. The script specifies a tool, str_replace_editor, which appears to be a text editor utility, possibly for string replacement tasks. The AI model is instructed to generate a response with a maximum of 1024 tokens. Finally, the script prints the response received from the AI. Overall, the script demonstrates how to use the Anthropic API to request assistance from an AI model for editing or correcting a file, leveraging a specific tool for text manipulation.",2025-04-30T03:46:33.370975+00:00
"context/auto_deps_main_example.py","This Python script defines a command-line interface (CLI) tool using the Click library. The tool, named 'auto-deps', is designed to process a prompt file and analyze code dependencies, likely for Python projects. It provides several command-line options, such as forcing file overwrites, suppressing output, setting parameters for dependency analysis (strength and temperature), specifying paths for the prompt file, directory of example files, a CSV file for dependencies, output location, and forcing a directory rescan.

The main function collects these options, sets up the Click context, and then calls the 'auto_deps_main' function (imported from 'pdd.auto_deps_main') with the relevant parameters. The results, including the modified prompt, total cost, and model name used for the analysis, are printed to the console. If an error occurs during execution, it is caught, printed, and the process is aborted using Click's error handling. The script is intended to be run as a standalone program, as indicated by the '__main__' block at the end.",2025-04-30T03:46:44.050799+00:00
"context/auto_include_example.py","This Python script demonstrates the usage of the `auto_include` function from the `pdd.auto_include` module, likely as part of a project dependency management or automation tool. The script begins by importing necessary modules, including `os`, `pandas`, and some custom modules. The main function reads a CSV file named 'project_dependencies.csv', which presumably contains information about project dependencies. It then defines a detailed multi-line string `input_prompt` that describes the intended behavior of a function called `generate_test`, which is designed to generate unit tests for code files using a language model (LLM) via Langchain. The prompt outlines a step-by-step process for generating, formatting, and validating unit tests, including cost calculation and model selection.

The script sets parameters such as the directory path to search for Python files, the strength and temperature for the LLM, and a verbosity flag. It then calls the `auto_include` function with these parameters, which processes the dependencies and returns the results, including the dependencies found, a CSV output, the total cost of the operation, and the name of the LLM model used. Finally, the script prints these results. The script is intended to be run as a standalone program.",2025-04-30T03:46:55.799083+00:00
"context/auto_update_example.py","The provided file is a Python script demonstrating the usage of the `auto_update` function from the `pdd.auto_update` module. The script defines a `main()` function that showcases several ways to utilize `auto_update` for managing package updates. The function's docstring explains that `auto_update` works by checking the currently installed version of a package, comparing it to the latest available version (either from PyPI or a specified version), and prompting the user to upgrade if a newer version is found. If the user agrees, the upgrade is performed using pip. If the package is already up to date, the function produces no output. The script provides three example usages: checking for updates to the default 'pdd' package, checking for updates to the 'requests' package, and checking if the 'pandas' package is at least version 2.0.0. The script is designed to run the `main()` function when executed directly. Overall, the file serves as a practical guide for developers on how to implement and use the `auto_update` function to keep Python packages current.",2025-04-30T03:47:04.801528+00:00
"context/autotokenizer_example.py","The provided file is a Python script that demonstrates how to count the number of tokens in a given text using a tokenizer from the Hugging Face Transformers library. The script imports the AutoTokenizer class and defines a function called count_tokens, which takes a text string and an optional model name (defaulting to 'deepseek-ai/deepseek-coder-7b-instruct-v1.5'). The function loads the appropriate tokenizer for the specified model, tokenizes the input text, and returns the number of tokens by counting the length of the 'input_ids' in the tokenized output. An example usage is provided, where the script counts the tokens in the sentence 'Write a quick sort algorithm in Python.' and prints the result. This script is useful for developers working with language models who need to determine token counts for input texts, which is often important for managing model input limits or estimating computational costs.",2025-04-30T03:47:11.518427+00:00
"context/bug_main_example.py","This Python script demonstrates how to use the `bug_main` function from the `pdd` package to automatically generate unit tests based on observed and desired outputs of a program. The script sets up a Click context with options for overwriting files, verbosity, model strength, and temperature. It then creates an output directory and writes several example files: a prompt describing the desired function (summing even numbers in a list), a Python implementation of that function, a main program that uses the function, and files containing the current (incorrect) and desired (correct) outputs. The script then calls `bug_main`, passing in the context and paths to these files, to generate a unit test that would help identify and correct the bug (in this case, not handling empty lists). Finally, it prints the generated unit test, the cost of the operation, and the model used. The script serves as an example of automating test generation and bug detection using AI-driven tools.",2025-04-30T03:47:18.877074+00:00
"context/bug_to_unit_test_example.py","This Python script demonstrates the usage of a function called `bug_to_unit_test` from the `pdd.bug_to_unit_test` module. The script is designed to automate the process of generating unit tests for code that may contain bugs, based on a prompt and the code under test. It uses the `rich` library for enhanced console output.

The main function sets up several parameters, including sample outputs (`current_output` and `desired_output`), the module name, and reads in the prompt, code under test, and the program used to run the code from corresponding files. It also sets parameters for the language model's strength and temperature, and specifies the programming language as Python.

The core of the script is a try-except block where it calls `bug_to_unit_test` with all the prepared inputs. If successful, it prints the generated unit test, the total cost of the operation, and the model used. If an error occurs, it prints an error message. The script is intended to be run as a standalone program and serves as an example or test harness for the `bug_to_unit_test` function, facilitating the debugging and unit test generation process for Python code.",2025-04-30T03:47:28.207151+00:00
"context/bug_to_unit_test_failure_example.py","The provided file contains a short snippet of Python code. It defines a single function named `add`. This function is designed to accept two input arguments, represented by the parameters `x` and `y`. The core logic implemented within the function calculates the difference between these two arguments, specifically `x - y`. The result of this subtraction operation is then returned by the function. Notably, there is a discrepancy between the function's name, `add`, which implies addition, and its actual implementation, which performs subtraction. While syntactically correct Python, the function's behavior contradicts its name, suggesting a potential logical error or misleading naming convention.",2025-04-30T03:47:35.315656+00:00
"context/change_example.py","This Python script demonstrates the usage of a function called `change` from the `pdd.change` module, likely designed to modify code or prompts using a language model. The script begins by importing necessary modules, including `os` for environment variables, `change` for the main functionality, and `Console` from the `rich` library for formatted output. The `main` function sets up example input data: a prompt asking for a factorial function, the corresponding Python code, and a change prompt requesting the function be modified to take the square root of the factorial output. It also defines parameters for the language model, such as `strength` and `temperature`. The script then calls the `change` function with these inputs, capturing the modified prompt, the total cost of the operation, and the model name used. Results are printed to the console in a formatted manner. Error handling is included to catch and display any exceptions. The script is designed to be run as a standalone program, with the `main` function executed when the script is run directly. Overall, it serves as an example or test harness for the `change` function's capabilities.",2025-04-30T03:47:44.555335+00:00
"context/change_main_example.py","This Python script serves as an example of how to use the `change_main` function from the `pdd.change_main` module, which is part of a command-line program named 'pdd'. The script demonstrates two primary modes of operation: single-change mode and CSV batch-change mode. 

First, it sets up a `click.Context` object with various configuration parameters like LLM strength, temperature, language, and budget. It then creates necessary output directories. 

For the single-change mode, the script creates sample input files: a change prompt file, an input code file (a simple Python division function), and an input prompt file. It then calls `change_main` to modify the input prompt based on the change prompt and the code, displaying the modified prompt, total cost, and the language model used. 

For the CSV batch-change mode, it creates a directory with sample code files (addition and subtraction functions) and their corresponding prompt files. A CSV file is then generated, specifying different modification instructions for each prompt. The `change_main` function is called again, this time with `use_csv=True`, to process these batch changes. The script concludes by printing a success message, the total cost for the batch operation, the model used, and the location where batch results are saved. The `rich` library is used for formatted output.",2025-06-03T06:20:17.447142+00:00
"context/cli_example.py","The file 'demo_run_pdd_cli.py' is a minimal, self-contained Python script that demonstrates how to programmatically invoke the PDD (Prompt-Driven Development) CLI using Python. The script is structured to perform three main tasks:

1. It creates a small prompt file in the './output/' directory. This prompt instructs the model to write a Python function `hello()` that returns the string Hello, PDD!.

2. It uses Click's `CliRunner` to invoke the PDD CLI's `generate` command with the `--local` flag, ensuring that the code generation uses a local model rather than contacting the cloud. The output is directed to './output/hello.py'. The `--quiet` flag is also used to suppress progress bars and other non-essential output.

3. After running the command, the script prints the CLI's output, the exit code, and checks if the generated Python file exists. If successful, it prints the path to the generated file and displays its first few lines; otherwise, it reports a failure.

The script serves as a concise example for developers on how to automate PDD CLI tasks within Python, especially for local, offline code generation scenarios.",2025-04-30T03:47:58.628805+00:00
"context/click_example.py","This Python script defines a command-line tool for processing images using the Pillow library, structured as a Unix-like pipeline using the Click library for command-line interfaces. The script allows users to chain multiple image processing commands, where the output of one command feeds into the next. The main commands implemented include opening images, saving processed images, displaying them, resizing, cropping, rotating, flipping, blurring, smoothening, embossing, sharpening, and pasting images together. Each command is implemented as a Click subcommand and uses decorators to handle the chaining of operations. The script supports processing multiple images at once and provides options for customizing each operation, such as specifying dimensions for resizing, the amount of blur, or the number of smoothening iterations. Error handling is included for file operations, and informative messages are printed to the console during processing. The design emphasizes modularity and extensibility, allowing users to build complex image processing pipelines directly from the command line, similar to Unix pipes. The script is well-documented with docstrings and usage examples, making it user-friendly for batch image processing tasks.",2025-04-30T03:48:06.013435+00:00
"context/cmd_test_main_example.py","The file 'test_cli_example.py' provides an example of how to build a command line interface (CLI) using the Click library to integrate the 'cmd_test_main' function, which is designed for generating or enhancing unit tests for source code files. The script defines a CLI with global options such as verbosity, AI generation strength, creativity (temperature), force overwrite, and quiet mode. The main command, 'test', requires two arguments: a prompt file (typically a text file that generated the code, such as a GPT prompt) and a code file (the source code to be tested). Optional arguments allow users to specify the output path for generated tests, override the programming language, provide a coverage report, supply existing tests for enhancement or merging, set a target coverage percentage, and enable merging of new tests into existing ones. When executed, the script calls 'cmd_test_main' with the provided parameters and outputs the generated tests, the total cost of generation, and the model used. The file includes detailed docstrings and usage examples, making it a practical template for integrating AI-powered test generation into a Python CLI workflow.",2025-04-30T03:48:14.678926+00:00
"context/code_generator_example.py","The provided file is a Python script that demonstrates how to use a function called `code_generator` from the `pdd.code_generator` module. The main purpose of the script is to generate code using a language model based on a prompt. The script reads a prompt from an external file located at `prompts/generate_test_python.prompt`. It then sets several parameters for the code generation process, including the programming language (Python), the strength and temperature of the language model, and a verbosity flag. The script attempts to call the `code_generator` function with these parameters, which returns the generated code, the total cost of the operation, and the name of the model used. These results are then printed to the console. If any exception occurs during this process, it is caught and an error message is displayed. The script is designed to be run as a standalone program, with the main logic encapsulated in a `main()` function that is executed when the script is run directly.",2025-04-30T03:48:21.076138+00:00
"context/code_generator_main_example.py","This Python script serves as a demonstration for a `code_generator_main` function, presumably part of a 'pdd' package. It showcases various code generation scenarios using mocked backend functions to avoid actual LLM calls and API key dependencies, ensuring the example is self-contained and predictable. Key features demonstrated include: full code generation, incremental code updates, and a simulated cloud generation attempt that falls back to local execution. The script defines mock functions (`mock_local_code_generator_func`, `mock_incremental_code_generator_func`) that return pre-defined code snippets, simulating the behavior of real code generators. It uses `unittest.mock.patch` to replace the actual generator functions within `code_generator_main` with these mocks. The `main_example` function orchestrates several test scenarios: generating Java code from scratch, incrementally updating Python code, forcing an incremental update, and attempting C# code generation via a mocked cloud service. It handles file I/O for prompts and outputs, manages a dedicated output directory, and uses a `MockContext` to simulate `click` CLI context. Helper functions are used for printing results. The script also temporarily sets and unsets mock environment variables for the cloud scenario. Overall, it provides a comprehensive example of `code_generator_main`'s capabilities in a controlled environment.",2025-06-03T06:20:25.287671+00:00
"context/comment_line_example.py","The provided file contains documentation and example usage for the `comment_line` function from the `comment_line.py` module. This function is designed to comment out a line of code using specified comment characters, making it adaptable for different programming languages and commenting styles. The function takes two parameters: `code_line`, which is the line of code to be commented, and `comment_characters`, which determines how the line is commented. If `comment_characters` is 'del', the function returns an empty string, effectively deleting the line. If it contains a space, it is interpreted as a pair of start and end comment characters (e.g., for HTML comments). Otherwise, it is treated as a single comment character (e.g., '#' for Python). The file includes example usage for Python-style comments, HTML-style comments, and the deletion case, demonstrating the function's flexibility. Additionally, the documentation clearly explains the input and output parameters, making it easy for users to understand how to use the function in various contexts.",2025-04-30T03:48:36.233490+00:00
"context/config_example.py","The provided Python code snippet demonstrates the initialization of a configuration. It begins by importing the `init_config` function from a module located at `utils.config`. Immediately following the import, this `init_config()` function is called. A comment, # Initialize configuration FIRST, emphasizes that this configuration setup is intended to be the very first operation performed in the script or module where this code resides. This suggests that the rest of the program's execution likely depends on the settings or state established by this initialization process.",2025-06-03T06:20:34.662300+00:00
"context/conflicts_in_prompts_example.py","This Python script demonstrates the use of a function called `conflicts_in_prompts` to detect and resolve potential conflicts between two prompt specifications for a software project. The script imports necessary modules, including a custom conflict detection utility and the `rich` library for formatted output.

The `main` function defines two detailed prompt strings:
- The first prompt describes requirements for an `auth_helpers.py` module, focusing on authentication helper functions for Firebase Cloud Functions, including token verification, user info extraction, permission checks, error handling, and security best practices.
- The second prompt outlines the design of a `User` data model class for the PDD Cloud platform, specifying fields, methods (such as validation, serialization, Firestore interaction, credit and permission management), dependencies, error handling, and code style guidelines.

The script sets parameters for model strength and temperature, then calls `conflicts_in_prompts` with the two prompts and these parameters. It prints the results, including the model used, total cost, and any suggested changes to resolve conflicts between the prompts. If no conflicts are found, it indicates so. The script is intended to help developers ensure consistency and compatibility between different prompt-based specifications in a project.",2025-04-30T03:48:42.306428+00:00
"context/conflicts_main_example.py","This Python script demonstrates the usage of the `conflicts_main` function, likely from a library named 'pdd'. The script begins by creating two distinct prompt files, `prompt1_LLM.prompt` and `prompt2_LLM.prompt`, in an './output/' directory. Each file contains a different instruction for an AI model: one defines an AI assistant and the other a helpful chatbot. To simulate a command-line interface context, the script defines a `MockContext` class. This class includes an `obj` attribute, which is initialized as an empty dictionary in this instance, and an `exit` method. An instance of `MockContext` is then created. The core of the script is the call to `conflicts_main`. This function is invoked with the mock context, the paths to the two prompt files, a path for an output CSV file (`./output/outputconflicts_output.csv`), and the `verbose` flag set to `True`. The `conflicts_main` function returns three values: `conflicts`, `total_cost`, and `model_name`. Finally, the script prints these returned values to the console, indicating that the detailed results of the prompt comparison are also saved in the specified CSV file. The script essentially serves as a test or example for the `conflicts_main` functionality.",2025-06-03T06:20:39.609130+00:00
"context/construct_paths_example.py","The Python script `demo_construct_paths.py` provides a concise end-to-end demonstration of the `pdd.construct_paths.construct_paths` function. It begins by creating a temporary prompt file named `Makefile_makefile.prompt` with a sample task. Next, it simulates the arguments typically supplied by a command-line interface (CLI) to `construct_paths`. These arguments include the path to the created prompt file, a `force` flag (to allow overwriting outputs), a `quiet` flag (to control terminal output), the PDD `command` being emulated (set to generate), and an empty dictionary for `command_options`. The script then calls `construct_paths` with these arguments. The core of the demonstration involves printing the three return values from this function: `input_strings` (containing the content of the prompt file), `output_file_paths` (specifying where PDD would write its results, derived from the input), and `language` (the language detected for the operation, inferred from the prompt file name). Finally, the script cleans up by removing the temporary prompt file, ensuring the directory remains unchanged. This effectively showcases how `construct_paths` processes inputs and determines output paths and language for PDD operations.",2025-06-03T06:20:56.510637+00:00
"context/context_generator_example.py","This Python script is designed to generate example code using a function called `context_generator` from the `pdd.context_generator` module. The script begins by importing necessary modules and ensuring that the `PDD_PATH` environment variable is set, raising an error if it is not. It then defines several input parameters: a simple Python function (`add`) as the code module, a prompt describing the function to be written, the programming language (Python), and two parameters (`strength` and `temperature`) that likely influence the behavior of the code generation model. The script calls the `context_generator` function with these parameters, requesting verbose output. The function returns three values: the generated example code, the total cost of the operation (possibly in terms of API usage or computation), and the name of the model used for generation. Finally, the script prints the generated code, the total cost, and the model name using the `rich` library for formatted output. The script is structured for testing or demonstration purposes, showing how to use the `context_generator` function in a controlled environment.",2025-04-30T03:49:05.316975+00:00
"context/context_generator_main_example.py","This Python script demonstrates how to use the Click library to set up a command-line context and interact with a function called `context_generator_main` from the `pdd.context_generator_main` module. The script first creates a Click context object and configures its parameters, such as 'force', 'quiet', and 'verbose', which control file overwriting, output verbosity, and message display. It also sets object attributes like 'strength' and 'temperature', which likely influence the behavior of an AI model (possibly controlling creativity and determinism). The script specifies file paths for a prompt file, a code file, and an output file. It then calls the `context_generator_main` function with these parameters, which presumably generates example code based on the provided prompt and code files, and saves the result to the output file. The function returns the generated code, the total cost (possibly in terms of API usage or computation), and the name of the AI model used. Finally, the script prints the generated code, the cost, and the model name. This script serves as an example of integrating command-line context management with AI-driven code generation workflows.",2025-04-30T03:49:12.123033+00:00
"context/continue_generation_example.py","This Python script demonstrates the use of the `continue_generation` function from the `pdd.continue_generation` module. The main purpose of the script is to continue generating text using a language model, starting from a given prompt and an initial output fragment. The script reads the input prompt from the file `context/cli_python_preprocessed.prompt` and the initial LLM output fragment from `context/llm_output_fragment.txt`. It sets specific parameters for the language model, such as `strength` (0.915) and `temperature` (0), and then calls the `continue_generation` function with these inputs. The function returns the final generated output, the total cost of the generation, and the model name used. The script prints the total cost and model name to the console, and writes the final generated output to `context/final_llm_output.py`. Error handling is included to catch and report file not found errors and other exceptions. The script is designed to be run as a standalone program, with the main logic encapsulated in the `main()` function and executed when the script is run directly.",2025-04-30T03:49:20.259323+00:00
"context/crash_main_example.py","This Python script demonstrates the use of the `crash_main` function from the `pdd.crash_main` module to automatically fix errors in a code module and its calling program. The script first ensures that an output directory exists, then creates several example files: a prompt describing the desired functionality (a factorial function that raises a ValueError for negative inputs), a buggy code module (factorial function without negative number check), a program that calls this buggy function with a negative input, and an error log capturing the resulting RecursionError. The script then sets up a mock Click context with model parameters and calls `crash_main`, passing in the paths to the created files, output locations for the fixed code, and parameters for iterative fixing (such as maximum attempts and budget). After attempting to fix the code, it prints whether the fix was successful, the number of attempts, the cost, the model used, and the contents of the fixed code and program. The script is designed to be run as a standalone demonstration of automated code repair using error logs and prompt-driven code generation.",2025-04-30T03:49:27.914328+00:00
"context/detect_change_example.py","The provided file is a Python script designed to analyze and detect changes in a set of prompt files using a function called `detect_change` from the `pdd.detect_change` module. The script utilizes the `rich` library for enhanced console output and manages file paths with the `pathlib` module. It specifies a list of prompt files located in the 'prompts' and 'context' directories, which are to be analyzed for changes based on a given description: making prompts more compact by using 'context/python_preamble.prompt'. The script sets parameters for the language model, such as 'strength' and 'temperature', which control the model's output characteristics. It then calls the `detect_change` function with these parameters and the list of prompt files. If changes are detected, the script prints out the details of each change, including the prompt name and specific instructions, as well as the total cost and the model used. If an error occurs during execution, it is caught and displayed in the console. The script is structured for easy modification and provides clear output for users analyzing prompt file changes.",2025-04-30T03:49:34.630962+00:00
"context/detect_change_main_example.py","This Python script is designed to simulate command-line interface (CLI) parameters and invoke a function called `detect_change_main` from the `pdd.detect_change_main` module. The script uses the `click` library to create a context object that mimics CLI input, specifying model parameters such as 'strength', 'temperature', 'force', and 'quiet'. It defines a list of prompt files and a change description file, writing a specific change instruction to the latter. The script ensures the output directory exists and specifies an output CSV file for results. It then calls `detect_change_main` with the context, prompt files, change file, and output path. The results, including the model name, total cost, and a list of required changes (with prompt names and instructions), are printed to the console. The script includes error handling to catch and report exceptions. Overall, this script automates the process of detecting and summarizing changes needed in prompt files, likely for use in a prompt-driven development or code generation workflow.",2025-04-30T03:49:40.844959+00:00
"context/edit_file_example.py","This Python script is designed as a test harness for the 'edit_file.py' module, specifically for the function 'run_edit_in_subprocess', which is assumed to perform file editing operations based on provided instructions. The script is intended to be run from within a 'pdd' directory, with the module located in the parent directory. It sets up the environment by adjusting the Python path and checking for the required 'PDD_PATH' environment variable, which should point to the 'pdd' directory.

The script includes helper functions to create example files and a configuration file needed for testing. The main test involves creating a dummy text file with several lines, then running the edit function with specific instructions: updating a word on line 1, replacing line 3, and adding a new line at the end. After the edit operation, the script verifies whether the changes were applied correctly by checking the file's contents and reporting the results.

The script also provides detailed error handling and verification output, making it useful for debugging and validating the integration of the file editing module. It is structured to be extensible for additional test cases, though only the main example is active in the current version. The script requires certain dependencies and environment variables to be set, as outlined in its documentation.",2025-04-30T03:49:46.855998+00:00
"context/find_section_example.py","The file provides a concise example and documentation for using the `find_section` function from the `find_section.py` module. The example demonstrates how to import the function, prepare a sample input string containing multiple code blocks in different programming languages (Python and JavaScript), and process this string by splitting it into lines. The `find_section` function is then called to extract code sections, and the results are printed, showing the language and the start and end line indices for each code block found. The documentation details the input parameters for the function: a list of lines (`lines`), an optional starting index (`start_index`), and an optional sub-section flag (`sub_section`). The output is described as a list of tuples, each containing the code language, start line, and end line indices. An example output is provided, illustrating how the function identifies and locates code blocks within the input text. Overall, the file serves as both a usage guide and reference for the `find_section` function, clarifying its purpose, inputs, outputs, and practical application.",2025-04-30T03:50:01.331727+00:00
"context/firecrawl_example.py","This Python script demonstrates how to use the `firecrawl-py` library to perform web scraping. It begins by importing the `FirecrawlApp` class from the `firecrawl` library and the `os` module. An instance of `FirecrawlApp` is created, using an API key retrieved either from the environment variable `FIRECRAWL_API_KEY` or a placeholder string 'YOUR_API_KEY' if the environment variable is not set. The script then calls the `scrape_url` method on the `app` object to scrape the content of 'https://www.google.com'. It specifically requests the scraped content to be returned in markdown format by passing `formats=['markdown']` as an argument. Finally, the script prints the markdown content obtained from the `scrape_result` object to the console. The initial comment indicates that the `firecrawl-py` library can be installed using pip.",2025-06-03T06:21:13.182178+00:00
"context/fix_code_loop_example.py","The provided Python script demonstrates the use of a function called `fix_code_loop` from the `pdd.fix_code_loop` module. The script is structured around a `main()` function, which performs several tasks to showcase how `fix_code_loop` can be used to automatically fix code errors.

First, the script creates two example files in an 'output' directory: a Python module (`module_to_test.py`) containing a function `calculate_average`, and a verification script (`verify_code.py`) that imports this function and intentionally calls it with incorrect input (a string instead of a list of numbers), which is expected to cause an error. The original prompt that led to the code's creation is also defined.

The core of the script is the invocation of `fix_code_loop`, which attempts to iteratively fix the code in `module_to_test.py` by running the verification script, analyzing errors, and making corrections. The function is called with parameters controlling the model's strength, determinism, number of attempts, and budget. After running, the script prints whether the fix succeeded, the number of attempts, the cost, the model used, and the final version of the code. There is also commented-out code for cleaning up the generated files. The script is intended to be run as a standalone program.",2025-04-30T03:50:15.536132+00:00
"context/fix_code_module_errors_example.py","This Python script demonstrates the use of an automated code error-fixing module, specifically 'fix_code_module_errors' from the 'pdd.fix_code_module_errors' package. The script defines a main function that sets up a scenario where a function intended to sum a list of numbers is incorrectly called with a string input, leading to a TypeError. The script provides the original code, the prompt that generated it, the error message encountered, and then calls the 'fix_code_module_errors' function with these inputs. The function is expected to analyze the error, suggest or apply fixes, and return several outputs: whether the program or code module needs updating, the fixed versions of the program and code, the raw output from the code-fixing model, the total cost of the API call, and the model's name. The script then prints these results. This setup is useful for testing or demonstrating automated code repair tools, especially those leveraging language models to interpret error messages and suggest corrections.",2025-04-30T03:50:23.190197+00:00
"context/fix_error_loop_example.py","The provided file is a Python script that demonstrates the usage of the `fix_error_loop` function, which is imported from the `pdd.fix_error_loop` module. The script is designed to automate the process of fixing errors in code by iteratively running a loop that attempts to correct issues based on a set of parameters. The main function sets up the necessary input parameters, such as the paths to the unit test file, code file, prompt file, and verification program. It also specifies parameters for the error-fixing process, including strength, temperature, maximum attempts, and budget. The script reads a prompt from a file, then calls the `fix_error_loop` function with all these parameters. The results, including whether the process was successful, the number of attempts, total cost, the model used, and the final versions of the unit test and code, are displayed using the `rich` library for enhanced terminal output. If an exception occurs during execution, it is caught and displayed. The script is intended to be run as a standalone program and serves as an example of how to use the `fix_error_loop` function for automated code error correction.",2025-04-30T03:50:29.598551+00:00
"context/fix_errors_from_unit_tests_example.py","The provided Python script demonstrates the usage of the function `fix_errors_from_unit_tests` from the `pdd.fix_errors_from_unit_tests` module. The script sets up example inputs, including a unit test for an `add` function, the implementation of the `add` function itself, a prompt describing the function's purpose, and an error message resulting from a failed assertion and a deprecation warning. Additional parameters such as the error log file name, LLM strength, and temperature are also specified. The main function calls `fix_errors_from_unit_tests` with these inputs and receives several outputs: updated unit test and code, fixed versions of both, analysis results, total cost, and the model name used. The results are then printed using the `rich` library for enhanced formatting. The script is designed to be run as a standalone program, serving as an example or test harness for the error-fixing functionality provided by the imported module.",2025-04-30T03:50:38.634595+00:00
"context/fix_main_example.py","This Python script defines a command-line interface (CLI) tool using the Click library, designed to automate the process of fixing errors in code and unit tests. The script imports necessary modules, including 'rich' for colored output and a custom 'fix_main' function from the 'pdd.fix_main' module. The main command, 'fix_command', is decorated with several Click options that allow users to specify file paths for prompts, code, unit tests, error logs, and output files. Additional options control the fixing process, such as enabling iterative fixing, setting a maximum number of attempts, budget, and auto-submission if tests pass. When executed, the script sets up a Click context with default parameters and simulates command-line arguments for demonstration. The 'fix_command' function calls 'fix_main' with the provided parameters, and based on the result, prints a success or failure message using rich formatting. The script is intended to be run as a standalone program and serves as a wrapper to facilitate automated code and test correction workflows, likely in a development or continuous integration context.",2025-04-30T03:50:46.451160+00:00
"context/fix_verification_errors_example.py","This file is an example Python script that demonstrates how to use the `fix_verification_errors` function from the `pdd` package to automatically identify and fix errors in a code module based on verification output logs. The script assumes that the `pdd` package and its dependencies (such as prompt templates and LLM API keys) are properly installed and configured in the environment.

The script sets up an example scenario where a buggy Python module (`calculator_module.py`) is supposed to implement an `add_numbers` function, but contains a bug (it subtracts instead of adds). A separate program script is provided that uses this module, takes command-line arguments, and includes verification logic to check if the module's output matches the expected result. The output from running this program (which indicates a verification failure) is captured.

The script then calls `fix_verification_errors`, passing in the program code, the original prompt, the buggy code, the verification output, and some LLM configuration parameters (strength, temperature, verbosity). The results, including whether issues were found, if the code or program was updated, the LLM model used, cost, and explanations, are printed in a formatted way. This example illustrates an automated workflow for debugging and repairing code using LLMs and verification feedback.",2025-04-30T03:50:55.579326+00:00
"context/fix_verification_errors_loop_example.py","This Python script demonstrates the usage of the `fix_verification_errors_loop` function, designed to automatically debug code. It sets up a test scenario by creating a 'program.py' file that uses a 'calculator_module.py'. The 'calculator_module.py' is intentionally written with a bug: its `add_numbers` function performs subtraction instead of addition. The script defines various inputs for the `fix_verification_errors_loop` function, including paths to these files, the original code generation prompt, a secondary verification script, LLM control parameters (strength, temperature), resource limits (max attempts, budget), logging options, and program arguments. After preparing these inputs, the script executes `fix_verification_errors_loop`, which attempts to identify and correct the bug in 'calculator_module.py' by iteratively running the 'program.py' and analyzing its output. Finally, the script prints the results of the debugging process, such as whether the fix was successful, the final corrected code, the number of attempts made, and the associated cost.",2025-06-03T06:21:18.475893+00:00
"context/fix_verification_main_example.py","This Python script demonstrates the usage of a `fix_verification_main` function, presumably from a package named `pdd`, designed for automated code verification and fixing using a Language Model (LLM). The script sets up an example scenario by creating several files: a prompt describing a simple Python calculator function, an intentionally buggy version of this calculator code, a program to execute the calculator code, and a verification program to check the correctness of the calculator's output. It then showcases two modes of operation for `fix_verification_main`: a single-pass mode where the LLM attempts to fix the code based on the prompt and program output, and an iterative loop mode where the LLM repeatedly tries to fix the code, using the verification program to confirm success, until a solution is found, maximum attempts are reached, or a budget is exceeded. The script simulates Click CLI context, defines default parameters, runs both modes with the buggy code, and prints detailed results including success status, attempts made, cost, model used, and paths to the generated output files. The `rich` library is used for enhanced console output.",2025-06-03T06:21:29.455676+00:00
"context/gemini_may_pro_example.py","The provided Python code snippet demonstrates how to use the `litellm` library to interact with a large language model, specifically Google's Vertex AI Gemini 1.5 Pro model (identified as `vertex_ai/gemini-1.5-pro-preview-0514`). The code initializes a `completion` call to this model. It constructs a `messages` list, which currently contains a single message dictionary with the role set to user and a placeholder content Your prompt here. This structure is typical for conversational AI interactions, where the user's input is passed to the model. After sending the request to the model, the script captures the `response` returned by the `completion` function. Finally, it prints this `response` to the console. In essence, the script is a basic example of sending a prompt to the Gemini 1.5 Pro model via `litellm` and displaying the generated output. It highlights the ease of use of `litellm` for accessing various LLMs through a unified interface. The user would replace Your prompt here with their actual query to get a meaningful response from the model.",2025-06-03T06:21:37.348902+00:00
"context/generate_output_paths_example.py","This file is a Python script that demonstrates and tests the usage of a function called `generate_output_paths`, which is imported from a module located in a parent directory. The script is structured to handle various scenarios for generating output file paths based on user input, environment variables, and default behaviors. It covers the following cases:

1. Default behavior when no user input or environment variables are provided, resulting in a default filename in the current working directory.
2. User-specified output filename, which is resolved to an absolute path.
3. User-specified output directory, where the default filename is placed inside the given directory.
4. Use of environment variables to specify output paths for the 'fix' command, including both file and directory cases.
5. Mixed user input and defaults, where some outputs are specified by the user and others use defaults.
6. A command ('preprocess') with a fixed output file extension, ignoring the provided extension argument.
7. Handling of an unknown command, which results in an empty output dictionary.
8. The case where the required basename is missing, also resulting in an empty output.

The script prints the inputs, results, and expected outputs for each scenario, and includes setup and cleanup code for directories and environment variables used in the tests. It serves as a comprehensive example and test suite for the `generate_output_paths` function.",2025-04-30T03:51:28.530833+00:00
"context/generate_test_example.py","This Python script demonstrates how to use the `generate_test` function from the `pdd.generate_test` module to automatically generate unit tests for a given code snippet. The script sets up a prompt asking for a function that calculates the factorial of a number and provides a sample implementation of such a function in Python. It also specifies parameters such as `strength`, `temperature`, and `language` to control the behavior of the test generation. The script attempts to call `generate_test` with these inputs and, if successful, prints the generated unit test, the total cost of the operation, and the model name used, all with formatted output using the `rich` library for enhanced terminal display. If an error occurs during the process, it catches the exception and prints an error message in red. The script includes a commented-out line for setting the `PDD_PATH` environment variable, which may be necessary for locating project resources. Overall, the script serves as an example or template for automating unit test generation for Python code using an external tool.",2025-04-30T03:51:36.963786+00:00
"context/get_comment_example.py","The file provides a concise example and documentation for using the `get_comment` function from the `get_comment.py` module. The example demonstrates how to import and call `get_comment` with various programming language names (such as 'Python', 'Java', and 'JavaScript'), showing that the function returns the appropriate comment character(s) for each language (e.g., '#' for Python, '//' for Java). If an unknown language is provided, or if there is an error (such as a missing environment variable or CSV file), the function returns the string 'del'.

The documentation specifies that the function takes a single string parameter, `language`, which is case-insensitive, and returns the comment character(s) as a string. It also notes the importance of setting the `PDD_PATH` environment variable to the directory containing the `data/language_format.csv` file, which must have columns for `language` and `comment`. The file serves as both a usage guide and a reference for the expected behavior and requirements of the `get_comment` function.",2025-04-30T03:51:47.777797+00:00
"context/get_extension_example.py","The file demonstrates the usage of a function called 'get_extension', which is imported from the 'pdd.get_extension' module. The purpose of this function appears to be returning the appropriate file extension for a given programming language or file type. The file includes three example calls to 'get_extension': one with the argument 'Python', which returns '.py'; one with 'Makefile', which returns 'Makefile' (indicating that Makefiles do not have an extension); and one with 'JavaScript', which returns '.js'. The examples are printed to the console, showing the expected output for each input. The file serves as a simple usage demonstration or test for the 'get_extension' function, illustrating how it maps language or file names to their standard file extensions.",2025-04-30T03:51:57.519245+00:00
"context/get_jwt_token_example.py","This Python script demonstrates an authentication process using Firebase with GitHub Device Flow. It utilizes the `get_jwt_token` function from the `pdd.get_jwt_token` module. The script begins by defining constants for Firebase API key, GitHub client ID, and an application name, retrieving sensitive keys from environment variables. The core logic resides in the asynchronous `main` function, which attempts to obtain a Firebase ID token. It includes comprehensive error handling for various scenarios such as authentication failures (`AuthError`, `UserCancelledError`), network issues (`NetworkError`), token-related problems (`TokenError`), and rate limiting (`RateLimitError`). Upon successful authentication, the script prints the Firebase ID token. A key feature is its ability to update a `.env` file: it reads the existing `.env` file, replaces or adds a line for `JWT_TOKEN` with the newly acquired token, and writes the changes back. The script is executed asynchronously using `asyncio.run(main())` when run directly.",2025-06-03T06:21:42.993431+00:00
"context/get_language_example.py","The Python script imports a function `get_language` from a module named `pdd.get_language`. It defines a `main` function that serves as the primary execution block. Inside `main`, a file extension, '.py', is set as an example. The script then calls `get_language` with this extension to determine the corresponding programming language. Based on the return value, it prints either the identified language or a message indicating that no language was found for the extension. The script includes a try-except block to handle potential errors during this process, printing an error message if an exception occurs. The standard `if __name__ == __main__:` construct ensures that the `main` function is called only when the script is executed directly. The overall purpose of the script is to demonstrate how to use the `get_language` function to identify a programming language from a file extension and to handle the possible outcomes, including errors.",2025-06-03T06:21:49.338609+00:00
"context/git_update_example.py","This Python script demonstrates the usage of a function called `git_update`, which appears to update code and prompts based on input parameters, likely leveraging a language model (LLM). The script begins by importing necessary modules and setting up parameters such as the prompt file name, the path to the code file to be modified, and LLM parameters like `strength` and `temperature`. It reads an input prompt from a file, then calls the `git_update` function with these parameters. If the update is successful, it prints the modified prompt, the total cost of the operation (suggesting usage of a paid LLM API), and the model name. The modified prompt is then saved back to the original prompt file. The script includes error handling for both value errors and general exceptions, providing informative messages in case of failure. The main function is executed when the script is run directly. Overall, the script automates the process of updating code and prompts using an LLM, with integration for file I/O and cost tracking.",2025-04-30T03:52:20.786150+00:00
"context/increase_tests_example.py","The provided file is a Python script that demonstrates how to use the `increase_tests` function from the `pdd.increase_tests` module, likely part of a test generation or code coverage improvement toolkit. The script defines an `example_usage` function, which showcases both basic and advanced usage of `increase_tests`. It sets up a simple example: a function to calculate the average of a list of numbers, a corresponding unit test, and a code coverage report indicating 60% coverage. The script then calls `increase_tests` twice: first with default parameters to generate additional tests, and second with more advanced, customized parameters (such as specifying the language, strength, temperature, and verbosity). The results, including the generated tests, total cost, and model used, are printed to the console. The script also includes error handling for input validation and unexpected exceptions. Finally, the script runs the `example_usage` function if executed as the main module. Overall, the file serves as a usage example and template for users looking to enhance their test coverage using the `increase_tests` function.",2025-04-30T03:52:27.508789+00:00
"context/incremental_code_generator_example.py","This Python script demonstrates the usage of the `incremental_code_generator` function from the `pdd` package. It showcases how to perform an incremental code update by providing an original prompt, an updated prompt, previously generated code, and the target programming language (Python in this example). The script defines example inputs, such as an initial request to write a factorial function and an updated request to add input validation. It then calls `incremental_code_generator` with these inputs and various configuration parameters like `strength`, `temperature`, and `verbose`. The core logic of the `incremental_code_generator` decides whether to apply a minimal patch to the existing code or to recommend a full regeneration if changes are too significant. The script uses the `rich` library to print the results, indicating if an incremental update was successful and displaying the updated code, or advising full regeneration. It also outputs the total cost of the operation and the language model used. The `main` function orchestrates this demonstration, providing a practical example for users of the `pdd` library's incremental code generation capabilities.",2025-06-03T06:22:01.531690+00:00
"context/insert_includes_example.py","This Python script demonstrates the usage of the `insert_includes` function from the `pdd.insert_includes` module to process project dependencies. The script is structured around an `example_usage` function, which sets up an input prompt describing a task (reading a CSV, processing data, and outputting results), specifies a directory path for relevant files, and names a CSV file containing dependency information. It then calls `insert_includes` with these parameters, including options for output strength and temperature, and enables verbose output. The results from `insert_includes`including a modified prompt, CSV output, model name, and total costare displayed using the `rich` library for enhanced console output. The script also saves the generated CSV output to a file. Error handling is implemented to catch missing files or other exceptions, providing user-friendly error messages. The script is designed to be run as a standalone program, with the main block calling `example_usage`. Overall, the file serves as an example of integrating dependency processing into a Python workflow, with clear input/output handling and robust error management.",2025-04-30T03:52:33.916523+00:00
"context/install_completion_example.py","This Python script serves as an example for installing shell completion for the PDD CLI tool using the pdd package. It demonstrates the use of two main functions: get_local_pdd_path(), which returns the absolute path to the PDD_PATH directory, and install_completion(), which sets up shell completion for the user's shell environment. The script is designed to be safe for demonstration purposes by creating dummy directories (example_home for the home directory and example_pdd_path for the PDD_PATH) within the current working directory, ensuring that no real user configuration files are affected.

The setup_example_environment() function configures environment variables (SHELL, HOME, PDD_PATH) to point to these dummy directories, creates a dummy shell RC file (like .bashrc), and a dummy completion script file (pdd_completion.sh). The main() function orchestrates the setup, displays the determined PDD_PATH, and invokes the installation of shell completion, with output provided via the Rich library for enhanced console formatting. The script is intended to be run directly and provides clear instructions and feedback throughout the process, making it a safe and informative example for users wishing to understand or test the PDD shell completion installation process.",2025-04-30T03:52:41.474044+00:00
"context/langchain_lcel_example.py","This Python script demonstrates various functionalities of the Langchain library for interacting with Large Language Models (LLMs). It begins by importing necessary modules, including prompt templates, LLM wrappers for multiple providers (OpenAI, Google, Anthropic, Fireworks, Groq, Together, DeepSeek, AWS Bedrock), local model interfaces (Ollama, MLX), output parsers (String, JSON, Pydantic), and caching utilities. A custom `CompletionStatusHandler` class is defined to track LLM call metrics like completion status, finish reason, and token usage. The script showcases setting up an SQLite cache for LLM responses. It then proceeds to illustrate creating and using different LLMs, including `ChatGoogleGenerativeAI`, `ChatVertexAI`, `ChatOpenAI` (with various models like GPT-4o-mini and DeepSeek), `AzureChatOpenAI`, `ChatAnthropic`, `ChatGroq`, `Together`, `OllamaLLM`, `MLXPipeline`, and `ChatBedrockConverse`. The script demonstrates Langchain Expression Language (LCEL) for chaining prompts, LLMs, and output parsers. It covers both simple string outputs and structured JSON outputs using Pydantic models and `JsonOutputParser`. Examples include generating jokes, answering questions, and explaining concepts. Advanced features like LLM fallbacks and configurable alternatives are also shown. The script emphasizes current Langchain best practices, such as using `invoke()` instead of `run()` for chain execution.",2025-06-03T06:22:09.891804+00:00
"context/llm_invoke_example.py","This Python script demonstrates the usage of an `llm_invoke` function, likely designed to interact with large language models. It defines a Pydantic model named `Joke` for structured output, specifying `setup` and `punchline` fields. The `main` function showcases two primary examples. First, it generates an unstructured joke about programmers, iterating through a 'strength' parameter from 0.0 to 0.5. During this process, it tracks which language model is used for different strength ranges and prints the joke, associated cost, and the model's name. Second, it attempts to generate a structured joke about data scientists, providing a specific JSON format and using the `Joke` Pydantic model for validation. This part also iterates through strength values, prints the structured result, cost, model name, and then accesses the individual joke components. The script concludes by printing a summary of the strength ranges for which each language model was utilized. The core purpose appears to be testing `llm_invoke`'s behavior with varying strength settings for both free-form and schema-constrained outputs, while also monitoring model selection and cost.",2025-06-03T06:22:22.246386+00:00
"context/llm_selector_example.py","The provided Python script demonstrates the usage of a function called `llm_selector` imported from the `pdd.llm_selector` module. The main function initializes two parameters, `strength` and `temperature`, which are likely used to configure the selection of a large language model (LLM). The script enters a loop, incrementing the `strength` parameter from 0.5 up to 1.1 in steps of 0.05. In each iteration, it calls `llm_selector` with the current `strength` and `temperature` values, and receives several outputs: the selected LLM object, a token counting function, input and output costs per million tokens, and the model's name. The script prints out these details for each model selection. It also demonstrates the use of the token counter by counting the tokens in a sample text. The script includes error handling for `FileNotFoundError` and `ValueError`, printing error messages if these exceptions occur. The main function is executed when the script is run directly. Overall, the script serves as a test or demonstration for selecting and evaluating different LLM models based on configurable parameters.",2025-04-30T03:53:11.010798+00:00
"context/load_prompt_template_example.py","The provided file is a Python script that demonstrates how to load and display a prompt template using a custom function and the 'rich' library for formatted output. The script imports the 'load_prompt_template' function from the 'pdd.load_prompt_template' module and the 'print' function from the 'rich' library. In the main function, it specifies a prompt template name ('generate_test_LLM'), loads the corresponding template using the imported function, and, if successful, prints a message indicating the template was loaded, followed by the template's contents. The script is designed to execute the main function when run as the main module. Overall, the script serves as a simple utility for loading and visually displaying prompt templates, likely for use in language model or prompt engineering workflows.",2025-04-30T03:53:17.709776+00:00
"context/postprocess_0_example.py","The provided file is an example script demonstrating how to use the `postprocess_0` function from the `pdd.postprocess_0` module. The script is designed to process the output from a language model (LLM) that may contain multiple code blocks in various programming languages, interspersed with text. The example shows how to extract and focus on code sections written in a specified language (e.g., Python), using the `postprocess_0` function. The function takes two parameters: `llm_output`, a string containing the LLM's output, and `language`, a string specifying the target programming language. The output of the function is a string where only the largest code section in the specified language remains uncommented, while all other code and text are commented out using the appropriate comment syntax for that language. The file also includes documentation for the input and output parameters, as well as notes indicating that helper functions like `get_comment`, `comment_line`, and `find_section` must be implemented and available for the example to work. This script serves as both a usage guide and a reference for integrating the `postprocess_0` function into other projects.",2025-04-30T03:53:23.853509+00:00
"context/postprocess_example.py","This Python script serves as an example demonstrating the `postprocess` function from the `pdd.postprocess` module, which is designed to extract code from text, such as the output from a Large Language Model (LLM). The script illustrates two distinct code extraction scenarios. The first is a 'simple extraction' (when `strength=0`), which uses basic string manipulation to quickly find code blocks enclosed in triple backticks without any cost. The second is an 'advanced extraction' (when `strength > 0`), which leverages an LLM for more robust and accurate code identification, though this method incurs a cost and requires more time.

To enable the demonstration of the advanced LLM-based extraction without requiring actual LLM API calls or prompt files, the script employs Python's `unittest.mock` library to simulate the behavior of internal dependencies like `load_prompt_template` and `llm_invoke`. The `main` function orchestrates the example by defining a sample LLM output containing code, setting a target programming language, and then running both extraction scenarios. For each scenario, it prints the input parameters, the extracted code, the associated cost, and the model identifier. The script also includes verification steps for the mocked calls in the advanced scenario and uses the `rich` library for formatted console output. Finally, it provides notes explaining the mocking strategy and the real-world requirements for LLM-based extraction.",2025-06-03T06:22:30.036423+00:00
"context/preprocess_example.py","The provided file is a Python script that demonstrates the use of a preprocessing function, likely for handling and formatting prompts. The script imports a 'preprocess' function from a module named 'pdd.preprocess' and uses the 'rich' library's Console for styled terminal output. A multi-line string named 'prompt' is defined, containing a mix of custom XML-like tags (<shell>, <pdd>, <web>), placeholders ({test}, {test2}), and a code block. The script sets two configuration flags: 'recursive' (set to False) and 'double_curly_brackets' (set to True), and specifies an 'exclude_keys' list containing 'test2', indicating that this key should be excluded from certain processing steps (such as doubling curly brackets). Debug information about the excluded keys is printed to the console. The 'preprocess' function is then called with the prompt and configuration options, and the processed result is printed. The script appears to be for testing or demonstrating how the 'preprocess' function handles prompt formatting, placeholder substitution, and exclusion logic, with a focus on debugging and output visualization.",2025-04-30T03:53:40.903078+00:00
"context/preprocess_main_example.py","This file defines a command-line interface (CLI) tool using the Click library in Python. The tool is designed to preprocess prompt files, likely for use in natural language processing or AI applications. The CLI accepts several options: the path to the prompt file (required), an optional output path, a flag to insert XML delimiters, a recursive flag to process referenced prompt files, a flag to double curly brackets, a list of keys to exclude from doubling, and a verbose flag for detailed logging. The main function, 'cli', prepares a context object with some default parameters (strength, temperature, verbose), then calls 'preprocess_main' from the 'pdd.preprocess_main' module, passing along the user-specified options. After processing, it outputs the processed prompt, the total cost, and the model name. Error handling is included to catch and report exceptions during preprocessing. The script is intended to be run as a standalone program. Overall, the file provides a flexible and user-friendly interface for preprocessing prompt files with various customization options.",2025-04-30T03:53:47.566047+00:00
"context/process_csv_change_example.py","The Python script `run_demo.py` serves as a concise example demonstrating the usage of the `process_csv_change` function from the `pdd.process_csv_change` module. It begins by setting up a small workspace on disk. This setup involves creating a directory structure and populating it with necessary files: a Python source code file (`factorial.py`) containing a simple factorial function, a corresponding prompt file (`factorial_python.prompt`) with an initial instruction to write a test for this function, and a CSV file (`tasks.csv`). The CSV file contains instructions to modify the prompt file, specifically to add an example section. The core of the script is the call to `process_csv_change`, invoked with parameters like the CSV file path, code directory, and control settings. Finally, the script prints the returned success status, cost, model name, and a list of JSONs detailing the modifications. The example output shows the successful addition of an example test section to the prompt file, illustrating the function's capability to automate changes based on CSV input.",2025-06-03T06:22:45.441550+00:00
"context/pytest_example.py","This Python script is designed to run pytest tests programmatically and collect detailed results, including failures, errors, warnings, and logs. It defines a class, TestResultCollector, which acts as a custom pytest plugin. This class tracks the number of test failures, errors, and warnings by implementing pytest hook methods such as pytest_runtest_logreport and pytest_sessionfinish. It also captures all standard output and error logs during the test run using an in-memory StringIO buffer.

The run_pytest function initializes the collector, redirects output streams, and runs pytest on a specific test file (tests/test_get_extension.py) with verbose output. After the test run, it resets the output streams and retrieves the captured logs. The function returns the counts of failures, errors, warnings, and the logs themselves.

When executed as a script, the main block prints the current working directory, runs the tests, and then prints the collected results and logs. The script includes several debug print statements to trace its execution flow. Overall, the script provides a way to programmatically run pytest, collect detailed test results, and capture all output for further analysis or reporting.",2025-04-30T03:54:02.853935+00:00
"context/pytest_output_example.py","This Python script demonstrates the usage of a module named `pytest_output` (imported as `pdd.pytest_output`) for running and capturing the results of pytest test files. The script uses several libraries, including argparse, json, pytest, and rich for pretty printing. It defines a helper function to create dummy test files and a main function (`main_example`) that showcases various use cases of the `pytest_output` module.

The main function creates several test files with different contents: a standard test file with passing, failing, error, and warning tests; a non-existent file; a non-Python file; and an empty test file. For each scenario, it demonstrates how to:
- Run pytest on the file and capture the output using `run_pytest_and_capture_output`.
- Save the output to a JSON file with `save_output_to_json`.
- Handle errors gracefully when files are missing or not Python files.
- Use the `TestResultCollector` class directly for advanced log and result collection, including capturing stdout, stderr, failures, errors, warnings, and passed tests.

The script is intended as an example or test harness for the `pytest_output` module, illustrating its features and error handling capabilities in various scenarios.",2025-04-30T03:54:11.904084+00:00
"context/split_example.py","The provided Python script demonstrates the usage of the `split` function from the `pdd.split` module. It begins by importing necessary modules, including `os` for environment management and `rich.console` for enhanced console output. The script defines a `main` function, which sets up the environment by configuring the `PDD_PATH` variable. It then prepares input parameters: a prompt asking for a Python factorial function, the corresponding code, an example usage snippet, and two float parameters (`strength` and `temperature`). The `split` function is called with these inputs, and it returns a tuple containing a sub-prompt and a modified prompt, as well as the total cost and the model name used. The results are printed to the console using Rich's formatting capabilities. The script includes error handling to catch and display any exceptions that occur during execution. The main function is executed when the script is run directly. Overall, the script serves as a demonstration or test harness for the `split` function, showcasing how to prepare inputs, call the function, and display its outputs in a user-friendly manner.",2025-04-30T03:54:21.123836+00:00
"context/split_main_example.py","The provided Python script, `example_split_usage.py`, demonstrates how to create a command-line interface (CLI) using the `click` library to interact with a function called `split_main` from the `pdd.split_main` module. The script defines a CLI command, `split-cli`, which accepts several options: paths to an input prompt file, a generated code file, an example usage code file, and optional output files for a sub-prompt and a modified prompt. Additional flags allow the user to force overwriting files and suppress console output. The CLI command sets up a context object with parameters like `strength` and `temperature` to control the behavior of `split_main`. When executed, the command calls `split_main` with the provided arguments, processes the results, and displays or saves the sub-prompt and modified prompt contents, along with information about the model used and the operation's cost. The script also includes error handling to report issues during execution. At the end, the script registers the `split-cli` command with a top-level CLI group and runs the CLI if the script is executed as the main module. This script serves as an example of integrating a prompt/code splitting function into a user-friendly CLI tool.",2025-04-30T03:54:27.265533+00:00
"context/summarize_directory_example.py","The provided file is a Python script that demonstrates how to use the 'summarize_directory' function from the 'pdd.summarize_directory' module. The script is structured around a 'main' function, which serves as an example of summarizing Python files within a specified directory. The process begins by defining a sample CSV string containing existing file summaries. The 'summarize_directory' function is then called with parameters such as the directory path (using a wildcard to match files), strength, temperature, verbosity, and the existing CSV content. The function returns a new CSV output, the total cost of the operation, and the model name used for summarization. The script prints the generated CSV content, the total cost, and the model name to the console. It also ensures that an 'output' directory exists, and writes the new CSV output to a file named 'output.csv' within this directory. Exception handling is included to catch and print any errors that occur during execution. The script is intended to be run as a standalone program, as indicated by the '__main__' guard at the end.",2025-04-30T03:54:33.511865+00:00
"context/tiktoken_example.py","The file contains a short Python script that demonstrates how to use the 'tiktoken' library to count the number of tokens in a given text. The script first imports the 'tiktoken' module, then retrieves a specific encoding scheme called 'cl100k_base' using the 'get_encoding' function. It then encodes a variable named 'preprocessed_prompt' using this encoding, and calculates the number of tokens by taking the length of the resulting encoded list. This token count is stored in the variable 'token_count'. The script is likely intended for use cases where it is important to know the number of tokens in a text, such as preparing prompts for language models that have token limits. The code assumes that 'preprocessed_prompt' is already defined elsewhere in the program.",2025-04-30T03:54:39.473311+00:00
"context/trace_example.py","The provided file is a Python script that demonstrates the use of the `trace` function from the `pdd.trace` module, likely related to program debugging or code analysis. The script imports necessary modules, including `os`, `trace` from `pdd.trace`, and `Console` from the `rich` library for formatted output. It defines a `main` function that sets up example code and prompt strings, specifies a line number to trace, and sets parameters for model strength and temperature, which are typical in language model (LLM) configurations. The script then calls the `trace` function with these parameters, aiming to map a line in the code to a corresponding line in the prompt, and retrieves the total cost and model name used. Results are printed to the console in a formatted manner. The script includes error handling for file not found, value errors, and general exceptions, providing user-friendly error messages. The main function is executed when the script is run directly. Overall, the script serves as an example or test harness for the `trace` function, showcasing its usage and output handling.",2025-04-30T03:54:46.463917+00:00
"context/trace_main_example.py","This file is a Python script that demonstrates the use of a tracing tool, likely for analyzing code with the help of a language model (LLM). The script first creates two example files: a prompt file (calculator.prompt) containing instructions for a simple calculator function, and a Python code file (calculator.py) that implements an addition function and prints the result of adding two numbers. The script then sets up a Click context with various configuration options, such as verbosity, whether to overwrite files, the strength of LLM analysis, and the randomness (temperature) of the LLM. It then calls the `trace_main` function from the `pdd.trace_main` module, passing in the context, file paths, the line number of the function to trace, and an output file for the results. The script prints out the line number in the prompt file corresponding to the traced code, the cost of the analysis, and the name of the LLM model used. If any errors occur during execution, they are caught and printed. Overall, the script serves as an example of how to set up and use a code tracing and analysis workflow with LLM support.",2025-04-30T03:54:56.972930+00:00
"context/track_cost_example.py","This Python script defines a command-line interface (CLI) for a tool called PDD, which is designed to process prompts and generate outputs with cost tracking. The script uses the Click library to handle command-line arguments and commands, and the Rich library for enhanced console output. The main CLI group accepts an optional '--output-cost' argument to enable cost tracking and output usage details to a CSV file.

The primary command implemented is 'generate', which requires a '--prompt-file' argument specifying the path to an input file containing a prompt. An optional '--output' argument allows the user to specify an output file for the generated result; otherwise, the output is printed to the console. The 'generate' function is decorated with a 'track_cost' decorator (imported from 'pdd.track_cost'), which likely handles the cost tracking functionality. The function reads the prompt from the input file, simulates processing it, and generates a placeholder output. It also simulates returning the cost of execution and the model name used (hardcoded as 0.05 dollars per million tokens and 'gpt-4', respectively). The script includes a main block that demonstrates how to invoke the CLI with example arguments.",2025-04-30T03:55:06.205668+00:00
"context/unfinished_prompt_example.py","This Python script serves as an example of how to use the `unfinished_prompt` function from the `pdd.unfinished_prompt` module. It begins by outlining the prerequisites for running the example, which include having the `pdd` package accessible in the Python environment, ensuring a prompt template file named unfinished_prompt_LLM is available, and configuring the `pdd.llm_invoke` function for LLM access (e.g., setting API keys). The script then demonstrates the usage of `unfinished_prompt` by defining an intentionally incomplete prompt about baking sourdough bread. It calls the function with this prompt, specifying custom `strength`, `temperature`, and `verbose` parameters. The `unfinished_prompt` function is expected to return a tuple containing the LLM's reasoning for its completeness assessment, a boolean flag indicating if the prompt is considered complete, the cost of the LLM call, and the name of the LLM model used. The script uses the `rich` library to print these results in a formatted manner. Additionally, a commented-out section shows how to call `unfinished_prompt` with default parameters using a different example prompt. The overall purpose is to guide users on integrating and utilizing the `unfinished_prompt` functionality for analyzing prompt completeness.",2025-06-03T06:23:04.546766+00:00
"context/update_main_example.py","This Python script demonstrates how to create a command-line interface (CLI) using the Click library, specifically for updating a prompt based on modified code. The script defines an 'update' command that accepts several options, such as paths to the original prompt file, modified code file, and optionally the original code file. It also supports using Git history to retrieve the original code, and includes options for output file path, strength and temperature parameters for the update process, and flags for verbosity, quiet mode, and force-overwriting files.

The core functionality is provided by the 'update_main' function, which is imported from another module. The script collects all user-specified options, stores some in the Click context, and passes them to 'update_main'. After execution, it displays the results, including a snippet of the updated prompt, the total cost, and the model name, unless quiet mode is enabled. The script is structured as a CLI group, allowing for future expansion with additional commands. It serves as an example of integrating Click for CLI development, handling file and Git inputs, and managing user feedback and logging.",2025-04-30T03:55:21.769753+00:00
"context/update_model_costs_example.py","The Python script `example_update_model_costs.py` serves as an end-to-end demonstration for the `update_model_costs.py` utility within the `pdd` package. It illustrates how to create or specify an `llm_model.csv` file, which stores details about various language models, including their provider, name, and cost metrics. The script demonstrates invoking the `pdd.update_model_costs.update_model_data` function both directly from Python and via its command-line interface (CLI). It outlines the expected CSV schema, which includes fields like provider, model, input/output costs (per million tokens), and other model-specific parameters. The example specifically shows how the script can automatically populate missing cost and `structured_output` fields. The script first creates a sample CSV with two models (OpenAI's gpt-4o-mini and Anthropic's claude-3-haiku), then calls the updater function to modify this CSV in place, and finally displays the CSV contents before and after the update to show the changes. An optional section with commented-out code demonstrates how to achieve the same update using the CLI.",2025-06-03T06:23:11.282953+00:00
"context/update_prompt_example.py","The provided file is a Python script that demonstrates the usage of the `update_prompt` function from the `pdd.update_prompt` module. The script defines a `main` function, which sets up example input parameters including an input prompt, original code, modified code, and parameters for a language model (strength and temperature). It then calls the `update_prompt` function with these parameters, including a verbosity flag. The function is expected to return a modified prompt, the total cost of the operation, and the model name used. The script prints these results if the function call is successful, or prints an error message if an exception occurs. The script is designed to be run as a standalone program, as indicated by the `if __name__ == __main__:` block. Overall, the file serves as a usage example or test harness for the `update_prompt` function, demonstrating how to supply inputs and handle outputs and errors.",2025-04-30T03:55:28.422608+00:00
"context/xml_tagger_example.py","The provided file is a Python script that demonstrates how to use the 'xml_tagger' function from the 'pdd.xml_tagger' module, along with the 'rich' library for formatted console output. The script sets up a sample prompt ('Write a story about a magical forest') and specifies two parameters: 'strength' (which appears to control the quality or cost of the language model used) and 'temperature' (which likely affects the randomness of the model's output). It then attempts to process the prompt using the 'xml_tagger' function, which returns an XML-tagged version of the prompt, the total cost of the operation, and the name of the model used. The results are printed to the console in a formatted manner. If an error occurs during processing, the script catches the exception and prints an error message in red. Overall, the script serves as an example or test for using the 'xml_tagger' function with customizable parameters and provides user-friendly output.",2025-04-30T03:55:34.441519+00:00
