full_path,file_summary,date
"context/DSPy_example.py","This code snippet demonstrates the initial setup and implementation of a DSPy-based application focused on example selection. The code begins with importing necessary DSPy components and configuring OpenAI's GPT-3.5-turbo-instruct model as the language model backend, with a maximum token limit of 250. The core of the implementation is a 'chainofthought' class that inherits from dspy.Module. This class is designed to implement a chain-of-thought reasoning process, with a basic structure that includes initialization and a forward method to process questions. The class uses DSPy's ChainOfThought feature to map questions to answers. However, the code appears to be incomplete, as indicated by empty spaces and an unfinished comment about compilation and optimization. The code represents a basic framework for implementing chain-of-thought reasoning in a DSPy environment, though it needs additional implementation details to be fully functional.",2024-12-07T01:53:08.162193+00:00
"context/anthropic_counter_example.py","This Python code demonstrates how to count tokens in a text string using the Anthropic API. The code begins by importing the Anthropic library and initializing an Anthropic client. It then defines a sample text string and uses the client's count_tokens method to calculate the number of tokens in that text. The result is printed to the console. However, there's an important note in the comments indicating that this token counting method is not accurate for Anthropic models version 3.0 and above. The code is straightforward and serves as a basic example of token counting functionality, which is often useful when working with language models to stay within token limits or estimate API costs.",2024-12-07T01:53:08.238576+00:00
"context/auto_deps_main_example.py","The provided Python script is a command-line tool implemented using the Click library. It defines a command named 'auto-deps' that facilitates dependency analysis and processing of prompt files. The script includes several command-line options, such as '--force' to overwrite files, '--quiet' to suppress output, '--strength' and '--temperature' to configure dependency analysis parameters, and paths for input prompt files, directories, and output files. The main function initializes the Click context with these options and invokes the 'auto_deps_main' function from the 'pdd.auto_deps_main' module. This function processes the specified prompt file, analyzes dependencies, and generates a modified prompt file. The script also calculates and displays the total cost of the operation and the model used. Error handling is implemented to catch exceptions and abort execution if necessary. The script is designed to be executed as a standalone program.",2024-12-13T21:07:14.439152+00:00
"context/auto_include_example.py","The provided Python script demonstrates the usage of the 'auto_include' function from the 'pdd.auto_include' module. The script begins by importing necessary libraries such as 'os' and 'pandas'. It defines a 'main' function that serves as the entry point of the program. Within the 'main' function, the script reads a CSV file named 'project_dependencies.csv' and sets up an input prompt that describes the functionality of a Python function called 'generate_test'. This function is designed to create unit tests from code files using a language model.

The script specifies parameters for the 'auto_include' function, including the input prompt, directory path, CSV file content, model strength, temperature, and verbosity. The 'auto_include' function is then called with these parameters, and its outputs—dependencies, CSV output, total cost, and model name—are printed to the console. The script also includes detailed steps for processing the input prompt, invoking a language model using Langchain, handling incomplete generations, and calculating costs. Finally, the 'main' function is executed when the script is run as the main module.",2024-12-21T03:10:12.884959+00:00
"context/autotokenizer_example.py","This Python code defines a function called 'count_tokens' that calculates the number of tokens in a given text using a specified language model's tokenizer. The function uses the Hugging Face transformers library, specifically the AutoTokenizer class. By default, it uses the 'deepseek-ai/deepseek-coder-7b-instruct-v1.5' model, but this can be changed through the model_name parameter. The function works by first loading the appropriate tokenizer, then tokenizing the input text, and finally returning the length of the resulting input_ids. The code includes a simple example that demonstrates how to use the function by counting tokens in the text 'Write a quick sort algorithm in Python.' This utility is particularly useful for working with language models where understanding token count is important for managing context windows and input limitations.",2024-12-07T01:53:08.299472+00:00
"context/bug_to_unit_test_example.py","This Python script demonstrates the usage of a bug_to_unit_test function for automated test generation. The main function serves as the entry point and orchestrates the process of generating unit tests from bug reports. The script reads input from three different files: a prompt file (from 'prompts' directory), a code file (from 'pdd' directory), and a context file (from 'context' directory). It processes two types of outputs (current and desired) that contain debug information and JSON-formatted explanations. The function takes several parameters including the outputs, prompt, code under test, program to run the code, strength (0.9), temperature (0), and language ('Python'). The script uses the Rich library for enhanced console output formatting, displaying the generated unit test, total cost, and model name used. Error handling is implemented to catch and display any exceptions that occur during execution. The code appears to be part of a larger system for automated testing and debugging, possibly using AI/LLM models given the presence of temperature and strength parameters.",2024-12-07T01:53:08.330225+00:00
"context/bug_to_unit_test_failure_example.py","This code snippet contains a Python function named 'add' that takes two parameters 'x' and 'y'. However, there appears to be a logical error in the implementation. Despite the function name suggesting addition, the function actually performs subtraction, returning 'x-y' instead of 'x+y'. This is likely a bug that needs to be fixed to match the intended functionality implied by the function name. The code is very simple, consisting of just two lines: the function definition and the return statement. The indentation suggests this might be part of a larger codebase, but only this single function is shown in the provided content.",2024-12-07T01:53:08.365017+00:00
"context/change_example.py","The provided Python script demonstrates the use of a `change` function from the `pdd.change` module. It includes a `main` function that sets up environment variables, defines input parameters, and calls the `change` function. The script outlines an example scenario where a function to calculate the factorial of a number is modified based on a prompt to take the square root of the factorial output. Parameters such as `strength` and `temperature` are used to control the behavior of the `change` function. The script also handles potential exceptions and prints the modified prompt, total cost, and model name using the `rich.console` module for formatted output. The `main` function is executed when the script is run directly.",2024-12-21T03:10:15.598748+00:00
"context/change_main_example.py","The provided Python script demonstrates the usage of the 'change_main' function from the 'pdd' module, which is part of a command-line program. The script outlines two primary modes of operation: single-change mode and CSV batch-change mode. It begins by setting up a Click context with parameters such as verbosity, file extension, programming language, and budget. In single-change mode, the script creates sample input files, including a Python code file and a prompt file, and then calls 'change_main' to modify the prompt based on specified instructions. The results, including the modified prompt, total cost, and model used, are displayed using the 'rich' library. In CSV batch-change mode, the script creates multiple code and prompt files, along with a CSV file specifying batch changes. It then calls 'change_main' to process these changes in bulk, displaying the results and saving them to an output file. The script is designed to be run as a standalone program and includes detailed comments explaining its functionality and prerequisites, such as the availability of the 'change_main' module and necessary environment setup.",2024-12-21T03:10:17.361682+00:00
"context/cli_example.py","This Python script demonstrates the usage of the 'generate' command from the PDD CLI module. The main purpose is to generate runnable code from a prompt file. The script defines a main function that sets up the necessary parameters and executes the code generation process. It uses two key file paths: a prompt file ('prompts/get_extension_python.prompt') and an output file ('get_extension.py'). The generation process is configured with specific parameters including a strength of 0.5 and a temperature of 0, with a force flag to bypass user confirmation. The script uses command-line arguments formatted as a list to control the generation process. Error handling is implemented through a try-except block that catches and prints any exceptions that occur during execution. The script is designed to be run directly and assumes that environment variables are set and required packages are installed. The output includes both the generated code saved to the specified file and console output showing the total cost and model name used for generation.",2024-12-07T01:53:08.480560+00:00
"context/click_example.py","This Python script implements a command-line image processing tool using Click and Pillow (PIL) libraries. It follows a Unix-like pipe pattern where multiple image processing commands can be chained together. The script includes various image manipulation functions such as opening, saving, displaying, resizing, cropping, transposing (rotation and flipping), blurring, smoothening, embossing, sharpening, and pasting images. Each command is implemented as a Click command group with specific options and parameters. The script uses decorator patterns (@processor and @generator) to handle the command chain processing. Key features include:

1. Command chaining functionality similar to Unix pipes
2. Support for multiple input images
3. Various image transformation options
4. Error handling for image operations
5. Flexible file naming for output
6. Preservation of image filenames through transformations

The script is well-documented with docstrings and comments, making it clear how to use each command and what it does. Example usage shows commands can be combined like 'imagepipe open -i example.jpg resize -w 128 display' to process images in sequence.",2024-12-07T01:53:08.509553+00:00
"context/code_generator_example.py","The provided Python script demonstrates the usage of a function called 'code_generator' from the 'pdd.code_generator' module. The script's main function reads a prompt from a file ('prompts/generate_test_python.prompt') and uses it to generate Python code with the help of a language model. Key parameters for the 'code_generator' function include the programming language ('python'), model strength (0.5), temperature (0.0), and verbosity (True). The script attempts to generate runnable code, calculate the total cost, and identify the model name. If successful, it prints the generated code, cost, and model name. In case of an error, it catches and prints the exception. The script is designed to be executed as a standalone program with the main function serving as the entry point.",2024-12-21T03:10:19.903389+00:00
"context/comment_line_example.py","This file provides documentation and examples for a Python function called `comment_line` that handles code commenting across different programming languages. The function takes two parameters: `code_line` (the line to be commented) and `comment_characters` (the commenting syntax to use). The function supports three commenting styles: single-character comments (like Python's #), paired comments (like HTML's <!-- -->), and line deletion (using 'del'). The documentation includes the function's definition with detailed parameter descriptions and return value information. The file also contains practical examples demonstrating all three use cases: commenting a Python print statement, commenting an HTML tag, and deleting a line of code. Each example is accompanied by its expected output. The function is designed to be flexible and can accommodate different programming language commenting syntaxes by appropriately formatting the input line based on the specified comment characters.",2024-12-07T01:53:08.574892+00:00
"context/conflicts_in_prompts_example.py","The provided Python script demonstrates the use of a function named `conflicts_in_prompts` to detect and resolve conflicts between two example prompts. The script imports necessary modules, including `conflicts_in_prompts` from `pdd.conflicts_in_prompts` and `print` from the `rich` library for formatted output. The main function defines two detailed prompts: one for creating an `auth_helpers.py` module for Firebase authentication and another for designing a `User` class for the PDD Cloud platform. Each prompt includes specific requirements, such as functionality, dependencies, error handling, and coding standards. The script sets parameters like `strength`, `temperature`, and `verbose` for the conflict detection process. It then calls the `conflicts_in_prompts` function with the prompts and parameters, capturing the suggested changes, total cost, and model name. The results are displayed using formatted output, highlighting any detected conflicts and proposed changes. If no conflicts are found, a corresponding message is printed. The script is structured to showcase the functionality of the `conflicts_in_prompts` function while adhering to Python best practices, including type hints, docstrings, and modular design.",2024-12-21T03:10:21.724675+00:00
"context/conflicts_main_example.py","The provided script is a Python program that utilizes the 'click' library and a custom module 'pdd.conflicts_main' to analyze conflicts between two AI-generated prompts. It begins by defining two prompt files ('prompt1_LLM.prompt' and 'prompt2_LLM.prompt') with predefined text for AI assistance. These files are created and written to disk. A mock Click context class ('MockContext') is then defined to simulate a command-line interface context, including parameters like 'force', 'quiet', 'strength', and 'temperature'. The script uses the 'conflicts_main' function from the 'pdd.conflicts_main' module, passing the mock context, the two prompt files, and an output file ('conflicts_output.csv') as arguments. The function returns the number of conflicts, the total cost, and the model name used in the analysis. These results are printed to the console and saved in the specified output file. The script also includes verbose output for detailed logging. Overall, the program is designed to evaluate and report conflicts between AI-generated prompts, providing insights into their compatibility and associated costs.",2024-12-21T03:10:24.366081+00:00
"context/construct_paths_example.py","This Python file demonstrates the usage of a construct_paths function from a 'pdd' module. The main function serves as an example implementation, showing how to set up and use the construct_paths function. The code defines two main input parameters: input_file_paths (a dictionary containing paths to code and prompt files) and command_options (a dictionary for additional command settings). The construct_paths function is called with these parameters along with additional boolean flags for force and quiet modes, and a command type. The function returns three values: input_strings (containing file contents), output_file_paths, and language. The code includes error handling through a try-except block and prints the results. The file appears to be part of a larger system that handles file processing and path construction, possibly for code generation or testing purposes. The code is well-documented with comments explaining the purpose of each dictionary and parameter.",2024-12-07T01:53:08.665271+00:00
"context/context_generator_example.py","This Python script demonstrates the usage of a context generator module for code generation. The script first checks for the required environment variable 'PDD_PATH'. It then sets up input parameters including a sample Python function (add), a prompt requesting to write an addition function, the programming language (Python), and model parameters (strength and temperature). The script calls the context_generator function with these parameters to generate example code. The function returns three values: the generated example code, the cost of generation, and the name of the model used. Finally, it prints the results using Rich library's formatted output, displaying the generated code, total cost (formatted to 6 decimal places), and the model name used. The code is structured to demonstrate a practical implementation of automated code generation with cost tracking and model identification.",2024-12-07T01:53:08.693613+00:00
"context/continue_generation_example.py","The provided Python script demonstrates the usage of the `continue_generation` function, which extends text generation using a language model and calculates associated costs. The script begins by loading input data from external files: a formatted input prompt from `context/cli_python_preprocessed.prompt` and an initial LLM output fragment from `context/llm_output_fragment.txt`. It sets parameters for the language model, including `strength` and `temperature`, and then calls the `continue_generation` function with these inputs. The function returns the final generated output, the total cost of the operation, and the model name. The script prints the total cost and model name to the console and writes the final generated output to a file named `context/final_llm_output.py`. Error handling is implemented to catch and report file-related or general exceptions. The script is designed to be executed as a standalone program with the `main` function serving as the entry point.",2024-12-21T03:10:26.974173+00:00
"context/detect_change_example.py","This Python script is designed to analyze and detect changes in prompt files using an LLM (Large Language Model). The script imports necessary modules including a custom 'detect_change' function and Rich console for formatted output. It defines a list of prompt files to analyze from 'prompts' and 'context' directories, though there's a commented-out alternative that would dynamically gather all .prompt files. The script specifies a change description aimed at using 'python_preamble.prompt' to make prompts more compact. It includes configuration for LLM parameters: strength (1.0) and temperature (0.0). The main functionality is wrapped in a try-except block that calls detect_change() with the specified parameters and prints the results using Rich console formatting. For each detected change, it displays the prompt name and change instructions. Finally, it outputs the total cost of the operation and the model name used. If any errors occur during execution, they are caught and displayed with red formatting.",2024-12-07T01:53:08.749981+00:00
"context/find_section_example.py","This file provides documentation and examples for using the `find_section` function from the `find_section.py` module. The function is designed to identify and extract code blocks from text content. The documentation includes a complete example that demonstrates how to use the function with a sample input containing multiple code blocks in different programming languages (Python and JavaScript). The file details both the input parameters (`lines`, `start_index`, and `sub_section`) and the output format (a list of tuples containing language, start line, and end line information). The example shows how to process a string containing code blocks by first splitting it into lines and then using the find_section function to locate the code sections. It also includes a practical example of reading from a file ('unrunnable_raw_llm_output.py') and processing its contents. The documentation concludes with sample output showing how the function identifies code blocks and their locations within the text. The function appears to be particularly useful for parsing and processing text that contains embedded code blocks in various programming languages.",2024-12-07T01:53:08.778520+00:00
"context/fix_code_loop_example.py","This Python script demonstrates the usage of a code fixing utility called 'fix_code_loop'. The main function sets up a test environment by creating two files: a code file containing a function with a potential error (calculate_average) and a verification file that tests the function. The script then uses the fix_code_loop function to automatically detect and fix issues in the code. The fix_code_loop takes several parameters including the code file path, original prompt, verification program, model strength, temperature, maximum attempts, budget limit, and error log file location. After running the fix attempt, it prints the results including whether the fix was successful, number of attempts made, total cost incurred, model used, and the final corrected code. The script also includes functionality to read prompts from files and handle module-specific paths. While there's code to clean up the created files at the end, it's currently commented out. The script is designed to work with a larger system that likely involves machine learning models for code correction.",2024-12-07T01:53:08.807027+00:00
"context/fix_code_module_errors_example.py","The provided Python script demonstrates a process for identifying and fixing errors in a code module using a function called `fix_code_module_errors`. The script begins with a sample program that contains an error, where a function `calculate_sum` attempts to sum a string instead of a list of numbers, resulting in a `TypeError`. The script also includes the original prompt that generated the code, the erroneous code module, and the error message encountered during execution. The `fix_code_module_errors` function is then called with these inputs, along with parameters for model strength and temperature, to generate a corrected version of the program and code module. The function returns several outputs, including whether updates are needed, the fixed program and code, the total cost of the API calls, and the name of the model used. The script concludes by printing these results to the console. Overall, the script serves as an example of automated error detection and correction in Python code using an external function or API.",2024-12-21T03:10:29.497557+00:00
"context/fix_error_loop_example.py","The provided Python script demonstrates the usage of a function called `fix_error_loop` from the `pdd.fix_error_loop` module. The script is designed to automate the process of fixing errors in a code file using a unit test file, a prompt, and a verification program. It sets up various parameters such as file paths for the unit test, code, prompt, and verification program, as well as configuration options like strength, temperature, maximum attempts, and budget for the error-fixing process. The script then calls the `fix_error_loop` function with these parameters and captures the results, including whether the process was successful, the final unit test and code, the total number of attempts, the total cost, and the model used. The results are displayed using the `rich` library for formatted output. If an exception occurs during execution, it is caught and displayed. The script is intended to be executed as a standalone program with the `main` function serving as the entry point.",2024-12-21T03:10:31.896964+00:00
"context/fix_errors_from_unit_tests_example.py","This Python script demonstrates the usage of a function called 'fix_errors_from_unit_tests' which is designed to automatically fix errors in code and unit tests. The main function sets up example inputs including a simple unit test for an addition function, the corresponding code implementation, and various parameters. The unit test deliberately includes an incorrect test case (asserting 0 == 1), which triggers an error. The script uses several parameters including a prompt describing the desired functionality, an error message, a file for logging fixes, and LLM (Language Learning Model) parameters like strength and temperature. The function returns multiple values including updated versions of both the unit test and code, fixed versions of both, the total cost of the operation, and the name of the model used. The results are printed using Rich library's pretty printing functionality. The script is structured as a demonstration tool, showing how the error-fixing functionality can be used in practice.",2024-12-07T01:53:08.893156+00:00
"context/generate_output_paths_example.py","The file is a Python script that demonstrates the usage of a function called `generate_output_paths` from the `pdd.generate_output_paths` module. The script sets up example inputs, including a command, output locations, a basename, a programming language, and a file extension. It then calls the `generate_output_paths` function to generate output paths based on these inputs. The script includes examples for different scenarios: a basic 'generate' command, a case where an environment variable (`PDD_GENERATE_OUTPUT_PATH`) is used to specify the output path, and a 'fix' command that generates multiple output paths for test and code files. The results of these function calls are printed to the console, showcasing the generated output paths for each scenario. The script is structured to demonstrate the flexibility and functionality of the `generate_output_paths` function in handling various input configurations and commands.",2024-12-13T21:07:19.225256+00:00
"context/generate_test_example.py","The file contains a Python script that demonstrates the generation of a unit test for a given function using the `generate_test` function from the `pdd.generate_test` module. The script begins by importing necessary modules, including `os` and `rich` for environment variable handling and formatted output, respectively. It defines a prompt for creating a factorial function, along with the corresponding Python code implementation. Additional parameters such as `strength`, `temperature`, and `language` are specified to guide the test generation process. The script attempts to call the `generate_test` function with these inputs and captures the resulting unit test, total cost, and model name. If successful, it prints the generated unit test, cost, and model details using formatted output. In case of an error, it catches the exception and displays an error message. The script also includes commented-out code for setting an environment variable (`PDD_PATH`) if needed.",2024-12-21T03:10:34.209936+00:00
"context/get_comment_example.py","This file provides documentation and usage examples for the `get_comment` function from the `get_comment.py` module. The function retrieves comment characters for different programming languages. The documentation includes a practical example showing how to import and use the function, with sample calls for languages like Python (#), Java (//), and JavaScript (// or /* */). The function takes a single case-insensitive string parameter 'language' and returns the corresponding comment character(s) as a string. If there's an error (missing environment variable PDD_PATH, unknown language, or other issues), it returns 'del'. The function relies on a CSV file (language_format.csv) that should be located in a directory specified by the PDD_PATH environment variable. This CSV file must contain at least two columns: 'language' and 'comment' that map programming languages to their respective comment characters.",2024-12-07T01:53:08.992494+00:00
"context/get_extension_example.py","The file contains a Python script that imports a function named 'get_extension' from the 'pdd.get_extension' module. The script demonstrates example calls to the 'get_extension' function, which appears to return the file extension or identifier associated with a given programming language or file type. Examples include returning '.py' for 'Python', 'Makefile' for 'Makefile', and '.js' for 'JavaScript'.",2024-12-21T03:10:37.702819+00:00
"context/get_language_example.py","This Python script demonstrates the usage of a 'get_language' function that determines the programming language associated with a file extension. The code consists of a main function that serves as an example implementation. Inside the main function, it defines a sample file extension '.py' and attempts to get the corresponding programming language using the imported get_language function. The code includes error handling through a try-except block to catch any potential exceptions during execution. If a language is successfully identified, it prints the file extension and its associated programming language. If no language is found, it displays a message indicating that no programming language was found for the given extension. The script uses type hints for better code readability and follows Python's standard if __name__ == '__main__' pattern for script execution. The code is well-structured and includes proper documentation through docstrings.",2024-12-07T01:53:09.050133+00:00
"context/git_update_example.py","The provided Python script demonstrates the usage of a function called `git_update_prompt` from the `pdd.git_update` module. The script begins by defining input parameters, including an input prompt loaded from a file, a file path for modified code, and parameters like `strength` and `temperature` for the language model. The `git_update` function is then called with these parameters, and its result is processed. If successful, the script prints the modified prompt, total cost, and model name, and saves the modified prompt back to the file. If the function fails, appropriate error messages are displayed. The script includes exception handling for `ValueError` and general exceptions. The main function is executed when the script is run directly.",2024-12-21T03:10:38.878580+00:00
"context/increase_tests_example.py","This Python file demonstrates the usage of an 'increase_tests' function, which is designed to generate additional unit tests for existing code. The file contains an 'example_usage' function that showcases both basic and advanced implementations of the test generation process. The example uses a simple 'calculate_average' function as the target code, along with an existing basic unit test and a coverage report showing 60% code coverage. The function demonstrates two ways to call 'increase_tests': a basic usage with default parameters and an advanced usage with custom parameters including language specification, strength, temperature, and verbosity settings. The code includes proper error handling using try-except blocks to catch both ValueError for input validation errors and general exceptions. The example is structured to run when the script is executed directly. The code serves as a practical demonstration of how to use the test generation functionality while handling potential errors and displaying relevant output information such as the generated tests, associated costs, and the model used for generation.",2024-12-07T01:53:09.111951+00:00
"context/insert_includes_example.py","This Python file demonstrates the usage of an insert_includes module for processing dependencies. The main function, example_usage(), showcases how to set up and execute dependency processing with error handling. The code initializes a Rich console for formatted output and defines input parameters including a prompt for a Python function that handles CSV files, a directory path for example files, and a CSV filename for dependency information. The insert_includes function is called with specific parameters like strength and temperature to control the output. The code then displays the results including the original prompt, modified prompt with dependencies, CSV output, model name, and total cost. The processed CSV output is saved to a file. Error handling is implemented for common issues like missing files or processing errors. The code uses the Rich library for enhanced console output formatting and the pathlib library for file operations. This appears to be part of a larger system for managing and processing project dependencies with AI model integration.",2024-12-07T22:11:27.202537+00:00
"context/langchain_lcel_example.py","This Python code demonstrates various implementations and uses of Language Models (LLMs) using the LangChain framework. The file includes examples of working with different LLM providers including Google's Gemini, OpenAI, Azure, Fireworks, Anthropic's Claude, Groq, Together.ai, and Ollama. It showcases several key features:

1. Setting up a SQLite cache for LLM responses
2. Implementation of a custom CompletionStatusHandler for tracking completion status and token usage
3. Various prompt templates and chain configurations
4. Different output parsing methods including JSON and Pydantic models
5. Structured output handling with a custom Joke class
6. Examples of fallback configurations and model alternatives
7. Integration with multiple API endpoints and services
8. Usage of both chat and completion models
9. Implementation of prompt templates with variable substitution

The code serves as a comprehensive example of how to integrate and use different LLM providers within the LangChain ecosystem, including proper error handling, response parsing, and output formatting. It also demonstrates modern best practices like using invoke() instead of deprecated run() methods.",2024-12-07T01:53:09.175039+00:00
"context/llm_invoke_example.py","The provided Python script demonstrates the use of the `llm_invoke` function to generate jokes using a language model. It employs the Pydantic library to define a structured output model (`Joke`) with fields for the joke's setup and punchline. The script iterates through different 'strength' values (ranging from 0.0 to 1.0) to test the model's performance and tracks the strength ranges for each model used.

Two examples are showcased:
1. Unstructured output: The script generates a joke based on a given topic (e.g., programmers) and prints the result, cost, and model used.
2. Structured output: The script generates a joke in a predefined JSON format using the `Joke` Pydantic model, ensuring structured data output. The setup and punchline are accessed and printed.

The script also tracks and prints the strength ranges for each model used during the iterations. It concludes by summarizing the strength ranges for all models. The main function serves as the entry point, and the script is designed to be run as a standalone program.",2024-12-13T21:07:21.656593+00:00
"context/llm_selector_example.py","This Python code demonstrates the usage of a language model selector function (llm_selector) through a main function. The code iteratively tests different strength values ranging from 0.5 to 1.1 in increments of 0.05, while maintaining a fixed temperature of 1.0. For each iteration, it calls the llm_selector function, which returns various parameters including the selected LLM model, token counter function, input and output costs per million tokens, and the model name. The code includes error handling for FileNotFoundError and ValueError exceptions. Additionally, it demonstrates the usage of the token counter function with a sample text. The main function is structured to showcase how to interact with different language models based on varying strength parameters, making it useful for testing and comparing different LLM configurations. The code is organized with type hints and proper error handling, following good programming practices.",2024-12-07T01:53:09.245457+00:00
"context/llm_token_counter_example.py","This Python script demonstrates different methods of counting tokens in text using various tokenization libraries. The code includes three main approaches: tiktoken, anthropic, and autotokenizer. The main function contains three try-except blocks, each showcasing a different tokenization method. For tiktoken, it uses the 'o200k_base' encoding. For anthropic, it utilizes the 'claude-3-sonnet-20240229' model. For autotokenizer, it employs the 'deepseek-ai/deepseek-coder-7b-instruct-v1.5' model. Each method processes a sample text string and prints the token count. The script imports necessary libraries including tiktoken, anthropic, and transformers.AutoTokenizer, and uses a custom token counter function 'llm_token_counter' from a module called 'pdd'. The code is structured with error handling to gracefully manage potential issues with each tokenization method. The script is designed to be run as a standalone program, with the main function being called only if the script is the primary program being executed.",2024-12-07T01:53:09.273521+00:00
"context/load_prompt_template_example.py","This Python script demonstrates a simple program that loads and displays a prompt template. The code imports two functions: 'load_prompt_template' from a custom module 'pdd.load_prompt_template' and 'print' from the 'rich' library, which provides rich text formatting capabilities. The main function defines a variable 'prompt_name' set to 'generate_test_LLM' (presumably the name of a prompt template file without its extension), loads the template using the imported function, and then displays it with blue-colored formatting if the template is successfully loaded. The script uses a standard Python idiom with the '__name__ == __main__' check to ensure the main function only runs when the script is executed directly. This appears to be part of a larger system dealing with prompt templates, possibly for working with Large Language Models (LLMs).",2024-12-07T01:53:09.302428+00:00
"context/postprocess_0_example.py","This file provides a comprehensive example of how to use the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The example demonstrates processing output from a language model (LLM) that contains multiple code sections in different programming languages. The main example shows how to extract and process Python code specifically, though the function can work with other languages. The code includes a sample LLM output containing Python and Java code blocks, along with regular text. The example demonstrates how to use the function to process this mixed content, focusing on extracting Python code while commenting out other content. The documentation section clearly outlines the input parameters (`llm_output` and `language`) and explains that the function returns a string with only the largest section of the specified language's code uncommented, while other content is commented out. The file concludes with a note about ensuring proper implementation of supporting functions (`get_comment`, `comment_line`, and `find_section`) for the example to work correctly.",2024-12-07T01:53:09.330350+00:00
"context/postprocess_example.py","This Python script demonstrates the usage of a postprocessing function designed to extract code from mixed text and code output typically generated by Large Language Models (LLMs). The main function contains an example LLM output that includes a factorial function implementation along with usage examples, all embedded within markdown-style code blocks. The script uses the 'postprocess' function from a 'pdd.postprocess' module to extract the pure code from this mixed output. The script also utilizes the 'rich' library for enhanced console output formatting. The postprocessing function takes several parameters including the target programming language (python), model strength (0.7), and temperature (0.2). The script outputs the extracted code, the total cost of the operation, and the model name used. The output is formatted with color coding using the rich console library's formatting capabilities. This appears to be a demonstration or utility script that might be part of a larger toolkit for handling LLM-generated code outputs.",2024-12-07T01:53:09.359769+00:00
"context/preprocess_example.py","This Python script demonstrates the usage of a preprocessing functionality, likely part of a larger system. The code performs several key operations:

1. It imports a preprocess function from 'pdd.preprocess' and sets up a Rich console for formatted output.

2. It processes multiple different prompts:
- An initial XML-style prompt containing various tags (include, shell, pdd)
- A prompt loaded from 'prompts/xml/change_LLM.prompt'
- A prompt from 'prompts/example_generator_LLM.prompt'
- A simple mock database prompt

3. For each prompt processing operation, it:
- Calls the preprocess function with specific parameters (recursive and double_curly_brackets flags)
- Prints the processed results using the Rich console
- In some cases, writes the processed output to test files

The script appears to be part of a prompt processing system, possibly for a language model interface, with functionality to handle different types of prompts and formatting options. It includes file I/O operations and uses the Rich library for enhanced console output formatting.",2024-12-07T01:53:09.387848+00:00
"context/process_csv_change_example.py","This Python code demonstrates the usage of a function called 'process_csv_change' from a module named 'pdd.process_csv_change'. The script is designed to process changes in code files based on CSV input. It initializes several parameters including the CSV file path, strength (0.8), temperature (0), code directory path, programming language (Python), file extension (.py), and a maximum budget of $10.00. The main function call is wrapped in a try-except block for error handling. If successful, it prints the total cost of changes, the AI model used, and displays modified prompts for each affected file. The code includes error handling for file not found scenarios and unexpected errors. The function appears to be part of a larger system that automates code modifications, possibly using AI models, while maintaining budget constraints and tracking costs.",2024-12-07T01:53:09.415833+00:00
"context/split_example.py","This Python script demonstrates the usage of a 'split' function from a 'pdd.split' module. The code is structured around a main function that showcases how to process and split programming-related prompts. The script begins by importing necessary modules, including 'rich.console' for enhanced console output formatting. It sets up environment variables by configuring the PDD_PATH variable to point to the parent directory. The main function includes example inputs: a prompt requesting a factorial function implementation, sample code for the factorial function, and example usage code. It also defines parameters like strength (0.7) and temperature (0.5) for controlling the split operation. The script calls the split function with these inputs and prints three outputs: a sub-prompt, a modified prompt, and the total cost of the operation. Error handling is implemented using a try-except block, and any errors are displayed with red formatting using the Rich console. The script is designed to be run as a standalone program, with the main function being called only if the script is the primary program being executed.",2024-12-07T01:53:09.443529+00:00
"context/summarize_directory_example.py","This Python script demonstrates the usage of the 'summarize_directory' module from the 'pdd' package. The main function showcases how to summarize Python files in a directory while handling existing summaries. It includes an example of existing CSV content and demonstrates calling the summarize_directory function with various parameters including directory path (with wildcard), strength (model capability), temperature (output determinism), verbosity, and existing CSV content. The script processes the files, generates summaries, and outputs the results including the generated CSV content, total cost in USD, and the model used. The results are both printed to console and saved to 'output.csv'. The code includes error handling and is structured as a standalone executable script with a main() function guard.",2024-12-07T01:53:09.471711+00:00
"context/tiktoken_example.py","This code snippet demonstrates the use of the tiktoken library for token counting in text processing. The code first imports the tiktoken library, then creates an encoding object using the 'cl100k_base' encoding scheme (which is commonly used with newer GPT models). Finally, it counts the number of tokens in a variable called 'preprocessed_prompt' by encoding the text and measuring the length of the resulting encoded sequence. The code is particularly useful for applications that need to track token usage when working with language models, as it provides a way to count tokens before sending text to an API. The 'cl100k_base' encoding is specifically mentioned, though the comment indicates other encoding options are available. This is a common pattern when working with OpenAI's GPT models, where understanding token count is important for both cost management and staying within model context limits.",2024-12-07T01:53:09.500027+00:00
"context/trace_example.py","This Python script demonstrates the usage of a `trace` function from the `pdd.trace` module. The main function sets up a demonstration environment with example inputs for code tracing. The script includes imports for required modules, including `os` and `rich.console` for enhanced console output. The main components include:

1. Example code that defines a simple 'hello_world' function
2. Configuration parameters such as code line number, strength (0.7), and temperature (0.2)
3. A prompt file containing instructions for the code implementation
4. Error handling for various potential exceptions (FileNotFoundError, ValueError, and general exceptions)

The script attempts to trace a specific line of code (line 3) using the provided parameters and outputs the results including the corresponding prompt line, total cost, and model name used. The output is formatted using the Rich console library for better readability with color-coded messages. The script is structured to run the main function when executed directly rather than being imported as a module.",2024-12-07T01:53:09.527136+00:00
"context/trace_main_example.py","This Python script implements a command-line interface (CLI) using the Click library for a code tracing tool. The script contains example content for both a prompt file (which requests implementation of a factorial function) and a code file (which contains the actual factorial implementation). The main CLI function 'cli' is decorated with various Click options including prompt-file path, code-file path, code line number, output file path, and several flags and parameters for controlling the trace analysis. The options include force overwrite, quiet mode, model strength, and temperature settings for LLM (Language Learning Model) generation. The script writes example files to disk and then processes them using a trace_main function imported from pdd.trace_main. When executed, it outputs the prompt line, total cost, and model name used for the analysis, unless quiet mode is enabled. The factorial function implementation included in the example handles negative numbers and uses recursion for the calculation.",2024-12-07T01:53:09.556510+00:00
"context/track_cost_example.py","This Python file implements a Command-Line Interface (CLI) tool called PDD using the Click library. The tool is designed for processing prompts and generating outputs with cost tracking functionality. The code consists of two main components: a base CLI group and a 'generate' command. The CLI accepts an optional '--output-cost' parameter to track costs and save usage details to a CSV file. The 'generate' command requires a prompt file input and has an optional output file parameter. It's decorated with a 'track_cost' decorator for cost monitoring. The command reads a prompt from the input file, processes it (currently using placeholder logic), and either writes the output to a specified file or prints it to the console. The function returns a tuple containing the generated output, the cost of execution (simulated at $0.05 per million tokens), and the model name (set to 'gpt-4'). The file includes proper type hints, documentation, and error handling through Click's built-in functionality. The main execution block demonstrates example usage of the CLI with specific command-line arguments.",2024-12-07T01:53:09.597187+00:00
"context/unfinished_prompt_example.py","This Python file demonstrates the usage of an 'unfinished_prompt' function from a 'pdd' module. The code consists of a main function that analyzes the completeness of a given text prompt using a Language Learning Model (LLM). The example uses a simple fairy tale beginning ('Once upon a time...') as the test prompt. The function takes several parameters including the prompt text, model strength (0.5), and temperature (0.0). The unfinished_prompt function returns four values: reasoning, whether the prompt is finished, the total cost of the analysis, and the model name used. The code includes error handling through a try-except block and is structured to run the main function only when the file is executed directly. This appears to be a demonstration or testing script for a larger prompt analysis system, likely used to determine if a given text prompt is complete or needs additional content.",2024-12-07T01:53:09.626009+00:00
"context/update_prompt_example.py","The provided Python script demonstrates the usage of the `update_prompt` function from the `pdd.update_prompt` module. The script defines a `main` function that sets up input parameters, including an input prompt, input code, modified code, and parameters like strength and temperature for a language model. It then calls the `update_prompt` function with these parameters. The function is expected to return a modified prompt, the total cost of the operation, and the model name. If successful, the script prints these results; otherwise, it handles exceptions and prints an error message. The script is designed to be executed as a standalone program, with the `main` function being called when the script is run directly.",2024-12-21T03:10:40.913389+00:00
"context/xml_tagger_example.py","This Python script demonstrates the usage of an XML tagging functionality using a custom module 'pdd.xml_tagger'. The code imports the 'xml_tagger' function and the 'rich' library for enhanced console printing. The script takes a raw text prompt ('Write a story about a magical forest') and processes it with two parameters: 'strength' (set to 0.5, representing base model performance) and 'temperature' (set to 0.7, controlling output randomness). The code is wrapped in a try-except block for error handling. When successful, it outputs the XML-tagged version of the prompt, the total cost of processing, and the name of the model used. The output is formatted using rich's printing capabilities with color coding (green for success, red for errors) and bold text formatting. This appears to be a demonstration or example implementation of an AI-powered text processing tool that adds XML markup to input text.",2024-12-07T01:53:09.681130+00:00
