full_path,file_summary,date
"context/DSPy_example.py","The provided file content is a Python code snippet demonstrating the initial setup and module definition for using the DSPy library to optimize prompts or examples. It begins by importing a local context module and configuring the DSPy settings to use the OpenAI 'gpt-3.5-turbo-instruct' model with a token limit of 250. Following the setup, the code defines a custom class named 'chainofthought' that inherits from 'dspy.Module'. Inside this class, the constructor initializes a 'dspy.ChainOfThought' program designed to map questions to answers. A 'forward' method is also defined to execute this program when the module is called with a question. The snippet ends abruptly with a comment indicating the next steps would involve compiling and optimizing the module.",2026-01-06T21:03:04.515704+00:00
"context/__init__example.py","This Python script serves as a practical demonstration of the command registration module within the PDD tool's architecture. It illustrates the standard procedure for initializing a main command-line interface (CLI) using the Click library and dynamically attaching subcommands. The script begins by importing the necessary modules, including the register_commands function from the pdd.commands package. It defines a primary Click group named main_cli, which acts as the central entry point for the application. The core logic resides in the run_example function, which performs three key steps: it initializes the CLI group, invokes register_commands to populate the group with subcommands defined elsewhere in the package, and finally executes the CLI with the --help argument. By setting standalone_mode to false, the script can display the generated help documentation—verifying that all subcommands were successfully registered—without terminating the process. This pattern is essential for maintaining a modular codebase where different functional areas of a tool are developed in separate files but aggregated into a single, cohesive user interface. The example provides a clear template for developers to follow when setting up the main entry point of the PDD application, ensuring that all features are accessible through a unified command structure.",2026-01-06T21:03:04.734897+00:00
"context/agentic_bug_example.py","This Python script serves as a demonstration for the run_agentic_bug functionality within the pdd.agentic_bug module, specifically designed for investigating GitHub issues. The code begins by configuring the system path to include the project root, ensuring proper module resolution. The core logic resides in the main function, which targets a placeholder GitHub issue URL. To facilitate a safe and cost-effective demonstration, the script utilizes the unittest.mock library to patch the run_agentic_bug_orchestrator. This mock simulates a successful eight-step investigation process, returning predefined values such as a success boolean, a descriptive message regarding the creation of a test file, a simulated API cost of $2.50, the model provider (Anthropic), and a list of modified files. Upon execution, the script invokes the run_agentic_bug function with verbose logging enabled. Finally, it prints a formatted summary of the investigation results to the console, detailing the outcome, resources used, and specific file changes. This example provides developers with a clear template for integrating automated bug investigation workflows into their projects while highlighting the orchestration and reporting capabilities of the underlying agentic system.",2026-01-06T21:03:22.217724+00:00
"context/agentic_bug_orchestrator_example.py","This Python script provides an example usage and simulation of the `agentic_bug_orchestrator` module. It demonstrates how to invoke the `run_agentic_bug_orchestrator` function to handle a bug investigation workflow. To avoid actual LLM costs and external dependencies, the script mocks key internal functions like `run_agentic_task` and `load_prompt_template`. The simulation follows a scenario where a user reports a 'ZeroDivisionError' in a calculator application. It steps through a nine-stage process including duplicate checking, documentation review, reproduction of the bug, root cause identification, test generation, and draft PR creation. The script utilizes the `unittest.mock` library to simulate successful agent outputs and costs for each step. Upon execution, it prints the simulation progress, final success status, total simulated cost, and any files changed during the process. This serves as a template for developers to understand the orchestrator's logic and integration without requiring a live GitHub environment or active API keys.",2026-01-06T21:03:29.917329+00:00
"context/agentic_common_example.py","This Python script serves as a demonstration for the 'pdd' package's agentic capabilities, specifically utilizing the 'agentic_common' module. The script begins by configuring the system path to include the project root, ensuring proper imports. The main execution flow illustrates a complete lifecycle for an agentic task: environment setup, provider discovery, task execution, and result verification. It creates a local './output' directory to serve as the agent's workspace and uses 'get_available_agents' to identify active providers like Claude, Gemini, or Codex based on installed CLI tools and API keys. The core of the script involves calling 'run_agentic_task' with a natural language instruction to generate a Python file named 'generated_math.py' containing a factorial function. Upon completion, the script parses and displays the execution status, the specific provider used, and the estimated financial cost. Finally, it performs a verification step by checking for the existence of the generated file and printing its contents to the console, showcasing how headless agents can perform file-system operations and code generation autonomously.",2026-01-06T21:03:32.820160+00:00
"context/agentic_crash_example.py","This Python script serves as a test harness and demonstration for an 'agentic crash' handling system within the pdd package. It simulates a software failure scenario by programmatically creating a suite of dummy files: a specification document, a buggy implementation of a factorial function that raises a NotImplementedError, a runner script, and a corresponding crash log. The script utilizes the unittest.mock library to simulate the behavior of an AI agent and subprocess execution, avoiding the need for actual API calls or external dependencies. Specifically, it mocks the 'run_agentic_task' function to simulate an agent analyzing the crash and applying a fix, while a side effect function updates the buggy source code with a working recursive factorial implementation. The main execution flow calls 'run_agentic_crash', which orchestrates the repair process and verifies the fix. Finally, the script prints a summary of the results, including the success status, the AI model used, the simulated cost, and the updated content of the fixed file, demonstrating how the system automates the debugging and repair of crashed programs.",2026-01-06T21:03:35.474822+00:00
"context/agentic_fix_example.py","This Python script demonstrates an automated 'agentic fix' workflow using an LLM-based agent to repair buggy code. The script defines a scenario involving a simple calculator function with a logic error (subtraction instead of addition) and a corresponding failing pytest suite. It utilizes a temporary directory to simulate a project environment, creating the buggy source file, a test file, a prompt file, and an initial error log generated by running subprocess commands. The core logic revolves around the `run_agentic_fix` function, which invokes an AI agent to analyze the error and apply a correction. The script includes prerequisite checks for necessary API keys (Anthropic, Google, or OpenAI) and manages environment variables for logging. Upon execution, it reports the success or failure of the fix, the specific model used, the estimated API cost, and the list of modified files. Finally, it displays the corrected code to verify that the agent successfully resolved the bug, ensuring a clean environment by restoring the original working directory after completion.",2026-01-06T21:03:38.068873+00:00
"context/agentic_langtest_example.py","The provided Python file, `pdd/agentic_langtest.py`, contains utility functions designed to automate the verification and testing of code across multiple programming languages, including Python, JavaScript, TypeScript, and Java. The script includes a project root discovery mechanism that searches for marker files like `package.json` or `pom.xml`. It features a core function, `default_verify_cmd_for`, which generates appropriate shell commands to execute unit tests based on the detected language and build system (e.g., pytest for Python, npm for JavaScript, or Maven/Gradle for Java). Additionally, the `missing_tool_hints` function checks the system's PATH for required toolchains and provides user-friendly, platform-specific installation instructions if dependencies are missing. The file also includes a demonstration suite that uses the `rich` library to display how the script handles mock JavaScript and Python projects within a temporary directory. This toolset is particularly useful for agentic workflows or CI/CD environments where automated test execution across diverse environments is required.",2026-01-06T21:03:40.363022+00:00
"context/agentic_update_example.py","This Python script serves as a demonstration and utility for the 'run_agentic_update' function within the pdd package. Its primary purpose is to automate the synchronization of a prompt file with the current state of its corresponding source code. The script begins by setting up a local environment and creating dummy files to simulate a common development scenario: a math module where the code has been updated to include a subtraction function, but the existing prompt file only describes an addition function. It then invokes the agentic update process, which utilizes an AI agent (such as Claude or Gemini) to analyze the code and tests, subsequently rewriting the prompt file to accurately reflect the new functionality. The script provides detailed output regarding the execution, including success status, financial cost of the API call, the specific model used, and a list of modified files. Finally, it verifies the update by printing the contents of the newly synchronized prompt file, illustrating how developers can maintain documentation and AI instructions in alignment with evolving codebases.",2026-01-06T21:03:42.688667+00:00
"context/agentic_verify_example.py","This Python script serves as a demonstration and test harness for the 'pdd.agentic_verify' module, specifically showcasing an automated code verification and repair workflow. The script begins by configuring the environment, ensuring the 'pdd' package is in the system path and creating a temporary directory containing four essential files: a specification (spec.md), a buggy implementation (calculator.py), a test driver (driver.py), and a verification log. To facilitate a standalone demonstration without external API dependencies, the script utilizes the 'unittest.mock' library to patch internal functions. It defines a 'mock_agent_behavior' function that simulates an AI agent identifying and fixing a logic error in the calculator code. When 'run_agentic_verify' is executed, the mocked agent replaces the subtraction operator with an addition operator in the source file. Finally, the script outputs the execution results, including success status, model metadata, and the corrected file contents, effectively illustrating how the agentic verification process integrates specification analysis with automated code correction.",2026-01-06T21:03:44.876089+00:00
"context/anthropic_counter_example.py","The provided file content consists of a commented-out Python script designed to demonstrate how to count tokens using the Anthropic API library. The script begins by importing the necessary anthropic module and initializing a client instance. It then defines a sample string and utilizes the client.count_tokens() method to calculate the number of tokens within that text. Notably, the code includes a warning comment stating that this specific method may not provide accurate results for Claude 3.0 or newer models, suggesting that users should consult updated documentation for more recent model versions. Finally, the script outputs the total token count to the console. This snippet serves as a basic utility example for developers looking to manage token usage or estimate costs when interacting with Anthropic's language models, though it highlights the importance of version-specific compatibility. The entire block is currently inactive due to the use of hash symbols for commenting, making it a reference or template rather than executable code in its current state.",2026-01-06T21:03:47.447644+00:00
"context/anthropic_tool_example.py","This Python script demonstrates a basic implementation of the Anthropic API using the 'anthropic' library. It initializes a client and sends a request to the 'claude-3-7-sonnet-20250219' model. The request includes a specific tool definition for 'str_replace_editor', which is part of the 'text_editor_20250124' toolset, indicating the model is being prepared for file editing tasks. The user message in the conversation history asks for assistance in fixing a syntax error within a file named 'patent.md'. The script concludes by printing the model's response to the console. This code serves as a template for integrating Claude's tool-use capabilities, specifically for text manipulation and error correction in external files.",2026-01-06T21:03:56.884910+00:00
"context/auto_deps_main_example.py","This Python script implements a command-line interface (CLI) for a tool called auto-deps, which is designed to analyze and process file dependencies within a project. Utilizing the Click library, the script defines a main function decorated with several configurable options. These options allow users to control the behavior of the dependency analysis, including flags for forcing file overwrites, suppressing output, and rescanning directories. Users can also specify parameters such as analysis strength, temperature for model processing, and paths for input prompts, directory contexts, and CSV dependency logs. The core logic is delegated to the auto_deps_main function imported from the pdd package. Upon execution, the script initializes a Click context, passes the user-defined parameters to the backend processing function, and then outputs the resulting modified prompt, the total financial cost of the operation, and the specific model utilized. Error handling is integrated to catch exceptions during the analysis process, providing feedback to the user and aborting the command if necessary. This script serves as the entry point for automating the identification and integration of project dependencies into prompt-based workflows.",2026-01-06T21:03:59.268347+00:00
"context/auto_include_example.py","This Python script demonstrates the practical application of the 'auto_include' function from the 'pdd' library. The script begins by loading project dependency data from a CSV file and defining a complex multi-step prompt designed for an LLM. This prompt instructs an expert Python Software Engineer agent to generate unit tests using Langchain, incorporating specific steps such as environment variable retrieval, prompt preprocessing, model selection via 'llm_selector', and cost estimation. The script sets execution parameters including model strength, temperature, and verbosity before invoking 'auto_include'. This function processes the input prompt against a specified directory path of Python files and the existing dependency CSV. Finally, the script outputs the identified dependencies, the updated CSV content, the total generation cost in dollars per million tokens, and the name of the LLM model utilized. It serves as an integration example for automated code analysis and prompt-based dependency management.",2026-01-06T21:04:16.712637+00:00
"context/auto_update_example.py","The provided Python script serves as a demonstration of the auto_update function from the pdd.auto_update module. This utility is designed to streamline the process of keeping Python packages current by automating version checks and upgrade procedures. The script outlines a clear workflow: first, it identifies the currently installed version of a specified package; second, it retrieves the latest version information, typically from the Python Package Index (PyPI) or a manually provided version string; third, if a discrepancy is found indicating a newer version is available, the function prompts the user for confirmation to proceed with an upgrade. Upon user approval, the function executes the update using the pip package manager. The code provides three distinct usage examples within its main function: a basic call targeting the 'pdd' package itself, a targeted check for the 'requests' library, and a version-specific check for 'pandas' against a hardcoded version number. This script is particularly useful for developers who want to ensure their software environments remain up-to-date with minimal manual intervention, providing a silent operation if the package is already at the latest version.",2026-01-06T21:04:21.531136+00:00
"context/autotokenizer_example.py","The provided Python script demonstrates how to calculate the number of tokens in a given text string using the Hugging Face Transformers library. Specifically, it utilizes the AutoTokenizer class to load a pre-trained tokenizer associated with a specific model, in this case, the DeepSeek Coder 7B Instruct v1.5. The script defines a core function named count_tokens, which accepts a text input and an optional model identifier. Within this function, the tokenizer is instantiated with the trust_remote_code parameter enabled to ensure compatibility with custom model architectures. The input text is then processed by the tokenizer to generate a sequence of input IDs, and the function returns the total count of these IDs, representing the token length. The script concludes with a practical example, applying the function to a sample prompt about writing a quicksort algorithm and printing the resulting token count to the console. This utility is particularly useful for developers working with large language models who need to monitor or limit input lengths to stay within specific context window constraints or to estimate API usage costs based on token consumption.",2026-01-06T21:04:40.408996+00:00
"context/bug_main_example.py","This Python script demonstrates the practical application of the `bug_main` function from the `pdd` package to automate unit test generation. The script begins by configuring a Click context object with parameters such as model strength, temperature, and output verbosity. It then programmatically creates a directory named 'output' and populates it with several necessary files: a prompt file defining the logic for summing even numbers, a Python source file containing the initial implementation, a main program file to execute the logic, and text files representing both the current (incorrect) and desired (correct) program outputs. By passing these file paths to `bug_main`, the script triggers an AI-driven process to analyze the discrepancy between the observed and expected behavior. The function returns a generated unit test, the associated API cost, and the specific model used. Finally, the script utilizes the `rich` library to print these results to the console in a formatted manner. This serves as a comprehensive example of how developers can use the `pdd` library to identify bugs and generate regression tests based on specific output requirements.",2026-01-06T21:04:56.891909+00:00
"context/bug_to_unit_test_example.py","This Python script serves as a demonstration and entry point for the `bug_to_unit_test` function within the `pdd` package. The script initializes a `main` function that sets up the necessary parameters to automatically generate a unit test based on observed versus desired program outputs. It defines `current_output` (containing error logs and trace information) and `desired_output` (showing the expected successful trace and cost metrics). The script dynamically loads the prompt used to generate the original code, the source code under test, and the example program used to execute that code from local files located in the `prompts/`, `pdd/`, and `context/` directories respectively. Using the `rich` library for formatted console output, it calls `bug_to_unit_test` with specified parameters like model strength, temperature, and language. Upon successful execution, it prints the generated unit test, the total API cost, and the specific LLM model used. The script includes error handling to catch and display exceptions during the test generation process, providing a robust example of how to leverage the `pdd` library for automated debugging and test creation.",2026-01-06T21:05:01.292486+00:00
"context/bug_to_unit_test_failure_example.py","The provided code snippet defines a Python function named 'add' that takes two parameters, 'x' and 'y'. Despite its name, the function performs a subtraction operation, returning the result of 'x' minus 'y'. This discrepancy between the function's identifier and its actual behavior is a notable characteristic of the code, potentially indicating a logic error or a placeholder implementation. The function is written using standard Python syntax, utilizing the 'def' keyword for definition and the 'return' keyword to output the result of the arithmetic expression. It is a minimal example of a function definition, lacking additional features such as documentation strings, type annotations, or error handling. In a practical programming scenario, such a function would be used to encapsulate a specific mathematical operation, though the naming would typically be expected to align with the logic performed. The simplicity of the code makes it easy to analyze, highlighting the fundamental structure of a function while simultaneously serving as a cautionary example of misleading naming conventions in software development.",2026-01-06T21:05:06.023984+00:00
"context/change/16/fix_code_module_errors_example.py","This Python script demonstrates the usage of the 'fix_code_module_errors' function from the 'pdd' library to automatically debug and correct code. The 'main' function defines a scenario where a 'calculate_sum' function incorrectly attempts to sum a string instead of a list of numbers, resulting in a TypeError. The script provides the original buggy program, the initial prompt used to generate it, the specific code module, and the resulting error message as inputs to the fixing utility. By calling 'fix_code_module_errors' with parameters for model strength and temperature, the script retrieves a corrected version of both the full program and the individual code module. Finally, the script outputs boolean flags indicating if updates were necessary, the fixed code blocks, the total API cost incurred for the correction, and the name of the model utilized. This serves as a practical example of using automated tools to resolve runtime exceptions in Python code.",2026-01-06T21:05:13.559711+00:00
"context/change/16/fix_errors_from_unit_tests_example.py","The provided Python script serves as a demonstration or entry point for the fix_errors_from_unit_tests utility, likely part of a larger package named pdd. The script's primary purpose is to showcase how to programmatically resolve discrepancies between source code and unit tests using an LLM-based approach. It initializes several variables representing a typical debugging scenario: a Python function for addition, a corresponding unit test containing a deliberate assertion error, a natural language prompt describing the intended functionality, and a specific error message. These inputs, along with configuration parameters like LLM strength and temperature, are passed to the core function. The script then captures the output, which includes boolean flags indicating whether the code or tests were updated, the corrected versions of the code and tests, the total API cost incurred, and the specific model utilized. Finally, it utilizes the rich library to output these results to the console in a formatted manner. This script acts as a practical example of automated error correction workflows, highlighting the integration of unit testing, error logging, and large language models to streamline the software development and debugging process.",2026-01-06T21:05:17.861253+00:00
"context/change_example.py","This Python script serves as a demonstration for the change function within the pdd.change library. It begins by importing necessary modules, including os for environment management and rich.console for enhanced terminal output. The core logic resides in the main function, which initializes several variables to simulate a code modification request. Specifically, it defines an initial prompt for a factorial function, the corresponding Python code for that function, and a modification prompt asking to incorporate a square root calculation into the result. The script also sets execution parameters such as strength and temperature to control the behavior of the underlying large language model. Upon execution, the change function is invoked with these inputs, returning a modified prompt, the total financial cost of the operation, and the name of the model used. The results are then displayed in a formatted manner using the rich library. Error handling is implemented via a try-except block to catch and report any issues during the process. Overall, the file acts as a practical example of how to programmatically request code alterations and track associated metadata like cost and model identity.",2026-01-06T21:05:25.921576+00:00
"context/change_main_example.py","This Python script serves as a demonstration and example usage of the 'change_main' function from the 'pdd' command-line utility. It illustrates how to programmatically interact with the tool's logic to modify prompts using Large Language Models (LLMs). The script begins by configuring a Click context object with global parameters such as LLM strength, temperature, language settings, and a budget limit. It then demonstrates two primary operational modes: Single-Change and CSV Batch-Change. In Single-Change mode, the script creates a sample prompt and code file, then calls 'change_main' to apply a specific modification, outputting the resulting prompt and associated costs. In CSV Batch-Change mode, it sets up a directory of multiple code and prompt files along with a CSV file containing batch instructions. It then executes 'change_main' with the 'use_csv' flag enabled to process multiple files simultaneously. Throughout the execution, the script uses the 'rich' library to provide formatted console output, displaying the modified prompts, total financial cost of the API calls, and the specific model used for the operations.",2026-01-06T21:05:41.358415+00:00
"context/cli_example.py","The provided Python script, `demo_run_pdd_cli.py`, serves as a minimal, self-contained demonstration of how to programmatically invoke the PDD (Prompt-Driven Development) Command Line Interface using the Click library's CliRunner. The script's primary objective is to automate the generation of Python code from a prompt file without requiring manual terminal interaction or network access. It begins by creating a directory named 'output' and writing a simple prompt file, 'hello_python.prompt', which instructs an AI to write a basic 'hello()' function. Subsequently, the script imports the CLI entry point from the 'pdd' package and executes the 'generate' command. By utilizing the '--local' flag, the script ensures the use of a local model rather than contacting cloud services, while the '--quiet' flag suppresses progress bars for cleaner output. After execution, the script captures and prints the CLI's exit code and console output. Finally, it verifies the creation of the resulting 'hello.py' file, printing its absolute path and the first few lines of the generated code to confirm a successful run. This demo is a practical example for developers looking to integrate PDD's code generation capabilities into larger automated workflows or testing suites.",2026-01-06T21:05:45.840027+00:00
"context/click_example.py","This Python script implements a command-line interface (CLI) for image processing using the Click library and the Pillow (PIL) image library. It utilizes Click's command chaining feature to create a pipeline-like workflow, where multiple image manipulation commands can be executed sequentially on a stream of images. The script defines several core decorators, such as `@processor` and `@generator`, to manage the flow of image objects through the command chain. Key functionalities include opening images from files or standard input, saving processed images with formatted filenames, and displaying them in a viewer. The tool provides a variety of image manipulation commands, including resizing, cropping, transposing (rotating and flipping), blurring, smoothening, embossing, and sharpening. Additionally, it includes a 'paste' command to overlay one image onto another. Each command is implemented as a subcommand that yields processed image objects, allowing for complex transformations to be performed in a single execution string, similar to a Unix pipe.",2026-01-06T21:05:50.961494+00:00
"context/cmd_test_main_example.py","This Python script serves as a demonstration and example for using the `cmd_test_main` function from the `pdd` package to automatically generate unit tests. The script begins by setting up a local environment, creating an 'output' directory, and generating two dummy files: a prompt file describing a simple calculator's requirements and a corresponding Python code file implementing addition and subtraction functions. It then mocks a Click context to simulate command-line execution, configuring parameters such as verbosity, local execution, and force-overwrite. The core of the script involves calling `cmd_test_main` with these inputs, specifying the target language as Python and setting model parameters like strength and temperature. Upon execution, the function generates unit test code, saves it to a specified output file, and returns metadata including the model name and estimated API cost. Finally, the script uses the Rich library to print a summary of the execution to the console, including the cost, the location of the generated test file, and a preview of the resulting test code. This example illustrates how the PDD tool can be integrated into automated workflows for test generation.",2026-01-06T21:05:56.053801+00:00
"context/code_generator_example.py","The provided Python script serves as a demonstration of the code_generator function from the pdd.code_generator module. The primary purpose of the script is to automate the generation of source code by leveraging a language model. The process begins by reading a specific prompt from an external file located at 'prompts/generate_test_python.prompt'. This prompt likely contains instructions for the model, such as creating a Python function. The script configures several parameters to control the generation process, including the target programming language (Python), a model strength of 0.5, a temperature of 0.0 for deterministic output, and a verbosity flag set to true for detailed logging. Within a try-except block to ensure robust error handling, the script invokes the code_generator function. Upon successful execution, it retrieves three key outputs: the generated runnable code, the total financial cost associated with the API call, and the name of the specific model used. Finally, the script prints these results to the console, providing a clear view of the generated logic and the operational metrics of the AI request. This script illustrates a typical workflow for integrating LLM-based code generation into a Python environment.",2026-01-06T21:06:00.729402+00:00
"context/code_generator_main_example.py","This Python script serves as a demonstration and testing suite for the `code_generator_main` function within the `pdd` package. It utilizes the `unittest.mock` library to replace actual Large Language Model (LLM) calls with predictable mock functions, specifically `mock_local_code_generator_func` and `mock_incremental_code_generator_func`. This setup allows for self-contained execution without requiring real API keys or network access. The script explores four primary scenarios: full code generation for a Java program, incremental code generation for a Python function using an explicit original prompt, forced incremental generation where the model might otherwise suggest a full rewrite, and a cloud generation attempt that demonstrates a fallback to local execution when authentication fails. Throughout these scenarios, the script manages file system operations using `pathlib` and `shutil` to create prompts and store generated outputs in a dedicated directory. It also simulates CLI context using a `MockContext` class to pass parameters like temperature and strength. The execution concludes by printing detailed results for each scenario, including the generated code snippets, cost estimates, and the specific model used.",2026-01-06T21:06:18.365088+00:00
"context/commands/__init___example.py","This Python script serves as a comprehensive demonstration of the command registration module for the PDD (Python Development Driver) CLI. It illustrates how to use the `register_commands` function to populate a Click command group with a wide array of subcommands categorized by functionality. The script defines a main CLI group and registers commands across several domains: Generation (e.g., generate, test), Fixing (fix), Modification (e.g., split, update), Maintenance (e.g., sync, setup), Analysis (e.g., detect_change, bug, trace), and Utilities (e.g., verify, install_completion). The file includes multiple examples, such as basic CLI setup, inspecting registered commands via a dictionary mapping, and creating custom CLI groups. The `if __name__ == __main__:` block provides a practical execution path that prints a categorized list of all registered commands along with their truncated help descriptions, demonstrating the total count of commands integrated into the system. This module acts as both a functional entry point and a developer guide for extending or utilizing the PDD command suite.",2026-01-06T21:06:23.587141+00:00
"context/commands/analysis_example.py","This Python script demonstrates the programmatic usage of the 'pdd' library's analysis commands by invoking their Click command callbacks. It establishes a Click context to manage global configurations such as verbosity, model strength, and temperature. The script systematically executes five primary analysis functions: 'detect_change', which identifies necessary prompt updates based on descriptions; 'conflicts', which checks for inconsistencies between prompt files; 'bug', which generates unit tests to reproduce logic errors; 'crash', which analyzes error logs to fix crashing code; and 'trace', which maps execution flow back to specific prompt instructions. For each demonstration, the script manages a local 'output' directory, creating dummy files (prompts, code, and logs) and cleaning previous results to prevent overwrite prompts. By using a context manager to push the Click context to the stack, it ensures that the '@click.pass_context' decorators within the library function correctly. The script serves as a comprehensive integration example for developers looking to automate prompt and code analysis workflows using the pdd framework.",2026-01-06T21:06:28.659663+00:00
"context/commands/fix_example.py","This Python script serves as a comprehensive demonstration of the 'pdd fix' command, a tool designed to automatically repair errors in source code and unit tests using AI. The script establishes a mock environment by creating a directory structure containing a buggy calculator module, failing unit tests, and error logs. It showcases several usage patterns, including basic command invocation, iterative 'loop mode' with verification programs, and budget-constrained fixing. Advanced features are also highlighted, such as agentic fallback (using models like Claude or Gemini for complex issues), automatic submission of successful fixes to a cloud platform, and detailed cost tracking via CSV exports. Additionally, the file provides examples of both CLI-based execution using Click's CliRunner and programmatic integration within Python applications. By providing clear setup routines and modular example functions, the script acts as both a functional test suite and a tutorial for developers looking to automate code maintenance and error resolution within the PDD ecosystem.",2026-01-06T21:06:33.313443+00:00
"context/commands/generate_example.py","This Python script serves as a comprehensive demonstration and documentation for the 'pdd.commands.generate' module, which provides CLI commands for LLM-driven code generation. The file outlines three primary Click-based commands: 'generate', which creates runnable source code from prompt files; 'example', which produces compact usage examples for existing code; and 'test', which generates or enhances unit tests, potentially using coverage reports to identify gaps. The script includes utility functions to set up output directories and create sample prompt/code files for a calculator module. It details the configuration of the Click context object, including parameters for model strength, temperature, and reasoning time. Furthermore, the file demonstrates programmatic invocation of these commands, the use of packaged templates with environment variable substitution, and the integration of the '@track_cost' decorator for monitoring LLM API expenditures. Overall, it acts as both a functional test suite and a guide for developers using the PDD CLI to automate software development tasks.",2026-01-06T21:06:36.182690+00:00
"context/commands/maintenance_example.py","This Python script serves as a comprehensive demonstration of the maintenance commands module within the PDD package. It showcases three primary Click-based CLI commands: 'sync', 'auto-deps', and 'setup'. The 'sync' command is used to synchronize prompt files with generated code and tests, including functional verification and coverage targets. The 'auto-deps' command analyzes prompt files to automatically identify and inject necessary dependencies from a specified directory. The 'setup' command provides an interactive utility for configuring API keys, shell completions, and configuration files. The script utilizes the Click testing framework's 'CliRunner' and 'unittest.mock' to simulate command execution without making actual LLM calls or system changes. It includes helper functions to create mock project structures, such as sample prompts and source files, and demonstrates both CLI-style invocation and programmatic usage of the command callbacks. By providing detailed examples of parameter handling and context management, the file acts as a developer guide for integrating and testing PDD maintenance operations.",2026-01-06T21:06:41.206819+00:00
"context/commands/misc_example.py","This Python script serves as a comprehensive demonstration of the 'preprocess' command within the pdd.commands.misc module, part of the PDD (Prompt Driven Development) toolkit. The script illustrates how to prepare prompt files for Large Language Models (LLMs) through various local operations that do not incur API costs. Key functionalities showcased include the resolution of <include> tags for external file embedding, the removal of <pdd> comment blocks, and the execution of <shell> or <web> directives. The code provides five distinct examples using the Click library's CliRunner to simulate command-line usage. These examples cover basic preprocessing, the insertion of XML delimiters for structural clarity, recursive processing of nested includes, and the doubling of curly brackets for template system compatibility (with specific key exclusions). By setting up a mock environment and generating sample input files in an output directory, the script provides a clear tutorial on transforming raw prompt templates into finalized strings ready for model consumption, returning a standardized cost tuple of (processed_prompt, 0.0, 'local') to maintain compatibility with the broader PDD ecosystem.",2026-01-06T21:06:45.882224+00:00
"context/commands/modify_example.py","This Python script serves as a comprehensive demonstration of the 'modify' module within the pdd CLI package, specifically showcasing the split, change, and update commands. The script is structured to provide a hands-on tutorial by first setting up a local directory environment and generating sample data, including prompt files, source code, and usage examples. The 'split' command example illustrates how to break down large prompt files into modular sub-prompts. The 'change' command demonstration shows how to modify existing prompts based on specific natural language instructions, including a batch processing mode using CSV files. Finally, the 'update' command example demonstrates synchronizing a prompt file with manual changes made to its corresponding source code by comparing original and modified versions. Each command is executed using the Click-based CLI runner and utilizes a cost-tracking decorator to monitor LLM usage. The script concludes by executing all examples sequentially and directing the user to the generated output files in the './output/' directory, providing a clear template for developers to integrate these automated prompt engineering tools into their workflows.",2026-01-06T21:06:48.658774+00:00
"context/commands/templates_example.py","This Python script serves as a comprehensive example and demonstration of the `pdd.commands.templates` module, which provides a CLI interface for managing PDD templates. The script utilizes the `click.testing.CliRunner` to programmatically simulate command-line interactions for three primary subcommands: `list`, `show`, and `copy`. The `list` command example demonstrates how to retrieve available templates in both text and JSON formats, including the use of tag-based filtering. The `show` command example illustrates how to display detailed metadata for a specific template, such as its version, variables, usage examples, and output schemas, rendered via Rich-formatted tables. Finally, the `copy` command example shows how to export a template file to a local directory for customization. Overall, the file acts as both a functional test and a guide for developers using the PDD CLI framework to discover, inspect, and deploy reusable prompt templates containing YAML front matter.",2026-01-06T21:06:51.439503+00:00
"context/commands/utility_example.py","This Python script serves as a comprehensive demonstration of the utility commands within the PDD (Program Design and Development) CLI tool, specifically focusing on the 'install_completion_cmd' and 'verify' commands. The script provides a practical walkthrough by setting up a mock environment in an './output' directory, where it generates a prompt file, a buggy Python calculator module, and a corresponding verification script. It explains the 'install_completion_cmd' functionality for setting up shell completion across various shells (bash, zsh, fish) without executing it to protect the user's configuration. The core of the example illustrates the 'verify' command, which uses an LLM to iteratively test and fix code until it passes verification or exhausts a set budget/attempt limit. The script details the command's parameters, CLI invocation patterns, and programmatic return structures, including success status, final code content, total USD cost, and the model used. It concludes by documenting the expected tuple-based return values and providing instructions for further exploration via the CLI help menu.",2026-01-06T21:06:53.895929+00:00
"context/comment_line_example.py","The provided file documents the usage and implementation of a Python function named `comment_line`, which is designed to programmatically comment out lines of code based on specified syntax rules. The function accepts two parameters: `code_line`, the string of code to be modified, and `comment_characters`, which dictates the commenting style. The documentation details three modes of operation for `comment_characters`: passing 'del' returns an empty string (effectively deleting the line); passing a string with a space (e.g., '<!-- -->') treats the input as start and end delimiters for block-style comments; and passing a single string (e.g., '#') applies a standard single-line comment prefix. The file includes the source code for the function itself, followed by practical examples demonstrating its application for Python-style comments, HTML-style comments, and line deletion. Finally, it provides a concise breakdown of the input parameters and the expected string output.",2026-01-07T18:42:53.556862+00:00
"context/config_example.py","The provided code snippet is a Python script fragment focused on the critical initial setup phase of an application. It demonstrates the prioritized execution of configuration logic, which is a common architectural pattern in software development. Specifically, the script imports the init_config function from a local utility module named utils.config. Immediately following the import, the function is invoked to prepare the application's environment. The presence of the comment '# Initialize configuration FIRST' underscores the importance of execution order, suggesting that subsequent components of the system likely depend on the global state or settings established by this call. This pattern is frequently used to load environment variables, parse configuration files like YAML or JSON, or set up logging and database connections before the rest of the application logic proceeds. By centralizing this logic and ensuring it runs at the very beginning, the developers maintain a predictable and stable runtime environment. This prevents errors related to missing configurations or uninitialized variables that might occur if other modules were loaded prematurely. In summary, the file serves as a foundational entry point or a mandatory preamble for a larger system, ensuring that all necessary parameters are correctly configured before any functional logic is executed.",2026-01-06T21:07:01.190931+00:00
"context/conflicts_in_prompts_example.py","This Python script serves as a demonstration for the `conflicts_in_prompts` function within the PDD Cloud project. The code defines a `main` function that initializes two complex LLM prompts: one for creating a Firebase authentication helper module (`auth_helpers.py`) and another for defining a user data model class (`User`) using Python dataclasses and Firestore. These prompts include detailed requirements regarding functionality, dependencies, error handling, and security practices. The script then executes the conflict detection logic by passing these prompts to the `conflicts_in_prompts` function with specific parameters for model strength, temperature, and verbosity. Finally, it utilizes the `rich` library to print a formatted report of the results, including the model used, the total API cost, and any suggested changes to resolve identified conflicts between the two prompts. The script is structured to be executed as a standalone module.",2026-01-06T21:07:18.166675+00:00
"context/conflicts_main_example.py","The provided Python script serves as a functional demonstration and test harness for the conflicts_main function, which is imported from the pdd.conflicts_main module. The script's primary purpose is to illustrate how to programmatically analyze potential conflicts between two different AI prompts. It begins by programmatically creating two example prompt files, prompt1_LLM.prompt and prompt2_LLM.prompt, each containing distinct instructions for an AI assistant. To simulate a command-line environment, the script defines a MockContext class that mimics a Click context object, storing configuration settings such as force, quiet, strength, and temperature. After initializing this context and defining an output path for a CSV file, the script executes the conflicts_main function. This function processes the input prompts and returns critical data including the detected conflicts, the total financial cost of the operation, and the specific model name used for the analysis. The script concludes by printing these results to the console and noting that the detailed findings are exported to a CSV file. This code effectively showcases the setup, execution, and output handling required to utilize the conflict detection utility within a larger software system.",2026-01-06T21:07:22.931231+00:00
"context/construct_paths_example.py","The provided Python script, `demo_construct_paths.py`, serves as a concise end-to-end demonstration of the `pdd.construct_paths.construct_paths` function within the Prompt-Driven Development (PDD) framework. The script follows a structured five-step process to illustrate how the PDD CLI handles file paths and content resolution. First, it creates a temporary toy prompt file named `Makefile_makefile.prompt` containing a simple task description. Second, it prepares a set of arguments—including input file paths, force flags, and command options—that mimic those typically supplied by the command-line interface. Third, it invokes the `construct_paths` helper function, which is the core logic used by the CLI to process these inputs. Fourth, the script prints the resulting data structures: `input_strings` (the read file contents), `output_file_paths` (the designated write locations for PDD results), and the detected `language`. Finally, the script performs a cleanup operation by deleting the temporary prompt file to maintain a clean directory. This demo effectively showcases the internal path construction and file reading mechanisms that underpin the PDD tool's generation and transformation commands.",2026-01-06T21:07:40.826305+00:00
"context/context_generator_example.py","The provided Python script demonstrates the usage of the context_generator function from the pdd.context_generator module. It begins by importing necessary libraries, including os for environment variable management and rich for enhanced console output. The script enforces a prerequisite by checking for the PDD_PATH environment variable, raising a ValueError if it is missing. It then initializes several input parameters: a code_module containing a basic addition function, a descriptive prompt, the target programming language (Python), and configuration settings like strength and temperature. The core logic involves calling context_generator with these arguments to produce an example code snippet, calculate the associated API cost, and identify the specific AI model utilized. Finally, the script utilizes the rich library to display the generated code, the total cost formatted to six decimal places, and the model name in a visually structured format. This script serves as a practical implementation example for generating context-aware code snippets using the PDD framework, highlighting the integration of environment configuration, function invocation, and formatted result reporting.",2026-01-06T21:07:43.357676+00:00
"context/context_generator_main_example.py","This Python script serves as a demonstration and example of how to utilize the 'context_generator_main' function from the 'pdd' package. The script begins by setting up a local directory structure and creating dummy input files, specifically a '.prompt' file containing task instructions and a '.py' file containing source code. It then mocks a Click Context object to simulate command-line interface parameters such as model strength, temperature, and verbosity. The core of the script executes the 'context_generator_main' wrapper, which processes the input files to generate example code using an AI model. Finally, the script outputs the generation results, including the model name, the total cost of the operation in USD, the file path where the generated code was saved, and a snippet of the resulting code. This file acts as a practical guide for developers to integrate the context generation logic into their own workflows, highlighting the necessary inputs, expected outputs, and environment configurations required for the underlying LLM operations.",2026-01-06T21:07:59.023018+00:00
"context/continue_generation_example.py","This Python script serves as a demonstration and execution entry point for the 'continue_generation' function from the 'pdd' package. The script's primary purpose is to resume text generation from a previously unfinished language model output. It begins by reading a preprocessed prompt from 'context/cli_python_preprocessed.prompt' and an incomplete output fragment from 'context/llm_output_fragment.txt'. These inputs, along with specific model parameters such as a strength of 0.915 and a temperature of 0, are passed to the 'continue_generation' function. Upon execution, the script calculates the total cost of the operation and identifies the model used. The resulting completed text is then saved to 'context/final_llm_output.py'. The code includes error handling for file-related issues and general exceptions, ensuring robust execution. It provides a clear example of how to integrate automated text continuation workflows, specifically tailored for Python code generation or processing, while tracking resource usage and outputting the final results to the local file system.",2026-01-06T21:08:03.733645+00:00
"context/core/cli_example.py","This Python script serves as a comprehensive demonstration of the PDD CLI module, showcasing how to programmatically interact with the PDD command-line interface using the Click library's testing utilities. The file highlights the core functionality of the PDDCLI class, a custom Click Group designed with enhanced help formatting and centralized error handling. It provides detailed examples of global options that control AI model behavior, such as --strength, --temperature, and --time, as well as operational flags like --verbose, --quiet, and --force. Additionally, the script demonstrates advanced features including cost tracking via CSV output, context management through .pddrc configuration files, and the generation of debug bundles using the --core-dump flag. By invoking various example functions—such as example_show_help, example_cost_tracking_setup, and example_pddcli_class_usage—the script illustrates how the CLI manages command execution, suppresses output in quiet mode, and organizes related commands like generate, test, and example into a 'Generate Suite.' This serves as both a functional test suite and a developer guide for integrating and configuring the PDD CLI within a larger project ecosystem.",2026-01-06T21:08:15.090531+00:00
"context/core/cloud_example.py","This Python script serves as a comprehensive example and demonstration of the `CloudConfig` module within the PDD CLI project. It illustrates how to manage centralized cloud configurations, specifically focusing on four key areas: URL configuration, feature availability checks, authentication token retrieval, and error handling. The script demonstrates how to retrieve default base URLs and specific endpoints, as well as how to override these defaults using environment variables for local development or emulation. It also shows how the system determines if cloud features are enabled based on the presence of required API keys like Firebase and GitHub. Furthermore, the example covers JWT token management, showcasing both the high-priority environment variable injection used in CI/CD pipelines and the interactive device flow fallback for end-users. To ensure the script is runnable and safe, it utilizes the `unittest.mock` library to simulate environment states and asynchronous network calls. This file acts as both a developer guide and a functional test suite for the `CloudConfig` class, ensuring that the CLI can correctly interact with cloud services under various operational scenarios.",2026-01-06T21:08:20.201452+00:00
"context/core/dump_example.py","This Python script serves as a comprehensive demonstration of the 'pdd.core.dump' module, which is designed for generating and managing core dumps within the PDD CLI debugging framework. The file illustrates several critical diagnostic functions: '_write_core_dump' for capturing execution context, errors, and environment data into a JSON file; '_github_config' for verifying environment variables required for issue reporting; and '_write_replay_script' for generating executable shell scripts that reproduce failed commands. Additionally, it showcases '_build_issue_markdown', which constructs detailed GitHub issue reports including system metadata, tracebacks, and truncated raw JSON, and provides a signature overview for '_post_issue_to_github' to automate issue creation. By utilizing mock objects and temporary directories, the script provides a safe environment to test how the CLI handles debugging information, cost tracking for AI models, and automated error reporting. The output is organized into a local directory, allowing developers to inspect generated core dumps, replay scripts, and markdown templates used for troubleshooting PDD CLI operations.",2026-01-06T21:08:22.605666+00:00
"context/core/errors_example.py","This Python script serves as a comprehensive demonstration of the `pdd.core.errors` module, which provides centralized error handling and rich console output for the PDD CLI. The script showcases several key functionalities, including the use of a custom-themed Rich console for styled messaging (info, warning, success, etc.) and the `handle_error` function for managing exceptions without crashing the application. It illustrates how different exception types, such as `FileNotFoundError`, `ValueError`, and `IOError`, can be processed in both verbose and quiet modes. In quiet mode, errors are recorded silently in a buffer, while verbose mode prints them to the console. The script also demonstrates the module's ability to collect error details—including command names, exception types, messages, and tracebacks—for core dump analysis. It concludes by showing how to retrieve these collected errors, export them to a JSON file for debugging, clear the error buffer using `clear_core_dump_errors`, and verify that the error handling mechanism allows the program to continue execution gracefully after a critical failure.",2026-01-06T21:08:25.553274+00:00
"context/core/utils_example.py","This Python script serves as a comprehensive demonstration and documentation for the 'pdd.core.utils' module, which provides essential helper functions for the PDD CLI. The file illustrates several key functionalities through practical examples and mock scenarios. These include '_first_pending_command' for parsing Click context arguments to identify subcommands, and '_api_env_exists' for verifying the presence of API configuration files in the user's home directory. It also demonstrates '_completion_installed', which checks shell RC files for PDD tab-completion markers, and '_project_has_local_configuration', which detects project-specific environment settings. A significant portion of the script is dedicated to '_should_show_onboarding_reminder', explaining the logic used to determine if a new user should be prompted to run the setup utility based on environment variables, existing configurations, and the current command context. Finally, the script provides informational details on '_run_setup_utility', which launches an interactive subprocess for API key configuration and shell integration. Overall, the file acts as both a functional test suite using 'unittest.mock' and a developer guide for maintaining the PDD CLI's onboarding and configuration infrastructure.",2026-01-06T21:08:28.103291+00:00
"context/crash_main_example.py","This Python script demonstrates the usage of the `crash_main` function from the `pdd` package to automatically debug and fix a crashed program. The script sets up a simulated environment by creating a directory containing four key files: a prompt file describing the intended logic, a buggy Python module (`calculator.py`), a main application script that triggers a `TypeError` by passing strings instead of integers, and an error log capturing the traceback. It then initializes a mock Click context to configure execution parameters such as verbosity and local execution. The core logic invokes `crash_main` with specific parameters including model strength, temperature, and a financial budget. Upon execution, the function attempts to resolve the bug and generate corrected versions of both the module and the application script. Finally, the script outputs the results using the `rich` library, displaying the success status, the model used, the total cost, and the content of the fixed files. This serves as a practical example of using automated tools to handle program crashes by analyzing error logs and source code.",2026-01-06T21:08:34.057071+00:00
"context/detect_change_example.py","This Python script utilizes the 'pdd.detect_change' module to analyze a specific set of prompt files for potential modifications. The script targets four specific files located in the 'context' and 'prompts' directories: 'python_preamble.prompt', 'change_python.prompt', 'fix_error_loop_python.prompt', and 'code_generator_python.prompt'. The primary objective is to determine how to use 'python_preamble.prompt' to make the other prompts more compact, acknowledging that some may already incorporate this preamble. The script configures Large Language Model (LLM) parameters, setting 'strength' to 1 for maximum model capability and 'temperature' to 0 for deterministic output. Upon execution, the 'detect_change' function processes the files based on the provided change description. The script then uses the 'rich' library to output a formatted report to the console, detailing the detected changes for each prompt, the specific instructions for those changes, the total API cost incurred, and the name of the model used. It also includes error handling to catch and display any exceptions that occur during the detection process.",2026-01-06T21:08:38.966952+00:00
"context/detect_change_main_example.py","This Python script serves as a driver or simulation tool for the `detect_change_main` function, which is part of a larger system likely focused on automated code or prompt modification. The script utilizes the `click` library to mock a command-line interface context, setting specific model parameters such as strength and temperature. It defines a set of input prompt files, including a Python preamble and various code generation prompts, and specifies a change description file that instructs the system to make prompts more compact. The script ensures the necessary output directories exist before executing the change detection logic. Upon execution, it captures the list of required changes, the total API cost, and the model name used. Finally, it outputs these results to the console, detailing the specific instructions for each prompt file processed. This script is essential for testing the change detection workflow and verifying how the model interprets instructions to modify existing prompt templates.",2026-01-06T21:08:41.827535+00:00
"context/edit_file_example.py","This Python script serves as a test runner and demonstration tool for a file editing module named 'edit_file'. It begins by configuring the environment, dynamically adding the parent directory to the system path to import 'run_edit_in_subprocess' from the 'pdd.edit_file' package. The script relies on a 'PDD_PATH' environment variable to locate its working directory. It includes a helper function, 'create_example_files', which generates a dummy text file with initial content in an 'output' subdirectory. The core logic is contained in 'run_edit_file_test', which executes the editing process by passing specific instructions to the subprocess function and then verifies the results. These instructions include replacing words, replacing entire lines, and appending new text. The script performs detailed verification by checking the final file content against expected outcomes, reporting successes or specific failures. Finally, the 'run_example' function orchestrates the setup, execution, and verification phases, providing a clear example of how to programmatically edit files using the associated MCP (Model Context Protocol) text editor service.",2026-01-06T21:08:44.106156+00:00
"context/find_section_example.py","This file provides documentation and usage examples for the `find_section` function located in the `pdd.find_section` module. The primary purpose of this function is to parse a list of strings—typically lines from a text output—to identify and extract code blocks. The documentation details three input parameters: `lines` (the list of strings to search), `start_index` (the starting point for the search, defaulting to 0), and `sub_section` (a boolean flag for sub-section searching). The function returns a list of tuples, each containing the identified programming language, the starting line index, and the ending line index of the code block. The provided Python example demonstrates how to split a multi-line string containing Python and JavaScript snippets and pass it to `find_section`. It also shows how to read from an external file to perform the same extraction. The example output illustrates that the function correctly identifies the language and line boundaries for each code block found within the input text, facilitating automated processing of mixed-content strings.",2026-01-06T21:08:49.103286+00:00
"context/firecrawl_example.py","The provided Python script serves as a basic implementation guide for the Firecrawl library, a tool designed for web scraping and data extraction. The code begins by indicating the necessary installation command, 'pip install firecrawl-py,' ensuring the environment is prepared. It then imports the FirecrawlApp class and the os module to handle environment variables. The script initializes the FirecrawlApp by fetching an API key, prioritizing the 'FIRECRAWL_API_KEY' environment variable while providing a placeholder for manual entry. The core functionality is demonstrated through the 'scrape_url' method, which targets 'https://www.google.com'. By specifying the 'formats' parameter as 'markdown,' the script instructs the library to convert the scraped web content into a structured markdown format. Finally, the script outputs the resulting markdown data to the console using a print statement. This concise example illustrates the ease of integrating Firecrawl into Python projects for automated web data retrieval, highlighting its capability to transform raw HTML into cleaner, more readable formats like markdown, which is particularly useful for LLM training or documentation purposes.",2026-01-06T21:08:53.900195+00:00
"context/fix_code_loop_example.py","This Python script serves as a demonstration for the `fix_code_loop` module, which is designed to automatically repair buggy code using an LLM-driven iterative process. The script begins by setting up the environment and importing necessary utilities, including the `rich` library for console output. It defines a helper function, `create_dummy_files`, which generates three essential components in a temporary directory: a buggy Python file (`calculator.py`) containing a type error, a verification script (`verify_calculator.py`) that tests the buggy function and fails, and a prompt file providing instructions for the fix. The `main` function orchestrates the demonstration by initializing these files and calling `fix_code_loop` with specific parameters such as the target code path, verification command, maximum attempts, and budget constraints. The loop attempts to fix the code until the verification script passes or the maximum attempts are reached. Finally, the script prints the results, including the success status, total attempts, cost incurred, and the corrected code if the repair was successful. This example illustrates an automated agentic workflow for software debugging and quality assurance.",2026-01-06T21:09:09.679983+00:00
"context/fix_code_module_errors_example.py","This Python script demonstrates the usage of the 'fix_code_module_errors' function from the 'pdd' library to automatically debug and correct code errors. The example defines a 'main' function containing a buggy program where a string is passed to a summation function, resulting in a TypeError. It provides the original prompt, the faulty code module, and the specific error message as inputs to the correction function. The 'fix_code_module_errors' function is called with parameters for model strength, temperature, and verbosity. The script then captures several outputs, including boolean flags indicating if updates were necessary, the corrected versions of both the full program and the specific code module, the raw LLM output, the total API cost in USD, and the name of the model utilized. Finally, the script prints these results to the console, showcasing an automated workflow for identifying and fixing software bugs using large language models.",2026-01-06T21:09:12.038864+00:00
"context/fix_error_loop_example.py","This Python script serves as a demonstration for the `pdd.fix_error_loop` module, which automates the process of repairing buggy code using Large Language Models (LLMs) and unit tests. The script initializes a mock environment by creating a directory containing a buggy 'calculator.py' file (which incorrectly subtracts instead of adding), a corresponding failing pytest file, a prompt file for LLM context, and a verification script. It then invokes the `fix_error_loop` function, configuring parameters such as LLM temperature, maximum repair attempts, and a financial budget. The automated loop executes the tests, identifies failures, sends the error logs and code to an LLM for correction, and applies the suggested fixes. Finally, the script outputs the results of the repair process, including whether the fix was successful, the number of attempts taken, the total cost incurred, and the final corrected source code. This example highlights the module's ability to iteratively debug and verify code through an agentic feedback loop.",2026-01-06T21:09:16.417982+00:00
"context/fix_errors.log","The provided file content describes a specific technical failure encountered during a test execution process. Specifically, the system reports a 'FileNotFoundError' (Errno 2), indicating that the application or script was unable to locate a required resource at a designated file path. The missing file is identified as 'language_format.csv', which is expected to reside within the 'data' subdirectory of the 'site-packages' folder in a Miniconda3 environment named 'pdd', running Python 3.12. This error typically suggests a configuration issue, a missing dependency, or an incorrect relative path reference within the codebase. The path points to a local user directory belonging to 'benjaminknobloch'. To resolve this issue, one would need to verify if the 'data' directory and the 'language_format.csv' file were correctly packaged with the library or if the environment was set up properly. It highlights a common problem in software development where hardcoded paths or missing data files in virtual environments lead to runtime exceptions. The error serves as a diagnostic starting point for debugging the installation or the data loading logic of the 'pdd' project.",2026-01-07T02:20:32.540828+00:00
"context/fix_errors_from_unit_tests_example.py","This Python script serves as a demonstration for the 'fix_errors_from_unit_tests' function within the 'pdd' package. The 'main' function initializes several variables to simulate a real-world debugging scenario: a unit test containing an intentional logic error, a code snippet with a deprecated NumPy function call, a descriptive prompt, and a multi-line error message including an AssertionError and a DeprecationWarning. It then executes the 'fix_errors_from_unit_tests' function synchronously, passing these inputs along with configuration parameters like LLM strength and temperature. The script concludes by using the 'rich' library to print a formatted report of the results, which includes boolean flags indicating if the test or code were updated, the actual corrected versions of the code and tests, a detailed analysis of the fixes, the total API cost incurred, and the specific model name used for the operation. This file acts as a practical example of how to automate code and test correction using LLM-driven analysis.",2026-01-06T21:09:18.784353+00:00
"context/fix_main_example.py","This Python script serves as a demonstration and test harness for the `fix_main` function within the `pdd` package, which is designed to automatically repair buggy code using Large Language Models (LLMs). The script begins by setting up a mock environment, creating a directory named 'output' containing a buggy calculator script, a failing unit test, a prompt file for context, a verification script, and an error log. It then simulates a command-line interface environment using the `click` library, configuring a context object with parameters like LLM temperature, execution strength, and local execution flags. The `main` function executes two primary scenarios: 'Loop Mode' and 'Single-Pass Mode'. In Loop Mode, the system iteratively attempts to fix the code by running tests and capturing errors until a verification script passes or the maximum attempts are reached. In Single-Pass Mode, the system performs a single repair attempt based on a pre-existing error log. Throughout the process, the script uses the `rich` library to provide formatted console output, reporting on success status, the number of attempts made, and the financial cost incurred by the LLM API calls.",2026-01-06T21:09:21.255646+00:00
"context/fix_verification_errors_example.py","This Python script serves as a demonstration for the `fix_verification_errors` function within the `pdd` package. The primary purpose of the script is to showcase how Large Language Models (LLMs) can be utilized to automatically identify and rectify bugs in a code module by analyzing verification logs. The example sets up a scenario involving a 'calculator_module' containing an intentional logic error (performing subtraction instead of addition) and a corresponding verification program that flags this discrepancy. The script defines several key inputs for the function, including the original prompt used to generate the module, the buggy source code, the captured verification output (containing a 'VERIFICATION_FAILURE' marker), and parameters for LLM strength and temperature. Upon execution, the `fix_verification_errors` function analyzes the failure logs and attempts to generate corrected versions of both the code module and the verification program. Finally, the script outputs the results of the operation, displaying whether issues were found, the specific updates made to the code, the name of the LLM model employed, and the estimated API cost associated with the process.",2026-01-06T21:09:24.080816+00:00
"context/fix_verification_errors_loop_example.py","This Python script serves as a demonstration for the 'fix_verification_errors_loop' function within the 'pdd' package. The script automates the process of identifying and correcting bugs in source code using an LLM-driven feedback loop. It begins by setting up a temporary environment containing three key components: a buggy 'calculator.py' file that incorrectly performs subtraction instead of addition, a verification script 'verify_calc.py' that tests the function and returns a failure code, and a prompt file defining the intended behavior. The 'main' function executes the fix loop, passing parameters such as maximum attempts, budget constraints, and temperature settings. During execution, the loop runs the verification script, detects the failure, and utilizes an LLM to iteratively rewrite the code until the verification passes or constraints are met. The script uses the 'rich' library for formatted console output, displaying the initial state, the progress of the fix attempts, and the final result, including the corrected code, total cost, and model statistics. This provides a practical example of automated program repair and agentic code correction.",2026-01-06T21:09:26.473807+00:00
"context/fix_verification_main_example.py","This Python script serves as a demonstration and test harness for the 'fix_verification_main' module, which is designed to automatically verify and fix code based on a prompt and a verification program. The script begins by configuring the system path and importing necessary utilities, including 'click' for command-line context and 'rich' for formatted console output. It defines a helper function, 'create_dummy_files', which sets up a temporary environment containing a prompt file, a buggy Python module (a calculator with a subtraction error in an addition function), and a verification script that tests the module. The 'main' function orchestrates the demonstration by initializing a mock Click context with specific settings like verbosity and temperature. It then invokes 'fix_verification_main' to process the dummy files, attempting to identify and correct the bug in a single pass. Finally, the script prints the execution results, including success status, number of attempts, total cost, and the resulting fixed code if the verification was successful. This file acts as an integration example for automated code repair workflows.",2026-01-06T21:09:29.030805+00:00
"context/gemini_may_pro_example.py","The provided Python script demonstrates the implementation of the LiteLLM library to facilitate a connection with Google's Gemini 2.5 Pro model via the Vertex AI platform. LiteLLM serves as a universal interface, allowing developers to call various LLM APIs using a consistent format. In this specific example, the code begins by importing the completion function from the litellm package. It then executes a request by specifying the model identifier vertex_ai/gemini-2.5-pro-preview-05-06. The request includes a messages parameter, which is structured as a list containing a dictionary with the user's role and the prompt content. This structure follows the standard chat completion format used by many modern AI integrations. Once the API call is processed, the resulting response object, which typically contains the generated text and metadata, is stored in the response variable and subsequently printed to the console. This snippet is a concise example of how LiteLLM abstracts the complexities of provider-specific authentication and endpoint management, providing a streamlined workflow for developers looking to leverage advanced generative AI models like Gemini within their Python applications.",2026-01-06T21:09:31.591771+00:00
"context/generate_output_paths_example.py","This Python script serves as a comprehensive demonstration and test suite for the 'generate_output_paths' function, likely part of a larger utility for managing file paths in a development workflow. The script begins by dynamically adjusting the system path to import the target function from a parent directory. It then systematically executes eight distinct scenarios to validate the function's logic across various use cases. These scenarios include testing default path generation for the 'generate' command, handling user-specified filenames and directories, and evaluating the influence of environment variables on output locations for the 'fix' command. Additionally, the script explores mixed user inputs, commands with fixed extension defaults like 'preprocess', and edge cases such as unknown commands or missing basenames. For each scenario, the script prints the inputs, the actual result, and the expected absolute path, providing a clear verification of the function's path resolution and naming conventions. It also includes setup and cleanup logic for temporary directories and environment variables used during testing.",2026-01-06T21:09:48.662484+00:00
"context/generate_test_example.py","This Python script serves as a practical implementation example for the pdd library, specifically demonstrating how to programmatically generate unit tests for existing code. The script begins by importing the necessary modules, including the generate_test function from the pdd.generate_test package and the rich library for stylized terminal output. It defines a set of input variables: a natural language prompt describing a factorial function, the corresponding Python source code for a recursive factorial implementation, and configuration parameters such as strength, temperature, and language. The script then executes the generate_test function within a try-except block to handle potential runtime errors. Upon successful execution, the function returns a generated unit test, the total API cost incurred, and the name of the AI model utilized. These results are printed to the console using rich's formatting capabilities, which highlight the unit test in green, the cost in blue, and the model name in bold. If an error occurs during the process, a red-formatted error message is displayed. Overall, the file provides a clear template for developers to integrate automated, AI-driven unit testing into their Python projects by leveraging the pdd framework and managing environment configurations.",2026-01-06T21:09:53.314825+00:00
"context/get_comment_example.py","The provided text serves as a guide for utilizing the get_comment function within the pdd.get_comment module. This utility is designed to retrieve the specific comment syntax associated with various programming languages. The documentation includes a practical Python example demonstrating how to import and call the function with different language strings, such as Python, Java, and JavaScript. A key feature of the function is its case-insensitivity regarding the input language name. The function's logic depends on an external data source: a CSV file named language_format.csv located in a data subdirectory. To function correctly, the system must have the PDD_PATH environment variable configured to point to the directory containing this data. In terms of error handling, the function is designed to return the string 'del' if the specified language is not found in the database, if the environment variable is unset, or if any other execution error occurs. This ensures a predictable fallback value. The CSV structure requires at least two columns, 'language' and 'comment', to map the names to their respective symbols. Overall, the file provides clear instructions for developers to integrate language-specific comment detection into their applications using this module.",2026-01-06T21:10:08.257125+00:00
"context/get_extension_example.py","The provided Python script serves as a demonstration for a utility module named pdd.get_extension. It specifically imports the get_extension function to showcase its functionality in mapping descriptive language names or file types to their respective file extensions or standard naming conventions. The script includes three distinct print statements that act as test cases or usage examples. In the first example, passing the string Python to the function returns the standard .py extension. The second example demonstrates how the function handles specific build files, where inputting Makefile returns the string Makefile itself, indicating the function can handle non-extension-based filenames. The third example shows that JavaScript is correctly mapped to the .js extension. Overall, the file acts as a concise piece of documentation or a functional test script, illustrating how the get_extension utility simplifies the process of identifying file suffixes based on human-readable language names. This type of utility is commonly used in file processing tasks, syntax highlighting engines, or project initialization tools where file type identification is necessary.",2026-01-06T21:10:26.855559+00:00
"context/get_jwt_token_example.py","This Python script implements an authentication workflow for a CLI application named 'Prompt Driven Development.' It primarily facilitates obtaining a Firebase ID token using the GitHub Device Flow. The script begins by dynamically loading configuration settings, such as the Firebase API key and GitHub Client ID, based on the target environment (local, staging, or production) defined by the 'PDD_ENV' environment variable. It includes a helper function, '_load_firebase_api_key', which searches for keys in various environment files like '.env.local' or '.env.production'. The core logic resides in an asynchronous 'main' function that calls 'get_jwt_token' to perform the authentication. It features robust error handling for various scenarios, including authentication failures, network issues, token exchange errors, and rate limiting. Upon a successful login, the script retrieves the JWT token and automatically updates or appends it to the local '.env' file using both environment-specific (e.g., 'JWT_TOKEN_STAGING') and generic ('JWT_TOKEN') keys. This ensures that subsequent requests by the CLI or associated frontend components can remain authenticated without manual token management.",2026-01-06T21:10:34.687748+00:00
"context/get_language_example.py","This Python script serves as a practical demonstration of the get_language function, which is imported from the pdd.get_language module. The primary objective of the code is to illustrate how a developer can programmatically determine the programming language associated with a specific file extension. In this specific implementation, the script defines a main function that sets a target file extension, specifically '.py', and then invokes the get_language utility to retrieve the corresponding language name. To ensure the program's reliability, the core logic is encapsulated within a try-except block, which allows the script to gracefully handle and report any unexpected errors or exceptions that might arise during execution. If the function successfully identifies a language, the script outputs a clear message to the console; otherwise, it notifies the user that no matching language was found for the provided extension. By utilizing the standard if __name__ == '__main__': boilerplate, the script ensures that the demonstration logic only runs when the file is executed directly, making it a clean and reusable example of integrating language detection features into a larger Python project.",2026-01-06T21:10:37.538180+00:00
"context/get_run_command_example.py","This Python script serves as a comprehensive demonstration and documentation for the `get_run_command` module, which is part of the `pdd` package. The module's primary purpose is to retrieve executable command templates for various programming languages based on their file extensions. It relies on a CSV configuration file located at `$PDD_PATH/data/language_format.csv`, which maps extensions to command templates containing a `{file}` placeholder. The script showcases several key functionalities: retrieving a raw command template for a specific extension (e.g., '.py'), demonstrating the module's ability to normalize input by handling missing dots or uppercase characters, and generating a fully formatted execution command for a specific file path. Additionally, the examples illustrate how the system handles edge cases, such as unknown file extensions or files lacking extensions entirely, by returning empty strings. By iterating through a list of common extensions like .js, .rb, and .sh, the script provides a clear overview of how developers can programmatically determine the correct way to execute different source files within the PDD environment.",2026-01-06T21:10:55.462032+00:00
"context/get_test_command_example.py","This Python script serves as a comprehensive demonstration of the `get_test_command` module, specifically showcasing the functionality of the `get_test_command_for_file()` function. The script illustrates a layered resolution strategy for determining the appropriate test command for various file types, which includes checking a CSV configuration, utilizing smart detection (such as identifying pytest for Python or npm for JavaScript), and returning a null result to signal the need for an agentic fallback. Through several dedicated functions—`demonstrate_python_test`, `demonstrate_javascript_test`, `demonstrate_language_override`, `demonstrate_multiple_files`, and `demonstrate_handling_none_result`—the module provides clear examples of how to handle different programming languages, override automatic detection, process batches of files, and manage scenarios where no command is found. The script utilizes the `rich` library to produce formatted, color-coded console output, including panels and tables, to clearly present the resolution results and status for each test case. It serves as both a functional test and a documentation guide for developers integrating the test command resolution logic into their own tools.",2026-01-06T21:10:58.124547+00:00
"context/git_update_example.py","This Python script demonstrates the usage of the `git_update` function from the `pdd` package to automate prompt and code updates. The script targets a specific prompt file and its corresponding modified Python code. It utilizes a routing logic that chooses between an AI-powered agentic update path or a legacy update path based on the provided parameters. The process involves reading the input prompt, defining LLM parameters like strength and temperature, and executing `git_update`. This function manages the restoration of original code from Git HEAD, performs the update, and then restores the modified code. If successful, the script outputs the modified prompt, the total API cost, and the model used. It also includes error handling for input and unexpected runtime issues, ensuring that the modified prompt is saved back to the source file if the legacy path is utilized.",2026-01-06T21:11:00.836036+00:00
"context/increase_tests_example.py","This Python script provides a practical demonstration of the increase_tests function from the pdd library, which is designed to automate the generation of comprehensive unit tests. The code defines an example_usage function that sets up a scenario involving a simple calculate_average function, its existing unit tests, a coverage report showing 60% coverage, and the original prompt used to generate the code. The script showcases two primary ways to utilize the increase_tests utility: a basic implementation using standard arguments and an advanced implementation that fine-tunes parameters such as language, strength, temperature, and verbosity. Upon execution, the function returns the newly generated test cases, the total API cost incurred, and the specific model used for the task. The script also includes robust error handling to manage ValueErrors and other unexpected exceptions during the test generation process. By providing a clear template for integrating AI-driven test expansion, this file serves as a guide for developers looking to improve code coverage and software reliability through automated test suite enhancement. The example concludes by printing the results of the generation process, highlighting the tool's ability to bridge gaps in existing testing frameworks.",2026-01-06T21:11:11.129230+00:00
"context/incremental_code_generator_example.py","This Python script serves as a demonstration for the `incremental_code_generator` function within the `pdd` package. It illustrates a workflow for updating existing source code based on changes to a natural language prompt. In this specific example, the script attempts to update a simple factorial function to include input validation. The process involves several parameters, including the original and new prompts, the existing code, the target language, and various LLM configuration settings such as strength, temperature, and time budget. The script utilizes the `rich` library to provide a formatted console output, indicating whether the update was achieved through an incremental patch or if a full regeneration was required. Additionally, it reports the total cost incurred during the operation and identifies the specific language model used. This utility is designed to minimize unnecessary code rewrites by applying structured, minimal diffs when possible, thereby maintaining code stability and reducing computational overhead during iterative development tasks.",2026-01-06T21:11:25.911965+00:00
"context/insert_includes_example.py","This Python script demonstrates the practical application of the `insert_includes` module from the `pdd` package. The script defines an `example_usage` function that sets up a scenario for processing code dependencies within a project. It begins by initializing a `rich` console for formatted output and defining a sample input prompt that requests a Python function for CSV processing. The core logic involves calling `insert_includes` with specific parameters, including a directory path for context files, a target CSV filename for dependency tracking, and configuration settings like strength and temperature to control the model's output behavior. The script includes robust error handling for file-related issues and general exceptions. Upon successful execution, it prints the original and modified prompts, the model name, and the calculated processing cost to the console. Finally, it persists the generated dependency information by writing the CSV output to a local file. This example serves as a template for developers looking to automate the inclusion of relevant code context and dependency management when interacting with large language models for code generation tasks.",2026-01-06T21:11:28.333990+00:00
"context/install_completion_example.py","This Python script serves as a demonstration for the PDD shell completion installation module. It showcases how to programmatically set up shell completion for the PDD CLI by utilizing functions from the pdd.install_completion package. The script includes a setup_example_environment function that safely simulates a real-world environment by creating dummy directories for the user's home and PDD path. It specifically forces the SHELL environment variable to bash, creates a mock .bashrc file, and generates a dummy completion script (pdd_completion.sh). The main execution flow involves retrieving the local PDD path via get_local_pdd_path and then calling install_completion to automate the process of appending the necessary source command to the shell's configuration file. Throughout the process, the script uses the Rich library to provide color-coded console feedback, ensuring the user can track the environment setup and installation steps. This example is designed to be run safely without modifying the user's actual system configuration files, providing a clear template for how the PDD package handles shell integration and path resolution.",2026-01-06T21:11:32.772633+00:00
"context/langchain_lcel_example.py","This Python script demonstrates the implementation of various Large Language Model (LLM) integrations using the LangChain framework. It showcases the use of LangChain Expression Language (LCEL) to build chains that connect prompts, models, and output parsers. The code features a wide array of providers, including OpenAI, Google (Generative AI and VertexAI), Azure, Anthropic, DeepSeek, Fireworks, Groq, Together AI, and AWS Bedrock. Additionally, it includes local execution examples using Ollama and MLX for Apple Silicon. A key component of the script is the 'CompletionStatusHandler', a custom callback handler that tracks execution status, finish reasons, and token usage metadata. The script also illustrates advanced LangChain features such as structured output using Pydantic and JsonOutputParsers, model fallbacks, and configurable alternatives. Practical examples range from simple joke generation to complex tasks like calculating cube roots with Claude's thinking mode or generating Python code. Finally, the script implements performance and cost-saving measures by setting up a local SQLite cache for LLM responses.",2026-01-06T21:11:37.703236+00:00
"context/llm_invoke_example.py","This Python script demonstrates the usage of the `llm_invoke` utility for interacting with various Large Language Models (LLMs) based on a 'strength' parameter. It begins by defining a Pydantic model, `Joke`, to facilitate structured data extraction. The core logic includes a helper function, `calculate_model_ranges`, which iterates through strength values from 0.0 to 1.0 to identify the specific ranges assigned to different models within the system's configuration. The `main` function executes a demonstration workflow: it first calculates and prints these model strength ranges, then iterates through each identified model. For every model, it performs two types of calls at the range's midpoint: an unstructured request asking for a joke about programmers and a structured request using the Pydantic model to generate a joke about data scientists. The script outputs the model name, the generated content, and the associated cost for each invocation, showcasing how the library handles dynamic model selection, prompt formatting, and response parsing.",2026-01-06T21:11:42.971543+00:00
"context/llm_selector_example.py","The provided Python script serves as a demonstration for the llm_selector utility, which is imported from the pdd.llm_selector module. The core functionality revolves around a main function that iterates through a range of model strength values, starting from 0.5 and increasing by increments of 0.05 until it reaches 1.1. For each iteration, the script calls the llm_selector function, passing in the current strength and a fixed temperature of 1.0. This function returns several key components: the LLM instance itself, a specialized token counter function, the input and output costs per million tokens, and the specific model name selected based on the input parameters. The script then outputs these details to the console, providing transparency into which model is chosen for a given strength level and its associated costs. Additionally, it demonstrates the practical application of the returned token_counter by calculating the token count for a sample text string. To ensure robustness, the execution is wrapped in a try-except block to handle potential FileNotFoundError or ValueError exceptions that might arise during the selection process. This script is a clear example of how to dynamically select and evaluate different large language models based on performance requirements and cost considerations.",2026-01-06T21:11:47.508726+00:00
"context/load_prompt_template_example.py","The provided Python script is a concise utility designed to load and display a specific prompt template, likely for use in Large Language Model (LLM) workflows. It begins by importing the load_prompt_template function from the pdd.load_prompt_template module and the print function from the rich library to enhance terminal output with color and formatting. The core logic resides within the main function, where a variable named prompt_name is initialized with the string 'generate_test_LLM', representing the target template file without its file extension. The script then attempts to fetch the template content by calling load_prompt_template with this identifier. Upon a successful retrieval, the script utilizes the rich library's capabilities to print a stylized blue header labeled 'Loaded Prompt Template:' followed by the actual content of the prompt. This script serves as a straightforward demonstration or testing tool for verifying that prompt templates are correctly stored and accessible within the project's directory structure. By leveraging the rich library, it ensures that the output is visually distinct and readable for developers during the debugging or development process. The execution is controlled by a standard Python entry point, ensuring the main function runs only when the script is executed directly.",2026-01-06T21:12:04.371335+00:00
"context/logo_animation_example.py","This Python script serves as a demonstration for the PDD branding animation module. It utilizes the pdd.logo_animation package to display a dynamic logo sequence within a terminal environment. The script begins by invoking start_logo_animation(), which launches a background thread to handle the visual rendering. The animation sequence is composed of three distinct phases: a logo formation period lasting 1.5 seconds, a hold duration of 1.0 second, and a transition to a box state taking 1.5 seconds, resulting in a total sequence time of approximately 4 seconds. To ensure the full animation and its final state are visible to the user, the main program executes a sleep loop for 7 seconds. The script highlights that because the animation utilizes full-screen terminal control, standard print statements issued during the animation's runtime may not be visible until the terminal is restored. Once the viewing period concludes, the script calls stop_logo_animation(), which terminates the background thread and returns control of the terminal to the main program. This example effectively illustrates how to integrate and manage background visual processes within a command-line interface while maintaining the flow of the primary application logic.",2026-01-06T21:12:26.907911+00:00
"context/output/prompts/calculator_python.prompt","The provided text contains a specific instruction or prompt directed toward the creation of a Python function intended to perform the arithmetic operation of adding two numbers together. This request serves as a fundamental programming task, typically encountered in introductory software development contexts or as a basic component of larger mathematical computations. The core objective is to define a reusable block of code that accepts two distinct numerical inputs as parameters and subsequently returns their calculated sum. Such a task highlights the basic syntax and structural requirements of the Python language, including the use of the 'def' keyword for function definition, the specification of arguments, and the implementation of the return statement to output the result. While the input itself is brief, it encapsulates a primary concept in functional programming: the encapsulation of logic into discrete, callable units to enhance code modularity and readability. This specific example focuses on binary addition, one of the most basic yet essential operations in any computational environment, providing a clear and direct goal for a developer or an automated code generation system to execute.",2026-01-07T02:20:33.073086+00:00
"context/postprocess_0_example.py","This file provides a concise guide and code example for utilizing the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The primary purpose of this function is to parse raw output from a Large Language Model (LLM) that contains a mix of natural language text and multiple code blocks in various programming languages. The function takes two inputs: `llm_output`, a string representing the raw model response, and `language`, a string specifying the target programming language (e.g., 'python'). The function processes this input to identify the largest code section matching the specified language. It then returns a modified string where this primary code block remains uncommented, while all other text and code sections in different languages are automatically commented out using the appropriate syntax for the target language. The documentation notes that the function relies on internal utilities like `get_comment`, `comment_line`, and `find_section` to operate correctly. The provided Python script demonstrates a practical implementation, showing how a multi-block LLM response containing Python and Java is transformed to isolate the Python logic.",2026-01-06T21:12:54.980613+00:00
"context/postprocess_example.py","This Python script demonstrates the functionality of the `postprocess` function within the `pdd.postprocess` module, which is designed to extract specific programming code from raw LLM text outputs. The demonstration covers two primary extraction methods: a simple, cost-free approach (strength = 0) that uses basic string manipulation to find triple-backtick code blocks, and an advanced, LLM-based approach (strength > 0) for more robust extraction. The script utilizes the `rich` library for formatted console output and `unittest.mock` to simulate internal dependencies like `llm_invoke` and `load_prompt_template`. This mocking allows the example to run without requiring actual LLM API keys or external prompt files. The code provides detailed examples of input parameters such as target language, strength, temperature, and time, while displaying the resulting extracted code, associated costs, and the model identifier used. It serves as a comprehensive guide for developers to understand how to integrate and configure code post-processing in their LLM-based applications.",2026-01-06T21:13:05.409784+00:00
"context/preprocess_example.py","This Python script demonstrates the functionality of a preprocessing utility imported from the pdd.preprocess module. It utilizes the rich library to provide formatted console output for debugging and final results. The script defines a template string called prompt, which incorporates several custom XML-style tags including shell, pdd, and web, alongside curly bracket placeholders and a markdown file reference. The primary logic involves configuring the preprocess function with specific flags: recursive is set to false, and double_curly_brackets is enabled. Notably, the script defines an exclusion list containing the key test2 to prevent it from being affected by the bracket doubling process. After initializing these settings, the script prints a debug message and executes the preprocessing routine on the input string. The final processed output is then displayed on the console. This code serves as a practical example of how to programmatically manipulate and clean template strings, likely for use in large language model prompting or automated documentation workflows, where specific tags need to be parsed or removed and bracket syntax must be strictly controlled.",2026-01-06T21:13:10.214957+00:00
"context/preprocess_main_example.py","This Python script defines a Command Line Interface (CLI) for preprocessing prompt files using the 'click' library. It serves as a wrapper for the 'preprocess_main' function, allowing users to specify a source prompt file and an optional output path. The CLI provides several configuration flags: '--xml' for adding structural delimiters, '--recursive' for handling nested prompt references, and '--double' for escaping curly brackets. Users can also exclude specific keys from bracket doubling via the '--exclude' option and enable detailed logging with '--verbose'. Upon execution, the script initializes a context object with default parameters like 'strength' and 'temperature', invokes the preprocessing logic, and outputs the resulting prompt, total processing cost, and the model name used. It includes error handling to catch and report exceptions during the preprocessing workflow.",2026-01-06T21:13:26.298759+00:00
"context/process_csv_change_example.py","The provided Python script, 'run_demo.py', serves as a concise usage example for the 'process_csv_change' function within the 'pdd' package. The script demonstrates how to programmatically modify prompt files based on instructions provided in a CSV file. It begins by setting up a temporary workspace and creating three essential components: a source code file ('factorial.py'), a corresponding prompt file ('factorial_python.prompt'), and a CSV file ('tasks.csv') containing specific change instructions. The script then invokes 'process_csv_change' with parameters such as 'strength', 'temperature', 'code_directory', and a financial 'budget'. This function processes the CSV to apply the requested edits to the prompt file using a language model. Finally, the script prints the results, including a success boolean, the total cost incurred, the model name used, and a list of JSON objects detailing the modifications made. The example output shows that the prompt was successfully updated to include a Python code example for testing the factorial function, illustrating the tool's utility in automating prompt engineering and maintenance tasks.",2026-01-06T21:13:30.282984+00:00
"context/pytest_example.py","The provided Python script implements a custom test execution and reporting mechanism using the pytest framework. At its core is the TestResultCollector class, which functions as a pytest plugin to programmatically capture test outcomes. This class tracks the number of failures, errors, and warnings encountered during a test session. It hooks into pytest_runtest_logreport to monitor the setup, call, and teardown phases of each test, incrementing counters based on the report's outcome. Additionally, it utilizes pytest_sessionfinish to extract warning counts from the terminal reporter plugin. Beyond outcome tracking, the script features a log redirection system using io.StringIO, allowing it to capture all standard output and error streams during the test run. The run_pytest function orchestrates the process by initializing the collector, triggering pytest.main on the target file tests/test_get_extension.py, and ensuring that system streams are restored even if an exception occurs. Finally, the script's entry point executes this routine and prints a detailed summary of the test results, including specific counts for failures and errors alongside the full captured log output. This setup is particularly useful for integrating pytest into larger automated workflows where programmatic access to test results and logs is required.",2026-01-06T21:13:32.803589+00:00
"context/pytest_output_example.py","This Python script serves as a comprehensive demonstration and example suite for the `pytest_output` module. It begins by importing necessary libraries such as `argparse`, `json`, `pytest`, and `rich` for enhanced console output. The script includes a utility function, `create_dummy_test_file`, which generates temporary Python test files containing various scenarios including passing tests, failing tests, errors, and warnings. The core of the script is the `main_example` function, which showcases six distinct use cases: executing the module via command-line simulation, capturing and saving test results to JSON files, handling non-existent or invalid file types, processing empty test files, and advanced usage of the `TestResultCollector` class to manually capture logs and test outcomes. By utilizing the `rich` library, the script provides clear, formatted console output for each example, making it an effective tool for understanding how to programmatically interact with pytest and manage test output data.",2026-01-06T21:13:49.408494+00:00
"context/python_env_detector_example.py","This Python script serves as a practical demonstration of the python_env_detector module, showcasing its capabilities for identifying and analyzing the host Python environment. The script begins by importing key functions from the pdd.python_env_detector package, including detect_host_python_executable, get_environment_info, is_in_virtual_environment, and get_environment_type. Within the main function, the script sequentially executes these utilities to provide a comprehensive overview of the current execution context. First, it locates and prints the path to the host Python executable. Next, it determines whether the script is running inside a virtual environment, followed by identifying the specific environment type. Finally, it calls get_environment_info to retrieve a dictionary of detailed metadata, which it then iterates through to display various environment-specific attributes. This example is designed to help developers understand how to programmatically inspect their Python runtime environment, which is particularly useful for debugging, configuration management, and ensuring compatibility across different deployment setups. By providing a clear, structured output of system paths and environment states, the script illustrates the module's utility in automating environment detection tasks within larger Python applications or deployment pipelines.",2026-01-06T21:13:54.248550+00:00
"context/render_mermaid_example.py","This Python script serves as a comprehensive demonstration and example suite for the 'render_mermaid' module, which is designed to visualize software architecture by converting JSON-based architecture definitions into interactive Mermaid diagrams. The script includes a helper function to generate a sample architecture dataset containing backend components (FastAPI, SQLAlchemy, Config) and frontend components (React, API client) with defined dependencies and tags. It showcases four distinct usage patterns: basic command-line execution, programmatic integration using module functions like 'generate_mermaid_code' and 'generate_html', processing existing 'architecture.json' files from the local filesystem, and a summary of customization features such as automatic categorization, color-coded subgraphs, and interactive tooltips. By executing these examples, users can learn how to transform structured architectural metadata into responsive HTML visualizations that illustrate module relationships and priorities, facilitating better system documentation and communication.",2026-01-06T21:14:09.730353+00:00
"context/split/simple_math/split_example.py","The provided Python script serves as a demonstration or placeholder for utilizing a module named split_script. The primary focus of the code is to illustrate the execution of an addition function, presumably defined within the imported module, while implementing robust error handling through try-except blocks. In the first scenario, the script successfully passes two integers, 10 and 20, to the add function and outputs the calculated sum. This represents a standard, successful function call. In the second scenario, the script intentionally attempts to pass incompatible data types—an integer and a string—to the same function. This action is designed to trigger a TypeError, which the script then catches and reports to the console. By structuring the code this way, the developer showcases how to manage potential runtime exceptions and ensure the program does not crash when encountering invalid input. Overall, the script acts as a functional test case or example for integrating external logic and managing data type integrity within a Python environment. It highlights the importance of defensive programming practices when interacting with modular components.",2026-01-06T21:14:14.322782+00:00
"context/split_example.py","This Python script serves as a demonstration and entry point for the `split` function within the `pdd.split` module. It begins by configuring the environment, specifically setting the `PDD_PATH` to the parent directory to ensure proper module resolution. The script utilizes the `rich` library to provide formatted console output. Within the `main` function, it defines several input parameters including a natural language prompt, a Python code snippet for calculating factorials, an example usage block, and configuration values for 'strength' and 'temperature'. These parameters are passed to the `split` function, which returns a tuple containing a sub-prompt and a modified prompt, along with the total execution cost and the name of the model used. The script then unpacks these results and prints them to the console using stylized headers. Error handling is implemented via a try-except block to catch and display any exceptions during execution. This file acts as a practical example of how to integrate and invoke the core splitting logic of the PDD library.",2026-01-06T21:14:29.322033+00:00
"context/split_main_example.py","The provided Python script, 'example_split_usage.py', serves as a demonstration of how to implement and utilize the 'split_main' function from the 'pdd.split_main' module within a command-line interface (CLI). Built using the Click library, the script defines a 'split-cli' command that accepts several mandatory and optional arguments, including paths for an input prompt file, a generated code file, and an example usage file. It also allows users to specify output paths for a sub-prompt and a modified prompt, with flags for forcing file overwrites and suppressing console output. Within the command logic, the script configures a context object with parameters like 'strength' and 'temperature' before invoking 'split_main'. Upon execution, the function returns a data dictionary, the total operation cost, and the model name used. If not in quiet mode, the script prints the resulting sub-prompt and modified prompt content, their save locations, and the associated costs. The script is structured with a top-level CLI group and a standard entry point, providing a clear template for developers to integrate prompt-splitting logic into their own automation workflows or tools.",2026-01-06T21:14:31.572000+00:00
"context/summarize_directory_example.py","This Python script demonstrates the practical application of the 'summarize_directory' module from the 'pdd' package. The script's primary function is to automate the summarization of Python files within a specified directory using AI models. It begins by defining a sample CSV string containing existing file summaries and timestamps to simulate a real-world update scenario. The 'main' function then invokes 'summarize_directory' with specific parameters: a wildcard path ('context/c*.py') to filter files, a model strength of 0.5, a deterministic temperature of 0.0, and verbose logging enabled. Upon execution, the module processes the files, calculates the total API cost, and identifies the specific model used. The script prints these results to the console, including the newly generated CSV content. Finally, it ensures an 'output' directory exists and saves the resulting CSV data to 'output/output.csv'. The code includes error handling via a try-except block to manage potential runtime issues, serving as a comprehensive template for developers looking to integrate automated code documentation and directory analysis into their workflows.",2026-01-06T21:14:36.592686+00:00
"context/sync_animation_example.py","This Python script provides a comprehensive example of how to implement and use the `sync_animation` module within a multi-threaded application. The primary purpose of the script is to demonstrate the synchronization between a main application workflow and a background animation thread that provides real-time visual feedback in the terminal. It begins by defining shared, mutable state variables—such as function names, accumulated costs, and file paths—using lists and a `threading.Event` to allow communication between threads. The script features a `mock_pdd_main_workflow` function that simulates various stages of a development process (e.g., checking, generating, testing, and fixing), updating the shared state at each step to trigger visual changes in the animation. The animation itself is launched in a separate daemon thread, ensuring it does not block the main logic. Finally, the script demonstrates proper cleanup procedures by signaling the animation to stop via the event object and joining the thread. It concludes by reporting the total elapsed time and final cost, serving as a template for integrating dynamic Rich-based terminal visualizations into complex, long-running Python tasks.",2026-01-06T21:14:41.409662+00:00
"context/sync_determine_operation_example.py","This Python script serves as a demonstration and test suite for the `sync_determine_operation` function within a PDD (Prompt-Driven Development) workflow. It simulates a project environment by creating a temporary './output' directory and manipulating files to represent various development states. The script covers four primary scenarios: initializing a new unit where only a prompt exists, handling test failures based on run reports, detecting manual code changes by comparing file hashes against stored fingerprints, and verifying a fully synchronized state where all hashes match and tests pass. By programmatically creating prompts, source code, and metadata files (like JSON fingerprints and run reports), the script illustrates how the PDD system decides whether to generate, fix, or maintain code based on the current state of the workspace. It utilizes standard libraries such as `pathlib` for file management and `hashlib` for integrity verification, ensuring a clean environment by resetting the working directory and cleaning up files between scenarios.",2026-01-06T21:14:43.971123+00:00
"context/sync_main_example.py","This Python script provides a demonstration and testing environment for the `sync_main` function within the `pdd` framework. It begins by defining `setup_mock_project`, which creates a temporary directory structure containing mock prompt files for Python and JavaScript to simulate a real project environment. The `main` function orchestrates the demonstration by initializing a mock Click context to simulate command-line arguments and global options. To avoid external dependencies and LLM costs, the script utilizes the `unittest.mock` library to patch internal functions like `construct_paths` and `sync_orchestration`. These mocks simulate path resolution and the results of code generation and testing for different languages. When executed, the script calls `sync_main` with a specific basename, allowing it to detect the mock languages and process them sequentially. Finally, it outputs the aggregated results, including success status, total simulated cost, and detailed JSON metadata to the console using the `rich` library for formatting. This serves as a controlled example of how the synchronization logic handles multi-language projects and error reporting.",2026-01-06T21:14:46.435212+00:00
"context/sync_orchestration_example.py","This Python script serves as a demonstration and usage example for the `sync_orchestration` module within a project likely named `pdd-cli`. The script primarily illustrates how to programmatically trigger the synchronization process that manages the lifecycle of prompts, code, examples, and tests. It begins by defining a helper function, `setup_example_project`, which creates a mock directory structure (prompts, src, examples, tests) and initializes a dummy prompt file to simulate a real project environment.

The main execution block showcases two key scenarios. First, it runs a full synchronization process for a calculator project using the `sync_orchestration` function. This step simulates the generation and testing workflow, passing in directory paths and a budget constraint, and then prints the resulting JSON summary indicating success or failure. Second, it demonstrates a dry-run mode by calling the same function with the `dry_run=True` flag. This allows users to preview the current state and potential actions without executing any changes. Overall, the file acts as both a functional test and a documentation guide for integrating the PDD orchestration logic.",2026-01-07T02:32:09.890571+00:00
"context/sync_tui_example.py","This Python script demonstrates the usage of a Textual-based Terminal User Interface (TUI) application named `SyncApp`, likely part of a larger project called `pdd`. The file defines a mock worker function, `mock_worker_logic`, which simulates a synchronization process to showcase various interactive features of the TUI. These features include thread-safe user input via modals, progress bar updates, interactive steering through choice selection with timeouts, and confirmation dialogs. The worker logic interacts directly with the `SyncApp` instance to trigger these UI elements while performing simulated tasks like file processing and strategy selection.

The script also includes a `run_sync_tui_example` function that configures the application state, including shared references for function names and costs, file paths, and color schemes for UI animation boxes. It initializes the `SyncApp` with these configurations and the mock worker, runs the application, and handles the final exit animation and result printing. The code serves as an integration test or example implementation, illustrating how to bridge background worker threads with a rich terminal interface for user interaction and status reporting.",2026-01-07T18:42:54.074010+00:00
"context/tests/test_calculator.py","This file contains a comprehensive test plan and implementation for a simple 'add_numbers' function, utilizing both traditional unit testing and Z3 formal verification. The unit tests, implemented with pytest, cover standard runtime scenarios including positive and negative numbers, zero as an identity element, floating-point precision (IEEE 754), large values, and special cases like infinity and NaN. It also includes error handling for incorrect types and subnormal number behavior. Complementing these are Z3 formal verification tests that use SMT solvers to prove mathematical properties across all real numbers. These proofs verify fundamental algebraic laws such as commutativity, associativity, and the identity property, as well as more complex relationships like the inverse property, monotonicity, and negation distribution. By combining runtime assertions with formal proofs, the file demonstrates a robust approach to verifying both the practical execution and the mathematical correctness of a basic arithmetic operation.",2026-01-07T02:20:40.611178+00:00
"context/tiktoken_example.py","The provided code snippet is a concise Python implementation utilizing the tiktoken library, a high-performance BPE tokenizer created by OpenAI. The script begins by importing the tiktoken module, which is designed for efficient text processing in the context of large language models. It then proceeds to initialize an encoding instance by calling the get_encoding method with the cl100k_base parameter. This specific encoding is the standard for modern OpenAI models such as GPT-4 and GPT-3.5-turbo. The final line of the snippet demonstrates the practical application of this library: it calculates the total token count of a string variable named preprocessed_prompt. This is achieved by first encoding the text into a list of integers (token IDs) and then determining the length of that list. Such functionality is critical for developers who need to monitor token usage to stay within model context limits, optimize prompt engineering, and accurately predict API usage costs. By providing a programmatic way to count tokens locally, tiktoken allows for more robust and cost-effective application development when integrating with sophisticated AI services.",2026-01-06T21:14:53.807406+00:00
"context/trace_example.py","This Python script serves as a demonstration for the `trace` function within the `pdd.trace` module. The script begins by importing necessary utilities, including environment management via `os`, the `trace` function, the `rich.console` for formatted output, and a default strength constant. The core logic is contained within a `main` function which defines several example inputs: a multi-line string representing a Python code file, a specific line number to trace, a prompt file describing the code's logic, and parameters for model strength and temperature. The script executes the `trace` function within a try-except block to handle potential errors such as missing files or value mismatches. Upon successful execution, it utilizes the `rich` library to print the corresponding prompt line, the total financial cost of the operation, and the name of the LLM model used. This file acts as a practical example of how to integrate the `pdd` library to map specific lines of code back to their originating natural language prompts while monitoring model performance and costs.",2026-01-06T21:15:02.489442+00:00
"context/trace_main_example.py","This Python script demonstrates the usage of the 'pdd' library's 'trace_main' function to perform code-to-prompt tracing. The script begins by programmatically creating two example files: a prompt file containing natural language instructions for a simple calculator and a Python code file implementing an 'add_numbers' function. It then configures a Click context object to define operational parameters such as output verbosity, file overwriting permissions, and Large Language Model (LLM) settings like analysis strength and temperature. The core of the script executes 'trace_main', which analyzes the relationship between a specific line of code (the function definition) and the original prompt instructions. Upon successful execution, the script outputs the corresponding line number in the prompt file, the financial cost of the LLM analysis, and the specific model name used. This serves as a practical example of how to integrate automated tracing tools into a development workflow to maintain alignment between requirements and implementation.",2026-01-06T21:15:04.959309+00:00
"context/track_cost_example.py","This Python script defines a Command-Line Interface (CLI) for a tool named PDD, which is designed to process prompts and generate outputs while tracking associated costs. Built using the Click library, the script establishes a command group that allows users to specify an output path for cost tracking data in CSV format. The primary functionality is contained within the 'generate' command, which is decorated with a custom 'track_cost' utility. This command requires a path to a prompt file and accepts an optional output file path. During execution, the script reads the input prompt, simulates a generation process using a placeholder model (gpt-4), and returns the generated text along with a simulated cost of 0.05 dollars per million tokens. If an output path is provided, the result is saved to a file; otherwise, it is printed to the console using the Rich library for enhanced formatting. The script includes a main execution block that demonstrates how to invoke the CLI with specific arguments for prompt processing and cost logging. It serves as a foundational structure for developers building LLM-integrated applications that require rigorous usage monitoring and file-based I/O operations.",2026-01-06T21:15:15.015836+00:00
"context/unfinished_prompt_example.py","This Python script serves as a comprehensive example for utilizing the `unfinished_prompt` function from the `pdd` package. The primary purpose of the script is to demonstrate how to programmatically assess whether a given text prompt is complete or requires further continuation. It begins by outlining necessary prerequisites, such as ensuring the `pdd` package is in the PYTHONPATH, having a specific prompt template file available, and configuring environment variables for LLM API access. The script then defines an intentionally incomplete prompt regarding sourdough bread and passes it to the `unfinished_prompt` function. This function accepts parameters like `strength`, `temperature`, and `verbose` to control the underlying LLM's behavior. The output is a tuple containing a detailed reasoning string, a boolean indicating completeness, the estimated API call cost, and the model name used. Finally, the script uses the `rich` library to print these results in a formatted, readable manner, providing a clear template for developers to integrate prompt analysis into their own workflows.",2026-01-06T21:15:24.824123+00:00
"context/update_main_example.py","This Python script provides a command-line interface (CLI) example for updating a prompt based on code modifications using the Click library. It serves as a wrapper for the `update_main` function from the `pdd.update_main` module. The script defines an `update` command that accepts several parameters, including paths for the original prompt file, the modified code file, and an optional original code file. Alternatively, users can enable a `--git` flag to retrieve the original code from Git history. Other configurable options include 'strength' for blending code differences, 'temperature' for model determinism, and flags for verbosity, quiet mode, and forcing file overwrites. The script also includes a `--simple` flag to bypass agentic routing in favor of a legacy update path. Upon execution, the script passes these configurations to `update_main`, captures the resulting updated prompt, total cost, and model name, and displays a summary of these results to the console using the Rich library for formatted output.",2026-01-06T21:15:29.531746+00:00
"context/update_model_costs_example.py","The provided Python script, `example_update_model_costs.py`, serves as an end-to-end demonstration of the `update_model_costs.py` utility within the `pdd` package. Its primary purpose is to show users how to programmatically or via CLI update a CSV file containing LLM model metadata, specifically focusing on filling in missing cost information and structured output capabilities using LiteLLM. The script outlines the required CSV schema, which includes fields such as provider, model name, input/output costs per million tokens, and various token limits. In the demonstration, the script first creates a sample CSV file with placeholder values for models like GPT-4o-mini and Claude-3-Haiku. It then imports and executes the `update_model_data` function, which modifies the CSV file in place by fetching the latest pricing and technical specifications. Finally, the script displays the updated data using pandas to illustrate the changes. This example highlights the utility's prerequisites, such as necessary environment variables for API keys and dependencies like `pandas`, `rich`, and `litellm`, providing a clear guide for integrating this automated cost-tracking tool into larger workflows.",2026-01-06T21:15:33.730700+00:00
"context/update_prompt_example.py","The provided Python script serves as a demonstration or entry point for utilizing the update_prompt function from the pdd package. It begins by importing necessary modules, including a specific strength constant and the core update function. Within the main function, the script initializes several variables representing a typical use case: an original natural language prompt, the corresponding original code, and a modified version of that code. It also sets parameters for the Large Language Model (LLM), such as strength and temperature. The script then executes the update_prompt function within a try-except block to ensure robust error handling. Upon a successful call, it retrieves and prints the newly generated prompt, the financial cost associated with the API call, and the name of the model used. If the function fails to return a prompt, it notifies the user. This script effectively illustrates how to programmatically refine LLM prompts based on code changes while monitoring performance metrics like cost. It acts as a practical example for developers looking to integrate automated prompt engineering or refinement into their workflows using the pdd library's capabilities.",2026-01-06T21:15:36.395451+00:00
"context/xml_tagger_example.py","This Python script demonstrates the practical application of the xml_tagger function from the pdd.xml_tagger library. The code begins by importing the core tagging utility along with the rich library for formatted console output. It initializes a sample raw prompt, 'Write a story about a magical forest', and configures two key parameters for the underlying Large Language Model: strength, set to 0.5 to balance cost and performance, and temperature, set to 0.7 to control output variability. The script utilizes a try-except block to handle the execution of the xml_tagger function, which processes the prompt and returns three distinct values: the XML-tagged version of the input, the total monetary cost of the API call, and the name of the specific model used. If successful, the script prints the results using color-coded formatting to highlight the tagged prompt, the precise cost, and the model metadata. In the event of a failure, it catches the exception and displays a formatted error message. Overall, the file serves as a concise example of how to programmatically enhance prompts with XML tags while monitoring resource consumption and model selection.",2026-01-06T21:15:53.406171+00:00
