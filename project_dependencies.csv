full_path,file_summary,date
"context/DSPy_example.py","The provided file content is a Python code snippet demonstrating the initial setup and module definition for using the DSPy library to optimize prompts or examples. It begins by importing a local context module and configuring the DSPy settings to use the OpenAI 'gpt-3.5-turbo-instruct' model with a token limit of 250. Following the setup, the code defines a custom class named 'chainofthought' that inherits from 'dspy.Module'. Inside this class, the constructor initializes a 'dspy.ChainOfThought' program designed to map questions to answers. A 'forward' method is also defined to execute this program when the module is called with a question. The snippet ends abruptly with a comment indicating the next steps would involve compiling and optimizing the module.",2026-01-03T03:51:06.988854+00:00
"context/__init__example.py","This Python script serves as an example usage guide for the command registration module within the 'pdd' application. It demonstrates the process of initializing the main Click CLI group and dynamically registering subcommands defined in the `pdd.commands` package. The script defines a main entry point function, `main_cli`, decorated with `@click.group()`, which acts as the root for the command-line interface. The core functionality is showcased in the `run_example` function, which calls `register_commands(main_cli)` to attach all available subcommands to the main group. To verify successful registration, the script simulates running the CLI with the `--help` argument, printing the resulting help message to the console. This output confirms that the commands have been correctly aggregated into the interface. The script is designed to be run directly to visualize how the application's entry point aggregates functionality.",2026-01-03T03:51:20.079790+00:00
"context/addition_of_time_param.prompt","This file contains a system prompt designed for the `pdd detect` command within the PDD CLI ecosystem. Its primary purpose is to instruct an AI model on how to analyze other PDD CLI command prompt files to determine if they require updates due to a new feature: a global `--time` parameter. The prompt details the background of this new parameter, explaining that it controls the thinking effort or reasoning allocation of the underlying Large Language Model (LLM) via an internal `llm_invoke` function. It specifies how the `time` float (0.0 to 1.0) maps to different reasoning types ('budget' for token limits or 'effort' for qualitative levels like low/medium/high). The task assigned to the AI is to review a target prompt and decide if changes are needed based on specific criteria: checking for conflicting hardcoded instructions regarding reasoning effort, assessing parameter relevance, and identifying outdated documentation. The ultimate goal is to ensure existing prompts are compatible with the new global reasoning control mechanism.",2026-01-03T03:51:27.359528+00:00
"context/agentic_bug_example.py","This file serves as an example script demonstrating how to utilize the `run_agentic_bug` function for investigating bugs based on GitHub issues. The script begins by setting up the Python path to ensure the project root is accessible for imports. The core of the example is the `main` function, which simulates an agentic bug investigation workflow without making real API calls. It achieves this by mocking the `run_agentic_bug_orchestrator` using `unittest.mock.patch`. The mock is configured to return a successful result, simulating an 8-step workflow that costs $2.50, uses an Anthropic model, and results in the creation of a test file named `test_calculator_bug.py`. The script then executes the `run_agentic_bug` function with a dummy GitHub issue URL and prints a formatted summary of the results, including success status, model usage, cost, changed files, and the final message.",2026-01-03T03:51:34.283268+00:00
"context/agentic_bug_orchestrator_example.py","This Python script serves as an example usage demonstration for the `agentic_bug_orchestrator` module. It simulates the execution of the `run_agentic_bug_orchestrator` function without requiring actual LLM API calls or a live GitHub repository. The script achieves this by mocking internal dependencies—specifically `load_prompt_template` and `run_agentic_task`—using the `unittest.mock` library.

The simulation walks through a hypothetical scenario involving a ZeroDivisionError in a calculator application. It defines mock responses for a 9-step bug investigation workflow, which includes checking for duplicates, reproducing the error, identifying the root cause, generating tests, and creating a pull request. The `main` function sets up dummy issue data, applies the patches, and executes the orchestrator, finally printing the simulation results such as success status, total cost, and changed files. This allows developers to verify the logic and flow of the bug orchestration process in a controlled, cost-free environment.",2026-01-03T05:22:10.753881+00:00
"context/agentic_common_example.py","This Python script serves as a demonstration and usage example for the `pdd.agentic_common` module within the `pdd` project. It illustrates how to programmatically interact with available AI agent providers (such as Claude, Gemini, or Codex) to perform headless tasks. The script begins by setting up the project path to ensure internal packages can be imported. It then defines a `main` function that executes a workflow in several steps: first, it establishes a local output directory for the agent's operations; second, it calls `get_available_agents()` to detect which CLI tools and API keys are configured on the system; third, it defines a natural language instruction asking an agent to create a Python file named `generated_math.py` containing a factorial function. The core of the script invokes `run_agentic_task`, passing in the instruction and working directory. Finally, the script parses and prints the execution results—including success status, the specific provider used, and estimated costs—and verifies the outcome by checking if the requested file was successfully created and displaying its content.",2026-01-03T03:51:47.696098+00:00
"context/agentic_crash_example.py","This Python script serves as a test harness or example usage for the `run_agentic_crash` function from the `pdd` package. It simulates an automated debugging scenario where an AI agent fixes a crashing program. The script first sets up a dummy environment by creating several files: a specification file (`factorial_spec.md`), a buggy Python implementation (`math_lib.py` raising a `NotImplementedError`), a runner script (`run_factorial.py`), and a crash log. It then mocks external dependencies—specifically the agentic task runner and subprocess execution—to simulate a successful repair process without requiring real API calls. A side effect is injected into the mock to actually update the buggy code file with a correct factorial implementation. Finally, the script executes `run_agentic_crash` with these parameters, prints a summary of the results (success status, cost, model used), and verifies that the code file was updated correctly.",2026-01-03T03:51:54.220938+00:00
"context/agentic_fix_example.py","This Python script serves as a demonstration and test harness for an agentic fix workflow, specifically utilizing the `run_agentic_fix` function from the `pdd.agentic_fix` module. The script simulates a software development scenario where a bug exists in a codebase, and an AI agent is tasked with fixing it based on failing tests and error logs.

The script defines a deliberately buggy function (`add` which performs subtraction) and a corresponding unit test that fails. It includes a `setup_scenario` function that creates a temporary directory environment, populating it with the buggy code, the test file, a prompt for the AI, and an initial error log generated by running `pytest`. The `main` function orchestrates the workflow: it checks for necessary API keys (Anthropic, Google, or OpenAI), sets up the temporary environment, and invokes `run_agentic_fix`. Finally, it reports the success or failure of the agent's attempt, including details like the model used, estimated cost, and the content of the corrected code. The script ensures clean execution by using temporary directories and restoring the original working directory upon completion.",2026-01-03T03:52:07.621023+00:00
"context/agentic_langtest_example.py","The provided text contains a Python module, `pdd/agentic_langtest.py`, and an example script demonstrating its usage. The module is designed to facilitate the automated execution of unit tests for multiple programming languages, including Python, JavaScript/TypeScript, Java, and C++.

Key features of the module include:
1.  **Project Root Detection**: The `_find_project_root` function traverses directory trees to locate project markers such as `package.json`, `pom.xml`, or `build.gradle`.
2.  **Command Generation**: The `default_verify_cmd_for` function constructs appropriate shell commands to run tests based on the detected language and build system (e.g., `pytest`, `npm test`, `mvn test`).
3.  **Environment Validation**: The `missing_tool_hints` function checks for the presence of necessary command-line tools (like `npm` or `javac`) and provides installation suggestions for macOS and Ubuntu if they are missing.

The appended example code utilizes the `rich` library to visually demonstrate these functions. It creates temporary mock projects for JavaScript and Python, showing how the system identifies project structures, generates test commands, and offers hints for missing dependencies.",2026-01-03T03:52:21.691754+00:00
"context/agentic_update_example.py","This Python script serves as a demonstration and utility for using the `run_agentic_update` function from the `pdd` package. Its primary purpose is to showcase how an AI agent can automatically update a documentation or prompt file to match the current state of a codebase.

The script sets up a simulated development environment by creating a local output directory and generating three dummy files: an outdated prompt file (`math_module.prompt`) that only specifies an addition function, a current code file (`math_module.py`) that includes both addition and subtraction functions, and a corresponding test file.

It then executes the `run_agentic_update` function, passing in the paths to the prompt and code files. This function is designed to invoke an AI agent (such as Claude or Gemini) to analyze the code and tests, and subsequently rewrite the prompt file to accurately reflect the new code features. Finally, the script prints the results of the operation, including success status, cost, the model used, and the content of the updated prompt file to verify that the agent successfully synchronized the documentation with the code.",2026-01-03T03:52:37.382885+00:00
"context/agentic_verify_example.py","This Python script serves as a demonstration and test harness for the `run_agentic_verify` function within the `pdd` package. It is designed to simulate an automated code verification and repair workflow without requiring actual external API calls or live agent interactions.

The script first sets up the environment by ensuring the `pdd` package is importable and creating a temporary directory populated with dummy files: a specification file (`spec.md`), a buggy implementation (`calculator.py` containing a subtraction error), a driver test script (`driver.py`), and a verification log.

To execute the verification process, the script uses `unittest.mock.patch` to mock internal dependencies. Specifically, it replaces the `run_agentic_task` function with a `mock_agent_behavior` function. This mock simulates an AI agent analyzing instructions, physically correcting the bug in `calculator.py` (changing subtraction to addition), and returning a structured JSON response indicating success. Finally, the script runs `run_agentic_verify`, prints the execution results (success status, cost, model used, and changed files), and displays the corrected content of the code file to verify the mock agent's actions.",2026-01-03T03:52:52.380336+00:00
"context/anthropic_counter_example.py","The provided file content consists of a Python script that demonstrates how to use the Anthropic API client to count tokens for a given text string. The code is currently entirely commented out. It begins by importing the 'anthropic' library and initializing an instance of the Anthropic client. A sample string variable named 'text' is defined with the value 'Sample text'. The script then attempts to calculate the token count for this text using the 'client.count_tokens(text)' method. A comment within the code notes a limitation, stating that this specific method is not accurate for Anthropic models version 3.0 and above. Finally, the script includes a print statement intended to output the total number of tokens calculated. The primary purpose of this snippet is to serve as a basic usage example for token counting with older versions of the Anthropic API.",2026-01-03T03:53:06.886710+00:00
"context/anthropic_thinking_test.py","This Python script demonstrates how to use the `litellm` library to interact with Anthropic's Claude models, specifically testing the thinking or reasoning capabilities of the `claude-3-7-sonnet-20250219` model. The script begins by setting up the environment, loading API keys from a `.env` file located in the project root, and verifying the presence of the `ANTHROPIC_API_KEY`. It configures a request payload that includes a user prompt asking for an explanation of recursion. Crucially, the configuration enables a thinking parameter with a specific token budget (1024 tokens) and sets a high maximum token limit for the response. The script then executes the completion call using `litellm.completion` and prints the results using the `rich` library for formatted output. It includes error handling to catch unsupported parameters or general exceptions and attempts to extract and display any specific reasoning content returned by the model, checking both hidden parameters and standard message fields.",2026-01-03T03:53:14.831623+00:00
"context/anthropic_tool_example.py","The provided file content is a Python script demonstrating how to interact with the Anthropic API, specifically utilizing the `claude-3-7-sonnet-20250219` model. The script begins by importing the `anthropic` library and initializing a client instance. It then constructs a request to create a message using `client.messages.create`. This request configures several parameters: the specific model version, a maximum token limit of 1024, and a specific tool definition. The tool included is a text editor tool named `str_replace_editor` with the type `text_editor_20250124`, indicating the model is being set up with capabilities to modify text files. The message payload simulates a user request asking for help fixing a syntax error in a file named `patent.md`. Finally, the script prints the API's response to the console. This snippet serves as a concise example of how to implement tool use (specifically text editing) within an Anthropic API call.",2026-01-03T03:53:30.334031+00:00
"context/auto_deps_main_example.py","This file defines a command-line interface (CLI) tool named `auto-deps` using the Python `click` library. The script serves as an entry point for a dependency analysis utility. It accepts several configuration options, including flags for forcing overwrites (`--force`) and suppressing output (`--quiet`), as well as parameters for analysis strength, temperature, input prompt file paths, directory paths for scanning, and output locations. The core logic is delegated to an imported function `auto_deps_main`. When executed, the script initializes a click context with the provided parameters, runs the main analysis function, and prints the resulting modified prompt, the total cost of the operation, and the model name used. It also includes error handling to catch exceptions and abort execution gracefully if issues arise.",2026-01-03T03:53:36.858419+00:00
"context/auto_include_example.py","This Python script demonstrates the usage of the `auto_include` function from the `pdd` package. The script defines a `main` function that sets up several parameters, including a detailed prompt string describing a task for an expert Python Software Engineer to generate a unit test function using Langchain. It reads a CSV file named `project_dependencies.csv` and defines a directory path pattern (`context/c*.py`). The script then calls `auto_include` with these inputs, along with configuration settings for model strength, temperature, and verbosity. Finally, it prints the returned dependencies, the updated CSV output, the total cost of the operation, and the name of the LLM model used.",2026-01-03T03:53:49.878509+00:00
"context/auto_update_example.py","This file provides a Python script demonstrating the usage of the `auto_update` function from the `pdd` package. The script defines a `main` function that showcases three different implementation scenarios for the auto-update feature. First, it demonstrates the basic usage by calling `auto_update()` without arguments to check for updates to the `pdd` package itself. Second, it shows how to check for updates for a specific third-party package by passing the `package_name` argument (e.g., requests). Finally, it illustrates how to check against a specific known version by providing both the `package_name` and a `latest_version` argument (e.g., checking pandas against version 2.0.0). The accompanying docstring explains that the function's workflow involves checking the installed version, comparing it with the latest available version, prompting the user for an upgrade if a newer version exists, and executing the upgrade via pip upon confirmation.",2026-01-03T03:54:02.842157+00:00
"context/autotokenizer_example.py","The provided file contains a Python script designed to count the number of tokens in a given text string using the Hugging Face `transformers` library. It defines a function named `count_tokens` which accepts a text string and an optional model name as arguments, defaulting to deepseek-ai/deepseek-coder-7b-instruct-v1.5. Inside the function, an `AutoTokenizer` is instantiated from the pretrained model specified, with the `trust_remote_code` parameter set to True to ensure compatibility with custom model architectures. The function then tokenizes the input text and returns the length of the resulting `input_ids` list, effectively providing the total token count. The script concludes with an example usage block where a sample string, Write a quick sort algorithm in Python., is passed to the function, and the resulting token count is printed to the console. This utility is useful for developers needing to estimate token usage for Large Language Model (LLM) inputs.",2026-01-03T03:54:15.423337+00:00
"context/bug_main_example.py","This Python script serves as a demonstration of how to utilize the `bug_main` function from the `pdd` library to automatically generate unit tests based on discrepancies between observed and desired program outputs. The script begins by setting up a Click context object configured with parameters such as force overwrite, verbosity, model strength, and temperature. It then prepares a testing environment by creating an `output` directory and populating it with several sample files: a prompt file describing a function to sum even numbers, a Python file containing the implementation of that function, a main program file that utilizes the function, and text files representing both the current (buggy) output and the desired correct output. The core of the script invokes `bug_main`, passing in paths to these generated files along with configuration options like the target language and output path for the resulting test. Finally, the script prints the generated unit test code, the total cost of the operation in USD, and the name of the AI model used to the console using the `rich` library for formatting. This example effectively illustrates the workflow for automated test generation using the `pdd` toolset.",2026-01-03T03:54:28.099409+00:00
"context/bug_to_unit_test_example.py","This Python script serves as a demonstration and entry point for using the `bug_to_unit_test` function from the `pdd` package. The script sets up a specific test scenario involving a mismatch between current and desired outputs, which simulates a bug where a prompt line was not correctly matched in a trace log. It loads necessary context from external files, including a prompt file (`trace_python.prompt`), the code under test (`pdd/trace.py`), and an example runner script (`context/trace_example.py`). The `main` function configures parameters such as the LLM strength, temperature, and language, then invokes `bug_to_unit_test` to generate a unit test that reproduces the identified issue. Finally, it uses the `rich` library to print the generated unit test, the cost of the operation, and the model used to the console, handling any exceptions that occur during execution.",2026-01-03T03:54:41.674737+00:00
"context/bug_to_unit_test_failure_example.py","The provided file content defines a single Python function named 'add' that takes two arguments, 'x' and 'y'. However, despite the function name suggesting an addition operation, the implementation performs subtraction, returning the result of 'x - y'. This discrepancy indicates a logical error or a misleading naming convention within the code.",2026-01-03T03:54:55.870284+00:00
"context/change/1/change.prompt","The provided file content is a concise directive regarding a code refactoring or dependency change within a software project. Specifically, it instructs the developer or user to switch the method used for counting tokens in a prompt. Instead of using the 'tiktoken' library directly, the instruction mandates the use of a 'token_counter' utility derived from a module named 'llm_selector'. This change likely aims to standardize token counting across the application, abstracting the specific tokenizer implementation behind the 'llm_selector' interface, or to ensure compatibility with specific large language model configurations managed by that selector.",2026-01-03T03:55:01.559510+00:00
"context/change/1/final_code_generator_python.prompt","The provided text outlines a specification for an expert Python engineer to develop a function named code_generator. This function is designed to compile a raw text prompt into a runnable code file, utilizing the Langchain library and the Python rich library for console output. The function accepts four inputs: a raw prompt string, the target programming language (e.g., Python, Bash), a strength parameter (float 0-1) for model selection, and a temperature setting for the LLM.

The process involves a seven-step workflow. First, the raw prompt is preprocessed. Next, a Langchain LCEL template is created. The system then selects an appropriate Large Language Model (LLM) based on the strength parameter and calculates input token costs. The prompt is executed through the model, with real-time console feedback on token usage and cost. The model's output, which mixes text and code blocks, is displayed using rich Markdown formatting. Finally, the output is post-processed to extract clean, runnable code, and the function returns both this code and the total calculated cost of the operation. The specification references several external context files for examples on preprocessing, LLM selection, and postprocessing.",2026-01-03T03:55:14.293660+00:00
"context/change/1/initial_code_generator.py","This file defines a Python function named `code_generator` designed to generate programming code based on a user-provided prompt. The function orchestrates a multi-step pipeline involving preprocessing, model selection, execution, and postprocessing. It utilizes the `langchain` library for prompt management and model interaction, and `tiktoken` for token counting to estimate costs.

The process begins by preprocessing the raw prompt and creating a Langchain template. It then selects an appropriate Large Language Model (LLM) using a custom `llm_selector` based on desired strength and temperature parameters. To optimize performance and cost, it employs an SQLite cache for LLM responses. The function executes the prompt, calculates the financial cost based on input and output token usage, and pretty-prints the raw result using the `rich` library. Finally, the output undergoes a postprocessing step to extract runnable code, and the function returns both the final code and the total calculated cost of the operation. Error handling is included to catch and report exceptions during execution.",2026-01-03T03:55:28.363802+00:00
"context/change/1/initial_code_generator_python.prompt","The provided text outlines a specification for an expert Python engineer to create a function named code_generator. This function is designed to compile a raw text prompt into a runnable code file using a Large Language Model (LLM). The function accepts four inputs: the raw prompt string, the target programming language (e.g., Python, Bash), a strength parameter (float 0-1) to determine model capability, and a temperature setting for the model. The expected outputs are a string of runnable code and the total cost of the operation. The implementation must utilize the LangChain library, specifically leveraging LangChain Expression Language (LCEL). The process involves several distinct steps: preprocessing the prompt, creating an LCEL template, selecting the appropriate LLM based on the strength parameter, and executing the model run. Throughout the execution, the function is required to provide console feedback using the rich Python library, displaying token counts (calculated via tiktoken), costs, and pretty-printed Markdown results. Finally, the raw model output must be post-processed to extract clean, runnable code, stripping away conversational text or markdown formatting, before returning the final code and total cost.",2026-01-03T03:55:35.690719+00:00
"context/change/10/change.prompt","The provided text outlines a specific modification required for the `fix_errors_from_unit_tests` function. The primary instruction is to update the function's signature to accept a new input parameter named `error_file`. This file is intended to serve as a destination for logging data. Specifically, the output generated by the first LCEL (LangChain Expression Language) run, which is currently printed to the console, needs to be appended to this `error_file`. Furthermore, the instructions emphasize the importance of readability within this log file. To achieve this, a clear separator must be inserted between the existing content of the `error_file` and the newly appended output from the LCEL execution. This separation is crucial for distinguishing which parts of the log originated from which specific function or execution run, thereby facilitating easier debugging and log analysis.",2026-01-03T03:55:42.993866+00:00
"context/change/10/final_fix_errors_from_unit_tests_python.prompt","This document outlines the requirements for creating a Python function named fix_errors_from_unit_tests. The function is designed to automatically resolve unit test errors by leveraging Large Language Models (LLMs) via the Langchain library. It takes inputs including the failing unit test code, the code under test, the error message, a file path for logging, and parameters for controlling the LLM's strength and temperature.

The process involves an 11-step workflow. First, it loads specific prompt templates from a project directory defined by an environment variable. It then reads existing error logs before initiating a two-stage Langchain LCEL process. The first stage uses an LLM to generate a fix based on the provided code and errors, logging the output and costs to both the console (using the rich library for formatting) and the error file. The second stage uses a JSON parser to extract specific fields—such as whether updates are needed and the fixed code strings—from the previous step's output. Finally, the function calculates the total cost of the LLM usage and returns the fixed code, update flags, and cost metrics, while ensuring robust error handling for file I/O and model interactions throughout execution.",2026-01-03T03:55:56.102752+00:00
"context/change/10/initial_fix_errors_from_unit_tests.py","This Python script defines a function named `fix_errors_from_unit_tests` designed to automatically resolve errors in unit tests and their corresponding code using Large Language Models (LLMs). The script leverages the LangChain library for orchestrating LLM interactions and utilizes a local SQLite cache to optimize performance and reduce costs. The core function takes a failing unit test, the associated source code, the error message, and configuration parameters (strength and temperature) as inputs. It operates in a two-step process: first, it uses a specific prompt template to generate a fix for the errors based on the provided context. Second, it employs a different prompt to extract the structured JSON output containing the corrected code and unit test from the initial generation. Throughout execution, the script uses the `rich` library to display formatted logs, including markdown-rendered LLM outputs and detailed cost calculations based on token usage. The function ultimately returns flags indicating whether updates were made, the fixed code strings, and the total financial cost of the LLM operations.",2026-01-03T03:56:09.840356+00:00
"context/change/10/initial_fix_errors_from_unit_tests_python.prompt","This document outlines the requirements for creating a Python function named fix_errors_from_unit_tests. The function is designed to automatically resolve errors encountered during unit testing by leveraging Large Language Models (LLMs) via the Langchain library. It takes the failing unit test, the code under test, the error message, and model configuration parameters (strength and temperature) as inputs.

The process involves a multi-step workflow using Langchain's LCEL (LangChain Expression Language). First, it loads specific prompt templates from a project directory defined by the $PDD_PATH environment variable. It then executes an initial LLM run to generate a fix for the errors, calculating and pretty-printing token usage and costs using the rich library. Subsequently, a second LLM run parses the previous output to extract structured data, specifically boolean flags indicating if updates are needed and the actual fixed code strings. Finally, the function returns the update status, the corrected code and unit test, and the total calculated cost of the LLM operations. The implementation must also handle potential errors gracefully.",2026-01-03T03:56:23.650359+00:00
"context/change/11/change.prompt","The file outlines a request to create a Python function named change. This function is designed to process a specific prompt file located at 'prompts/change_LLM.prompt' using LangChain Expression Language (LCEL) and apply post-processing similar to the current context. The primary goal of the function is to output a modified prompt. The document further specifies the expected inputs and outputs for the underlying 'change_LLM' prompt. The inputs include 'input_prompt' (the original prompt to be modified), 'input_code' (code generated from the original prompt), and 'change_prompt' (instructions for modification). The single output is defined as 'modified_prompt', which represents the updated prompt string resulting from the applied changes.",2026-01-03T03:56:37.762529+00:00
"context/change/11/initial_code_generator.py","This Python script defines a `code_generator` function designed to automate the creation of programming code from natural language prompts using Large Language Models (LLMs) via the LangChain framework. The process involves a multi-step pipeline. First, the raw input prompt is preprocessed, likely to handle specific formatting or file inclusions. Next, the script utilizes a custom `llm_selector` to choose an appropriate LLM based on a desired strength and temperature setting, while also retrieving cost metrics. The prompt is then executed through a LangChain pipeline consisting of a template, the selected model, and an output parser. Throughout execution, the script uses the `rich` library to print detailed, color-coded logs to the console, including the preprocessed prompt, model output (rendered as Markdown), token usage, and estimated costs for both input and output. Finally, the raw model output undergoes a post-processing step to extract or refine runnable code for a specific target language. The function returns the final executable code string and the total calculated cost of the operation. An example usage block demonstrates how to invoke the generator with a sample prompt requesting a Python factorial function.",2026-01-03T03:56:50.296167+00:00
"context/change/11/initial_code_generator_python.prompt","The provided text outlines a specification for an expert Python engineer to develop a function named code_generator. This function is designed to compile a raw text prompt into a runnable code file, utilizing the Langchain library and the Python rich library for console output. The function accepts four inputs: a raw prompt string, the target programming language (e.g., Python, Bash), a strength parameter (float 0-1) for model selection, and a temperature setting for the LLM.

The process involves a seven-step workflow. First, the raw prompt is preprocessed. Next, a Langchain LCEL template is created. The system then selects an appropriate Large Language Model (LLM) based on the strength parameter and calculates input token costs. The prompt is executed through the model, with real-time console feedback on token usage and cost. The model's output, which mixes text and code blocks, is displayed using rich Markdown formatting. Finally, the output is post-processed to extract clean, runnable code, and the function returns both this code and the total calculated cost of the operation. The specification references several external context files for examples on preprocessing, LLM selection, and postprocessing.",2026-01-03T03:57:04.445920+00:00
"context/change/11/initial_split_python.prompt","This document outlines the requirements for an expert Python Software Engineer to create a function named split. The primary objective of this function is to decompose a given input_prompt into two distinct components: a sub_prompt and a modified_prompt, ensuring no loss of functionality in the resulting code generation. The function takes several inputs, including the original prompt, the code generated from it, an example usage code snippet, and LLM parameters like strength and temperature.

The process involves several specific steps: loading prompt templates from a specified path, preprocessing them (including doubling curly brackets), and utilizing Langchain LCEL templates for execution. The function must employ an llm_selector for model selection and token counting. It executes a two-stage LLM process: first, to generate a raw split based on the inputs, and second, to extract the specific sub_prompt and modified_prompt as JSON data. Throughout execution, the function is required to pretty-print outputs, token counts, and estimated costs using the Python Rich library. Finally, it returns the two resulting prompts and the total calculated cost, while handling potential edge cases and errors robustly.",2026-01-03T03:57:11.298871+00:00
"context/change/12/change.prompt","The provided file content is a concise instruction intended for a Large Language Model (LLM) or a developer. It requests the transformation of the current text into a specific prompt designed to generate a function named unfinished_prompt. This function is intended to handle the execution or processing of another prompt referred to as unfinished_prompt_LLM. The file explicitly references an external resource or file path, indicated by the XML-like tags <include>prompts/unfinished_prompt_LLM.prompt</include>, suggesting that the content of that external file is central to the task. Essentially, this is a meta-prompt: a directive to create a tool (the function) that manages a specific interaction defined in a separate prompt file.",2026-01-03T03:57:24.235645+00:00
"context/change/12/final_unfinished_prompt_python.prompt","This document outlines the requirements for creating a Python function named 'unfinished_prompt', designed to analyze whether a given text prompt is complete or requires continuation. The function takes 'prompt_text', 'strength', and 'temperature' as inputs and returns a boolean 'is_finished', a 'reasoning' string, and an optional 'total_cost'. The implementation must utilize the Langchain LCEL framework, loading a specific prompt template from an environment-variable-defined path. It involves selecting an LLM model via a helper function ('llm_selector'), calculating token usage and costs, and pretty-printing status updates using the 'rich' library. The process includes invoking the model with the prompt text, parsing the JSON output for reasoning and completion status, and handling errors gracefully.",2026-01-03T03:57:31.000332+00:00
"context/change/12/initial_postprocess.py","This Python script defines a function named `postprocess` designed to extract and format code snippets from the raw text output of a Large Language Model (LLM). The function takes the LLM's output, the target programming language, and parameters for model strength and temperature as inputs. It first checks if the strength is zero, in which case it defaults to a simpler `postprocess_0` function. Otherwise, it utilizes the LangChain library to construct a processing chain involving a prompt template loaded from an external file, an LLM selected via `llm_selector`, and a JSON output parser based on a Pydantic model (`ExtractedCode`). The script calculates and displays the estimated token usage and financial cost for both input and output using the `rich` library for console formatting. Finally, it cleans the extracted code by removing markdown code block delimiters (triple backticks) and returns the sanitized code string along with the total operation cost.",2026-01-03T03:57:43.822319+00:00
"context/change/12/initial_postprocess_python.prompt","The file outlines the requirements for creating a Python function named 'postprocess' designed to refine the string output of a Large Language Model (LLM) into executable code. The function accepts inputs including the raw 'llm_output', the target programming 'language', a 'strength' parameter (defaulting to 0.9), and a 'temperature' setting. The primary goal is to extract code blocks while properly commenting out non-code text. The process involves a conditional logic: if the strength is 0, it utilizes a zero-cost 'postprocess_0' method. Otherwise, it employs a more complex workflow involving the loading of a specific prompt file ('extract_code_LLM.prompt'), creating a Langchain LCEL template, and selecting an LLM model via an 'llm_selector' helper. The function must handle JSON outputs, calculate and print token usage and costs, strip markdown code fences (triple backticks), and pretty-print the results using the 'rich' library. It returns the cleaned 'extracted_code' string and the total processing cost, while ensuring robust error handling and using relative imports for dependencies.",2026-01-03T03:57:57.735708+00:00
"context/change/13/change.prompt","The provided file contents describe the logic and purpose of a Python function named 'continue_generation'. This function is designed to handle scenarios where a Large Language Model (LLM) output is incomplete or truncated. It operates by taking a formatted input prompt (with variables already substituted) and the initial output from the LLM as inputs. The core mechanism involves detecting whether the generation is unfinished using a helper function called 'unfinished_prompt'. If the output is deemed incomplete, the function triggers a specific prompt template located at 'prompts/continue_generation_LLM.prompt'. The process enters a loop, repeatedly invoking the continuation prompt and appending the new output to the existing 'LLM_OUTPUT' until the generation is fully complete. Essentially, this script ensures that long or complex responses from an LLM are captured in their entirety by automatically requesting continuations when necessary.",2026-01-03T03:58:11.602274+00:00
"context/change/13/initial_split.py","This Python script defines a function named `split` designed to process and split input code using a Large Language Model (LLM). The script leverages the `langchain` library for LLM interactions and the `rich` library for formatted console output. It begins by setting up an SQLite cache to optimize performance and reduce costs. The core `split` function takes an input prompt, input code, example code, and model parameters (strength and temperature) as arguments. It loads specific prompt templates from files, preprocesses them, and uses a custom `llm_selector` to choose an appropriate model. The function then executes a two-step chain: first, it generates a raw split of the code based on the inputs; second, it parses this output into a structured JSON format to extract a 'sub_prompt' and a 'modified_prompt'. Throughout the process, the script calculates and displays token usage and estimated costs. Finally, it returns the extracted prompts and the total cost. An example usage block at the end demonstrates how to call the function.",2026-01-03T03:58:23.952326+00:00
"context/change/13/initial_split_python.prompt","The provided text outlines the requirements for an expert Python Software Engineer to create a function named split. This function is designed to decompose a single input_prompt into two distinct components: a sub_prompt and a modified_prompt, ensuring no loss of original functionality. The process involves using the Langchain LCEL framework and specific prompt files located via environment variables. The function requires several inputs, including the original prompt, generated code, example usage code, and model parameters like strength and temperature. It executes a multi-step workflow: preprocessing prompt templates, invoking an LLM to generate the split, and then running a second extraction step to parse the results into JSON. The output must include the two resulting prompts and the total computational cost. Additionally, the implementation must utilize the Python Rich library for console output, handle internal module imports relatively, and include robust error handling for edge cases.",2026-01-03T03:58:37.258529+00:00
"context/change/13/modified_initial_split.prompt","The file outlines a task for an expert Python Software Engineer to develop a function named continue_generation. This function is designed to ensure the completeness of a language model's (LLM) output by detecting unfinished generations and iteratively prompting the model for continuations. The function takes a formatted input prompt and the current LLM output as inputs and returns the final, complete string. Key implementation steps include loading a specific prompt file, preprocessing it, creating a Langchain LCEL template, and utilizing an llm_selector for model selection and token counting. The process involves a loop that checks for incompleteness using an unfinished_prompt helper and appends content until finished. All console output must be formatted using the Python Rich library, displaying token counts and estimated costs. The function must also handle edge cases and errors gracefully.",2026-01-03T03:58:49.644035+00:00
"context/change/14/change.prompt","The provided file contents outline a prompt designed to generate Python code for a function named 'detect_change'. The primary objective of this function is to identify and list necessary modifications to a set of prompts based on a given change description. The function takes a list of prompt filenames (e.g., [prompt1_filename, prompt2_filename]) and a textual description of the desired changes as input. To accomplish its task, the 'detect_change' function utilizes two specific embedded LLM prompts: 'detect_change_LLM', which likely analyzes the files against the description, and 'extract_detect_change_LLM', which is responsible for parsing the analysis to extract a structured list of prompts requiring updates along with the specific details of those updates.",2026-01-03T03:58:55.931917+00:00
"context/change/14/initial_change.py","This Python file defines a single function named `change`, which is designed to process and modify input prompts using a Large Language Model (LLM). The function takes an initial prompt, input code, a description of desired changes, and model parameters (strength and temperature) as arguments. It operates in a multi-step workflow: first, it loads specific prompt templates from a directory specified by an environment variable. Next, it selects an appropriate LLM using a helper function `llm_selector` and preprocesses the prompt templates. The core logic involves two main LLM invocations: one to generate a modification based on the inputs and a second to extract the final modified prompt from the previous step's output in a structured JSON format. Throughout the process, the function calculates and prints token usage and estimated costs using the `rich` library for console output. Finally, it returns the modified prompt, the total cost of the operation, and the name of the model used, while also including error handling for file access and JSON parsing issues.",2026-01-03T03:59:01.699351+00:00
"context/change/14/initial_change_python.prompt","This document outlines the requirements for creating a Python function named change designed to modify an existing prompt based on specific instructions. The function takes several inputs, including an initial prompt string, the code generated from it, a change instruction string, and model parameters like strength and temperature. It is expected to output a modified prompt string, the total operational cost, and the name of the LLM model used. The implementation must utilize the Python Rich library for console output and employ relative imports. The process involves several steps: loading specific prompt files, preprocessing them (handling curly brackets), creating Langchain LCEL templates, and selecting an LLM model using a helper function. The function executes a two-step LLM process: first, generating a raw modified prompt based on the inputs, and second, extracting the final JSON-formatted modified_prompt from that output. Throughout execution, the function must calculate and pretty-print token counts and estimated costs, finally returning the modified prompt, total cost, and model name while handling potential errors gracefully.",2026-01-03T03:59:17.597928+00:00
"context/change/14/modified_initial_change.prompt","This file outlines the requirements for an expert Python Software Engineer to create a function named detect_change. The function's primary purpose is to analyze a list of prompt files against a provided change description to identify which prompts require modification. It takes inputs including a list of filenames, a description of the changes, and LLM parameters like strength and temperature. The expected output is a list of JSON objects detailing necessary changes for each prompt, the total cost of the operation, and the model name used. The process involves several steps: loading specific prompt templates, preprocessing them, utilizing Langchain LCEL for model interaction, and employing a custom llm_selector for model choice and token counting. The function must execute two main LLM calls—one to detect changes and another to extract those changes into a structured JSON format. All console output should be formatted using the Python Rich library, and the code must handle edge cases and errors robustly while using relative imports.",2026-01-03T03:59:31.756497+00:00
"context/change/15/README.md","The provided text documents the PDD (Prompt-Driven Development) Command Line Interface, a version 0.1.0 tool designed to streamline software development using AI models. PDD enables developers to generate code, create examples, run unit tests, and manage prompt files across various programming languages like Python, JavaScript, and Java. The CLI supports a specific naming convention for prompt files (`<basename>_<language>.prompt`) and offers global options for controlling AI model strength, temperature, and output verbosity, as well as tracking operational costs via CSV.

The documentation details eleven core commands, including `generate` (create code), `example` (create usage examples), `test` (generate unit tests), `fix` (iteratively repair code errors), `split` (break down large prompts), and `crash` (fix runtime crashes). It highlights advanced features such as multi-command chaining for complex workflows, environment variables for default output paths, and conflict detection between prompts. Additionally, the text covers security considerations regarding code execution and data privacy, troubleshooting tips, and integration strategies for CI/CD pipelines, emphasizing PDD's role in enhancing efficiency and code quality through an iterative, prompt-centric workflow.",2026-01-03T03:59:39.447626+00:00
"context/change/15/change.prompt","The provided file content is a brief instruction related to updating documentation or prompts. It specifically notes that new commands have been introduced to a README file, referenced by the path context/change/15/README.md. The core directive is to incorporate these newly added commands, such as 'detect', into the relevant prompt structure. Essentially, it serves as a reminder or a task to ensure that the system's prompts are synchronized with the latest command updates documented in the specified change log.",2026-01-03T03:59:52.839911+00:00
"context/change/15/initial_cli.py","This Python file defines a command-line interface (CLI) tool named `pdd` (Prompt-Driven Development), built using the `click` library. The tool facilitates various AI-assisted development tasks by integrating with a suite of backend modules. Key functionalities include generating code from prompt files (`generate`), creating example files (`example`), generating unit tests (`test`), preprocessing prompts (`preprocess`), and iteratively fixing code errors based on test failures (`fix`). It also includes commands for splitting complex prompts (`split`), modifying prompts based on changes (`change`), and updating prompts to reflect code modifications (`update`). The CLI supports configuration options for AI model strength, temperature, and verbosity, and includes a mechanism for tracking and logging API costs to a CSV file. Additionally, it features a utility command to install shell tab completion scripts for Bash, Zsh, and Fish shells.",2026-01-03T03:59:58.024230+00:00
"context/change/15/initial_cli_python.prompt","The provided text outlines the specifications for building a Python command-line interface (CLI) tool named pdd. This tool is designed to assist with prompt-driven development workflows. It leverages the `click` library for CLI command management and the `rich` library for pretty-printing console output. The program's architecture includes specific directories for prompts, context, and data.

The core functionality is divided into several commands: `generate` creates runnable code from prompt files; `example` produces example code from source files; `test` generates unit tests based on code and prompts; and `preprocess` prepares prompts, with an optional XML output format. Additionally, the tool includes a `fix` command to resolve code errors based on unit test feedback, which can be run in a loop. Other utility commands include `split` for dividing prompt files, `change` and `update` for modifying prompts, and `install_completion` to set up shell autocompletion. The instructions emphasize using relative imports, avoiding naming conflicts between CLI commands and internal functions, and utilizing helper functions for file loading and path construction.",2026-01-03T04:00:12.850099+00:00
"context/change/15/modified_initial_cli.prompt","The provided file serves as a comprehensive prompt or specification for an AI engineer to build a Python command-line interface (CLI) tool named pdd. The tool is designed to be built using the `click` library for CLI management and the `rich` library for pretty-printed console output. The specification outlines the project's directory structure, which includes folders for prompts, context, and data.

The core functionality of pdd is broken down into specific commands, each associated with a distinct operation and referencing example files for implementation details. Key commands include `generate` (creating code from prompts), `example` (generating examples from code), `test` (creating unit tests), and `preprocess` (handling prompt formatting, including XML tagging). The tool also features debugging and maintenance capabilities such as `fix` (resolving errors based on unit tests, with a looping option), `crash` (fixing crash-causing errors), and `conflicts` (resolving prompt conflicts). Additional utility commands include `split`, `change`, `update`, `detect`, and `install_completion` for shell integration. The prompt emphasizes avoiding naming conflicts by using distinct import names for functions that share names with CLI commands.",2026-01-03T04:00:27.274724+00:00
"context/change/16/change.prompt","The provided file outlines a series of configuration changes or directives intended for a code generation or refactoring system. It specifies modifications to how error handling loops are processed. Specifically, it instructs the system to update the 'fix_error_loop' prompt by referencing a new file path ('context/change/16/fix_error_loop_python.prompt'). Furthermore, it dictates a functional change in the error-fixing workflow: instead of invoking a function or module named 'fix_errors_from_units_tests', the system should now call 'fix_code_module_errors'. To support this transition, the file provides references to the new example implementation ('fix_code_module_errors_example.py') and the corresponding prompt definition ('fix_code_module_errors_python.prompt'). Essentially, this file acts as a patch or instruction set for swapping out an old unit-test-based error fixer with a module-based error fixer.",2026-01-03T04:00:41.314987+00:00
"context/change/16/fix_code_module_errors_example.py","This file is a Python script that demonstrates the usage of the `fix_code_module_errors` function from the `pdd` library. The script defines a `main` function that simulates a scenario where a piece of Python code contains a runtime error. Specifically, it sets up variables representing a buggy program (which attempts to sum a string instead of a list of numbers), the original prompt used to generate that code, the specific code module involved, and the resulting `TypeError` message.

The script then calls `fix_code_module_errors` with these inputs, along with configuration parameters for model strength and temperature. This function is designed to analyze the error and return corrected versions of the program and code module. Finally, the script prints the results of this operation, including boolean flags indicating if updates were necessary, the fixed source code for both the full program and the module, the total cost of the API usage, and the name of the AI model employed for the fix.",2026-01-03T04:00:48.134969+00:00
"context/change/16/fix_code_module_errors_python.prompt","This file provides a detailed specification for a Python function named fix_code_module_errors. The function is designed to automatically repair errors in a code module that caused a program crash. It takes several inputs, including the original program code, the prompt that generated the module, the problematic code module itself, the error logs, and parameters for the LLM model (strength and temperature).

The process involves a multi-step workflow using LangChain Expression Language (LCEL). First, it loads specific prompt templates from a project directory defined by the $PDD_PATH environment variable. It then selects an appropriate Large Language Model (LLM) using an internal llm_selector tool. The function executes two distinct LLM calls: the first analyzes the errors and generates a fix using the fix_code_module_errors prompt, while the second extracts structured data (booleans for updates and the fixed code strings) from that analysis using the extract_program_code_fix prompt and a JSON parser. Throughout the execution, the function tracks and pretty-prints token usage and costs. Finally, it returns the fixed program and code, flags indicating if updates are needed, the total cost of the operation, and the name of the model used.",2026-01-03T04:01:01.237863+00:00
"context/change/16/fix_error_loop.py","This Python script implements an automated iterative process to fix errors in unit tests and their corresponding code files. It defines a primary function, `fix_error_loop`, which orchestrates a cycle of running tests, analyzing failures, and attempting repairs using an external `fix_errors_from_unit_tests` module. The script utilizes `pytest` to execute tests and regex to parse the output for failure and error counts.

Key features include a `IterationResult` dataclass to track and compare the success of different attempts, ensuring the process can revert to the best-performing version if subsequent fixes degrade quality. The script manages file backups before every modification, appending failure metrics to filenames for version control. It also integrates a verification step where a separate program checks the validity of code changes; if verification fails, the code is immediately rolled back.

The loop continues until tests pass, a maximum attempt count is reached, or a budget is exceeded. Finally, the script performs a final test run and, if necessary, restores the files from the most successful iteration before returning the final status, code content, and cost metrics.",2026-01-03T04:01:14.932245+00:00
"context/change/16/fix_error_loop_python.prompt","This document outlines the specifications for an AI agent acting as an expert Python Software Engineer to create a function named fix_error_loop. The function's primary purpose is to iteratively attempt to fix errors in a unit test and its corresponding code file. It takes inputs such as file paths, a prompt, verification program, LLM parameters (strength, temperature), maximum attempts, and a budget. The process involves a loop that runs pytest, analyzes the output for failures, and uses a helper function (`fix_errors_from_unit_tests`) to generate fixes if tests fail. It manages file backups, tracks costs, and ensures code integrity by running a verification program after code changes. The function also includes logic to restore the best-performing iteration if the final attempt is not successful. The output includes the success status, final file contents, total attempts, and total cost.",2026-01-03T04:01:40.786048+00:00
"context/change/16/fix_errors_from_unit_tests_example.py","This file serves as a demonstration script for the `fix_errors_from_unit_tests` function, which is imported from the `pdd` package. The script defines a `main` function that sets up a mock scenario involving a buggy unit test and a simple Python function. Specifically, it defines a unit test string with an intentional error (asserting that 0 equals 1), a corresponding code snippet for an addition function, a prompt description, and simulated error details. It then calls `fix_errors_from_unit_tests` with these inputs, along with parameters for model strength and temperature. Finally, the script uses the `rich` library to print the results of the operation to the console, displaying whether the unit test or code was updated, the fixed versions of the test and code, the total cost of the operation, and the name of the model used.",2026-01-03T04:01:46.886018+00:00
"context/change/16/modified_fix_error_loop_python.prompt","This file defines a prompt for an AI agent acting as an expert Python Software Engineer. The agent's primary task is to implement a Python function named `fix_code_loop`. This function is designed to iteratively repair errors in a given code module by running a verification program and utilizing an LLM to generate fixes.

The prompt outlines the specific inputs required for the function, including file paths for the code and verification program, LLM parameters (strength, temperature), and constraints like maximum attempts and budget. It also details the expected outputs, such as success status, final code content, and total cost.

The core logic involves a loop that executes the verification program, captures errors, and calls a helper function `fix_code_module_errors` to generate patches. The process includes robust error handling, such as backing up files before modification, restoring previous versions if a fix fails verification, and logging all attempts. The loop terminates upon success, budget exhaustion, or reaching the maximum attempt limit.",2026-01-03T04:01:59.636326+00:00
"context/change/17/change.prompt","The file provides instructions for updating a prompt to reflect a change in methodology. Specifically, it notes that the current method has been replaced by `llm_invoke` and `load_prompt_template`. The document includes placeholders for before and after examples, referencing external files (`unfinished_prompt_python.prompt` and `updated_unfinished_prompt_python.prompt`) to demonstrate how the prompt should be modified to accommodate these new function calls.",2026-01-03T04:02:12.722178+00:00
"context/change/17/continue_generation.py","This Python file defines a module for managing and continuing the generation of text or code using Large Language Models (LLMs) via the LangChain framework. The core functionality is encapsulated in the `continue_generation` function, which orchestrates a multi-step process to extend incomplete LLM outputs. It begins by loading and preprocessing specific prompt templates from a directory specified by an environment variable. The script utilizes an `llm_selector` to choose appropriate models based on desired strength and temperature settings, calculating costs for input and output tokens throughout the process.

The workflow involves an iterative loop where the system generates content, checks if the generation is complete using a helper function (`unfinished_prompt`), and appends the new content to the existing code block. It employs Pydantic models (`TrimResultsOutput`, `TrimResultsContinuedOutput`) to structure the parsing of LLM responses, specifically for extracting code blocks and trimming continued generations. The module relies heavily on the `rich` library for console logging, providing detailed feedback on costs, token counts, and intermediate generation states. Ultimately, it returns the fully generated code block, the total financial cost of the API calls, and the name of the model used.",2026-01-03T04:02:18.570855+00:00
"context/change/17/continue_generation_python.prompt","This file outlines the specifications for a Python function named continue_generation, designed to complete the generation of a prompt using a Large Language Model (LLM). The function takes a formatted input prompt, existing LLM output, and model parameters (strength and temperature) as inputs. It returns the completed LLM output, the total cost of execution, and the name of the model used.

The process involves several steps: loading specific prompt files from an environmental path, preprocessing them, and creating Langchain LCEL templates. It utilizes an llm_selector to choose the appropriate model and track token costs. The core logic involves trimming the initial LLM output to extract code blocks, then running a generation loop. This loop uses a continue_generation prompt to extend the output and an unfinished_prompt function to detect if the generation is incomplete. If incomplete, the function loops until finished; if complete, it trims the final result to ensure seamless concatenation. Throughout the process, the function must track and calculate token costs for all model invocations. Finally, it pretty-prints the result using Rich Markdown and returns the final output string, total cost, and model name.",2026-01-03T04:02:32.990125+00:00
"context/change/17/unfinished_prompt_python.prompt","This file contains a prompt specification for an AI agent acting as an expert Python engineer. The objective is to generate a Python function named 'unfinished_prompt' that evaluates whether a given text prompt is complete or requires continuation. The function takes 'prompt_text', 'strength', and 'temperature' as inputs and returns a structured reasoning string, a boolean status ('is_finished'), and optional cost metrics. The implementation steps require loading a specific prompt file from the project path, creating a LangChain LCEL template that outputs JSON, and utilizing an internal 'llm_selector' module to choose the model and calculate token costs. The process involves invoking the model with the prompt text, pretty-printing status updates and costs using the 'rich' library, and parsing the JSON response to return the final assessment and usage data.",2026-01-03T04:02:47.668454+00:00
"context/change/17/updated_continue_generation_python.prompt","This file outlines the specifications for creating a Python function named continue_generation, designed to complete the generation of a prompt using a large language model (LLM). The function takes inputs such as a formatted input prompt, current LLM output, strength, temperature, and a verbose flag. Its primary goal is to iteratively append to the initial LLM output until the generation is deemed complete. The process involves several steps: loading and preprocessing specific prompt templates ('continue_generation_LLM', 'trim_results_start_LLM', 'trim_results_LLM'), trimming the initial output, and entering a loop where the model continues generation. Inside the loop, the function checks for completeness using an 'unfinished_prompt' utility; if incomplete, it appends the new content and repeats. Once complete, it trims the final segment and returns the consolidated 'final_llm_output', the total cost of operations, and the model name used. The instructions also include references to internal example modules for preprocessing, loading templates, and invoking the LLM.",2026-01-03T04:03:00.787005+00:00
"context/change/17/updated_unfinished_prompt_python.prompt","This file outlines the specifications for a Python function named 'unfinished_prompt', designed to evaluate whether a given text prompt is complete or requires continuation. Acting as an expert Python engineer, the developer is tasked with creating this function, which accepts 'prompt_text', 'strength', 'temperature', and a 'verbose' flag as inputs. The function's primary purpose is to leverage an LLM to analyze the input text. The process involves loading a specific prompt template named 'unfinished_prompt_LLM' and executing it using an internal 'llm_invoke' module. The function is expected to return a tuple containing a structured reasoning string explaining the assessment, a boolean 'is_finished' flag indicating completeness, the total cost of the operation, and the name of the model used. The file includes references to internal modules for loading templates and invoking LLMs, along with specific steps for passing parameters like 'PROMPT_TEXT' and handling Pydantic outputs.",2026-01-03T04:03:07.396812+00:00
"context/change/18/change.prompt","The file provides instructions for updating a prompt to reflect a change in methodology. Specifically, it notes that the current method has been replaced by `llm_invoke` and `load_prompt_template`. The document includes placeholders for before and after examples, referencing external files (`unfinished_prompt_python.prompt` and `updated_unfinished_prompt_python.prompt`) to demonstrate how the prompt should be modified to accommodate these new function calls.",2026-01-03T04:03:20.219136+00:00
"context/change/18/code_generator.py","This Python file defines a `code_generator` function designed to generate code based on a user-provided prompt using a Large Language Model (LLM). The process involves a multi-step pipeline: first, the raw prompt is preprocessed to handle formatting and recursion. Next, an appropriate LLM is selected via an `llm_selector` module based on desired strength and temperature parameters. The prompt is then executed through a LangChain pipeline, and the initial result is displayed using Markdown formatting. The function calculates the cost of input and output tokens throughout the process. Crucially, the script checks if the generated code is incomplete using an `unfinished_prompt` module; if incomplete, it triggers a continuation process. If complete, the output undergoes post-processing to ensure it is valid runnable code in the specified language. Finally, the function returns the final code, the total calculated cost, and the name of the model used, while handling potential errors gracefully.",2026-01-03T04:03:20.322242+00:00
"context/change/18/code_generator_python.prompt","This file outlines the specifications for a Python function named code_generator, designed to compile a raw text prompt into a runnable code file using a Large Language Model (LLM). The function takes inputs including the raw prompt, the target programming language (e.g., Python, Bash), a strength parameter (0-1), and a temperature setting. It returns the resulting runnable code, the total cost of the operation, and the name of the model used.

The process leverages LangChain Expression Language (LCEL) and involves an eight-step workflow. First, the prompt is preprocessed. Next, an LCEL template is created, and an appropriate LLM is selected via an internal `llm_selector` module. The system then executes the prompt, calculating and displaying token usage and costs. It includes logic to detect incomplete outputs using an `unfinished_prompt` function; if incomplete, it triggers a `continue_generation` function. Otherwise, the output is refined using a `postprocess` module. Finally, the function calculates the total cost across all steps and returns the final code artifact. The file also references several external context files for examples of internal modules like preprocessing, postprocessing, and token counting.",2026-01-03T04:03:27.320167+00:00
"context/change/18/unfinished_prompt_python.prompt","This file contains a prompt specification for an AI agent acting as an expert Python engineer. The objective is to generate a Python function named 'unfinished_prompt' that evaluates whether a given text prompt is complete or requires continuation. The function takes 'prompt_text', 'strength', and 'temperature' as inputs and returns a structured reasoning string, a boolean status ('is_finished'), and optional cost metrics. The implementation steps require loading a specific prompt file from the project path, creating a LangChain LCEL template that outputs JSON, and utilizing an internal 'llm_selector' module to choose the model and calculate token costs. The process involves invoking the model with the prompt text, pretty-printing status updates and costs using the 'rich' library, and parsing the JSON response to return the final assessment and usage data.",2026-01-03T04:03:34.610392+00:00
"context/change/18/updated_code_generator_python.prompt","This file outlines the specifications for an AI agent acting as an expert Python engineer to create a function named code_generator. The primary purpose of this function is to compile a raw text prompt into a runnable code file in a specified language (e.g., Python, Bash). The function accepts inputs such as the raw prompt, target language, model strength, temperature, and a verbosity flag. It returns the generated runnable code, the total cost of the operation, and the name of the LLM model used. The process involves a multi-step workflow using Langchain: preprocessing the prompt, loading templates, invoking the LLM, and handling incomplete generations via a specific unfinished_prompt check. If the output is incomplete, it triggers a continuation; otherwise, it post-processes the result. The file also references several internal modules for tasks like loading templates, invoking the LLM, and post-processing, and emphasizes graceful error handling and detailed verbose logging of costs and token usage.",2026-01-03T04:03:41.148292+00:00
"context/change/18/updated_unfinished_prompt_python.prompt","This file outlines the specifications for a Python function named 'unfinished_prompt', designed to evaluate whether a given text prompt is complete or requires continuation. Acting as an expert Python engineer, the developer is tasked with creating this function, which accepts 'prompt_text', 'strength', 'temperature', and a 'verbose' flag as inputs. The function's primary purpose is to leverage an LLM to analyze the input text. The process involves loading a specific prompt template named 'unfinished_prompt_LLM' and executing it using an internal 'llm_invoke' module. The function is expected to return a tuple containing a structured reasoning string explaining the assessment, a boolean 'is_finished' flag indicating completeness, the total cost of the operation, and the name of the model used. The file includes references to internal modules for loading templates and invoking LLMs, along with specific steps for passing parameters like 'PROMPT_TEXT' and handling Pydantic outputs.",2026-01-03T04:03:47.662922+00:00
"context/change/19/change.prompt","The provided file content is a directive for updating a prompt or code snippet to reflect a change in methodology. Specifically, it notes that the functions `llm_invoke` and `load_prompt_template` have replaced a previous method, necessitating an update to the prompt to align with this new approach. The file includes an example structure to illustrate the required change, utilizing XML-like tags `<example>`, `<before_example>`, and `<after_example>`. These tags enclose references to external files (`unfinished_prompt_python.prompt` and `updated_unfinished_prompt_python.prompt`) which likely contain the concrete code or text before and after the modification. Essentially, this snippet serves as a migration guide or a task description for a developer or an automated system to refactor existing prompts to use the new `llm_invoke` and `load_prompt_template` functions.",2026-01-03T04:03:54.014663+00:00
"context/change/19/context_generator.py","This file defines a Python function named `context_generator` designed to generate concise usage examples for a given code module using a Large Language Model (LLM). The function orchestrates a multi-step process involving prompt loading, preprocessing, model selection, and post-processing. It relies on the `langchain` library for chaining LLM operations and `rich` for console output.

The workflow begins by verifying the `PDD_PATH` environment variable to locate prompt templates. It then loads and preprocesses an `example_generator_LLM.prompt` file. The function selects an appropriate LLM based on specified strength and temperature parameters using `llm_selector`. It constructs a LangChain template and invokes the model to generate the example code. Crucially, the script includes logic to handle incomplete generations by detecting unfinished outputs and continuing generation if necessary. Finally, the output undergoes post-processing to refine the code snippet. Throughout the execution, the function tracks and calculates the estimated financial cost of the API usage, returning the generated example code, the total cost, and the name of the model used.",2026-01-03T04:04:00.865349+00:00
"context/change/19/context_generator_python.prompt","The provided text is a detailed prompt specification for an expert Python engineer to create a function named context_generator. This function is designed to generate concise usage examples for a given code module. The prompt outlines specific inputs (code_module, prompt, language, strength, temperature) and outputs (example_code, total_cost, model_name). It includes references to external context files for Python preambles, LangChain Expression Language (LCEL) examples, and internal module usage (preprocessing, LLM selection, unfinished prompt detection, etc.). The core logic involves a nine-step process using LangChain: loading a specific prompt file, preprocessing it, selecting an LLM, invoking the model via LCEL, handling incomplete generations, post-processing the result, and calculating the total cost based on token usage.",2026-01-03T04:04:08.636012+00:00
"context/change/19/unfinished_prompt_python.prompt","This file contains a prompt specification for an AI agent acting as an expert Python engineer. The objective is to generate a Python function named 'unfinished_prompt' that evaluates whether a given text prompt is complete or requires continuation. The function takes 'prompt_text', 'strength', and 'temperature' as inputs and returns a structured reasoning string, a boolean status ('is_finished'), and optional cost metrics. The implementation steps require loading a specific prompt file from the project path, creating a LangChain LCEL template that outputs JSON, and utilizing an internal 'llm_selector' module to choose the model and calculate token costs. The process involves invoking the model with the prompt text, pretty-printing status updates and costs using the 'rich' library, and parsing the JSON response to return the final assessment and usage data.",2026-01-03T04:04:14.403478+00:00
"context/change/19/updated_context_generator_python.prompt","This file outlines a prompt for an expert Python engineer to create a function named context_generator. The primary purpose of this function is to generate a concise usage example for a given code module. The function takes inputs such as the code module itself, the original prompt used to create it, the programming language (defaulting to Python), and parameters for the LLM model like strength and temperature. It outputs a tuple containing the generated example code, the total cost, and the model name used. The process involves several steps: loading a specific prompt template (example_generator_LLM), invoking the LLM with the provided parameters, checking for incomplete output using an internal unfinished_prompt utility, and either continuing generation or post-processing the result. The file also references several internal modules for handling tasks like loading templates, invoking the LLM, and managing unfinished prompts.",2026-01-03T04:04:21.084680+00:00
"context/change/19/updated_unfinished_prompt_python.prompt","This file outlines the specifications for a Python function named 'unfinished_prompt', designed to evaluate whether a given text prompt is complete or requires continuation. Acting as an expert Python engineer, the developer is tasked with creating this function, which accepts 'prompt_text', 'strength', 'temperature', and a 'verbose' flag as inputs. The function's primary purpose is to leverage an LLM to analyze the input text. The process involves loading a specific prompt template named 'unfinished_prompt_LLM' and executing it using an internal 'llm_invoke' module. The function is expected to return a tuple containing a structured reasoning string explaining the assessment, a boolean 'is_finished' flag indicating completeness, the total cost of the operation, and the name of the model used. The file includes references to internal modules for loading templates and invoking LLMs, along with specific steps for passing parameters like 'PROMPT_TEXT' and handling Pydantic outputs.",2026-01-03T04:04:27.491815+00:00
"context/change/2/change.prompt","The provided file content is a concise directive regarding a code refactoring or dependency change within a software project. Specifically, it instructs the developer or user to switch the method used for counting tokens in a prompt. Instead of using the 'tiktoken' library directly, the instruction mandates the use of a 'token_counter' utility derived from a module named 'llm_selector'. This change likely aims to standardize token counting across the application, abstracting the specific tokenizer implementation behind the 'llm_selector' interface, or to ensure compatibility with specific large language model configurations managed by that selector.",2026-01-03T04:04:34.344453+00:00
"context/change/2/final_context_generator_python.prompt","The file outlines a task for an expert Python engineer to develop a function named context_generator. The primary goal of this function is to produce a concise usage example for a specific code module. The function accepts several inputs: the code module itself, the prompt used to generate it, the programming language (defaulting to Python), and parameters for the Large Language Model (LLM) such as strength and temperature.

The process involves a multi-step workflow using Langchain LCEL. First, it retrieves a prompt template from a specific file path defined by an environment variable. It then selects an appropriate LLM and preprocesses the input prompt. The core execution involves running the code through the model via Langchain LCEL, passing necessary parameters, and calculating token usage and cost. Finally, the function post-processes the mixed text and code output to extract a runnable example and returns both the generated example code and the total calculated cost.",2026-01-03T04:04:40.761400+00:00
"context/change/2/initial_context_generator.py","This Python script defines a function named `context_generator` designed to generate example code snippets using a Large Language Model (LLM). The function orchestrates a multi-step process that begins by loading a specific prompt template from a file path defined by an environment variable. It utilizes a custom `llm_selector` to choose an appropriate LLM based on provided strength and temperature parameters. The input prompt is preprocessed before being fed into a LangChain pipeline, which combines the prompt template, the selected LLM, and a string output parser. Before execution, the script calculates and prints the estimated token count and input cost using `tiktoken`. After invoking the model chain, the output undergoes a postprocessing step. Finally, the function calculates the total cost of the operation (including input, output, and postprocessing costs) and returns a tuple containing the generated example code and the total financial cost.",2026-01-03T04:04:54.024952+00:00
"context/change/2/initial_context_generator_python.prompt","The file outlines a task for an expert Python engineer to develop a function named context_generator. The primary objective of this function is to produce a concise usage example for a specific code module. The function accepts several inputs: the code module itself, the original prompt used to generate it, the programming language (defaulting to Python), and parameters for the Large Language Model (LLM) such as strength and temperature. The expected outputs are the generated example code string and the total computational cost. The implementation process involves a seven-step workflow using Langchain LCEL. This includes loading a prompt template from a specific environment path, selecting an appropriate LLM, preprocessing the input prompt, and invoking the model with specific parameters. Additionally, the function must calculate and print token usage and costs using tiktoken, and finally post-process the model's output to extract runnable code.",2026-01-03T04:05:07.398746+00:00
"context/change/20/change.prompt","The provided file outlines a specific refactoring task related to updating prompt handling methods within a codebase. It states that the functions `llm_invoke` and `load_prompt_template` have superseded a previous method, necessitating changes to the `input_prompt`. To guide this update, the file includes a section of examples, enclosed in `<examples>` tags. These examples demonstrate the required transformation by pairing `before_example` files with their corresponding `after_example` versions. Specifically, it lists four distinct examples (IDs 1 through 4), referencing file paths for prompts such as `unfinished_prompt_python.prompt`, `context_generator_python.prompt`, `code_generator_python.prompt`, and `generate_test_python.prompt`, along with their updated counterparts. The structure suggests this file serves as a dataset or instruction set for an automated refactoring tool or a developer guide for migrating to the new prompt invocation system.",2026-01-03T04:05:20.235743+00:00
"context/change/20/generate_test.py","This Python file defines a function named `generate_test` designed to automatically generate unit tests for a given piece of code using Large Language Models (LLMs) via the Langchain library. The process begins with input validation for parameters like model strength, temperature, and the target programming language. It then loads a specific prompt template from a file path defined in environment variables. The function utilizes a helper module, `llm_selector`, to choose an appropriate LLM based on the desired strength and temperature, calculating estimated costs for input tokens. The core logic involves creating a Langchain pipeline (LCEL) that feeds the preprocessed prompt, original code, and language into the model to generate the test code. The output is displayed using the `rich` library for formatting. Crucially, the function includes logic to handle incomplete generations by checking the output's ending and invoking a `continue_generation` function if necessary. Finally, the result undergoes post-processing, and the function returns the generated unit test code, the total calculated cost of the operation, and the name of the model used.",2026-01-03T04:05:32.460324+00:00
"context/change/20/generate_test_python.prompt","This file contains a detailed prompt specification for an expert Python Software Engineer AI agent. The objective is to write a Python function named `generate_test` that automatically creates unit tests for a given code file. The prompt outlines the function's inputs (including the original prompt, code, model strength, temperature, and language) and outputs (the generated unit test, total cost, and model name). It provides context by including references to a Python preamble, a LangChain Expression Language (LCEL) example, and several internal module examples for tasks like preprocessing, LLM selection, and postprocessing. The prompt details a nine-step execution plan using LangChain, which involves loading a specific prompt file, preprocessing inputs, selecting an LLM model, running the model via LCEL, handling incomplete generations, postprocessing the output, and calculating the total cost.",2026-01-03T04:05:46.635323+00:00
"context/change/20/updated_generate_test_python.prompt","This file contains a detailed prompt specification for an AI agent acting as an expert Python Software Engineer. The objective is to write a Python function named generate_test that automatically creates unit tests for a given code file. The specification outlines the function's inputs (including the original prompt, source code, model strength, temperature, language, and a verbose flag) and expected outputs (the generated unit test code, total cost, and model name). It provides specific instructions on using internal modules for tasks such as loading prompt templates, invoking the LLM, preprocessing inputs, handling unfinished generations, and postprocessing results. The workflow is broken down into six steps: loading the 'generate_test_LLM' template, preprocessing it, invoking the model with specific parameters, handling verbose logging, detecting and fixing incomplete outputs, and finally returning the results. The prompt also emphasizes error handling and cost tracking throughout the process.",2026-01-03T04:05:52.968807+00:00
"context/change/21/auto_deps_main_python.prompt","The provided file is a prompt template designed for an AI model to generate a specific Python function named `auto_deps_main`. This function is intended to be a core component of a command-line interface (CLI) tool called `pdd`, specifically handling the logic for an `auto-deps` command. The prompt outlines the function's inputs, which include a Click context object, file paths for prompts and directories, a CSV path for dependency tracking, an output path, and a force scan flag. It also defines the expected outputs: a tuple containing the modified prompt string, the operation cost, and the model name. The prompt includes references to external context files, such as a Python preamble, examples of using the Click library, and internal module usage examples for path construction and include insertion. Additionally, it references a README file to provide further context on how the `auto-deps` command should function within the broader application.",2026-01-03T04:05:59.563880+00:00
"context/change/21/auto_include.py","This Python file defines a function named `auto_include` designed to automatically identify and insert necessary file dependencies into a given input prompt. The process involves several steps orchestrated through Large Language Model (LLM) interactions. First, it validates inputs and loads specific prompt templates. It then calls `summarize_directory` to generate summaries of files within a specified path, potentially using an existing CSV cache. These summaries are parsed and fed into an LLM alongside the original input prompt to determine which files are relevant context. A subsequent extraction step, also powered by an LLM and structured via a Pydantic model (`AutoIncludeOutput`), isolates the specific string of dependencies. The function returns the identified dependencies, the CSV summary data, the total cost of the LLM operations, and the model name used. The file also includes a `main` function demonstrating example usage with `rich` library logging for verbose output.",2026-01-03T04:06:06.092611+00:00
"context/change/21/auto_include_python.prompt","This document outlines the specifications for creating a Python function named auto_include, designed to automatically identify and generate necessary dependencies for a given prompt. Acting as an expert Python Software Engineer, the developer is tasked with implementing this function, which takes inputs such as an input prompt, a directory path for dependencies, a CSV file string, and parameters for the LLM model like strength and temperature. The function's expected output is a tuple containing a string of dependencies, an updated CSV string, the total generation cost, and the model name used.

The process involves several key steps: loading specific prompt templates (auto_include_LLM and extract_auto_include_LLM), summarizing directory contents to create a list of available includes, and invoking an LLM to select relevant dependencies based on the input prompt. A second LLM invocation extracts the final string of includes. The document also provides references to internal module examples for loading templates, invoking LLMs, and summarizing directories to guide the implementation.",2026-01-03T04:06:12.933748+00:00
"context/change/21/change.prompt","The provided file outlines instructions for creating a wrapper prompt, specifically designated as `*_main`, to simplify the integration of prompts into a main CLI program. The primary goal is to reduce code complexity by enabling the CLI to execute a prompt with a single line of code. The instructions specify that only the `_main` wrapper prompt should be created, as the core input prompt resides in a separate file. The file includes an example structure demonstrating a `before_example` with an initial prompt and code, followed by an `after_example` showing the resulting `auto_deps_main_python.prompt`. Additionally, it references a `cli_command_readme` which includes the contents of a `./README.md` file to provide further context on how the CLI command operates.",2026-01-03T04:06:26.295257+00:00
"context/change/22/change.prompt","The provided file contains instructions and examples for creating a wrapper prompt, specifically designated with a '_main' suffix. The primary objective outlined in the file is to simplify the main Command Line Interface (CLI) program by encapsulating complexity within this wrapper. This allows the CLI to execute the prompt using a single line of code. The file structure includes a directive to generate only the '_main' wrapper prompt, noting that the core input prompt resides in a separate file. It provides XML-structured examples demonstrating the transformation from an initial state (containing a prompt and associated code) to a final state (the resulting '_main' prompt file). Additionally, the file references an external README.md file to provide context on how the CLI command operates.",2026-01-03T04:06:38.059868+00:00
"context/change/22/preprocess.py","This Python script defines a module for preprocessing text prompts, likely intended for dynamic content generation or preparing inputs for Large Language Models. The core functionality is encapsulated in the `preprocess` function, which orchestrates a pipeline of text transformations while using the `rich` library for console logging.

The script handles three main tasks. First, it processes file inclusions using two distinct syntaxes: a backtick pattern (` ```<path>``` `) and an XML-style tag (`<include>`). It supports recursive processing, allowing included files to trigger further inclusions. Second, it processes specific tags, most notably `<shell>`, which executes system commands via `subprocess` and embeds the standard output directly into the prompt text.

Finally, the script includes a sophisticated function, `double_curly`, to manage curly bracket escaping (converting `{` to `{{`). This is crucial for preventing errors in Python string formatting. This function intelligently parses the text to distinguish between code blocks and regular content, ensuring brackets inside code blocks are always escaped, while allowing specific keys (via `exclude_keys`) to remain unescaped in the main text to serve as valid formatting placeholders.",2026-01-03T04:06:49.558128+00:00
"context/change/22/preprocess_main_python.prompt","This file contains a prompt template designed for an AI model to generate a Python function named `preprocess_main`. The goal of this function is to serve as a Command Line Interface (CLI) wrapper for preprocessing prompt files. The prompt specifies that the function should accept inputs such as a Click context object, a prompt file path, an optional output path, and a boolean flag for XML delimiters. It outlines the expected outputs as a tuple containing the preprocessed prompt string, the operation cost, and the model name. The prompt includes references to external context files, such as Python preambles, examples of using the `click` library, and internal module usage examples for path construction, preprocessing, and XML tagging. It also references a README file to ensure the generated code aligns with the CLI command's documentation and supports global options like verbosity.",2026-01-03T04:07:07.410593+00:00
"context/change/22/preprocess_python.prompt","This file outlines the requirements for a Python function named 'preprocess_prompt', designed to prepare prompt strings for Large Language Models (LLMs). The function takes a prompt string, along with optional boolean flags for recursion and curly bracket doubling, and an optional list of keys to exclude from doubling. Its primary purpose is to parse and process specific XML-like tags within the prompt. Specifically, it handles an 'include' tag to insert file contents directly, a 'pdd' tag to remove comments, and a 'shell' tag to execute commands and capture their output. The function must support recursive processing for nested includes, which can appear as standard XML tags or within triple backticks. Additionally, it includes logic to double single curly brackets (escaping them for format strings) unless the keys are excluded or already doubled, handling nested brackets recursively. The implementation also requires a helper function, 'get_file_path', to resolve file paths relative to the current directory.",2026-01-03T04:07:13.170901+00:00
"context/change/3/change.prompt","The provided file content is a concise directive regarding a code refactoring or dependency change within a software project. Specifically, it instructs the developer or user to switch the method used for counting tokens in a prompt. Instead of using the 'tiktoken' library directly, the instruction mandates the use of a 'token_counter' utility derived from a module named 'llm_selector'. This change likely aims to standardize token counting across the application, abstracting the specific tokenizer implementation behind the 'llm_selector' interface, or to ensure compatibility with specific large language model configurations managed by that selector.",2026-01-03T04:07:25.651892+00:00
"context/change/3/final_test_generator_python.prompt","The file outlines the requirements for a Python function named `test_generator`, designed to automatically create unit tests from a given code file using a Large Language Model (LLM). Acting as an expert Python Software Engineer, the function takes inputs such as the original prompt, the source code, model strength, temperature, and the target language. It returns the generated unit test code and the total cost of the operation. The process involves several steps leveraging the Langchain library: loading a prompt template from a specific environment path (`$PDD_PATH`), preprocessing the prompt, selecting an appropriate LLM via `llm_selector`, and executing the model using Langchain's LCEL (LangChain Expression Language). The function is also required to provide console feedback using the `rich` library, displaying token counts, costs, and pretty-printed markdown results. Finally, it post-processes the model's output to extract runnable code before returning the final unit test and cost metrics.",2026-01-03T04:07:32.192648+00:00
"context/change/3/initial_test_generator_python.prompt","The file outlines the requirements for a Python function named `test_generator`, designed to automatically create unit tests from a given code file using a Large Language Model (LLM). Acting as an expert Python Software Engineer, the function takes inputs such as the original prompt, the code to be tested, model strength, temperature, and the target language. It outputs the generated unit test code and the total cost of the operation. The process involves several steps: loading a specific prompt file from the project path, preprocessing the prompt, creating a LangChain LCEL template, selecting an appropriate LLM, and invoking the model with specific parameters. The function is required to use the `rich` library for pretty-printing console output, including token counts calculated via `tiktoken` and cost estimates. Finally, the output must be post-processed to extract runnable code from the model's response, stripping away markdown formatting, before returning the final unit test and total cost.",2026-01-03T04:07:45.137810+00:00
"context/change/3/intial_test_generator.py","This Python script defines a function named `test_generator` designed to automate the creation of unit tests for a given piece of code using a Large Language Model (LLM). The process begins by loading a specific prompt template from a file path defined in the environment variables. This template is then preprocessed and converted into a Langchain `PromptTemplate`. The script utilizes a custom `llm_selector` to choose an appropriate LLM based on provided strength and temperature parameters. It calculates the estimated cost of the operation by counting input tokens using `tiktoken` before invoking the model chain. Once the model generates a response, the script calculates the output token cost, pretty-prints the result using Markdown, and then passes the output to a `postprocess` function to refine the unit test code. Finally, the function returns the generated unit test and the total calculated cost (input, output, and post-processing), while handling and logging any exceptions that occur during execution.",2026-01-03T04:07:59.290161+00:00
"context/change/4/change.prompt","The provided file content is a concise directive regarding a code refactoring or dependency change within a software project. Specifically, it instructs the developer or user to switch the method used for counting tokens in a prompt. Instead of using the 'tiktoken' library directly, the instruction mandates the use of a 'token_counter' utility derived from a module named 'llm_selector'. This change likely aims to standardize token counting across the application, abstracting the specific tokenizer implementation behind the 'llm_selector' interface, or to ensure compatibility with specific large language model configurations managed by that selector.",2026-01-03T04:08:12.539063+00:00
"context/change/4/final_postprocess_python.prompt","The file outlines the requirements for creating a Python function named `postprocess` designed to refine the string output of a Large Language Model (LLM) into executable code. The function takes `llm_output`, `language`, `strength`, and `temperature` as inputs and returns the cleaned `extracted_code` along with the `total_cost` of the operation. The logic dictates that if the `strength` parameter is 0, a zero-cost method (`postprocess_0`) is used. Otherwise, the function utilizes a Langchain LCEL pipeline. This involves loading a specific prompt template from a project path, selecting an LLM model via a helper function, and invoking the model to extract code in JSON format. The process includes steps for token counting, cost calculation, and pretty-printing status updates and results using the `rich` library. Finally, the function cleans up formatting artifacts like triple backticks before returning the processed code string and the calculated cost, while ensuring robust error handling throughout execution.",2026-01-03T04:08:18.624266+00:00
"context/change/4/initial_postprocess.py","This Python script defines a function named `postprocess` designed to extract code from the output of a Large Language Model (LLM). The function operates based on a `strength` parameter; if the strength is zero, it defaults to a simpler extraction method via `postprocess_0`. For non-zero strength values, the script employs a more sophisticated approach using LangChain. It loads a specific prompt template (`extract_code_LLM.prompt`) from a directory specified by the `PDD_PATH` environment variable. The function then selects an appropriate LLM using `llm_selector`, calculates token usage and estimated costs using `tiktoken`, and processes the input through a LangChain LCEL pipeline (prompt template, LLM, and JSON parser). Finally, it extracts the code from the model's JSON response, prints the result and cost metrics using the `rich` library for formatting, and returns both the extracted code string and the total calculated cost.",2026-01-03T04:08:32.340079+00:00
"context/change/4/initial_postprocess_python.prompt","The file outlines the requirements for an expert Python engineer to create a function named `postprocess`. This function is designed to sanitize string output from a Large Language Model (LLM), which typically contains a mix of conversational text and code blocks, into a clean, executable code string. The function accepts four inputs: the raw `llm_output`, the target programming `language`, and model parameters `strength` and `temperature`. It returns the `extracted_code` and the `total_cost` of the operation.

The logic involves a multi-step process. If the `strength` parameter is 0, it defaults to a zero-cost processing method. Otherwise, it utilizes a specific prompt file (`extract_code_LLM.prompt`) and constructs a Langchain LCEL pipeline to process the text via an LLM. The function must handle token counting (using tiktoken), cost calculation, and pretty-printing of status updates and results using the `rich` library. It specifically requires stripping markdown code fences (triple backticks) from the final output. The instructions also emphasize robust error handling and provide references to context examples for Langchain usage, LLM selection, and token counting.",2026-01-03T04:08:45.758419+00:00
"context/change/5/change.prompt","The provided file content is a concise directive regarding a code refactoring or dependency change within a software project. Specifically, it instructs the developer or user to switch the method used for counting tokens in a prompt. Instead of using the 'tiktoken' library directly, the instruction mandates the use of a 'token_counter' utility derived from a module named 'llm_selector'. This change likely aims to standardize token counting across the application, abstracting the specific tokenizer implementation behind the 'llm_selector' interface, or to ensure compatibility with specific large language model configurations managed by that selector.",2026-01-03T04:09:00.634580+00:00
"context/change/5/final_split_python.prompt","This file outlines the requirements for developing a Python function named split, designed to decompose a single input prompt into a sub_prompt and a modified_prompt without losing functionality. The function targets an expert Python Software Engineer context and mandates the use of the Python Rich library for pretty-printing console outputs.

The function accepts five inputs: the original prompt, the generated code, an example usage code, and LLM parameters for strength and temperature. It returns the two split prompts and the total execution cost. The implementation process involves seven specific steps: loading prompt templates from a specified path, preprocessing them, creating Langchain LCEL templates, and utilizing an llm_selector for model selection and token counting. The workflow requires running the input through an LLM to generate a split strategy, followed by a second LLM pass to extract the specific prompts in JSON format. The function must calculate and display token counts and estimated costs at each stage, pretty-print the final results using Rich Markdown, and handle edge cases or errors gracefully.",2026-01-03T04:09:07.485489+00:00
"context/change/5/initial_split.py","The provided file contains a Python implementation of a function named `split`, designed to process prompts and code using Large Language Models (LLMs) via the Langchain library. The function orchestrates a multi-step workflow that begins by loading specific prompt templates from file paths defined by environment variables. It utilizes a custom `llm_selector` to choose an appropriate model based on desired strength and temperature settings. The core logic involves two main LLM interactions: first, generating an initial output based on input prompts and code, and second, extracting structured JSON data (specifically a `sub_prompt` and `modified_prompt`) from that output. Throughout the process, the script employs the `tiktoken` library to calculate token usage and estimate costs for both input and output. It also leverages the `rich` library for enhanced console output, displaying progress updates, cost estimates, and the final extracted prompts in Markdown format. The function ultimately returns the extracted sub-prompt, the modified prompt, and the total calculated cost of the operations.",2026-01-03T04:09:20.360083+00:00
"context/change/5/initial_split_python.prompt","This file outlines the requirements for developing a Python function named split, designed to divide a single input prompt into a sub_prompt and a modified_prompt without losing functionality. The function targets an expert Python Software Engineer and mandates the use of the Python Rich library for pretty-printing console outputs.

The function takes several inputs: the original prompt, generated code, example usage code, and LLM parameters like strength and temperature. It outputs the two split prompts and the total execution cost. The implementation steps involve loading specific prompt files (`split_LLM.prompt` and `extract_prompt_split_LLM.prompt`), preprocessing them, and utilizing Langchain LCEL templates. The process requires selecting an LLM via `llm_selector`, running the input through the model to generate a string, and then running a second pass to extract the split prompts as JSON.

Crucially, the function must calculate and display token counts and estimated costs using `tiktoken` at each stage. Finally, it must pretty-print the results using Rich Markdown and handle edge cases or errors gracefully.",2026-01-03T04:09:34.093770+00:00
"context/change/6/change.prompt","This file outlines specific modifications required for the codebase regarding token counting and cost calculation mechanisms. The primary directive is to replace the existing usage of the 'tiktoken' library with a custom 'token_counter' utility imported from the 'llm_selector' module for the purpose of counting tokens within prompts. This change suggests a move towards a centralized or standardized method for handling token metrics across the application. Additionally, the file specifies an enhancement for the 'xml tagger' component. It is now required to calculate and return the total financial cost associated with executing LangChain Expression Language (LCEL) chains. This implies that the system needs to track usage metrics more comprehensively, likely for billing, auditing, or optimization purposes, ensuring that the expense of running these specific language model operations is transparent and accessible.",2026-01-03T04:09:47.992583+00:00
"context/change/6/final_xml_tagger_python.prompt","The provided text outlines a specification for an expert Python engineer to create a function named xml_tagger. This function is designed to enhance raw Large Language Model (LLM) prompts by adding XML tags, thereby improving their structure and readability. The function takes a raw prompt string, along with strength and temperature parameters for the model, as input. It outputs the enhanced, XML-tagged prompt string and the total cost of the operation. The implementation requires using the Langchain library and its LCEL (LangChain Expression Language) syntax. The process involves a multi-step workflow: loading specific prompt templates from a project path, selecting an LLM model with token counting capabilities, and executing two distinct LCEL chains. The first chain generates an analysis with XML tags, while the second extracts the clean XML content into a JSON format. The specification emphasizes user feedback via pretty-printed console messages using the Python rich library, detailing token counts and costs at each step. Finally, the function must handle errors gracefully and return the final tagged string and the cumulative cost.",2026-01-03T04:10:01.519005+00:00
"context/change/6/initial_xml_tagger.py","This Python script defines a function named `xml_tagger` designed to process raw text prompts by applying XML tagging through a multi-step Large Language Model (LLM) workflow using Langchain. The script initializes a SQLite cache for LLM responses and utilizes a custom `llm_selector` to choose an appropriate model based on strength and temperature parameters. The core process involves two main stages: first, converting a raw prompt into an XML-structured analysis using a specific prompt template (`xml_convertor_LLM.prompt`), and second, extracting the final XML-tagged content using another template (`extract_xml_LLM.prompt`) and a JSON output parser based on a Pydantic model. Throughout execution, the script calculates and displays token counts and estimated costs using `tiktoken`, and utilizes the `rich` library for formatted console output. It relies on environment variables to locate prompt files and handles errors gracefully by printing them to the console.",2026-01-03T04:10:14.886833+00:00
"context/change/6/initial_xml_tagger_python.prompt","This document outlines the requirements for creating a Python function named xml_tagger designed to enhance Large Language Model (LLM) prompts by adding XML tags for better structure and readability. The function takes a raw prompt string, along with strength and temperature parameters, and returns an XML-tagged version of the prompt. The implementation must utilize the Langchain library, specifically its LCEL (LangChain Expression Language) syntax, and incorporate the Python rich library for pretty-printing console output.

The process involves a multi-step workflow: first, loading specific prompt templates from a project directory defined by the $PDD_PATH environment variable; second, running the raw prompt through an LLM to generate an analysis containing XML tags; and third, using a secondary extraction prompt to isolate the final XML-tagged string from the initial analysis. The function is also required to calculate and display token usage and associated costs using tiktoken at various stages. Error handling for missing parameters or model response issues is mandatory.",2026-01-03T04:10:29.347382+00:00
"context/change/7/change.prompt","The provided text outlines the usage and specifications for a Python code module designed to automatically fix errors arising from unit tests. Instead of operating as a Command Line Interface (CLI) tool named 'pdd', the functionality is encapsulated within a Python function named 'fix_errors_from_unit_tests'. This function accepts four primary input parameters: the failing 'unit_test' code, the implementation 'code' being tested, the specific 'error' message generated during execution, and a 'strength' float parameter (0-1) that guides the selection of the Large Language Model (LLM) used for the fix. The module returns four outputs: boolean flags indicating if the unit test or code was updated, and the actual strings for the 'fixed_unit_test' and 'fixed_code'. Additionally, the documentation notes a modification where the module will be implemented as a function named 'fix_error_loop' which accepts an additional 'temperature' parameter, further customizing the LLM's generation behavior. The example usage demonstrates a standard main execution block where inputs are defined, the function is called, and the resulting corrected code and test status are printed to the console.",2026-01-03T04:10:43.339903+00:00
"context/change/7/final_fix_errors_python.prompt","This document outlines the requirements for creating a Python function named fix_error_loop, designed to iteratively repair errors in unit tests and their corresponding code files. The function takes inputs including file paths for the unit test, the code being tested, and a verification program, along with parameters for LLM strength, temperature, and a maximum attempt limit. The process involves a loop that runs pytest, captures output to an error log, and analyzes failures. If tests fail, the function creates backups of the current files, calls a helper function fix_errors_from_unit_tests to generate fixes using an LLM, and verifies the new code using the provided verification program. If verification fails, backups are restored. The loop continues until the tests pass or the maximum attempts are reached. The function utilizes the rich library for pretty-printed console output and returns the success status, final file contents, and total attempt count. It emphasizes robust error handling for file I/O and subprocess execution.",2026-01-03T04:10:57.882563+00:00
"context/change/7/initial_fix_errors.py","The provided file contains the source code and documentation for a Python script named `fix_errors.py`. This script is designed to automate the debugging and repair process for software projects by integrating unit testing with an automated fix tool (referred to as PDD). The core functionality revolves around a loop that runs `pytest` on a specified unit test file, captures the output, and logs any failures or errors to `error.log`. If tests fail, the script creates backup copies of the current code and test files, tagged with the iteration count and error statistics. It then invokes the PDD tool to attempt an automated fix based on the error log and a specified strength parameter. After applying a potential fix, the script runs a verification program to ensure the application's integrity is maintained. If verification fails, the script reverts to the backup files. This cycle repeats until all tests pass or a maximum number of iterations is reached. The file includes helper functions for file management, error extraction, and subprocess execution, along with a main execution block that parses command-line arguments for file paths, fix strength, and iteration limits.",2026-01-03T04:11:04.985832+00:00
"context/change/7/initial_fix_errors_python.prompt","The file outlines a prompt for an expert Python software engineer to create a script named fix_errors.py. This script is designed to automate the debugging process using a CLI tool called pdd and the rich library for pretty-printed console output. The script accepts arguments for a unit test file, a code file, a verification program, a strength parameter, and a retry limit. The workflow involves an iterative loop: first, it removes old logs and runs pytest, piping output to an error log. If tests fail, it analyzes the error count, backs up the current files with versioned filenames, and invokes pdd to attempt a fix. It then verifies that the code still runs; if successful, the loop repeats with the updated files. If the fix breaks the program, it reverts to the original files and retries the fix step. The process concludes with a final pytest run, logging and displaying the results.",2026-01-03T04:11:11.834282+00:00
"context/change/8/change.prompt","The provided text outlines specific technical modifications required for a software project, likely involving Large Language Model (LLM) integration. The primary directive is to replace the current token counting mechanism; specifically, the developer must switch from using the 'tiktoken' library to a custom 'token_counter' function located within the 'llm_selector' module for processing prompts. Additionally, the instructions mandate an update to the 'fix_errors_from_unit_tests' function, requiring it to accept a 'temperature' parameter and utilize it during execution, which suggests a need for adjustable creativity or randomness in the model's output during error correction. Finally, the update must include functionality to calculate and output the total financial cost associated with the LangChain Expression Language (LCEL) execution runs, ensuring better resource tracking and budget management.",2026-01-03T04:11:18.577410+00:00
"context/change/8/final_fix_errors_from_unit_tests_python.prompt","This document outlines the requirements for creating a Python function named `fix_errors_from_unit_tests`, designed to automatically resolve errors in code and unit tests using Large Language Models (LLMs) via Langchain. The function takes inputs such as the unit test code, the code under test, error messages, and LLM configuration parameters (strength and temperature). It operates in a multi-step process: first, it loads specific prompt templates from a project directory and constructs a Langchain LCEL pipeline to generate a fix based on the provided errors. Second, it uses a subsequent LCEL pipeline with a JSON parser to extract structured data—specifically boolean flags for updates and the corrected code strings—from the initial LLM response. The process involves detailed console output using the `rich` library to display progress, markdown-formatted results, token counts, and cost calculations. The final output includes the corrected code, flags indicating what was updated, and the total financial cost of the LLM operations.",2026-01-03T04:11:31.159760+00:00
"context/change/8/initial_fix_errors_from_unit_tests.py","This Python script defines a function, `fix_errors_from_unit_tests`, designed to automatically repair code and unit tests based on reported errors using Large Language Models (LLMs). The process involves a two-step chain. First, it loads prompt templates from a specified directory and uses an LLM (selected via a custom `llm_selector` based on a strength parameter) to analyze the provided unit test, source code, and error message. It calculates and prints the token usage and estimated cost for this operation. Second, the script uses another LLM call to parse the initial analysis into a structured format defined by the `FixResult` Pydantic model. This structured output separates the results into boolean flags indicating if updates are needed, along with the actual fixed code strings for both the unit test and the implementation. The script utilizes LangChain for orchestration, SQLite for caching LLM responses to reduce costs, and `rich` for formatted console output. An example usage block at the end demonstrates how to call the function with sample inputs.",2026-01-03T04:11:44.545294+00:00
"context/change/8/initial_fix_errors_from_unit_tests_python.prompt","This document outlines the requirements for creating a Python function named fix_errors_from_unit_tests designed to resolve unit test failures using Large Language Models (LLMs) and LangChain. The function takes the existing unit test, the code under test, the error message, and a model strength parameter as inputs. It aims to output boolean flags indicating necessary updates and the corrected strings for both the unit test and the code.

The process involves a multi-step workflow using LangChain's LCEL syntax. First, it loads specific prompt templates from a project directory defined by the $PDD_PATH environment variable. It then invokes an LLM (selected based on the strength parameter) to analyze the errors and generate a fix, while calculating and pretty-printing token usage and costs using the `rich` library and `tiktoken`. Subsequently, a second LLM call parses the initial analysis into a structured JSON format to extract the specific code and test modifications. The final output includes the corrected code components and the total calculated cost of the LLM operations.",2026-01-03T04:11:58.246179+00:00
"context/change/9/change.prompt","The file outlines specific updates required for a prompt or function definition related to an automated code repair tool. It requests updating the usage example of the `fix_errors_from_unit_tests` function, providing a Python code snippet that demonstrates how to call this function with inputs like unit test code, code under test, error messages, strength, and temperature. The example also shows how to handle the output, which includes updated tests, updated code, and total cost. Additionally, the file specifies two functional enhancements: first, the inclusion of a 'budget' input to halt iterations if the total cost exceeds a limit; and second, a logic update to ensure the function returns the most successful iteration based on a priority hierarchy where minimizing 'ERROR's is more critical than minimizing 'FAILED' tests. This selection logic must consider all runs, including the final one, to ensure the best possible version of the code and tests is saved.",2026-01-03T04:12:11.884047+00:00
"context/change/9/final_fix_error_loop_python.prompt","The provided text outlines the specifications for a Python function named `fix_error_loop`, designed to iteratively repair errors in a unit test and its associated code file using an LLM. The function takes inputs including file paths for the unit test, code, and a verification program, as well as parameters for the LLM (strength, temperature), a maximum attempt limit, and a budget cap. The process involves a loop that runs `pytest`, captures errors, and utilizes a helper function `fix_errors_from_unit_tests` to generate fixes. It manages file backups with versioning based on failure counts, tracks costs, and verifies fixes using a separate program. If the budget is exceeded or attempts run out, the loop terminates. The function ensures the best-performing iteration is restored if the final attempt is not optimal. It returns the success status, final file contents, total attempts, and total cost, while using the `rich` library for formatted console output.",2026-01-03T04:12:24.184621+00:00
"context/change/9/initial_fix_error_loop.py","This Python script defines a function named `fix_error_loop` designed to iteratively repair errors in code and unit tests. The function takes paths to a unit test file, a code file, and a verification program, along with parameters for an error-fixing algorithm (strength, temperature, and maximum attempts). It operates within a loop that runs `pytest` on the unit test file, capturing output to an error log. If tests pass, the loop terminates successfully. If tests fail, the script creates backups of the current files and invokes an external function, `fix_errors_from_unit_tests`, to generate corrected code based on the error log. The script then applies these fixes and runs a separate verification program to ensure the changes are valid. If verification fails, the script reverts to the backup files. This process repeats until the tests pass or the maximum number of attempts is reached. Finally, the function performs a last `pytest` run and returns a tuple containing the success status, the final contents of the unit test and code files, and the total attempt count.",2026-01-03T04:12:30.675203+00:00
"context/change/9/initial_fix_error_loop_python.prompt","This document outlines the requirements for creating a Python function named fix_error_loop, designed to iteratively repair errors in unit tests and their corresponding code files. The function takes inputs including file paths for the unit test, the code being tested, and a verification program, along with parameters for LLM strength, temperature, and a maximum attempt limit. The process involves a loop that runs pytest, captures output to an error log, and analyzes failures. If tests fail, the function creates backups of the current files, calls a helper function fix_errors_from_unit_tests to generate fixes using an LLM, and verifies the new code using the provided verification program. If verification fails, backups are restored. The loop continues until the tests pass or the maximum attempts are reached. The function utilizes the rich library for pretty-printed console output and returns the success status, final file contents, and total attempt count. It emphasizes robust error handling for file I/O and subprocess execution.",2026-01-03T04:12:37.490936+00:00
"context/change/simple_math/change.prompt","The provided file content appears to be a set of instructions or a prompt designed for a software development context, specifically targeting a regression test for a module or function named simple_math. The text outlines a specific task: to modify an existing function by adding type hints to both its signature (parameters) and its return value. This suggests a code refactoring or modernization effort aimed at improving code readability, static analysis capabilities, and type safety within the codebase. The context implies that this change is part of a testing scenario, likely to verify that adding these type annotations does not break existing functionality or to demonstrate how an AI coding assistant should handle such a request.",2026-01-03T04:12:37.584232+00:00
"context/change_example.py","This Python script serves as a demonstration and entry point for utilizing the `change` function from the `pdd.change` module. The script imports necessary libraries, including `os` for environment management and `rich.console` for formatted terminal output. The core functionality is encapsulated within the `main` function, which sets up example parameters such as an initial prompt, a snippet of Python code (a factorial function), a modification request (to calculate the square root of the result), and configuration settings for model strength and temperature. It then attempts to execute the `change` function with these inputs inside a try-except block to handle potential errors gracefully. Upon successful execution, the script prints the modified prompt, the total cost of the operation, and the name of the model used to the console using rich formatting. If an exception occurs, it outputs a red error message.",2026-01-03T04:12:49.199318+00:00
"context/change_main_example.py","This Python script serves as a demonstration and usage example for the `change_main` function from the `pdd` command-line program. It illustrates how to programmatically invoke the `change` command functionality in two distinct modes: single-change mode and CSV batch-change mode.

The script first sets up a `click.Context` object with necessary configuration parameters like force overwrite, verbosity, LLM strength, temperature, and budget. It then creates necessary output directories and sample input files (prompts and Python code) to simulate a real-world scenario.

In the single-change mode section, the script generates a specific change prompt, input code, and input prompt, then calls `change_main` to modify a single prompt based on the instructions, printing the result and cost. Subsequently, the CSV batch-change mode section creates multiple sample scripts and a CSV file containing change instructions for each. It then calls `change_main` with `use_csv=True` to process these changes in batch, outputting the results to a specified directory. The script uses the `rich` library for formatted console output.",2026-01-03T04:13:02.662511+00:00
"context/cli_example.py","This file, `demo_run_pdd_cli.py`, serves as a minimal, self-contained demonstration of how to programmatically invoke the PDD (Prompt Driven Development) Command Line Interface (CLI). It utilizes the `click.testing.CliRunner` to simulate command-line execution within a Python script. The process involves three main steps: first, it creates a simple prompt file (`hello_python.prompt`) in a local output directory. Second, it executes the PDD CLI using the `generate` command with the `--local` flag to force local model usage and avoid network calls, targeting the created prompt file to generate a Python script (`hello.py`). Finally, the script prints the execution results, including the exit code, console output, and the content of the generated file to verify success. This demo illustrates how to integrate PDD's generation capabilities directly into Python workflows without manual terminal interaction.",2026-01-03T04:13:17.686410+00:00
"context/cli_python_preprocessed.prompt","This document outlines the functionality and usage of the PDD (Prompt-Driven Development) Command Line Interface, a Python-based tool designed to streamline development workflows using AI. PDD facilitates the generation of code, unit tests, and examples from prompt files, adhering to a specific naming convention (`<basename>_<language>.prompt`). The CLI supports a wide range of commands including `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `change`, and `update`, allowing users to manage the entire lifecycle of prompt-driven coding. Key features include multi-command chaining for complex workflows, cost tracking via CSV output, and configurable environment variables for default file paths. The tool leverages the Python `rich` library for formatted console output and `click` for CLI management. Additionally, the document provides detailed examples of Python implementation for each command, demonstrating how to integrate internal functions like `code_generator`, `context_generator`, and `fix_error_loop` to build the CLI's logic.",2026-01-03T04:13:30.444952+00:00
"context/click_example.py","This Python script defines a command-line image processing tool that utilizes the `click` library to create a chained execution pipeline, mimicking the behavior of Unix pipes. By leveraging `click`'s chaining capability and a custom result callback, the script allows users to perform a sequence of operations on images in a single command line entry. The underlying image manipulation is handled by the Pillow (PIL) library.

The script employs custom decorators, `@processor` and `@generator`, to structure functions so they accept and yield streams of image objects. The `open` command acts as a generator, loading images into the pipeline. Subsequent commands process this stream. Supported operations include geometric transformations like resizing, cropping, and transposing (rotating and flipping). The tool also offers various visual filters, including Gaussian blur, smoothening, embossing, and sharpening. Furthermore, it includes a `paste` command for compositing images. The pipeline typically concludes with the `save` command to write results to disk or `display` to view them. This architecture provides a flexible and modular way to batch-process images programmatically.",2026-01-03T04:13:44.370421+00:00
"context/cloud_function_call.py","This Python script demonstrates how to invoke a secured Google Cloud Function using an HTTP GET request. It utilizes the `requests` library to send the request to a specific URL (`https://us-central1-prompt-driven-development.cloudfunctions.net/on_request_example`). The core functionality is encapsulated in the `call_cloud_function` function, which accepts a Firebase authentication token as an argument. This token is included in the request headers under the Authorization key using the Bearer scheme to authenticate the call. The script includes a hardcoded example of a JSON Web Token (JWT) assigned to the variable `firebase_token`, representing a user named Greg Tanaka. Finally, the script executes the function using this token and prints the JSON response returned by the Cloud Function to the console.",2026-01-03T04:14:02.166767+00:00
"context/cmd_test_main_example.py","This Python script serves as a demonstration and usage example for the `cmd_test_main` function from the `pdd` package. The script, designed to be run directly, illustrates how to programmatically generate unit tests for a simple calculator module. It begins by setting up a local environment, creating dummy input files (a prompt file describing the task and a corresponding Python code file with `add` and `subtract` functions) within an output directory. It then mocks a `click.Context` object to simulate command-line arguments such as verbosity and force-overwrite flags. The core of the script executes `cmd_test_main`, passing in the paths to the created files, configuration parameters like model strength and temperature, and the target output path. Finally, it displays the results of the operation using the `rich` library, printing the model used, the estimated cost, the location of the saved test file, and a preview of the generated test code.",2026-01-03T04:14:08.442317+00:00
"context/code_generator_example.py","This file is a Python script designed to demonstrate the functionality of a `code_generator` module. It serves as an entry point or usage example for generating code programmatically using a Large Language Model (LLM). The script begins by importing the `code_generator` function from the `pdd.code_generator` package. Inside the `main` function, it sets up necessary parameters, specifically reading a prompt from a local file named `prompts/generate_test_python.prompt`. It configures other arguments such as the target programming language (Python), the model strength (0.5), the temperature (0.0 for deterministic output), and a verbosity flag. The script then executes the `code_generator` function within a try-except block to handle potential errors gracefully. Upon successful execution, it prints the generated runnable code, the total cost of the API call, and the name of the model used to the console. Finally, the standard `if __name__ == __main__:` block ensures the `main` function runs when the script is executed directly.",2026-01-03T04:14:21.651231+00:00
"context/code_generator_main_example.py","This Python script serves as a standalone example and test harness for the `code_generator_main` function from the `pdd` package. It demonstrates various code generation scenarios—specifically full generation, incremental generation, forced incremental updates, and cloud generation with fallback—without requiring actual LLM API calls. To achieve this, the script uses `unittest.mock.patch` to replace the internal generator functions (`local_code_generator_func` and `incremental_code_generator_func`) with local mock implementations that return predictable, dummy code strings. The script sets up a temporary output directory, creates dummy prompt files, and executes `code_generator_main` under different configurations (local vs. cloud, varying temperatures, and incremental flags). It then prints the results of these simulations, including the generated code, cost, and model used, to the console.",2026-01-03T04:14:35.193339+00:00
"context/commands/__init___example.py","This Python script serves as an example and documentation for using the `register_commands` module within the PDD (Python Development Driver) CLI tool. It demonstrates how to initialize a Click-based command-line interface and populate it with a comprehensive suite of subcommands. The script defines a main `cli` group and uses `register_commands(cli)` to attach various functionalities categorized into Generation, Fixing, Modification, Maintenance, Analysis, Miscellaneous, Utility, and Templates. It includes three specific examples: setting up a basic CLI with versioning, inspecting the dictionary of registered commands, and creating a custom CLI instance. The `__main__` block executes a demonstration that prints a categorized list of all registered commands along with their help text summaries to the console, verifying the successful registration of tools like `generate`, `fix`, `sync`, and `detect-change`. Finally, it provides usage examples for invoking specific subcommands.",2026-01-03T04:14:48.793959+00:00
"context/commands/analysis_example.py","This Python script serves as a demonstration and usage guide for programmatically invoking analysis commands from the `pdd` library. It specifically showcases how to use the `detect_change`, `conflicts`, `bug`, `crash`, and `trace` commands by calling their Click command callbacks directly within a Python script rather than via the command line interface.

The script begins by setting up a Click Context object, which is essential for passing global configuration settings—such as verbosity, model strength, and cost tracking—to the underlying commands. It then proceeds through five distinct examples, each corresponding to a specific analysis tool. For each example, the script creates necessary dummy files (such as prompts, code snippets, error logs, or change requests) in a local output directory. It then executes the relevant command callback (e.g., `detect_change.callback`, `crash.callback`) using the context manager to ensure the configuration is active. The examples cover scenarios like detecting prompt changes based on descriptions, identifying conflicts between prompt files, generating unit tests for bugs, fixing code crashes, and tracing execution flow back to prompt logic. The script concludes by printing the results of each operation.",2026-01-03T04:14:55.125955+00:00
"context/commands/fix_example.py","This Python script serves as a comprehensive demonstration and documentation for the `pdd fix` command, a tool designed to automatically correct errors in code and unit tests. The script sets up a simulated environment by creating example files in an `./output` directory, including a prompt file, a Python module with an intentional bug (missing division-by-zero handling), a failing unit test, and an error log.

The file defines several functions illustrating different usage patterns of the command via the `click` testing library. These patterns include a basic fix invocation, a loop mode that iteratively attempts fixes using a verification program, and advanced configurations featuring agentic fallback (using models like Claude or Gemini) and automatic submission to the PDD Cloud. It also demonstrates how to track costs via CSV output and how to invoke the command programmatically within Python code. The `main` function orchestrates the setup but leaves the actual execution of examples commented out to prevent accidental API costs, serving primarily as a reference for developers on how to integrate and utilize the automated fixing capabilities of the PDD package.",2026-01-03T04:15:10.590260+00:00
"context/commands/generate_example.py","This Python script serves as a comprehensive demonstration and documentation for the `pdd.commands.generate` module within the PDD CLI tool. It illustrates how to programmatically invoke and utilize three primary Click commands: `generate`, `example`, and `test`. The script defines helper functions to set up a local output directory and create sample prompt and code files (specifically a simple calculator module) to simulate a working environment.

The `main` function orchestrates five specific examples: using the `generate` command to create code from prompts, using the `example` command to produce usage demonstrations, using the `test` command to generate or enhance unit tests (including coverage-based enhancements), invoking commands programmatically via Click contexts, and utilizing templates with environment variable substitution. The script emphasizes that all these commands are decorated with `@track_cost` to monitor LLM API usage. It provides detailed print outputs explaining the command signatures, arguments, options (such as model strength, temperature, and local execution), and expected context objects, acting as both a functional test script and a user guide for developers integrating with the PDD tool.",2026-01-03T04:15:17.448069+00:00
"context/commands/maintenance_example.py","This Python script serves as a demonstration and test harness for the maintenance commands module of the `pdd` package. It showcases how to programmatically invoke and test three specific Click-based CLI commands: `sync` (for synchronizing prompts with code and tests), `auto-deps` (for analyzing and injecting dependencies into prompt files), and `setup` (for running the interactive setup utility).

The script includes helper functions to create a mock project environment with sample directories and files (prompts, source code, examples). It defines four main example functions: `example_sync_command`, `example_auto_deps_command`, `example_setup_command`, and `example_programmatic_usage`. These functions utilize `unittest.mock` to simulate the underlying logic (avoiding actual LLM calls or system modifications) and `click.testing.CliRunner` to execute the commands within a simulated CLI context. The `example_programmatic_usage` function specifically demonstrates how to bypass the CLI parser and invoke the command callbacks directly while managing the Click context manually.",2026-01-03T04:15:30.815626+00:00
"context/commands/misc_example.py","This Python script serves as a comprehensive demonstration and test suite for the `pdd.commands.misc.preprocess` module, specifically focusing on the `preprocess` Click command. It illustrates how to prepare prompt files for Large Language Models (LLMs) by handling directives such as file includes, comments, shell commands, and web scraping locally without API calls.

The script defines a `setup_example_files` function to generate sample artifacts (a prompt file and an included text file) in an `./output` directory. It then executes five distinct examples using `click.testing.CliRunner` to simulate command-line usage:

1.  **Basic Preprocessing:** Demonstrates standard file processing with default settings.
2.  **XML Delimiters:** Shows how to insert XML tags for better prompt structure using the `--xml` flag.
3.  **Curly Bracket Doubling:** Illustrates converting single brackets to double brackets (for template safety), while using the `--exclude` option to preserve specific variables.
4.  **Recursive Processing:** Demonstrates resolving nested includes via the `--recursive` flag.
5.  **Combined Options:** Shows the simultaneous use of recursion, bracket doubling, and exclusions.

The script concludes with a main execution block that runs all examples and prints explanatory context about the command's capabilities.",2026-01-03T04:15:37.552464+00:00
"context/commands/modify_example.py","This Python script serves as a comprehensive demonstration and test suite for the `modify` module within the `pdd` CLI tool. It illustrates how to programmatically use three core commands—`split`, `change`, and `update`—to manage prompt engineering workflows. The script sets up a local directory structure (`./output/`) to generate sample files, including prompts, source code, and examples, needed to run these commands effectively.

The `split` command example demonstrates breaking down large prompt files into smaller sub-prompts while maintaining references. The `change` command is showcased in two modes: modifying a single prompt based on specific instructions (like adding logging) and a batch mode using a CSV file to process multiple prompts simultaneously. Finally, the `update` command example shows how to synchronize a prompt file to reflect changes made in the corresponding source code, such as adding new features to a module. Each command execution is wrapped in a Click-based CLI runner to simulate real-world usage, and the script verifies the successful creation of modified output files.",2026-01-03T04:15:45.713957+00:00
"context/commands/templates_example.py","This Python script serves as a usage example and demonstration for the `pdd.commands.templates` module within the PDD (Project Design Document) tool. It illustrates how to programmatically interact with the CLI command group responsible for managing templates. The script imports the `templates_group` and utilizes the `click.testing.CliRunner` to simulate command-line invocations for three primary functions: listing, showing, and copying templates.

The `example_list_templates` function demonstrates the `list` command, showing how to display available templates in both text and JSON formats, as well as how to filter them by tags. The `example_show_template` function highlights the `show` command, which outputs detailed metadata about a specific template, including variables, usage examples, and schemas. Finally, the `example_copy_template` function exhibits the `copy` command, simulating the extraction of a template file to a local destination directory. The script concludes with a `main` function that orchestrates the execution of these examples, providing a comprehensive overview of the template management capabilities available in the PDD CLI.",2026-01-03T04:15:52.893200+00:00
"context/commands/utility_example.py","This Python script serves as a demonstration and documentation for the `pdd.commands.utility` module, specifically showcasing the usage of two Click commands: `install_completion_cmd` and `verify`. It includes functions to set up a dummy environment by creating example files (a prompt, a buggy calculator script, and a verification script) in an output directory. The script then programmatically explains how to invoke the `install_completion` command to set up shell autocompletion and the `verify` command to iteratively fix code using an LLM and a verification program. Rather than executing the actual heavy-lifting commands which require API keys or shell modifications, the script prints detailed usage examples, CLI invocation patterns, programmatic execution methods using `Click.testing`, and the structure of return values (including cost tracking and success metrics). It uses the `rich` library for formatted terminal output.",2026-01-03T04:15:59.584748+00:00
"context/comment_line_example.py","The provided file documents the usage and implementation of a Python function named `comment_line`, which is designed to programmatically comment out lines of code based on specified syntax rules. The function accepts two parameters: `code_line`, the string of code to be modified, and `comment_characters`, which dictates the commenting style. The documentation details three modes of operation for `comment_characters`: passing 'del' returns an empty string (effectively deleting the line); passing a string with a space (e.g., '<!-- -->') treats the input as start and end delimiters for block-style comments; and passing a single string (e.g., '#') applies a standard single-line comment prefix. The file includes the source code for the function itself, followed by practical examples demonstrating its application for Python-style comments, HTML-style comments, and line deletion. Finally, it provides a concise breakdown of the input parameters and the expected string output.",2026-01-03T04:16:12.699231+00:00
"context/config_example.py","This file contains a brief Python script that serves as an initialization step for a larger application. Its primary and sole function is to set up the configuration settings before any other operations occur. The code imports a function named `init_config` from a module located at `utils.config`. Immediately following the import, it executes this `init_config()` function. A comment at the top of the file explicitly emphasizes the importance of order, stating Initialize configuration FIRST, indicating that this script is likely an entry point or a preamble intended to ensure that environment variables, logging settings, or other configuration parameters are correctly loaded and available to the rest of the system. The brevity of the code suggests a modular design where configuration logic is encapsulated elsewhere to maintain clean entry points.",2026-01-03T04:16:18.742000+00:00
"context/conflicts_in_prompts_example.py","This file is a Python script designed to demonstrate the functionality of a `conflicts_in_prompts` utility. It defines a `main` function that sets up two detailed example prompts: one for creating an authentication helper module (`auth_helpers.py`) using Firebase, and another for defining a `User` data model class (`user.py`) with specific fields and methods. The script then configures parameters such as model strength, temperature, and verbosity before calling the `conflicts_in_prompts` function to analyze these two prompts for potential contradictions or inconsistencies. Finally, it prints the results of the analysis to the console using the `rich` library, displaying the model used, the total cost of the operation, and any suggested changes if conflicts were detected.",2026-01-03T04:16:29.752039+00:00
"context/conflicts_main_example.py","This Python script serves as a demonstration or test harness for the `conflicts_main` function from the `pdd.conflicts_main` module. The script begins by creating two sample prompt files, `prompt1_LLM.prompt` and `prompt2_LLM.prompt`, in an output directory, populating them with slightly different system instructions for an AI assistant. It then defines a `MockContext` class to simulate a Click command-line interface context, initializing it with configuration parameters such as `force`, `quiet`, `strength`, and `temperature`, although the object is immediately reset to an empty dictionary in the provided snippet. The core of the script executes `conflicts_main` using this mock context, the two generated prompt files, and a specified output path for a CSV file. Finally, the script prints the results of the execution to the console, including the detected conflicts, the total cost of the operation, and the name of the model used. The output indicates that the detailed results of the conflict analysis are saved to `outputconflicts_output.csv`.",2026-01-03T04:16:42.420548+00:00
"context/construct_paths_example.py","This file, `demo_construct_paths.py`, serves as a concise end-to-end demonstration of the `construct_paths` function from the `pdd` library. It illustrates how the library processes input arguments typically supplied via a command-line interface (CLI). The script begins by creating a temporary prompt file (`Makefile_makefile.prompt`) containing a simple task description to simulate a real-world usage scenario. It then sets up the necessary arguments—such as input file paths, force flags, and command types—to mimic a `generate` command execution. The core of the script invokes `construct_paths`, capturing its return values: the resolved configuration, the content of the input strings, the calculated output file paths, and the detected programming language. Finally, the script prints these returned structures to the console for inspection and cleans up by deleting the temporary prompt file, ensuring the working directory remains tidy.",2026-01-03T04:16:56.070962+00:00
"context/context_generator_example.py","This Python script serves as a demonstration or test harness for the `context_generator` function from the `pdd.context_generator` module. It begins by importing necessary libraries, including `os` for environment variable management and `rich` for formatted console output. The script first verifies that the `PDD_PATH` environment variable is set, raising a `ValueError` if it is missing, which suggests the tool relies on a specific path configuration. It then defines a set of input parameters required for the generation process: a simple Python code module (an addition function), a corresponding prompt, the programming language ('python'), a strength parameter (0.5), and a temperature setting (0.0). These parameters are passed to the `context_generator` function with the verbose flag enabled. The function returns three values: the generated example code, the total cost of the operation, and the name of the model used. Finally, the script utilizes the `rich` library to print these results to the console in a readable format, displaying the generated code, the cost formatted to six decimal places, and the specific model identifier.",2026-01-03T04:17:10.125414+00:00
"context/context_generator_main_example.py","This Python script serves as a demonstration and test harness for the `context_generator_main` function, which is part of a larger system (likely named `pdd`) designed to generate code examples using Large Language Models (LLMs). The script defines a `run_example` function that sets up a mock environment by creating a temporary output directory and generating dummy `.prompt` and source code files. It simulates a `click.Context` object to mimic command-line arguments such as model strength, temperature, and verbosity. The script then executes `context_generator_main` with these inputs to generate an example usage file (`math_utils_example.py`) based on the dummy math utility code. Finally, it prints the results of the operation, including the model used, the calculated cost, and a snippet of the generated code. The main execution block also sets mock environment variables to ensure dependencies run without errors.",2026-01-03T04:17:24.284932+00:00
"context/continue_generation_example.py","This Python script serves as a demonstration and entry point for utilizing the `continue_generation` function from the `pdd.continue_generation` module. The script defines a `main` function that orchestrates the process of extending text generation using a language model. It begins by loading necessary context data from local files: a preprocessed prompt is read from `context/cli_python_preprocessed.prompt`, and an initial fragment of LLM output is read from `context/llm_output_fragment.txt`.

The script sets specific configuration parameters for the generation process, including a `strength` of 0.915 and a `temperature` of 0. It then invokes the `continue_generation` function with these inputs and parameters, enabling verbose mode for detailed feedback. Upon successful execution, the script captures the final generated text, the total cost of the operation, and the model name used. It prints the cost and model name to the console and saves the completed text generation to a file named `context/final_llm_output.py`. The code includes error handling to manage potential `FileNotFoundError` or other general exceptions that might occur during file operations or the generation process.",2026-01-03T04:17:30.902628+00:00
"context/core/cli_example.py","This Python script serves as a comprehensive demonstration and documentation of the PDD CLI module's capabilities. It illustrates how to programmatically interact with the main CLI entry point and its custom `PDDCLI` Click Group class. The script defines several example functions that showcase specific features: displaying help and version information, listing available configuration contexts, and invoking the CLI with various global options such as AI model strength, temperature, and verbosity settings.

Furthermore, the code demonstrates advanced functionality like enabling quiet mode, setting up cost tracking via CSV output, overriding automatic context detection, and generating core dumps for debugging purposes. It highlights the `PDDCLI` class's enhancements, including custom help formatting and centralized error handling. The script concludes with a `main` function that sequentially executes these examples, providing a practical reference for developers on how to utilize the PDD tool's command-line interface, manage configuration contexts, and handle output options effectively.",2026-01-03T04:17:45.248806+00:00
"context/core/cloud_example.py","This Python script serves as a comprehensive demonstration of the `pdd.core.cloud` module, illustrating how to manage cloud configurations within the PDD CLI environment. It defines a `CloudConfig` usage guide through three distinct example functions.

The first function, `example_url_configuration`, details how to retrieve default production URLs and specific API endpoints, while also demonstrating how to override these settings using environment variables to point to local emulators. The second function, `example_authentication_flow`, simulates the authentication process, showing how the system handles pre-injected JWT tokens for CI/testing environments and how it behaves when necessary credentials are missing. The third function, `example_feature_flags`, checks if cloud features are enabled based on the presence of required API keys (Firebase and GitHub) and lists the available cloud endpoints. The script utilizes the `rich` library to render formatted status messages and logs to the console.",2026-01-03T04:17:52.640433+00:00
"context/core/dump_example.py","This Python script serves as a demonstration and usage guide for the `pdd.core.dump` module, which handles core dump generation and debugging utilities for the PDD CLI tool. The script illustrates how to capture execution contexts, errors, and environment details to facilitate issue reproduction and diagnosis.

The file defines several example functions that showcase key capabilities: `example_write_core_dump` simulates a CLI run to generate a JSON core dump containing cost, model usage, and command results; `example_github_config` verifies the presence of required environment variables (`PDD_GITHUB_TOKEN` and `PDD_GITHUB_REPO`) for automated issue reporting; `example_write_replay_script` demonstrates how to generate an executable shell script that reconstructs the original environment and command arguments to replay a failed run; and `example_build_issue_markdown` shows how to format a GitHub issue body with platform details, tracebacks, and reproduction steps. Finally, `example_post_issue_to_github` outlines the interface for submitting these issues via the GitHub API. The script includes a main execution block that runs these examples sequentially, outputting results to a local directory.",2026-01-03T04:18:17.283481+00:00
"context/core/errors_example.py","This Python script serves as a comprehensive demonstration of the `pdd.core.errors` module, illustrating centralized error handling capabilities for the PDD CLI. It showcases the use of a `Rich` console instance with custom themes for styled output (info, warning, success, etc.). The script provides step-by-step examples of handling various exception types (FileNotFoundError, ValueError, IOError, RuntimeError) using the `handle_error` function, demonstrating both verbose and quiet modes. Furthermore, it details how the module collects error data into a buffer for core dumps, how to retrieve and inspect these errors using `get_core_dump_errors`, and how to persist them to a JSON file. Finally, the script demonstrates the ability to clear the error buffer and confirms that the error handling mechanism allows for graceful execution flow without crashing the application.",2026-01-03T04:18:24.838066+00:00
"context/core/utils_example.py","This Python script serves as a comprehensive demonstration and usage guide for the `pdd.core.utils` module within the PDD CLI project. It illustrates the functionality of key helper functions responsible for environment detection, configuration management, and CLI argument parsing.

Through a series of isolated example functions, the script demonstrates how to:
1. **Parse CLI Arguments:** Use `_first_pending_command` with mocked Click contexts to identify subcommands while ignoring option flags.
2. **Verify Environment Setup:** Check for the existence of global API configuration files (`_api_env_exists`) and verify if shell tab completion is installed (`_completion_installed`).
3. **Detect Local Configuration:** Determine if the current working directory contains project-specific settings like `.env` files or `.pdd` folders via `_project_has_local_configuration`.
4. **Manage User Onboarding:** Execute logic for `_should_show_onboarding_reminder` to decide when to prompt new users to run the setup tool, taking into account environment variables and existing configurations.

Additionally, the file provides informational details about the `_run_setup_utility` function, which handles the interactive installation process. The script includes a `main()` entry point that executes these demonstrations sequentially, printing diagnostic information to the console.",2026-01-03T04:18:37.975574+00:00
"context/crash_main_example.py","This Python script demonstrates the usage of the `crash_main` function from the `pdd` library to automatically fix coding errors. The script sets up a test scenario by creating four files in an output directory: a prompt file describing a desired factorial function, a buggy implementation of that function (which fails to handle negative numbers), a main program that incorrectly calls the function with a negative number, and a simulated error log showing a `RecursionError`. It then initializes a mock Click context with specific model parameters. Finally, the script calls `crash_main` with these files as inputs, enabling an iterative fixing loop with a defined budget and maximum attempt count. The results of the automated fix—including success status, cost, the model used, and the corrected code for both the module and the main program—are printed to the console.",2026-01-03T04:18:57.874285+00:00
"context/create_gcp_credential.py","This Python script demonstrates the process of retrieving and decoding a Google Cloud Platform (GCP) service account key stored within an Azure Key Vault. The code is structured to fetch a specific secret named GCP-VERTEXAI-SERVICE-ACC, which is expected to be a base64-encoded JSON string containing service account credentials.

The script includes a placeholder class, `AzureKeyVaultService`, to simulate the interaction with Azure Key Vault for testing purposes, providing a dummy base64-encoded JSON payload when queried. The main execution flow checks if the secret string is retrieved successfully. If present, it uses the `base64` library to decode the string into bytes and then converts it into a UTF-8 string. Finally, it parses this string into a JSON object using the `json` library. The script includes error handling for JSON decoding issues and general exceptions, and it suggests how the resulting `credentials_info` dictionary can be used to initialize Google OAuth2 credentials for services like Vertex AI.",2026-01-03T04:19:12.116717+00:00
"context/ctx_obj_params.prompt","This file documents the standard parameters available within the `ctx.obj` dictionary for PDD CLI commands. It details the keys populated by the main CLI group, which include configuration settings such as 'verbose' for debugging, 'force' for overwriting files, 'quiet' for suppressing output, and 'local' for execution preference. Additionally, it covers LLM-specific parameters like 'strength', 'temperature', and 'time' (relative thinking time). The documentation also explains the resolution logic for `strength` and `temperature` parameters in command functions: explicit function arguments take precedence over the values stored in `ctx.obj`, allowing orchestrators to override global CLI context settings when necessary.",2026-01-03T04:19:26.495705+00:00
"context/detect_change/1/change_python.prompt","This document outlines the requirements for creating a Python function named change designed to modify an existing prompt based on specific instructions. The function takes several inputs, including an initial prompt string, the code generated from it, a change instruction string, and model parameters like strength and temperature. It is expected to output a modified prompt string, the total operational cost, and the name of the LLM model used. The implementation must utilize the Python Rich library for console output and employ relative imports. The process involves several steps: loading specific prompt files, preprocessing them (handling curly brackets), creating Langchain LCEL templates, and selecting an LLM model using a helper function. The function executes a two-step LLM process: first, generating a raw modified prompt based on the inputs, and second, extracting the final JSON-formatted modified_prompt from that output. Throughout execution, the function must calculate and pretty-print token counts and estimated costs, finally returning the modified prompt, total cost, and model name while handling potential errors gracefully.",2026-01-03T04:19:32.734998+00:00
"context/detect_change/1/code_generator_python.prompt","This file outlines a prompt for an expert Python engineer to create a function named code_generator. The function's purpose is to compile a raw text prompt into a runnable code file in a specified language (e.g., Python, Bash). The prompt details specific inputs for the function, including the raw prompt string, target language, model strength (0-1), and temperature. It also defines the expected outputs: the resulting runnable code, the total cost of the operation, and the name of the LLM model used. The instructions mandate the use of LangChain Expression Language (LCEL) and provide a step-by-step workflow. This workflow involves preprocessing the prompt, selecting an LLM via an internal `llm_selector` module, executing the prompt, and handling token counting and cost estimation. Furthermore, the process includes logic to detect incomplete generations using an `unfinished_prompt` function and either continue generation or post-process the result based on completion status. The file includes references to several external context files for examples of internal modules like preprocessing, postprocessing, and LLM selection.",2026-01-03T04:19:40.070104+00:00
"context/detect_change/1/detect_change_output.txt","The document outlines a proposal to standardize Python-related instructions across various prompt files by utilizing a shared `context/python_preamble.prompt`. The primary goal is to reduce redundancy and improve consistency. After evaluating three implementation strategies—Direct Inclusion, Conditional Inclusion, and Reference-based Inclusion—the author selects Plan C (Reference-based Inclusion) as the optimal approach. This method uses references to the preamble file, allowing for easy updates and flexibility without duplicating content. The document concludes with specific instructions for modifying two files: `prompts/change_python.prompt` and `prompts/fix_error_loop_python.prompt`. For each, the plan involves inserting an XML include tag for the preamble and removing specific redundant instructions (e.g., regarding the Python Rich library or relative imports) that are now covered by the shared file. Other files like `code_generator_python.prompt` are noted as already compliant.",2026-01-03T04:19:47.408689+00:00
"context/detect_change/1/fix_error_loop_python.prompt","The provided text outlines the specifications for a Python function named fix_error_loop, designed to iteratively repair errors in unit tests and their corresponding code files. The function takes inputs such as file paths for the unit test, code, and a verification program, along with LLM parameters (strength, temperature) and constraints like maximum attempts and budget. Its process involves a loop that runs pytest, logs errors, and utilizes a helper function, fix_errors_from_unit_tests, to generate fixes based on the error output. Key features include creating backups of files with iteration-specific filenames, tracking costs against a budget, and verifying code integrity using a separate program after modifications. The function tracks the best-performing iteration (lowest errors/failures) and restores those files if the final attempt is not optimal. It returns the success status, final file contents, total attempts, and total cost, while using the rich library for formatted console output.",2026-01-03T04:19:54.458518+00:00
"context/detect_change/1/prompt_list.txt","The provided input contains a collection of prompt definitions designed for an AI-based Python software engineering agent. These prompts guide an LLM through specific coding tasks using the LangChain framework. First, the `python_preamble` establishes core coding standards, such as using relative imports and the Rich library for output. The `change_python` prompt outlines a function to modify existing code, detailing steps for preprocessing, token counting, and cost estimation. The `fix_error_loop_python` prompt describes a robust, iterative debugging process where the agent runs unit tests, analyzes error logs, and attempts fixes within a set budget, while managing file backups and verification. Finally, the `code_generator_python` prompt defines a workflow for compiling raw prompts into runnable code, including logic for detecting incomplete generations and post-processing results. Together, these prompts structure a comprehensive pipeline for automated code generation, modification, and self-correction.",2026-01-03T04:20:01.809731+00:00
"context/detect_change/1/python_preamble.prompt","The provided text outlines specific requirements for developing a Python function within a larger package structure. It mandates that the function must utilize relative imports, specifically denoted by a single dot, to access internal modules, ensuring proper integration within the package hierarchy. Furthermore, the requirements specify that all console output generated by the function must be formatted and displayed using the Python Rich library to enhance readability and visual appeal through pretty printing. The instructions also emphasize robust error handling, requiring the function to gracefully manage edge cases such as missing inputs or failures within the model. In these scenarios, the function is expected to provide clear, informative error messages to the user rather than failing silently or crashing, thereby ensuring a stable and user-friendly experience.",2026-01-03T04:20:23.060099+00:00
"context/detect_change/2/change.prompt","The provided file content appears to be a fragment of a prompt engineering template or instruction set, likely used within a system for generating code or managing LLM contexts. It specifically instructs the user or system to utilize a file named 'context/python_preamble.prompt' to make prompts more compact, noting that some prompts may already include it. The file includes placeholders or tags (e.g., <preamble>, <include>, <example>) that demonstrate how to embed external content, specifically showing where the 'python_preamble.prompt' content would be inserted and providing an example reference to 'prompts/code_generator_python.prompt'. Essentially, it serves as a directive and a structural guide for including standard Python-related context to optimize prompt length and consistency.",2026-01-03T04:20:29.587750+00:00
"context/detect_change/2/change_detect.csv","The provided file content is a CSV-formatted list detailing specific modification instructions for three distinct prompt files located within the 'context/detect_change/2/' directory. The files listed are 'change_python.prompt', 'preprocess_python.prompt', and 'unfinished_prompt_python.prompt'.

For each of these files, the instruction is identical: the user is directed to insert the contents of a shared preamble file, specifically './context/python_preamble.prompt', immediately following the role and goal statement. This insertion should be marked using XML 'include' tags. Furthermore, the instructions mandate the removal of any redundant guidelines that are already addressed by this new preamble, specifically citing examples like pretty printing and edge case handling. Despite these removals, the instructions emphasize the importance of maintaining the logical flow of the original prompts and retaining any unique, specific instructions not covered by the preamble. The goal appears to be standardizing the Python-related prompts by centralizing common instructions into a single preamble file while preserving the specific functionality of each individual prompt.",2026-01-03T04:20:36.165154+00:00
"context/detect_change/2/change_python.prompt","The file outlines a task for an expert Python Software Engineer to create a function named change. This function is designed to modify an existing input_prompt based on instructions provided in a change_prompt. The function takes several inputs, including the original prompt, generated code, modification instructions, and LLM parameters like strength and temperature. It outputs the modified prompt, the total cost of the operation, and the model name used. The implementation requires using relative imports, the Python Rich library for console output, and Langchain LCEL for processing. Specific steps include loading prompt templates from file paths, preprocessing them, selecting an LLM via a selector utility, and executing a two-step chain: first to generate a raw modification and second to extract the final prompt as JSON. The process involves calculating and displaying token counts and costs at each stage.",2026-01-03T04:20:43.020166+00:00
"context/detect_change/2/detect_change_output.txt","This document outlines a plan to standardize Python-related prompts by incorporating a shared preamble file (`context/python_preamble.prompt`). The primary goal is to reduce redundancy and improve maintainability by centralizing common instructions, such as those for relative imports, pretty printing with the Python Rich library, and edge case handling.

The document evaluates three implementation strategies: direct inclusion, conditional inclusion, and a hybrid approach. It selects the hybrid approach (Plan C) as the best option because it offers the flexibility to include the preamble only where it adds value while retaining specific existing instructions.

The analysis identifies four specific prompts for review. `xml_tagger_python.prompt` requires no changes as it already includes the preamble. However, `change_python.prompt`, `preprocess_python.prompt`, and `unfinished_prompt_python.prompt` are targeted for updates. The specific instructions for these files involve inserting the preamble immediately after the role/goal statements using XML include tags and removing any redundant text that the new preamble now covers, ensuring the logical flow of the prompts remains intact.",2026-01-03T04:20:55.790805+00:00
"context/detect_change/2/preprocess_python.prompt","The file outlines the requirements for creating a Python function named 'preprocess_prompt' designed to prepare prompt strings for Large Language Models (LLMs). The function takes a prompt string and several configuration options as input, including flags for recursive processing and curly bracket doubling, as well as a list of keys to exclude from doubling. Its primary task is to parse and transform specific XML-like tags within the prompt using regular expressions. Specifically, it must handle 'include' tags by replacing them with file contents, 'pdd' tags by deleting them and their content (acting as comments), and 'shell' tags by executing commands and capturing their output. The function supports nested includes, either via the XML tag or by detecting angle brackets within triple backticks, and should process these recursively if enabled. Additionally, it manages the doubling of single curly brackets to escape them, unless the keys are excluded or already doubled. File paths are resolved using a 'PDD_PATH' environment variable via a helper function 'get_file_path'. The implementation must also utilize the Python 'rich' library to provide pretty-printed progress updates to the console.",2026-01-03T04:21:02.906181+00:00
"context/detect_change/2/prompt_list.json","The provided file contains a JSON array defining four distinct prompt specifications designed to generate Python utility functions for a prompt engineering workflow. Each specification instructs an expert Python engineer to create specific tools using the LangChain LCEL framework and the Rich library for console output.

The first prompt, change_python.prompt, outlines a function to modify existing prompts based on user instructions, involving preprocessing and cost calculation. The second, preprocess_python.prompt, describes a recursive text processing function that handles XML-like tags (such as include and shell) and manages curly bracket escaping. The third, unfinished_prompt_python.prompt, defines a function that analyzes text to determine if a prompt is complete or requires continuation, returning structured reasoning. Finally, xml_tagger_python.prompt details a function that enhances raw prompts by adding XML tags for better organization using a multi-step LLM process. Common requirements across all prompts include using relative imports, handling environment variables like $PDD_PATH, and ensuring robust error handling.",2026-01-03T04:21:16.567518+00:00
"context/detect_change/2/python_preamble.prompt","The provided text outlines specific requirements for developing a Python function within a larger package structure. It mandates that the function must be integrated into a Python package ecosystem, explicitly requiring the use of relative imports (denoted by a single dot) to access internal modules, ensuring modularity and correct package hierarchy. Furthermore, the directive specifies that all console output generated by the function must be formatted using the Python Rich library to enhance readability and visual appeal through pretty printing. The requirements also emphasize robust error handling; the function is expected to gracefully manage edge cases, such as missing input data or failures within the model. In these scenarios, the function must provide clear, informative error messages to the user rather than failing silently or crashing, thereby ensuring a stable and user-friendly experience.",2026-01-03T04:21:32.502714+00:00
"context/detect_change/2/unfinished_prompt_python.prompt","This document outlines the requirements for creating a Python function named 'unfinished_prompt', designed to analyze whether a given text prompt is complete or requires continuation. The function takes 'prompt_text', 'strength', and 'temperature' as inputs and returns a structured reasoning string, a boolean status ('is_finished'), the total cost, and the model name. The implementation must utilize the Langchain LCEL framework, loading a specific prompt template from a project path defined by the '$PDD_PATH' environment variable. It involves selecting an LLM model via a helper function ('llm_selector'), calculating token usage and costs, and pretty-printing the analysis process and results using the 'rich' library. The function is expected to handle errors gracefully and return the analysis results in a specific format.",2026-01-03T04:21:45.044429+00:00
"context/detect_change/2/xml_tagger_python.prompt","This document outlines the requirements for creating a Python function named xml_tagger designed to enhance LLM prompts by adding structural XML tags. The function takes a raw prompt string, along with strength and temperature parameters, and returns the tagged prompt, the total cost of execution, and the model name used. The implementation involves a multi-step process using LangChain Expression Language (LCEL). First, it loads specific prompt templates from a project directory defined by the $PDD_PATH environment variable. It then utilizes an internal `llm_selector` module to select an LLM and calculate token costs. The process executes in two main stages: first, generating an analysis of the prompt with suggested tags using an xml_convertor template, and second, extracting the clean, tagged XML content using an extract_xml template that outputs JSON. The workflow requires pretty-printing status updates, token counts, and costs at each step using the `rich` library, culminating in the calculation of the total cost and the return of the final tagged string.",2026-01-03T04:21:56.909594+00:00
"context/detect_change_example.py","This Python script demonstrates the usage of a `detect_change` function, likely part of a larger system named `pdd`, to analyze a set of prompt files for necessary modifications based on a specific change description. The script begins by importing necessary modules, including `detect_change` and `rich.console` for formatted output. It defines a list of specific prompt files to be examined, such as `python_preamble.prompt` and `change_python.prompt`, although a commented-out section suggests an alternative method to dynamically list all prompt files in specific directories. A `change_description` variable is set to instruct the system to incorporate `context/python_preamble.prompt` to make prompts more compact. The script also configures Large Language Model (LLM) parameters, specifically `strength` and `temperature`. The core logic is wrapped in a try-except block where `detect_change` is called with the file list, description, and model parameters. Upon success, it prints a formatted list of detected changes, including the prompt name and specific instructions for each, followed by the total cost of the operation and the model name used. If an error occurs during execution, the script catches the exception and prints an error message in red.",2026-01-03T04:22:04.560376+00:00
"context/detect_change_main_example.py","This Python script serves as a driver or entry point for executing a change detection process, likely within a larger system for managing prompt engineering or code generation workflows. It imports the `detect_change_main` function from a local module and the `click` library to simulate command-line interface (CLI) context. The `main` function initializes a `click.Context` object with specific model parameters such as strength, temperature, and flags for force and quiet modes. It defines a list of target prompt files (e.g., `python_preamble.prompt`, `change_python.prompt`) and creates a change description file containing instructions to make prompts more compact using a specific preamble. The script ensures the output directory exists before invoking `detect_change_main` with the configured context, file lists, and output path. Upon execution, it captures and prints the results, including the model name used, the total calculated cost, and a detailed list of necessary changes for each prompt file. Error handling is included to catch and display exceptions during the process.",2026-01-03T04:22:11.185779+00:00
"context/device_flow.txt","The provided text outlines the GitHub Device Flow, an authorization protocol designed for headless applications such as CLI tools or the Git Credential Manager. To utilize this flow, developers must first enable it in their application settings. The process involves three primary steps: the application requests verification codes from GitHub, the user enters a specific user code into a browser at a provided URL, and the application polls GitHub to check for successful authorization.

The initial request returns a device code, user code, verification URI, and a polling interval. The application must adhere to this interval when polling for the access token to avoid rate limits; failing to do so triggers a 'slow_down' error, which increases the wait time between requests. The codes expire after 15 minutes. Upon successful user authorization, the app receives an access token to make API calls. The documentation also details input parameters, response formats (JSON or XML), and specific error codes such as 'authorization_pending', 'expired_token', and 'access_denied'.",2026-01-03T04:22:23.746164+00:00
"context/edit_file_example.py","This Python script serves as a test harness and example runner for the `edit_file` module within the `pdd` package. It demonstrates how to programmatically edit files using an LLM-driven process. The script first configures the environment by resolving paths and importing the `run_edit_in_subprocess` function. It verifies the existence of the `PDD_PATH` environment variable, which dictates where output files are stored. The core functionality is encapsulated in the `run_example` function, which generates a dummy text file with initial content, defines a set of natural language editing instructions (e.g., replacing lines, adding text), and then executes the edit process. Post-execution, the script verifies the results by checking if the file content matches the expected changes and prints a detailed success or failure report to the console.",2026-01-03T04:22:46.226075+00:00
"context/example.prompt","The provided text outlines specific guidelines for generating usage examples for a software module. It assumes that the necessary environment variables and packages are pre-installed, focusing solely on demonstrating module functionality. Key instructions include handling imports correctly by treating the example as residing in a separate 'pdd' directory, necessitating absolute references like 'from pdd.module_name import module_name'. The guidelines emphasize clarity regarding units for inputs and outputs, such as costs in dollars per million tokens. When creating examples using the Click library, arguments must be passed directly to ensure execution without user intervention. Furthermore, the instructions specify that any required input files must be created within the example script, and all file operations—including outputs and prompts—must be directed to a './output' directory. Error handling via try/except blocks should be avoided to maintain simplicity. Finally, it notes that the 'PDD_PATH' environment variable is already configured to point to the correct directory.",2026-01-03T04:22:52.676212+00:00
"context/execute_bug_to_unit_test_failure.py","The provided file content is a very short Python script that demonstrates a basic usage of an imported function. Specifically, it imports a function named 'add' from a module located at 'context.bug_to_unit_test_failure_example'. After importing the function, the script executes a print statement that calls 'add' with the integer arguments 2 and 3. The purpose of this script appears to be a simple test or demonstration to verify that the 'add' function works as expected, likely outputting the result '5' to the console. The file structure suggests it might be part of a larger project involving unit testing or debugging examples, given the module path name which references 'bug_to_unit_test_failure_example'. It serves as a minimal entry point or driver code to invoke specific logic defined elsewhere in the project's context.",2026-01-03T04:23:04.389254+00:00
"context/final_llm_output.py","This Python script implements the Command Line Interface (CLI) for a Prompt-Driven Development (PDD) tool. Built using the `click` library, the application facilitates an AI-assisted coding workflow. The main `cli` group configures global parameters such as AI model strength, temperature, and output verbosity, while supporting chained command execution.

The script defines several subcommands that interface with backend logic imported from the `pdd` package. Core functionalities include `generate` (creating code from natural language prompts), `test` (generating unit tests), and `fix` (iteratively debugging code based on error logs and test results). Advanced prompt management is handled via commands like `split` (breaking down complex prompts), `change` (modifying prompts), `update` (syncing prompts with code changes), and `preprocess`.

Additionally, the tool includes utility functions for installing shell completion, displaying version information, and tracking API costs via CSV output. The structure suggests a workflow where developers interact primarily through natural language prompts to generate, test, and maintain software, with the CLI acting as the orchestrator between the user's file system and AI generation models.",2026-01-03T04:23:16.663139+00:00
"context/find_section_example.py","The provided text serves as documentation and an usage guide for the `find_section` function within the `pdd.find_section` module. It primarily focuses on how to extract code blocks from a list of text lines, such as output generated by a Large Language Model (LLM). The guide includes a practical Python code example that demonstrates importing the function, processing a sample string containing Python and JavaScript code blocks, and iterating through the results to print the language and line indices. Additionally, it shows how to apply the function to a file read from disk. The documentation section details the input parameters—`lines`, `start_index`, and `sub_section`—and describes the output format, which is a list of tuples containing the programming language, start line index, and end line index. Finally, it provides an example of the expected output based on the sample input.",2026-01-03T04:23:37.867947+00:00
"context/firecrawl_example.py","This file contains a Python code snippet demonstrating the basic usage of the `firecrawl-py` library for web scraping. It begins with a comment instructing the user to install the package via pip. The script imports the `FirecrawlApp` class from the `firecrawl` module and the standard `os` library. It then initializes an instance of `FirecrawlApp`, attempting to retrieve the API key from an environment variable named `FIRECRAWL_API_KEY`, with a fallback placeholder provided. Finally, the code uses the `scrape_url` method to scrape content from 'https://www.google.com', specifying the output format as markdown, and prints the resulting markdown content to the console.",2026-01-03T04:23:50.156081+00:00
"context/fix_code_loop_example.py","This Python script serves as a demonstration and test harness for the `fix_code_loop` function imported from the `pdd` package. The script defines a `main` function that orchestrates a scenario to automatically fix buggy code. It begins by creating two temporary files in an `output` directory: `module_to_test.py`, which contains a simple function to calculate averages, and `verify_code.py`, a verification script designed to fail by passing a string instead of a list of numbers to that function.

The script then defines the original prompt used to generate the code. It subsequently calls `fix_code_loop` with specific parameters, including the paths to the code and verification files, a model strength setting, a temperature of 0 for deterministic output, a maximum of 5 attempts, and a budget constraint. The function attempts to iteratively fix the code based on the errors generated by the verification script. Finally, the script prints the outcome of the process, including success status, total attempts made, the financial cost incurred, the model used, and the final corrected code. Cleanup code for removing the temporary files is present but currently commented out.",2026-01-03T04:24:01.165786+00:00
"context/fix_code_module_errors_example.py","This Python script serves as a demonstration or test harness for the `fix_code_module_errors` function, imported from the `pdd.fix_code_module_errors` package. The script defines a `main` function that simulates a scenario where a piece of generated code contains a runtime error. Specifically, it sets up variables representing a buggy program (which attempts to sum a string instead of a list of numbers), the original prompt that generated it, the isolated code module, and the resulting `TypeError` message.

The core of the script involves calling `fix_code_module_errors` with these inputs, along with configuration parameters for model strength, temperature, and verbosity. This function is designed to use an LLM to analyze the error and automatically generate fixed versions of both the full program and the specific code module. Finally, the script captures the return values—including boolean flags for updates, the corrected code strings, raw LLM output, API cost, and the model name—and prints them to the console to verify the repair process.",2026-01-03T04:24:16.167289+00:00
"context/fix_error_loop_example.py","This Python script serves as a demonstration of the `pdd.fix_error_loop` module, illustrating how to automate the repair of buggy code using Large Language Models (LLMs) and unit tests. The script begins by setting up a temporary environment and creating mock files in an output directory. These files include a deliberately buggy Python function (`calculator.py` which subtracts instead of adds), a failing unit test (`test_calculator.py`), a prompt file describing the desired functionality, and a verification script to check importability.

The core of the script is the execution of the `fix_error_loop` function. It configures various parameters such as LLM strength, temperature, maximum attempts, and a financial budget. The loop iteratively runs the unit tests, detects failures, sends the code and error logs to an LLM for analysis, applies the suggested fixes, and re-tests until the code passes or the attempt limit is reached. Finally, the script prints the results to the console using the `rich` library, displaying whether the fix was successful, the number of attempts used, the total cost incurred, and the final corrected code.",2026-01-03T04:24:29.786735+00:00
"context/fix_errors_from_unit_tests/1/conflicts_in_prompts.py","This Python script defines a function, `conflicts_in_prompts`, designed to analyze two text prompts for logical or thematic conflicts using a Large Language Model (LLM). The script utilizes the LangChain framework and Pydantic for data validation. It begins by defining data structures (`Conflict` and `ConflictOutput`) to standardize the output format, which includes descriptions, explanations, and resolution suggestions for any identified conflicts. The core function loads prompt templates from external files specified by an environment variable (`PDD_PATH`). It then employs a two-step LLM process: first, it runs a conflict analysis chain to generate a raw assessment of the two input prompts; second, it runs an extraction chain to parse that raw output into a structured JSON format. The script includes logic for selecting an LLM based on desired strength and temperature, calculating token usage and estimated costs, and printing status updates to the console using the `rich` library. Finally, it returns a list of identified conflicts, the total calculated cost of the operation, and the name of the model used.",2026-01-03T04:24:36.291922+00:00
"context/fix_errors_from_unit_tests/1/conflicts_in_prompts_python.prompt","This file provides a detailed specification for an expert Python engineer to create a function named conflicts_in_prompts. The primary purpose of this function is to analyze two input prompts, identify any conflicts between them, and offer suggestions for resolution. The function takes two prompt strings, along with optional strength and temperature parameters for the Large Language Model (LLM), as inputs. It returns a list of conflict dictionaries (containing descriptions, explanations, and resolution suggestions for each prompt), the total cost of the operation, and the name of the model used.

The implementation instructions require the use of the LangChain Expression Language (LCEL). The process involves several specific steps: loading prompt templates from a project path defined by an environment variable, selecting an LLM using a custom llm_selector module, and executing a two-step chain. First, the function runs a conflict detection prompt. Second, it runs an extraction prompt to parse the initial output into a structured JSON format. The specification also mandates pretty-printing status messages regarding token counts and costs during execution, ensuring transparency about resource usage.",2026-01-03T04:24:48.353686+00:00
"context/fix_errors_from_unit_tests/1/test_conflicts_in_prompts.py","This file contains a suite of unit tests for the `conflicts_in_prompts` function, which appears to be part of a larger system (likely `pdd`) designed to analyze conflicts between two text prompts using Large Language Models (LLMs). The tests are written using the `pytest` framework and rely heavily on `unittest.mock` to isolate the function from external dependencies such as file systems, environment variables, and actual LLM API calls.

The test suite includes several fixtures: `mock_environment` to simulate the `PDD_PATH` environment variable, `mock_prompt_files` to provide fake prompt template content, `mock_llm_selector` to simulate the LLM selection and cost calculation logic, and `mock_json_parser` to handle the parsing of LLM outputs. The specific test cases cover various scenarios: successful execution returning a list of conflicts and costs, error handling for missing environment variables (`ValueError`), handling of missing prompt files (`FileNotFoundError`), resilience against unexpected exceptions (returning empty results), and verification that custom parameters like `strength` and `temperature` are correctly passed to the underlying LLM selector.",2026-01-03T04:25:01.472265+00:00
"context/fix_errors_from_unit_tests/2/code_generator.py","This Python script defines a `code_generator` function designed to generate code based on a user-provided prompt using a Large Language Model (LLM). The process begins by preprocessing the raw prompt to handle recursive elements and formatting. It then selects an appropriate LLM based on a specified strength and temperature using an `llm_selector` module. The script constructs a LangChain prompt template, invokes the model, and calculates the associated costs for input and output tokens. After generating the initial response, the code checks for completeness; if the output appears unfinished, it triggers a continuation process. Otherwise, it post-processes the result to extract clean, runnable code in the target language. Throughout execution, the script utilizes the `rich` library to print formatted logs, including token counts, cost estimates, and the generated markdown content, to the console. Finally, it returns the runnable code, the total calculated cost, and the name of the model used, while handling potential errors gracefully.",2026-01-03T04:25:14.841028+00:00
"context/fix_errors_from_unit_tests/2/code_generator_python.prompt","The provided text outlines a prompt for an expert Python engineer to create a function named code_generator. This function is designed to compile a raw text prompt into a runnable code file. The function accepts inputs including the raw prompt, the target programming language (e.g., Python, Bash), a strength parameter (0-1), and a temperature setting for the LLM. It outputs the resulting runnable code, the total cost of the operation, and the name of the model used. The process involves several steps leveraging LangChain Expression Language (LCEL): preprocessing the prompt, selecting an appropriate LLM via an internal `llm_selector` module, and executing the prompt while tracking token usage and cost. The workflow also includes logic to detect incomplete generations using an `unfinished_prompt` function; if incomplete, it triggers a continuation, otherwise, it post-processes the result. Finally, the function calculates and prints the total cost before returning the final code, cost, and model name. The prompt includes references to external context files for examples of LCEL usage and internal module implementations.",2026-01-03T04:25:20.910894+00:00
"context/fix_errors_from_unit_tests/2/test_code_generator.py","This file contains a suite of unit tests for the `code_generator` function within the `pdd.code_generator` module. Utilizing the `pytest` framework and `unittest.mock` for dependency isolation, the tests verify the function's behavior under various scenarios. The test suite includes three primary test cases: `test_code_generator_success`, which validates the standard successful execution path where code is generated, processed, and cost-calculated correctly; `test_code_generator_incomplete_generation`, which checks the logic for handling incomplete prompts that require a continuation step; and `test_code_generator_exception_handling`, which ensures the function gracefully handles errors by returning empty values and zero cost when an exception occurs during preprocessing. The tests mock several internal dependencies such as `preprocess`, `llm_selector`, `unfinished_prompt`, `continue_generation`, and `postprocess` to simulate different states of the Large Language Model (LLM) interaction pipeline without making actual API calls.",2026-01-03T04:25:27.450422+00:00
"context/fix_errors_from_unit_tests/3/context_generator.py","This file defines a Python function named `context_generator` designed to generate example code using a Large Language Model (LLM). The function orchestrates a multi-step process involving prompt loading, preprocessing, model selection, and execution via LangChain. It begins by verifying the `PDD_PATH` environment variable to locate prompt templates. The workflow includes loading a specific prompt template (`example_generator_LLM.prompt`), preprocessing it, and selecting an appropriate LLM based on strength and temperature parameters using `llm_selector`. The function then invokes the model to generate code based on a user-provided prompt and code module. Crucially, it includes logic to handle incomplete generations by checking the output's tail and continuing generation if necessary. Finally, the output undergoes post-processing to refine the code. Throughout the execution, the function calculates and prints estimated costs based on token usage and returns the generated code, total cost, and the name of the model used.",2026-01-03T04:25:33.187119+00:00
"context/fix_errors_from_unit_tests/3/context_generator_python.prompt","The provided text outlines a detailed specification for an AI agent acting as an expert Python engineer. The primary objective is to develop a Python function named context_generator. This function is designed to automatically generate concise usage examples for a given code module. The specification details the function's inputs (code module, original prompt, language, model strength, and temperature) and outputs (the generated example code, total cost, and model name). It includes references to external context files for Python preambles and LangChain Expression Language (LCEL) examples. Furthermore, the text provides a step-by-step algorithm for the function's logic, which involves loading a specific prompt file, preprocessing it, selecting an LLM via a selector module, invoking the model using LCEL, and handling potential incomplete outputs through a continue_generation function. It also mandates post-processing of the result and calculating the total financial cost of the operation based on token usage.",2026-01-03T04:25:45.589692+00:00
"context/fix_errors_from_unit_tests/3/test_context_generator.py","This file contains a suite of unit tests for the `context_generator` function within the `pdd` module, utilizing the `pytest` framework and `unittest.mock` library. The tests are designed to verify the behavior of the `context_generator` under various scenarios, including successful execution, handling of unfinished prompts, missing environment variables, file not found errors, and general exception handling.

Key components include several pytest fixtures (`mock_environment`, `mock_file_content`, `mock_llm_selector`, `mock_chain_run`) that simulate external dependencies like environment variables, file I/O, and Large Language Model (LLM) interactions. The test cases cover:
1.  **Success Path:** Verifies that the generator correctly processes inputs, interacts with the LLM chain, and returns a tuple containing the post-processed output, cost, and model name.
2.  **Unfinished Prompt:** Tests the logic for handling cases where the initial generation is incomplete, ensuring `continue_generation` is called.
3.  **Error Handling:** Checks for appropriate responses when the `PDD_PATH` environment variable is missing (raising a `ValueError`), when the prompt file is not found, or when an unexpected exception occurs during preprocessing (returning empty/zero values).",2026-01-03T04:25:51.793814+00:00
"context/fix_errors_from_unit_tests/4/detect_change.py","This Python file defines a function named `detect_change` designed to analyze a list of prompt files against a change description to determine necessary modifications. The process involves several steps: loading specific prompt templates (`detect_change_LLM.prompt` and `extract_detect_change_LLM.prompt`) from a directory specified by an environment variable, preprocessing these templates, and selecting an appropriate Large Language Model (LLM) based on strength and temperature parameters using an `llm_selector`. The function reads the content of the provided prompt files, constructs a prompt list, and executes a LangChain-based processing chain to generate an initial analysis. Subsequently, a second LLM call extracts structured change instructions from the initial analysis into a JSON format. The script calculates and reports token usage and estimated costs for each step using the `rich` library for console output. Finally, it returns a list of required changes, the total cost of the operation, and the name of the model used, while handling potential errors like file not found or JSON decoding issues.",2026-01-03T04:26:05.949969+00:00
"context/fix_errors_from_unit_tests/4/detect_change_1_0_1.py","This file defines a Python function named `detect_change` designed to analyze a list of prompt files against a specific change description using a Large Language Model (LLM). The function's primary goal is to determine which prompts require modification and to generate detailed instructions for those changes.

The process involves several steps: loading specific prompt templates (`detect_change_LLM.prompt` and `extract_detect_change_LLM.prompt`) from a directory specified by an environment variable; preprocessing these templates; and selecting an appropriate LLM based on strength and temperature parameters via a helper function `llm_selector`. The function then reads the content of the provided prompt files, packages them with the change description, and invokes the LLM using LangChain to generate an analysis.

A secondary extraction step uses another LLM call to parse the initial analysis into a structured JSON format containing a list of changes. The function calculates and reports token usage and estimated costs for both LLM interactions. Finally, it returns a list of necessary changes, the total cost, and the model name used, while handling potential errors like file not found or JSON decoding issues with console error logging.",2026-01-03T04:26:18.124825+00:00
"context/fix_errors_from_unit_tests/4/detect_change_python.prompt","This file provides a detailed specification for an AI agent acting as an expert Python Software Engineer. The objective is to create a Python function named `detect_change` that analyzes a list of prompt files against a provided change description to identify necessary modifications. The specification outlines the function's inputs (prompt files, change description, strength, temperature) and outputs (a list of changes, total cost, and model name). It includes references to external context files for Python preambles, LangChain LCEL examples, and internal module usage (preprocessing, LLM selection, token counting, etc.). The document concludes with a step-by-step algorithm for the function, detailing how to load specific prompts, preprocess inputs, execute LangChain LCEL chains to detect and extract changes, calculate costs based on token usage, and format the final output.",2026-01-03T04:26:30.807393+00:00
"context/fix_errors_from_unit_tests/4/test_detect_change.py","This file contains a suite of unit tests for the `detect_change` function within the `pdd.detect_change` module, utilizing the `pytest` framework. The tests rely heavily on mocking external dependencies and internal components to isolate the function's logic. Several mock classes (`MockPromptTemplate`, `MockLLM`, `MockOutputParser`) and fixtures (`mock_environment`, `mock_file_contents`, `mock_llm_selector`, `mock_preprocess`) are defined to simulate the behavior of language models, file systems, and environment variables without making actual API calls or file operations.

The test suite covers four main scenarios: a successful execution path where changes are correctly detected and parsed; a scenario where the target prompt file is not found (`FileNotFoundError`); a case handling JSON decoding errors during output parsing; and a general catch-all for unexpected exceptions during execution. Each test verifies that the function returns the expected structure—typically a list of results, a total cost, and a model name—ensuring robustness against various failure modes.",2026-01-03T04:26:37.403143+00:00
"context/fix_errors_from_unit_tests/4/test_detect_change_1_0_1.py","This file contains a suite of unit tests for the `detect_change` function within the `pdd.detect_change` module, utilizing the `pytest` framework. The tests are designed to verify the function's behavior under various scenarios, including successful execution, file handling errors, and data parsing issues.

The file defines several fixtures to mock external dependencies and environment variables. `mock_environment` sets up a dummy path for `PDD_PATH`, while `mock_file_contents` provides sample strings for prompts and file content. `mock_llm_selector` and `mock_preprocess` simulate the behavior of the Large Language Model (LLM) selection and preprocessing steps, ensuring tests run in isolation without making actual API calls.

The test cases cover four main scenarios: `test_detect_change_success` verifies that the function correctly processes a prompt file and returns the expected change instructions and cost; `test_detect_change_file_not_found` ensures the function handles missing files gracefully by returning empty results; `test_detect_change_json_decode_error` checks resilience against malformed JSON output from the LLM; and `test_detect_change_unexpected_error` confirms that general exceptions are caught and handled without crashing the application.",2026-01-03T04:26:43.694523+00:00
"context/fix_errors_from_unit_tests/5/continue_generation.py","This Python script implements a sophisticated text generation pipeline using LangChain and various utility modules to manage Large Language Model (LLM) interactions. The core functionality is encapsulated in the `continue_generation` function, which orchestrates a multi-step process to extend and refine generated text, specifically focusing on code blocks. The process begins by loading and preprocessing prompt templates from a specified directory. It then utilizes an `llm_selector` to choose appropriate models for generation and trimming tasks based on desired strength and temperature parameters.

The script employs a loop to iteratively generate text, checking for completion using an `unfinished_prompt` helper. It handles the extraction of clean text from JSON responses and calculates the cost of API usage at each step (trimming start, continuing generation, checking completion, and final trimming). The `rich` library is used for console output, providing formatted markdown displays of the generated code and cost metrics. Ultimately, the script returns the fully generated code block, the total accumulated cost, and the name of the model used.",2026-01-03T04:26:56.800010+00:00
"context/fix_errors_from_unit_tests/5/continue_generation_2_0_1.py","This Python script defines a function named `continue_generation` designed to extend and refine text generation from a Large Language Model (LLM). The process begins by loading and preprocessing specific prompt templates from a directory specified by an environment variable. It utilizes LangChain to create processing chains for continuing generation and trimming results, selecting appropriate LLMs based on strength and temperature parameters via a helper function. The core logic involves an iterative loop where the script extracts an initial code block from previous output, generates a continuation, and checks if the generation is complete using a separate `unfinished_prompt` utility. If the generation is deemed unfinished, the loop continues; otherwise, the result is trimmed and finalized. Throughout the execution, the script calculates and logs the cost of API usage for each step, displaying intermediate outputs and costs using the `rich` library for formatted console output. Finally, it returns the complete generated code block, the total cost, and the model name used.",2026-01-03T04:27:09.724621+00:00
"context/fix_errors_from_unit_tests/5/continue_generation_python.prompt","The provided text outlines a prompt for an expert Python Software Engineer to create a function named continue_generation. This function is designed to complete the generation of text from a Large Language Model (LLM) when the initial output is truncated or incomplete. The prompt details specific inputs (formatted prompt, current LLM output, strength, and temperature) and outputs (final complete string, total cost, and model name). It includes instructions to load specific prompt files from an environment path, preprocess them, and set up Langchain LCEL templates. The core logic involves a loop: checking if the current output is unfinished using a helper function, invoking the LLM to continue generation if necessary, and trimming the results to ensure seamless concatenation. The instructions also emphasize tracking token costs across different model invocations and using specific internal modules for preprocessing and model selection.",2026-01-03T04:27:22.018202+00:00
"context/fix_errors_from_unit_tests/5/error_log.txt","The provided file content is a log output from a failed `pytest` execution session. The session was initiated on a Darwin (macOS) platform using Python 3.12.4 and pytest 8.3.2 within a specific Anaconda environment. The log indicates that the test collection process started but ultimately collected zero items. The execution concluded almost immediately (0.00s) with a specific error message stating file or directory not found, pointing to an invalid path: `/path/to/non/existent/file.py`. This suggests the user attempted to run tests on a file that does not exist at the specified location, resulting in no tests being run.",2026-01-03T04:27:34.320137+00:00
"context/fix_errors_from_unit_tests/5/test_continue_generation.py","This file contains a suite of unit tests for the `continue_generation` module within the `pdd` package, utilizing the `pytest` framework. The tests are designed to verify the functionality of the `continue_generation` function, which appears to manage iterative interactions with a Large Language Model (LLM). Key components include several fixtures (`mock_environment`, `mock_prompts`, `mock_llm_selector`, `mock_unfinished_prompt`) that simulate the runtime environment, prompt loading, LLM responses, and completion checks. The test cases cover various scenarios: successful generation sequences (`test_continue_generation_success`), error handling for missing environment variables (`test_continue_generation_missing_env_variable`) and missing files (`test_continue_generation_file_not_found`), the handling of multiple generation iterations (`test_continue_generation_multiple_iterations`), and the accurate calculation of token costs (`test_continue_generation_cost_calculation`). The tests mock external dependencies like file I/O and the `rich` console to ensure isolation.",2026-01-03T04:27:45.142088+00:00
"context/fix_errors_from_unit_tests/5/test_continue_generation_2_0_1.py","This file contains a suite of unit tests for the `continue_generation` module within the `pdd` package, utilizing the `pytest` framework. The tests verify the functionality of the `continue_generation` function, which appears to manage iterative interactions with a Large Language Model (LLM) to complete generated text or code.

The test suite includes several fixtures to mock the environment (`PDD_PATH`), prompt templates, and the LLM selector mechanism, simulating responses for generation, trimming, and token counting. Specific test cases cover various scenarios: a successful generation workflow where the output is correctly processed and costs are calculated; error handling for missing environment variables and missing prompt files; and a scenario involving multiple iterations where the generation is not immediately finished. Additionally, there is a specific test to ensure the accurate calculation of total costs associated with the LLM usage. The tests heavily rely on `unittest.mock.patch` to isolate the function from external dependencies like file I/O and actual API calls.",2026-01-03T04:27:56.823871+00:00
"context/fix_errors_from_unit_tests_example.py","This Python script serves as a demonstration and entry point for testing the functionality of the `fix_errors_from_unit_tests` module. The script defines a `main` function that sets up a hypothetical scenario involving a buggy unit test and a corresponding piece of code containing a deprecated NumPy function call. Specifically, it provides an example unit test for an addition function with an intentional assertion error, alongside an implementation of that function that triggers a `DeprecationWarning`.

The script initializes necessary parameters such as the prompt description, error logs, and configuration settings for an underlying Large Language Model (LLM), including strength and temperature. It then invokes the `fix_errors_from_unit_tests` function synchronously, passing in these inputs to simulate an automated debugging process. Finally, the script uses the `rich` library to print a formatted report of the results, displaying whether the unit test or code was updated, the fixed versions of the content, the analysis provided by the LLM, the total cost of the operation, and the specific model used. The script is designed to be executed directly as a standalone program.",2026-01-03T04:28:12.303569+00:00
"context/fix_main_example.py","This Python script serves as a demonstration and test harness for the `fix_main` function from the `pdd` package, illustrating how to programmatically invoke automated code repair functionality. The script first sets up a mock environment by creating a directory named `output` and populating it with several necessary files: a buggy Python script (`calculator.py` containing a subtraction error in an addition function), a failing unit test, a prompt file describing the desired functionality, a verification script, and a sample error log. It then initializes a `click.Context` object with configuration parameters (like verbosity, LLM strength, and budget) to simulate a command-line execution environment. The `main` function executes two distinct examples of invoking `fix_main`: first in Loop Mode, which attempts to iteratively fix the code using the verification script and a maximum attempt limit; and second in Single-Pass Mode, which performs a one-time fix attempt based on an existing error log. The script outputs the success status, number of attempts, cost, and the resulting fixed code for these operations using the `rich` library for formatted console output.",2026-01-03T04:28:19.149947+00:00
"context/fix_verification_errors_example.py","This file is a Python example script demonstrating the usage of the `fix_verification_errors` function from the `pdd` package. The script illustrates an automated workflow for identifying and repairing bugs in code modules using Large Language Models (LLMs) based on verification logs.

The example sets up a scenario with a buggy code module (`calculator_module.py` which incorrectly subtracts instead of adds) and a driver program that verifies its output. It defines necessary inputs such as the original prompt, the buggy code, the verification program, and the captured failure output. The script then calls `fix_verification_errors` with parameters for LLM strength, temperature, and verbosity. Finally, it parses and displays the results returned by the function, including whether issues were found, if the code or program was updated, the fixed source code, the specific LLM model used, and the associated cost. The code relies on specific environment configurations, such as API keys and prompt template directories, to function correctly.",2026-01-03T04:28:31.947933+00:00
"context/fix_verification_errors_loop_example.py","This Python script serves as a demonstration and test driver for the `fix_verification_errors_loop` function, which is designed to iteratively fix code based on verification failures. The script sets up a scenario involving a buggy calculator module (`calculator_module.py`) that incorrectly subtracts instead of adds, and a driver program (`program.py`) that tests this module. It creates these files on disk, along with a secondary verification script. The script then configures various parameters for the repair loop, including model strength, temperature, maximum attempts (set to 5), budget, and logging options. Finally, it executes the `fix_verification_errors_loop` with specific command-line arguments (`5`, `3`) to trigger the bug, and prints the results of the repair process, including success status, the final fixed code, total attempts made, and the cost incurred.",2026-01-03T04:28:43.967683+00:00
"context/fix_verification_main_example.py","This Python script serves as a demonstration and test harness for the `fix_verification_main` function from the `pdd` package. It illustrates how to programmatically invoke the verification logic, simulating a CLI environment using a dummy Click context. The script sets up a test scenario by creating a temporary `output` directory containing a prompt, an intentionally buggy Python module (`calculator.py` which subtracts instead of adds), a runner program, and a verification script with assertions. It then executes `fix_verification_main` in two distinct modes: a single-pass mode (`loop=False`) where the LLM judges output once, and an iterative loop mode (`loop=True`) where the system attempts to fix the code over multiple attempts until a budget or attempt limit is reached or the verification script passes. The script prints detailed results to the console using the `rich` library, including success status, costs, attempt counts, and file paths for the generated outputs.",2026-01-03T04:28:50.177837+00:00
"context/gcs_hmac_test.py","This Python script demonstrates how to upload a file to Google Cloud Storage (GCS) using the `boto3` library, which is typically used for AWS S3 but is compatible with GCS via HMAC keys. The script begins by loading environment variables to retrieve necessary credentials, specifically the GCS HMAC access key, secret key, and the target bucket name. It configures logging to track the execution flow and errors. The core functionality involves creating an S3 client pointed at the Google Storage endpoint (`https://storage.googleapis.com`). It then attempts to upload a simple text string as a file named `test-hmac-upload.txt` to the specified bucket. The script includes robust error handling, catching `ClientError` exceptions to provide specific feedback on issues like missing buckets or authentication failures, as well as general exceptions. Finally, it prints a verification link to the Google Cloud Console upon success.",2026-01-03T04:29:02.426602+00:00
"context/gemini_may_pro_example.py","The provided file content is a concise Python script demonstrating how to utilize the `litellm` library to interact with Google's Vertex AI service. Specifically, the code imports the `completion` function from the `litellm` package, which serves as a unified interface for various Large Language Model (LLM) providers. The script then executes a call to the `completion` function, specifying the model identifier `vertex_ai/gemini-2.5-pro-preview-05-06`. This indicates the user intends to access a preview version of the Gemini 2.5 Pro model hosted on Vertex AI. The function call includes a standard message structure with a user role and a placeholder prompt content. Finally, the script captures the response from the API call in a variable named `response` and prints the output to the console. This snippet effectively serves as a minimal working example or boilerplate code for developers looking to integrate Vertex AI's Gemini models into their applications using the LiteLLM abstraction layer.",2026-01-03T04:29:08.949430+00:00
"context/generate/1/fix_errors_from_unit_tests.py","This Python script defines a function, `fix_errors_from_unit_tests`, designed to automatically correct coding errors identified during unit testing using Large Language Models (LLMs). The process begins by setting up an SQLite cache to optimize performance and cost. The core function takes the failing unit test, the source code, the error message, and LLM configuration parameters (strength and temperature) as inputs. It operates in two main stages: first, it uses a specific prompt template to generate a fix for the errors, calculating and displaying the token usage and cost for this operation. Second, it employs a different prompt template to parse the initial LLM response into a structured JSON format, extracting the corrected code and unit test. The script utilizes the `langchain` library for chaining LLM operations and `rich` for formatted console output. Finally, it returns flags indicating whether the unit test or code was updated, the fixed code strings, and the total calculated cost of the LLM interactions.",2026-01-03T04:29:20.623650+00:00
"context/generate/1/fix_errors_from_unit_tests_python.prompt","This document outlines the requirements for developing a Python function named fix_errors_from_unit_tests designed to resolve unit test failures using Large Language Models (LLMs) via the Langchain library. The function takes the failing unit test, the code under test, error messages, and LLM configuration parameters (strength and temperature) as inputs. It returns boolean flags indicating necessary updates, the corrected code and test strings, and the total operational cost.

The process involves a multi-step workflow using Langchain's LCEL (LangChain Expression Language). First, it loads specific prompt templates from a project directory defined by the $PDD_PATH environment variable. It then executes an initial LLM run to generate a fix based on the provided errors, utilizing a helper `llm_selector` to manage model selection and cost estimation. A second LLM run parses this output into structured JSON data to extract the specific code modifications. Throughout the execution, the function is required to use the Python `rich` library for pretty-printing console output, including markdown formatting, token counts, and cost calculations. Finally, it aggregates the costs from both steps and returns the parsed results.",2026-01-03T04:29:33.593385+00:00
"context/generate/2/cli.py","This Python script defines a command-line interface (CLI) tool named PDD (Prompt-Driven Development), built using the `click` and `rich` libraries. The tool facilitates an AI-assisted development workflow by providing various commands to generate, test, and modify code based on natural language prompts. Key functionalities include `generate` for creating code from prompt files, `example` for generating example usage, `test` for creating unit tests, and `fix` for iteratively resolving errors in code and tests using AI.

Additional commands include `preprocess` for formatting prompts (optionally with XML tagging), `split` for breaking down complex prompts, `change` for modifying prompts based on new instructions, and `update` for synchronizing prompts with manually modified code. The script integrates with LangChain for AI model interaction and includes a caching mechanism (SQLite) to optimize costs and speed. It also features a cost-logging system that tracks AI usage expenses in a CSV file. Finally, the tool includes a utility to install shell tab completion for Bash, Zsh, and Fish environments.",2026-01-03T04:29:45.765883+00:00
"context/generate/2/cli_python.prompt","The provided text outlines the specifications for building a Python command-line interface (CLI) tool named pdd. The tool is designed to be built using the `Click` library for CLI management and the `rich` library for pretty-printing console output. The project structure includes specific directories for source code, prompts, context, and data. The core functionality revolves around several commands: `generate` (creates runnable code from prompts), `example` (generates examples from code), `test` (creates unit tests), `preprocess` (processes prompts, optionally to XML), `fix` (corrects code errors based on unit tests, with a looping sub-command), `split` (divides prompt files), `change` (modifies prompt files), and `update` (updates prompts). Additionally, there is an `install_completion` command to manage shell auto-completion. The specification emphasizes using relative imports, avoiding naming conflicts between CLI commands and internal functions, and utilizing helper functions like `construct_paths` for file handling. It references external context files (e.g., `README.md` and various `*_example.py` files) to guide the implementation of each specific feature.",2026-01-03T04:29:59.815788+00:00
"context/generate/3/cli.py","This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` library. It serves as the main entry point for the application, orchestrating various AI-driven development tasks. The CLI includes a global configuration for model strength, temperature, verbosity, and cost tracking via a CSV log.

The file implements several commands, including `generate` (creating code from prompts), `example` (generating examples from code/prompts), `test` (creating unit tests), and `preprocess` (formatting prompts). It also features advanced workflows like `fix` (iteratively repairing code based on test failures), `split` (breaking down complex prompts), `change` (modifying prompts based on new requirements), and `update` (synchronizing prompts with code changes). Additionally, it provides utilities for detecting necessary changes across multiple files (`detect`), analyzing prompt conflicts (`conflicts`), and fixing crash-causing errors (`crash`). A helper function `track_cost` decorates these commands to log execution metrics and API costs. Finally, it includes a utility to install shell completion for the CLI.",2026-01-03T04:30:12.435031+00:00
"context/generate/3/cli_python_preprocessed.prompt","The provided file describes pdd (Prompt-Driven Development), a Python command-line interface (CLI) tool designed to streamline software development using AI models. Built with the Click and Rich libraries, PDD facilitates a workflow where developers define functionality through prompt files (e.g., `basename_language.prompt`). The tool supports multiple programming languages and offers a suite of commands to generate runnable code, create unit tests, produce usage examples, and preprocess prompts. Advanced features include a `fix` command that iteratively repairs code based on test errors and a budget, a `split` command for breaking down complex prompts, and conflict detection between prompt files. PDD also includes cost tracking capabilities, allowing users to monitor AI model usage expenses via CSV reports. It supports multi-command chaining for complex workflows and allows configuration via environment variables for output paths. The system emphasizes a structured approach to AI-assisted coding, ensuring consistency through specific naming conventions and offering tools for maintaining and updating prompts as code evolves.",2026-01-03T04:30:28.224150+00:00
"context/generate/4/cli.py","The provided code defines the main entry point for a Prompt-Driven Development (PDD) Command Line Interface tool, built using the Python `click` library. It serves as an orchestration layer that connects user commands to various backend AI modules for code generation and maintenance.

The CLI defines a global context allowing users to configure AI model parameters like strength and temperature, toggle verbosity, and track API costs. It exposes a comprehensive suite of commands designed to automate the software development lifecycle. Key functionalities include `generate` for creating runnable code from text prompts, `test` for producing unit tests, and `fix` for an iterative, AI-driven debugging loop that resolves errors.

Advanced features include `update` (synchronizing prompts with modified code), `split` (breaking down complex prompts), `detect` (identifying necessary changes), and `trace` (mapping code lines back to prompt instructions). The tool also handles file preprocessing and shell completion installation. Each command wraps specific logic imported from local modules, manages file input/output paths, and utilizes a `@track_cost` decorator to monitor the financial usage of the underlying AI models.",2026-01-03T04:30:35.221284+00:00
"context/generate/4/cli_python.prompt","This file contains a detailed prompt for generating a Python command-line interface (CLI) tool named pdd using the Click library. The prompt outlines the directory structure, imports, and specific functionality for various commands including `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, `bug`, and `install_completion`. It provides instructions on how to structure the CLI, handle imports to avoid naming conflicts (e.g., aliasing `preprocess` to `preprocess_func`), and utilize internal modules for specific tasks like code generation, unit test creation, and error fixing. The prompt also includes references to example code snippets for implementing each command and using decorators like `track_cost`.",2026-01-03T04:31:07.054382+00:00
"context/generate/5/generate_output_paths.py","This Python code defines a utility function named `generate_output_paths` designed to determine the appropriate file paths for various outputs based on a specific command context. The function takes inputs such as the command type (e.g., 'generate', 'test', 'fix', 'split'), a dictionary of explicit output locations, a file basename, a programming language, and a file extension. It employs nested helper functions to handle logic for default filename generation and environment variable lookups (prefixed with `PDD_`). The core logic iterates through required output keys, prioritizing explicitly provided paths, falling back to environment variables, and finally defaulting to constructed filenames if no path is specified. It also intelligently handles directory paths by appending the appropriate default filename. The function supports a wide range of commands, including 'generate', 'example', 'test', 'preprocess', 'fix', 'split', 'change', 'update', 'detect', 'conflicts', and 'crash', returning a dictionary mapping output keys to their resolved string file paths.",2026-01-03T04:31:16.401147+00:00
"context/generate/5/generate_output_paths_python.prompt","This file outlines a programming task for a Python software engineer to implement a function named `generate_output_paths` for the PDD (Prompt-Driven Development) CLI tool. The core objective is to construct appropriate output filenames and paths based on user commands, input parameters, and environment variables. The file includes a comprehensive manual for the PDD tool (version 0.1.0), detailing its functionality, supported languages, and prompt file naming conventions (`<basename>_<language>.prompt`). It describes eleven specific commands (generate, example, test, preprocess, fix, split, change, update, detect, conflicts, and crash), each with unique default output naming schemas. For instance, `generate` defaults to `<basename>.<extension>`, while `fix` produces multiple outputs like fixed code and test files. The document specifies global options, cost tracking features, and a hierarchy for determining output locations: explicit CLI arguments take precedence, followed by specific environment variables (e.g., `PDD_GENERATE_OUTPUT_PATH`), and finally default naming conventions in the current directory. The task requires handling edge cases, such as missing extensions, and implementing logic to prioritize these path definitions efficiently.",2026-01-03T04:31:28.570840+00:00
"context/generate/6/conflicts_main.py","This file defines the `conflicts_main` function, which serves as the core logic for a command-line interface (CLI) tool designed to analyze conflicts between two text prompts. The function takes two prompt file paths and an optional output path as arguments. It utilizes helper functions to construct file paths and read the content of the prompts. The core analysis is performed by the `conflicts_in_prompts` function, which evaluates the prompts based on strength and temperature parameters derived from the CLI context. The results, including specific conflict instructions and the prompt names, are written to a CSV file. The function also handles user feedback by printing the analysis status, model usage, total cost, and a detailed list of detected conflicts to the console using the `rich` library. Error handling is implemented to catch exceptions and exit gracefully.",2026-01-03T04:31:35.890331+00:00
"context/generate/6/conflicts_main_python.prompt","This document provides a prompt for an expert Python engineer to generate the `conflicts_main` function for the `pdd` command-line program. The function is designed to handle the logic for a `conflicts` command, which analyzes potential conflicts between two prompt files. The prompt specifies the function's inputs (Click context, two prompt file paths, and an optional output path) and expected outputs (a tuple containing analysis results, cost, and model name). It outlines a seven-step process for the function: constructing file paths using internal utilities, loading input files, analyzing conflicts via the `conflicts_in_prompts` function, updating result data with actual file paths, saving the output to a CSV file, providing user feedback via the Rich library, and implementing robust error handling. The document also references included context files for Python preambles and examples of using the Click library and internal modules like `construct_paths`.",2026-01-03T04:31:47.531974+00:00
"context/generate/7/README.md","This document serves as the comprehensive user manual for the PDD (Prompt-Driven Development) Command Line Interface, a tool designed to streamline software development using AI models. PDD facilitates code generation, unit testing, debugging, and prompt management through a suite of specific commands. Key features include generating code from prompt files (`generate`), creating examples (`example`), generating and enhancing unit tests (`test`), and iteratively fixing code errors (`fix`, `crash`). It supports multiple programming languages and integrates with major AI providers like OpenAI and Anthropic via API keys.

The manual details installation procedures, environment configuration (including auto-updates and output paths), and cost tracking mechanisms. It explains the prompt file naming convention (`<basename>_<language>.prompt`) and outlines advanced workflows like splitting complex prompts (`split`), detecting necessary changes (`detect`), and managing dependencies (`auto-deps`). The tool emphasizes a prompt-first approach, where the prompt file is the primary source of truth, and includes features for maintaining synchronization between prompts and code (`update`, `change`). Security considerations, troubleshooting tips, and workflow integration strategies are also provided to ensure effective and safe usage in development environments.",2026-01-03T04:31:53.607706+00:00
"context/generate/7/code_generator_main.py","This file defines the main logic for a code generation command-line utility, encapsulated within the `code_generator_main` function. The script orchestrates the process of reading a prompt file, generating code based on that prompt, and optionally saving the output. It utilizes the `click` library for command-line context management and `rich` for formatted terminal output. The workflow begins by constructing necessary file paths and loading the content of the specified prompt file using a helper function `construct_paths`. It then invokes the core `code_generator` function, passing in parameters such as the prompt content, target language, and model settings like strength and temperature derived from the command context. Upon successful generation, the script writes the resulting code to an output file if a path is provided. It also provides user feedback via the terminal, displaying the model used, the total cost of the operation, and the location of the saved file, unless quiet mode is enabled. Error handling is implemented to catch exceptions, print error messages, and exit the program gracefully.",2026-01-03T04:32:06.101834+00:00
"context/generate/7/code_generator_main_python.prompt","The provided file is a prompt template designed for an AI code generation task. It instructs an AI, acting as an expert Python engineer, to create a specific Python function named 'code_generator_main'. This function serves as a Command Line Interface (CLI) wrapper using the 'click' library. Its primary purpose is to read a prompt file, invoke a 'code_generator' function to produce code, and manage the output destination. The prompt details the function's inputs (Click context, prompt file path, and optional output path) and expected outputs (a tuple containing the generated code, cost, and model name). It includes references to external context files, such as a Python preamble, examples of using the 'click' library, internal module usage (specifically 'construct_paths' and 'code_generator'), and a README file explaining the CLI command's operation. The structure relies heavily on XML-like tags to include these external resources dynamically.",2026-01-03T04:32:18.750433+00:00
"context/generate/8/llm_invoke.py","This Python module, `llm_invoke.py`, provides a flexible framework for invoking various Large Language Models (LLMs) using the LangChain library. It is designed to dynamically select models based on a strength parameter, which balances cost against performance (measured by coding arena ELO). The core functionality revolves around the `llm_invoke` function, which accepts a prompt, input data, strength, and temperature settings.

The module loads model configurations from a CSV file (`llm_model.csv`), parsing details such as provider (OpenAI, Anthropic, Google, etc.), costs, and API keys. It includes a `ModelInfo` class to store these details and a `CompletionStatusHandler` to track token usage and completion reasons. The `select_model` function implements logic to choose a model: a strength below 0.5 prioritizes cost savings relative to a base model (defaulting to `gpt-4o-mini`), while a strength above 0.5 prioritizes higher performance.

Additionally, the script handles model instantiation for multiple providers, calculates invocation costs based on token usage, and supports both string and structured (Pydantic) outputs. It also integrates SQLite caching for LLM responses and includes verbose logging capabilities for debugging and cost analysis.",2026-01-03T04:32:25.037159+00:00
"context/generate/8/llm_invoke_python.prompt","The provided text outlines a prompt for an expert Python engineer to create a function named llm_invoke within a file called llm_invoke.py. This function is designed to execute a prompt using a Large Language Model (LLM) while utilizing the Langchain cache. The function takes several inputs, including the prompt string, a JSON input object, a strength float (0-1) for model selection, temperature, a verbose flag, and an optional Pydantic output schema. It returns the LLM result, the cost of the run, and the name of the model used.

The prompt specifies complex logic for model selection based on the strength parameter: values below 0.5 interpolate based on cost relative to a base model (defaulting to gpt-4o-mini or an environment variable), while values above 0.5 interpolate based on ELO score. It also details critical implementation rules, such as handling structured outputs via `.with_structured_output()`, managing token limits specific to Google and OpenAI models, and setting specific kwargs for reasoning models. Finally, it mandates verbose logging requirements, including printing model details, token costs, and inputs/outputs using rich printing where possible.",2026-01-03T04:32:39.258599+00:00
"context/generate_output_paths_example.py","This file is a Python script designed to demonstrate and test the functionality of a module named `generate_output_paths`. The script begins by dynamically adjusting the system path to locate and import the `generate_output_paths` function from a parent directory, ensuring portability across different file structures. It then proceeds to execute a series of eight distinct scenarios to validate the function's behavior under various conditions.

The scenarios cover a wide range of use cases, including generating default paths based on basenames and extensions, handling user-specified filenames versus directories, and integrating environment variables to override defaults. Specific tests check the function's response to different commands like 'generate', 'fix', and 'preprocess', verifying that output keys and file extensions are handled correctly. The script also includes edge cases, such as handling unknown commands or missing basenames, where it expects empty results. Throughout the execution, the script prints the inputs, the actual results returned by the function, and the expected absolute paths, serving as both a usage example and a verification tool for the path generation logic.",2026-01-03T04:32:47.003938+00:00
"context/generate_test_example.py","This Python script demonstrates how to utilize the `generate_test` function from the `pdd` library to automatically create unit tests for a given piece of code. The script begins by importing necessary modules, including `os`, the `generate_test` function itself, and the `rich` library for enhanced console output. It sets up several key input parameters required for test generation: a descriptive prompt asking for a factorial function, the actual Python implementation of that factorial function, a `strength` parameter set to 0.5, a `temperature` of 0.0 for deterministic output, and the target language specified as python. The core logic is wrapped in a try-except block to handle potential errors gracefully. Inside this block, the script calls `generate_test` with the defined parameters. Upon success, it uses `rich` to print the generated unit test code in green, followed by the total cost of the operation and the name of the AI model used. If an exception occurs during the process, it catches the error and prints a formatted error message in red.",2026-01-03T04:32:58.769475+00:00
"context/get_comment_example.py","The provided file offers documentation and usage examples for the `get_comment` function within the `pdd.get_comment` module. It includes a Python code snippet demonstrating how to import the function and call it with various programming language names, such as 'Python', 'Java', and 'JavaScript', to retrieve their respective comment syntax. The example also shows the function returning 'del' for an unknown language. The documentation section details the function's input parameter, `language`, noting it is a case-insensitive string. It explains that the output is a string representing the comment characters for the specified language, or 'del' if an error occurs or the language is not found. Additionally, the notes emphasize the necessity of setting the `PDD_PATH` environment variable to point to the directory containing a `data/language_format.csv` file, which serves as the data source mapping languages to their comment styles.",2026-01-03T04:33:11.377024+00:00
"context/get_extension_example.py","This file demonstrates the usage of the `get_extension` function imported from the `pdd.get_extension` module. It includes three example print statements that call this function with different string arguments representing programming languages or file types: Python, Makefile, and JavaScript. The comments accompanying each call illustrate the expected return values, showing that the function maps language names to their corresponding file extensions (e.g., .py for Python, .js for JavaScript) or preserves the name if it is a specific filename like Makefile.",2026-01-03T04:33:22.660825+00:00
"context/get_jwt_token_example.py","This Python script serves as a command-line interface (CLI) utility for authenticating a user via GitHub Device Flow to obtain a Firebase ID token. It is designed to support multiple environments (local, staging, production) by dynamically loading the appropriate Firebase API key and GitHub Client ID based on the `PDD_ENV` environment variable. The script defines a helper function, `_load_firebase_api_key`, to locate credentials in environment-specific files if they are not already set in the system environment. The main asynchronous execution flow attempts to retrieve a valid JWT using the imported `get_jwt_token` function. It includes comprehensive error handling for various authentication failures, such as user cancellation, network issues, or rate limits. Upon successful authentication, the script updates a local `.env` file with the retrieved token, saving it under both a generic `JWT_TOKEN` key and an environment-specific key (e.g., `JWT_TOKEN_STAGING`) to facilitate subsequent authenticated requests.",2026-01-03T04:33:32.823460+00:00
"context/get_language_example.py","This file serves as a demonstration script for the `pdd` package, specifically showcasing the functionality of the `get_language` module. The script begins by importing the `get_language` function from `pdd.get_language`. It defines a `main` function which acts as the entry point for execution. Inside this function, a sample file extension (`.py`) is defined to simulate a typical use case. The script then attempts to retrieve the programming language associated with this extension by calling `get_language`. It includes basic error handling using a try-except block to catch and print any exceptions that might occur during the process. If the function successfully returns a language, it prints a formatted message confirming the association between the extension and the language; otherwise, it informs the user that no language was found. Finally, the standard `if __name__ == __main__:` block ensures that the `main` function is executed only when the script is run directly, making it a standalone example of how to utilize the library's language detection capabilities.",2026-01-03T04:33:38.848856+00:00
"context/get_run_command_example.py","This Python script serves as a demonstration and documentation for the `pdd.get_run_command` module. It illustrates how to programmatically retrieve execution commands for various programming languages based on file extensions. The script relies on a CSV configuration file (located via the `PDD_PATH` environment variable) that maps extensions like `.py` or `.js` to their respective run command templates (e.g., `python {file}`).

The `main` function walks through six distinct examples showing the module's capabilities. These examples cover: retrieving a basic command template for a Python file; demonstrating input normalization where the module handles missing dots or uppercase extensions; generating a complete, executable command string for a specific file path; handling unknown extensions gracefully by returning empty strings; managing files without extensions; and iterating through a list of common language extensions to display their configured run commands. Overall, the file acts as a usage guide for developers integrating the `get_run_command` and `get_run_command_for_file` functions into their workflows.",2026-01-03T04:33:51.359224+00:00
"context/get_test_command_example.py","This Python script serves as a demonstration and documentation for the `get_test_command` module, specifically illustrating how to use the `get_test_command_for_file()` function. The script showcases a layered resolution strategy for determining test commands: checking a CSV configuration, using smart detection, or returning `None` to signal the need for an agentic fallback. It includes five distinct functions that demonstrate different use cases: resolving commands for Python files (typically pytest), resolving for JavaScript files (npm-based), overriding automatic detection by manually specifying a language (e.g., TypeScript), batch processing multiple files with a results table, and handling scenarios where no command is found (unknown file extensions). The `main` function orchestrates these examples, using the `rich` library to provide formatted, color-coded console output to explain the behavior of the underlying module.",2026-01-03T04:33:57.649600+00:00
"context/git_update_example.py","This Python script serves as a demonstration and entry point for utilizing the `git_update` function from the `pdd` package. The script's primary purpose is to showcase how to programmatically update a prompt file based on modifications made to a corresponding code file, leveraging Git to manage file states during the process.

The `main` function orchestrates this workflow by defining key parameters such as the prompt name (`fix_error_loop`), the path to the prompt file, and the target code file (`pdd/fix_error_loop.py`). It reads the initial prompt content and sets configuration options like `strength` and `temperature` for the underlying Large Language Model (LLM). The core logic invokes `git_update` with specific flags: `simple=False` to enable advanced agentic updates if available, and `verbose=True` for detailed logging. The script handles the result by printing the modified prompt, the total cost of the operation, and the model used. Finally, it includes error handling for value errors and unexpected exceptions, and ensures the updated prompt is saved back to the file system if a result is returned.",2026-01-03T04:34:03.937915+00:00
"context/increase_tests_example.py","This file serves as a demonstration script for the `pdd` library, specifically showcasing the functionality of the `increase_tests` module. The script defines a main function, `example_usage`, which illustrates how to programmatically generate additional unit tests for existing code to improve test coverage. It sets up a mock scenario involving a simple Python function for calculating averages, a basic existing unit test, and a sample coverage report indicating partial coverage. The script then calls `increase_tests` twice: first with basic parameters to demonstrate default behavior, and second with advanced parameters such as explicit language definition, strength settings, temperature control, and verbose logging. It captures and prints the outputs, including the newly generated test code, the associated cost, and the model name used. Additionally, the script includes error handling blocks to catch and display `ValueError` for input validation issues and generic exceptions for unexpected runtime errors. The file concludes by executing the `example_usage` function if run as the main program.",2026-01-03T04:34:17.049641+00:00
"context/incremental_code_generator_example.py","This Python script serves as a demonstration and usage example for the `incremental_code_generator` function from the `pdd` package. The script defines a `main` function that sets up a scenario for updating existing code based on a modified prompt. Specifically, it attempts to update a simple Python factorial function to include input validation, transitioning from an original prompt to a new prompt.

The script configures various parameters for the generation process, including the source language (Python), model strength, temperature, time budget, and flags for verbose output and prompt preprocessing. It calls `incremental_code_generator` with these inputs to determine whether the code can be patched incrementally or requires full regeneration. Finally, it uses the `rich` library to display the results in a formatted console output, showing the updated code (if patched), the success status of the incremental update, the total cost incurred, and the specific model used for the operation.",2026-01-03T04:34:23.397683+00:00
"context/insert/1/dependencies.prompt","The provided file serves as a concise documentation guide for utilizing internal modules within a specific development context. It outlines two primary functionalities: loading prompt templates and executing prompts using a function named 'llm_invoke'. The document is structured using XML-like tags to organize these instructions clearly. Specifically, it directs users to reference external example files for implementation details. For loading prompt templates, it points to 'context/load_prompt_template_example.py', and for running prompts via 'llm_invoke', it refers to 'context/llm_invoke_example.py'. Essentially, this file acts as an index or a quick-start reference, linking developers to practical code examples that demonstrate how to interact with the system's prompt management and Large Language Model invocation capabilities.",2026-01-03T04:34:35.939688+00:00
"context/insert/1/prompt_to_update.prompt","This document outlines the specifications for a Python function named `postprocess`, designed to extract code blocks from a mixed text and code string generated by a Large Language Model (LLM). The function is intended to be written by an expert Python Software Engineer and utilizes the `rich` library for pretty-printing console output. It accepts inputs including the raw `llm_output`, the target programming `language`, a `strength` parameter (defaulting to 0.9), `temperature` (defaulting to 0), and a `verbose` flag. The function returns a tuple containing the cleaned `extracted_code`, the `total_cost` of the operation, and the `model_name` used. The processing logic involves a conditional check: if `strength` is 0, it delegates to a `postprocess_0` function. Otherwise, it loads a prompt template (`extract_code_LLM.prompt`) and invokes an LLM using `llm_invoke` to parse the output. The resulting code is cleaned by removing markdown code fences (triple backticks) and language identifiers before being returned along with cost and model metadata.",2026-01-03T04:34:41.541692+00:00
"context/insert/1/updated_prompt.prompt","This document outlines the specifications for a Python function named postprocess, designed to extract code blocks from a mixed-text string output generated by a Large Language Model (LLM). Acting as an expert Python Software Engineer, the developer is instructed to utilize the rich library for console output. The function accepts inputs including the raw LLM output string, the target programming language, and configuration parameters like model strength, temperature, and verbosity. It returns a tuple containing the cleaned code string, the total operation cost, and the model name used. The logic flow involves a conditional check where a strength of 0 triggers a simpler postprocess_0 function. Otherwise, the process involves loading a specific prompt template (extract_code_LLM.prompt) and invoking an LLM via an internal llm_invoke module to parse the text. The resulting code is then cleaned by stripping markdown-style triple backticks and language identifiers before being returned. The document also includes references to internal modules for loading templates and invoking LLMs, providing context for implementation.",2026-01-03T04:34:53.935323+00:00
"context/insert/2/dependencies.prompt","The provided file content appears to be a documentation snippet or a configuration block illustrating how to utilize internal modules within a larger system. Specifically, it showcases an example of how to select a Langchain Large Language Model (LLM) and count tokens using a tool called 'llm_selector'. The content is structured with XML-like tags, enclosing a reference to an external Python script located at './context/llm_selector_example.py' via an include directive. This suggests the file serves as a guide or a template for developers on integrating specific internal utilities for LLM operations.",2026-01-03T04:35:06.344484+00:00
"context/insert/2/prompt_to_update.prompt","This file outlines the specifications for an expert Python engineer to create a function named conflicts_in_prompts. The purpose of this function is to analyze two input prompts, identify any conflicts between them, and suggest resolutions. The function takes two prompt strings, a model strength float (default 0.5), and a temperature float (default 0) as inputs. It returns a list of recommended changes in JSON format, the total cost of the operation, and the name of the LLM model used. The implementation requires using LangChain Expression Language (LCEL) and involves several steps: loading specific prompt templates from a project path, selecting an LLM model based on strength, calculating and printing token usage and costs, and running the prompts through a two-step LLM process. The first step generates a markdown analysis of conflicts, and the second step extracts a structured JSON list of changes from that analysis.",2026-01-03T04:35:11.633981+00:00
"context/insert/2/updated_prompt.prompt","This file contains a prompt specification for an expert Python engineer to create a function named conflicts_in_prompts. The purpose of this function is to analyze two input prompts for conflicts and suggest resolutions using a Large Language Model (LLM). The function takes two prompt strings, a strength parameter, and a temperature parameter as inputs. It is expected to output a list of recommended changes in JSON format, the total cost of the operation, and the name of the model used. The implementation details require using LangChain Expression Language (LCEL) and specific internal modules for LLM selection and token counting. The process involves a multi-step workflow: loading prompt templates from environment paths, running an initial conflict analysis using an LLM, printing cost and token usage, and then running a second extraction step to parse the analysis into a structured JSON list of changes.",2026-01-03T04:35:23.925783+00:00
"context/insert_includes_example.py","This Python script serves as a demonstration of how to use the `insert_includes` function from the `pdd.insert_includes` module. The script defines a single function, `example_usage`, which orchestrates the process of handling project dependencies and modifying prompts. It begins by setting up necessary inputs, including a sample prompt requesting a Python function for CSV processing, a directory path pattern for context files, and a filename for dependency tracking. The core of the script executes the `insert_includes` function with specific parameters such as a high strength value (0.93) and zero temperature to ensure focused and consistent results. It captures the returned values, which include the modified prompt, CSV output data, processing cost, and the model name used. The script utilizes the `rich` library to provide formatted, color-coded console output, displaying the success status, original and modified prompts, and cost metrics. Finally, it handles potential errors like missing files or general exceptions and saves the resulting dependency data to a CSV file.",2026-01-03T04:35:35.010900+00:00
"context/install_completion_example.py","This Python script serves as an example demonstration for using the shell completion installation module of the `pdd` package. It showcases two primary functions: `get_local_pdd_path()`, which retrieves the absolute path to the PDD directory, and `install_completion()`, which automates the setup of shell completion for the PDD CLI. To ensure safety and prevent modification of the user's actual configuration, the script includes a `setup_example_environment()` function. This function mocks the necessary environment variables (forcing the shell to `/bin/bash` and setting dummy `HOME` and `PDD_PATH` directories) and creates dummy files, including a fake completion script and a shell RC file. The `main()` function orchestrates the process by initializing this test environment, printing the detected PDD path, and executing the installation routine, with feedback provided via the `rich` library.",2026-01-03T04:35:48.272082+00:00
"context/langchain_lcel_example.py","This Python script serves as a comprehensive demonstration and testing ground for integrating various Large Language Models (LLMs) using the LangChain framework. It begins by importing necessary components from core LangChain libraries and specific provider packages, including OpenAI, Google Vertex/Generative AI, Anthropic, Groq, Together, AWS Bedrock, and local options like Ollama and MLX. A key feature of the script is the implementation of a custom CompletionStatusHandler callback, designed to monitor generation status and extract token usage metrics. The code establishes an SQLite cache to optimize performance. The script systematically executes examples across different providers. It showcases standard text generation using Google's Gemini and Vertex AI models. A significant portion focuses on structured output, utilizing Pydantic models to force LLMs (specifically OpenAI's GPT-4o and O1, and DeepSeek via the OpenAI client) to return JSON-formatted data. Furthermore, the script explores advanced LangChain functionalities, such as configuring model fallbacks, using RunnablePassthrough, and setting up configurable fields. It concludes with examples of invoking models via Groq, Together AI, and running local models on Apple Silicon via MLX and Ollama.",2026-01-03T04:35:54.071180+00:00
"context/litellm_bedrock_sonnet.py","This Python code snippet demonstrates how to utilize the `litellm` library to interface with AWS Bedrock's AI models. Specifically, it imports the `completion` function from `litellm` to send a request to the Anthropic Claude 3.7 Sonnet model (identified as `bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0`). The code includes commented-out lines for setting necessary AWS environment variables, such as the access key ID, secret access key, and region name, which are required for authentication. The core functionality is shown in the `completion` call, where a user message asking What is the capital of France? is sent to the model. Additionally, a `reasoning_effort` parameter is set to low. Finally, the script prints the response received from the model to the console.",2026-01-03T04:36:17.895029+00:00
"context/llm_invoke_example.py","This Python script serves as a demonstration and utility for invoking Large Language Models (LLMs) with varying capabilities based on a 'strength' parameter. It imports core functionality from a local `pdd.llm_invoke` module, specifically utilizing `llm_invoke`, model data loading, and candidate selection functions. The script defines a Pydantic model named `Joke` to illustrate structured output generation. A key function, `calculate_model_ranges`, dynamically determines the effective strength ranges (from 0.0 to 1.0) for different models by sampling values against a CSV dataset of model capabilities. The `main` function orchestrates the execution flow: it first calculates and prints these model strength ranges. Then, it iterates through each identified model range, executing `llm_invoke` at the midpoint strength value. For each model, it performs two tests: generating an unstructured joke about programmers and generating a structured joke about data scientists that conforms to the `Joke` Pydantic schema. The script outputs the results, costs, and specific model names used for each invocation, providing a comprehensive test of the system's dynamic model selection and structured output handling.",2026-01-03T04:36:29.686784+00:00
"context/llm_output_fragment.txt","The provided file, `pdd.py`, serves as the entry point for a Prompt-Driven Development (PDD) command-line tool built using the `click` library. It establishes a CLI architecture that supports command chaining and global configuration options for AI model behavior, such as `strength`, `temperature`, and output verbosity.

The script defines a series of subcommands that orchestrate various AI-driven coding tasks. Key commands include `generate` for creating code from prompts, `example` for deriving examples from code-prompt pairs, and `test` for automatically generating unit tests. Additionally, it includes utility commands like `preprocess` for formatting prompts (with optional XML tagging), `split` for decomposing complex prompts, and `fix`, which offers both single-pass and iterative loops to resolve errors in code and tests.

Internally, the script relies on a helper function `construct_paths` to manage file I/O and imports specific logic modules (e.g., `code_generator`, `fix_error_loop`) to execute the core functionality. It uses the `rich` library to provide formatted console output, including status updates and API cost tracking for each operation.",2026-01-03T04:36:42.457276+00:00
"context/llm_selector_example.py","This Python script serves as a demonstration and test harness for the `llm_selector` function, which is imported from the `pdd.llm_selector` module. The script defines a `main` function that iterates through a range of `strength` values, starting at 0.5 and incrementing by 0.05 until it exceeds 1.1. Inside this loop, the script calls `llm_selector` with the current `strength` and a fixed `temperature` of 1.0. For each iteration, it retrieves and prints key details about the selected Large Language Model (LLM), including the model name, input cost per million tokens, and output cost per million tokens. Additionally, the script demonstrates the utility of the returned `token_counter` function by calculating and printing the token count for a sample string. The execution is wrapped in a try-except block to handle potential `FileNotFoundError` or `ValueError` exceptions gracefully. Finally, the standard `if __name__ == __main__:` block ensures the `main` function runs when the script is executed directly.",2026-01-03T04:37:18.099782+00:00
"context/load_prompt_template_example.py","This Python script demonstrates the usage of a custom function to load and display a prompt template. It imports the `load_prompt_template` function from the `pdd.load_prompt_template` module and the `print` function from the `rich` library for formatted output. The script defines a `main` function where it specifies a target prompt name, generate_test_LLM. It then attempts to load this template using the imported utility. If the prompt is successfully loaded, the script prints a blue-styled header followed by the content of the prompt template to the console. Finally, the script includes a standard conditional block to execute the `main` function when the file is run directly.",2026-01-03T04:37:30.236634+00:00
"context/logo_animation_example.py","This Python script serves as a demonstration for utilizing the PDD branding animation module. It imports the `start_logo_animation` and `stop_logo_animation` functions from the `pdd.logo_animation` package. The `main` function orchestrates the demo by first initiating the animation, which runs in a background thread and takes over the terminal screen. The script calculates an appropriate duration to let the animation play out fully—accounting for formation, holding, and transition phases—setting a wait time of approximately 7 seconds. During this interval, the main program simulates ongoing work using a loop with `time.sleep`, although output from this phase is noted to be obscured by the full-screen animation. Finally, the script calls `stop_logo_animation` to cease the visual effects and restore standard terminal control, printing confirmation messages to indicate that the main program has resumed normal operation. The code includes comments explaining the timing logic and the behavior of terminal output during the animation sequence.",2026-01-03T04:37:40.992282+00:00
"context/no_include_conflicts_in_prompts_python.prompt","This file outlines the specifications for an expert Python engineer to create a function named conflicts_in_prompts. The purpose of this function is to analyze two input prompts to identify logical or instructional conflicts between them and provide suggestions for resolution. The function takes two prompt strings, a model strength parameter (defaulting to 0.5), and a temperature setting (defaulting to 0) as inputs. It is required to return a list of conflict dictionaries—each detailing the conflict description, explanation, and specific modification instructions for both prompts—along with the total cost and model name used.

The implementation steps involve using Langchain LCEL templates. First, it loads specific prompt files from a project path defined by an environment variable. It then utilizes a custom `llm_selector` to choose a model and run the initial comparison. A second step involves a stronger model (0.8 strength) to extract the conflict data into a structured JSON format. Throughout the process, the function must calculate and pretty-print token usage and costs before returning the final structured data.",2026-01-03T04:37:53.110747+00:00
"context/o4-mini-test.ipynb","The provided file is a Jupyter Notebook containing a single Python code cell that demonstrates how to interact with the Azure OpenAI API. The script initializes an `AzureOpenAI` client using specific configuration details, including an endpoint URL, a model deployment named o4-mini, an API version of 2024-12-01-preview, and a subscription key. It constructs a chat completion request with a conversation history where a user asks for travel recommendations in Paris, the assistant suggests the Eiffel Tower, the Louvre, and Notre-Dame, and the user follows up by asking what makes the Eiffel Tower special. The code then sends this request to the API and prints the response. The notebook's output displays the generated text, which details the Eiffel Tower's significance as an architectural marvel, an iconic symbol of France, a provider of spectacular views, a day-to-night attraction with sparkling lights, and a cultural hub for events.",2026-01-03T04:38:06.680914+00:00
"context/pdd_discussiion.txt","This transcript documents a conversation between a developer and ChatGPT regarding the concept and methodology of Prompt-Driven Development (PDD). The developer argues that traditional coding, where code is the primary artifact, leads to high maintenance costs and spaghetti code due to constant patching. In contrast, PDD proposes that the prompt should be the main artifact. By maintaining high-level prompts and regenerating code, examples, and tests from them, developers can reduce technical debt, improve modularity, and facilitate easier refactoring.

The discussion covers the PDD workflow, which involves creating a prompt, generating code, creating usage examples, and generating unit tests, all while keeping these four components synchronized. The developer highlights the benefits of PDD, such as token efficiency, better collaboration through readable prompts, and the ability to use batch processing for cost savings. They also introduce tools like the PDD-CLI and PDD Cloud for sharing few-shot examples. Potential challenges, such as the learning curve for writing effective prompts and managing dependencies between modules, are discussed, along with solutions like comprehensive testing and automated back-propagation of changes to keep documentation and prompts in sync.",2026-01-03T04:38:12.807370+00:00
"context/postprocess_0_example.py","This file provides a concise usage example and documentation for the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The example demonstrates how to import and utilize the function within a Python script. It sets up a mock `llm_output` string containing mixed text and code blocks in different languages (Python and Java) to simulate a Large Language Model's response. The script then calls `postprocess_0` with this output and a target language (e.g., python). The accompanying documentation explains that the function's purpose is to process the input string by isolating the largest code block of the specified language, leaving it uncommented, while commenting out all other text and code sections using language-appropriate syntax. It lists the input parameters (`llm_output` and `language`) and describes the expected return value. Finally, a note advises that dependency functions like `get_comment`, `comment_line`, and `find_section` must be available for the code to execute successfully.",2026-01-03T04:38:26.450240+00:00
"context/postprocess_example.py","This Python script serves as a demonstration for the `postprocess` function within the `pdd.postprocess` module, illustrating how to extract code blocks from Large Language Model (LLM) text outputs. The script showcases two distinct extraction scenarios. The first scenario uses a strength of 0, triggering a simple, cost-free method that relies on basic string manipulation to parse content between triple backticks. The second scenario uses a strength greater than 0, simulating an advanced, LLM-based extraction method capable of more robust parsing but incurring computational costs.

To function without external dependencies or API costs, the script utilizes Python's `unittest.mock` library to patch internal functions (`load_prompt_template` and `llm_invoke`). This allows the example to simulate the behavior of an LLM extractor—including returning Pydantic models and cost metrics—without making actual network calls. The script uses the `rich` library for formatted console output, guiding the user through the inputs, configuration parameters (like temperature and time), and the resulting extracted code and associated costs for both scenarios.",2026-01-03T04:38:39.255135+00:00
"context/postprocessed_runnable_llm_output.py","The provided text outlines the implementation of a Python function named `context_generator`. This function is designed to automate the creation of usage examples for Python modules. The process involves several key steps: reading a specified Python source file, preprocessing its content (though the specific preprocessing logic is abstracted in the snippet), and constructing a prompt for an AI model (specifically GPT-4). This prompt instructs the AI to act as an expert Python engineer and generate a concise example of how to properly use the code provided. The implementation includes error handling for file operations, specifically checking if the input file exists. Additionally, the text provides an explanation of the code's functionality, detailing the file reading and preprocessing stages, and offers a usage example demonstrating how to call the function with input and output filenames. The snippet appears to be part of a larger documentation or tutorial on generating code documentation or context using AI.",2026-01-03T04:39:03.023464+00:00
"context/preprocess_example.py","This Python script demonstrates the usage of a `preprocess` function from the `pdd.preprocess` module, utilizing the `rich` library for console output. The script initializes a `Console` object and defines a multi-line string variable named `prompt` containing XML-like tags (e.g., `<shell>`, `<pdd>`, `<web>`), placeholders (`{test}`, `{test2}`), and a markdown code block. The script sets configuration variables: `recursive` is set to `False`, `double_curly_brackets` is set to `True`, and a list `exclude_keys` is defined containing the string test2. It prints debug information regarding the excluded keys using formatted console output. The core functionality involves calling `preprocess` with the defined prompt and configuration arguments. Finally, the script prints a header followed by the result of the processed prompt to the console, showcasing how the preprocessing logic handles the input string and specific formatting rules.",2026-01-03T04:39:08.695652+00:00
"context/preprocess_main_example.py","This file implements a command-line interface (CLI) using the `click` library to facilitate the preprocessing of prompt files. It serves as a wrapper around a core function named `preprocess_main`, imported from the `pdd.preprocess_main` module. The CLI command, defined as `cli`, accepts several options to customize the preprocessing behavior. Key arguments include `--prompt-file` (required path to the input file), `--output` (optional path for saving results), and flags like `--xml` (to insert XML delimiters), `--recursive` (to process referenced files), and `--double` (to double curly brackets). Users can also exclude specific keys from bracket doubling using the `--exclude` option and enable verbose logging with `--verbose`. Inside the command execution, the script initializes a context object with default settings for strength and temperature, then invokes `preprocess_main` with the provided parameters. Upon successful execution, it prints the processed prompt content, the total cost of the operation, and the model name used. The script includes error handling to catch and display exceptions that occur during the process.",2026-01-03T04:39:21.791304+00:00
"context/process_csv_change_example.py","The file `run_demo.py` serves as a concise usage example for the `process_csv_change` function from the `pdd.process_csv_change` module. It demonstrates how to programmatically set up a workspace and execute a batch processing task driven by a CSV file.

The script first establishes a temporary directory structure, creating a sample Python source file (`factorial.py`) and a corresponding prompt file (`factorial_python.prompt`). It then generates a CSV file (`tasks.csv`) containing instructions to modify the prompt file. The core of the script invokes `process_csv_change` with specific parameters such as edit strength, temperature, code directory, language, and a budget limit.

Finally, the script captures and prints the results of the operation, including a success flag, the total cost incurred, the model name used (e.g., `gpt-5-nano`), and a list of JSON objects detailing the modifications made. The output shows that the function successfully updated the prompt file by appending an example section as requested in the CSV instructions.",2026-01-03T04:39:33.780895+00:00
"context/prompt_caching.ipynb","This file is a Jupyter Notebook titled Prompt caching through the Anthropic API. It serves as a technical cookbook demonstrating how to utilize prompt caching features to reduce latency and costs when using the Claude 3.5 Sonnet model. The notebook begins by setting up the environment and fetching the full text of Jane Austen's Pride and Prejudice from Project Gutenberg to serve as a large context document (approximately 187,000 tokens).

The guide presents two main examples. The first example compares a single-turn API call without caching against one with caching enabled. It highlights a significant performance improvement, showing a reduction in processing time from over 21 seconds to under 4 seconds. The second example illustrates a multi-turn conversation simulation. It defines a `ConversationHistory` class to manage dialogue turns and dynamically applies cache breakpoints to the last two user messages. This section demonstrates incremental caching, where subsequent turns achieve nearly 100% cache hit rates, drastically lowering input processing time while maintaining response quality. The notebook concludes by emphasizing the efficiency gains, noting that response times dropped from ~24 seconds to ~8-9 seconds after the initial cache setup.",2026-01-03T04:39:46.362493+00:00
"context/pytest_example.py","The provided Python script defines a custom test runner mechanism using the `pytest` framework, specifically designed to programmatically execute tests and capture detailed results. At its core is the `TestResultCollector` class, which acts as a plugin to intercept pytest hooks. This class tracks the number of test failures, errors, and warnings by monitoring the `pytest_runtest_logreport` and `pytest_sessionfinish` events. Additionally, it includes methods to redirect standard output and standard error streams into an in-memory string buffer, allowing for the capture of all console logs generated during the test session.

The script includes a `run_pytest` function that instantiates the collector, initiates log capturing, and executes `pytest.main` specifically targeting the `tests/test_get_extension.py` file with verbose output. It ensures that system output streams are reset in a `finally` block to prevent interference with the main process. The script's entry point prints the current working directory and debugging information before invoking the test runner and displaying the final counts of failures, errors, and warnings, along with the full captured logs, to the console.",2026-01-03T04:40:00.090945+00:00
"context/pytest_output_example.py","This Python script serves as a comprehensive demonstration and usage guide for a module named `pdd.pytest_output`. It illustrates how to programmatically run pytest, capture its output, and handle various testing scenarios. The script begins by creating a dummy Python test file containing passing, failing, error-raising, and warning-triggering tests. It then proceeds through six distinct examples: invoking the module's main function via simulated command-line arguments, using the `run_pytest_and_capture_output` function to get results and save them to JSON, and handling edge cases like non-existent files, non-Python files, and empty test files. Finally, it showcases an advanced usage pattern by directly utilizing the `TestResultCollector` class to manually capture logs and test outcomes (failures, errors, warnings, passed tests). The script uses the `rich` library to format console output for better readability during execution.",2026-01-03T04:40:12.706319+00:00
"context/python_env_detector_example.py","This Python script serves as a demonstration and usage example for the `python_env_detector` module within the `pdd` package. It is designed to showcase the primary capabilities of the module by invoking its key functions to analyze the current Python runtime environment. The script begins by importing four specific functions: `detect_host_python_executable`, `get_environment_info`, `is_in_virtual_environment`, and `get_environment_type`.

Inside the `main` function, the script systematically executes these functions and prints their outputs to the console. First, it identifies and displays the path to the host Python executable. Next, it checks whether the script is currently running inside a virtual environment and prints the boolean result. It then determines and outputs the specific type of the environment (e.g., virtualenv, conda, or system). Finally, the script retrieves a dictionary of detailed environment information using `get_environment_info` and iterates through the key-value pairs to display comprehensive details about the runtime context. The script includes a standard `if __name__ == __main__:` block to ensure the `main` function runs only when the script is executed directly.",2026-01-03T04:40:25.161408+00:00
"context/python_preamble.prompt","The provided text outlines a set of strict coding guidelines and requirements for a Python development task, positioning the recipient as an expert Python engineer. Key constraints include mandatory use of `from __future__ import annotations` at the beginning of files and comprehensive type hinting for all functions. Output handling is restricted to the `rich.console.Console` library for all printing operations. Structurally, the code must be designed as part of a package, utilizing relative imports (e.g., `from .module_name`) for internal dependencies. It specifically references a `./pdd/__init__.py` file that should contain global constants like `EXTRACTION_STRENGTH`, `DEFAULT_STRENGTH`, and `DEFAULT_TIME`. Finally, the instructions emphasize robust error handling, requiring the code to manage edge cases such as missing inputs or model errors gracefully with clear, informative error messages.",2026-01-03T04:40:37.575731+00:00
"context/regression_example.sh","This file is a Bash shell script designed to execute a comprehensive regression test suite for a command-line tool named `pdd`. The script begins by defining various environment variables for paths, filenames, and logging configurations. It includes helper functions for verbose logging to the console and timestamped logging to a file. The core functionality involves creating a staging directory and sequentially running a series of `pdd` commands to test different features, including code generation (`generate`), example creation (`example`), testing (`test`), preprocessing (`preprocess`), updating (`update`), code modification (`change`), bug fixing (`fix`), code splitting (`split`), detection (`detect`), conflict analysis (`conflicts`), and crash handling (`crash`). Throughout the execution, the script logs the success or failure of each command and tracks the associated costs in a CSV file. Finally, it calculates and displays the total cost of the operations performed.",2026-01-03T04:40:47.983030+00:00
"context/render_mermaid_example.py","This Python script serves as a comprehensive usage guide and demonstration for the `render_mermaid` module, specifically designed to visualize software architecture defined in JSON format. The script illustrates how to convert `architecture.json` files into interactive HTML Mermaid diagrams through four distinct examples. It begins by defining a helper function, `create_sample_architecture`, which generates a mock JSON structure representing a full-stack application with backend (FastAPI, SQLAlchemy) and frontend (React) components. The script then walks through usage scenarios: basic command-line execution, programmatic usage where Mermaid code and HTML are generated via function calls, and a real-world example that attempts to locate and process existing architecture files on the disk. Additionally, it highlights customization features such as automatic categorization, priority indicators, and interactive tooltips. The `main` function orchestrates these examples, printing instructional output to the console to guide the user in generating and viewing the resulting HTML diagrams.",2026-01-03T04:41:00.399661+00:00
"context/simple_math.py","The provided file content is extremely brief, consisting of a single line of text that reads '# Intentionally left blank for pdd generate'. This line appears to be a comment or a placeholder marker within a code file or configuration script. The use of the hash symbol (#) typically denotes a comment in many programming languages (such as Python, Shell, or YAML) or configuration formats. The phrase 'Intentionally left blank' is a standard convention used to indicate that a section of a document or file is empty on purpose, rather than due to an error or omission. The specific mention of 'pdd generate' suggests that this file is likely a template or a target for an automated process involving a tool or script named 'pdd'. It implies that content will be programmatically generated and inserted into this file at a later stage. Consequently, the file currently holds no functional logic or data beyond this placeholder instruction.",2026-01-03T04:41:06.921204+00:00
"context/split/1/final_pdd_python.prompt","The document outlines the specifications for a Python command-line tool named pdd. This utility is designed to compile prompt files into runnable code or generate example code files from existing source code. The tool utilizes the Python `rich` library for formatted console output. Input files follow a specific naming convention (e.g., `basename_language.prompt`), which the program parses to determine the target language and appropriate file extensions.

The CLI supports flexible output options, allowing users to specify filenames, paths, or rely on defaults that save files relative to the input location. Key features include a `--force` flag to overwrite existing files without confirmation, and specific flags (`-o` and `-oe`) to control the destination of runnable code and generated examples respectively. The program logic involves reading the input, determining file paths based on user arguments, and executing a multi-step process: generating code from prompts via a `code_generator` module, and creating examples via a `context_generator` module. It handles both prompt-to-code and code-to-example workflows seamlessly.",2026-01-03T04:41:13.394555+00:00
"context/split/1/initial_pdd_python.prompt","The document outlines the specifications for a Python command-line tool named pdd. This utility is designed to compile prompt files (formatted as 'basename_language.prompt') into runnable code files, or to generate example code files from existing source code. The program utilizes the Python 'rich' library for pretty-printed console output. Key features include flexible output handling, allowing users to specify filenames, paths, or rely on defaults derived from the input basename. It supports options like '--force' to overwrite existing files without confirmation, '-o' to define the output path for runnable code, and '-oe' to trigger and direct the generation of example code. The workflow involves parsing the input filename to determine the language and basename, resolving output paths based on user arguments, and then invoking specific generators ('code_generator' for prompts and 'context_generator' for examples) to produce the final files.",2026-01-03T04:41:25.452740+00:00
"context/split/1/pdd.py","The provided file contains the Python implementation for a command-line interface (CLI) tool named `pdd`. This program is designed to automate the generation of runnable code from prompt files and subsequently create example usage code. It leverages the `argparse` library for parsing command-line arguments and the `rich` library for enhanced console output and user interaction, such as confirmation prompts.

The core functionality revolves around the `main` function, which determines whether the input is a prompt file or a code file. It calculates appropriate output paths and file extensions based on the input language. The script integrates with two external modules, `code_generator` and `context_generator`, to perform the actual content creation. Specifically, it generates runnable code files (e.g., Python, Bash, Makefile) from `.prompt` files and can also generate example Python scripts demonstrating how to use that code. The tool includes safety features like overwrite confirmation, which can be bypassed using a `--force` flag. Usage instructions and dependency requirements (including `rich`, `langchain`, and `tiktoken`) are provided in the comments.",2026-01-03T04:41:37.513097+00:00
"context/split/1/split_get_extension.py","The provided file content consists of a single line of code, likely written in a high-level programming language such as Python. This line performs an assignment operation where a variable named file_extension is being set to the return value of a function call. Specifically, the function get_extension is invoked with a single argument, language. The purpose of this code snippet appears to be determining the appropriate file extension associated with a specific programming language. For example, if the variable language held the value Python, the function would likely return .py. This operation is common in software development tools, such as code editors, linters, or build scripts, where handling files dynamically based on their language type is necessary. The brevity of the snippet suggests it is part of a larger logic block responsible for file management or configuration settings.",2026-01-03T04:41:49.861898+00:00
"context/split/1/sub_pdd_python.prompt","The provided text outlines a task for an expert Python engineer to create a function named get_extension. This function is designed to accept a programming language name as a string input (e.g., Bash, Python) and return the corresponding file extension associated with that language. The logic for the function involves three primary steps: first, converting the input language string to lowercase to ensure case-insensitive comparison; second, performing a lookup to identify the correct file extension for the specified language; and third, returning the identified extension as a string. The prompt clearly defines the expected inputs and outputs, specifying that the goal is to automate the retrieval of file extensions based on language names.",2026-01-03T04:42:01.763644+00:00
"context/split/2/consolidated.prompt","The file contains a prompt engineering task designed for an expert LLM Prompt Engineer. The primary objective is to split a complex `input_prompt` into two distinct components: a `sub_prompt` and a `modified_prompt`, ensuring no loss of functionality. The context involves a Python command-line program named pdd that compiles prompts into code files or generates example code using the `rich` library for output.

The file provides specific inputs: the original `input_prompt` describing the pdd program's requirements (file handling, output paths, and execution logic), the resulting `input_code` (a Python script implementing pdd), and `example_code` showing how a specific function (`construct_output_paths`) should be used. It also includes a complete example of how to perform this split, demonstrating the extraction of a helper function (`get_extension`) into a sub-prompt and modifying the main prompt to utilize it. The user is instructed to follow this pattern to extract a logic block related to constructing output paths into a new sub-prompt and update the main prompt accordingly.",2026-01-03T04:42:12.638802+00:00
"context/split/2/final_pdd_python.prompt","The file outlines the specifications for a Python command-line utility named pdd. This tool is designed to streamline development workflows by compiling prompt files into runnable code or generating example usage files from existing code. The program utilizes the Python `rich` library for formatted console output.

Input files follow specific naming conventions (e.g., `basename_language.prompt`), which the tool parses to determine the target language and file extensions. Key features include a `--force` flag to overwrite existing files without confirmation, and options (`-o`, `-oe`) to specify custom output paths for runnable code and example files respectively.

The utility operates through a multi-step process: reading input filenames, determining extensions, constructing output paths, and then executing generation logic. If a `.prompt` file is provided, pdd generates the corresponding code using a `code_generator`. If a standard code file is provided or the example flag is set, it uses a `context_generator` to create an example file. The document includes references to helper scripts for tasks like looking up extensions and constructing paths.",2026-01-03T04:42:19.072554+00:00
"context/split/2/initial_pdd_python.prompt","The document outlines the specifications for a Python command-line tool named pdd. This utility is designed to compile prompt files into runnable code or generate example code files from existing source code. The tool utilizes the Python `rich` library for formatted console output. Input files follow a specific naming convention (e.g., `basename_language.prompt`), which the program parses to determine the target language and appropriate file extensions.

The CLI supports flexible output options, allowing users to specify filenames, paths, or rely on defaults that save files relative to the input location. Key features include a `--force` flag to overwrite existing files without confirmation, and specific flags (`-o` and `-oe`) to control the destination of runnable code and generated examples respectively. The program logic involves reading the input, determining file paths based on user arguments, and executing a multi-step process: generating code from prompts via a `code_generator` module, and creating examples via a `context_generator` module. It handles both prompt-to-code and code-to-example workflows seamlessly.",2026-01-03T04:42:31.186003+00:00
"context/split/2/pdd.py","The provided text outlines the implementation of a Python command-line tool named `pdd.py`. This utility is designed to compile prompt files into runnable code or generate example code files from existing source code. It utilizes the `rich` library for enhanced console output and `argparse` for handling command-line arguments such as input files, output paths (`-o`), and example output paths (`-oe`). The script's logic includes automatic file extension handling (defaulting to `.prompt`), language extraction from filenames, and intelligent output path construction. Depending on the input file type, it invokes external modules—`code_generator` to create runnable code from prompts, and `context_generator` to produce example code. Additionally, the script includes safety checks for file overwrites, which can be bypassed using a `--force` flag.",2026-01-03T04:42:43.122072+00:00
"context/split/2/split_pdd_construct_output_path.py","The provided code snippet demonstrates a function call in Python used to determine output file paths. Specifically, it calls a function named `construct_output_paths` which takes four arguments: `basename`, `file_extension`, `argv_output_path`, and `argv_example_output_path`. The `basename` argument represents the core name of the file, while `file_extension` specifies the file type based on the programming language being used. The other two arguments, `argv_output_path` and `argv_example_output_path`, appear to be derived from command-line arguments, specifically flags like `-o` and `-oe`. The function returns two values, `runnable_output_path` and `example_output_path`, which are assigned to variables of the same names. This logic suggests the code is part of a larger script or CLI tool responsible for generating or processing files where distinct paths for a runnable version and an example version need to be established dynamically based on user input and file attributes.",2026-01-03T04:42:56.680387+00:00
"context/split/2/sub_pdd_python.prompt","The provided text outlines the requirements for an expert Python engineer to develop a function named construct_output_paths. This function is designed to generate file paths for runnable code and example outputs within a command-line program called pdd. The function takes four inputs: the file's basename, the file extension corresponding to the programming language, and optional output paths provided via command-line flags ('-o' and '-oe'). It returns a tuple containing the paths for the runnable output and the example output. The prompt details four scenarios for handling output paths: filenames without paths, filenames with paths, paths without filenames, and default behaviors when nothing is specified. It also explains the naming convention for prompt files (e.g., 'basename_language.prompt') and provides several example command-line usages to illustrate expected behaviors. Finally, the text breaks down the implementation into four steps: defining a helper function to handle path construction logic, using this helper to determine the runnable output path, using it again for the example output path, and returning the final tuple.",2026-01-03T04:43:07.979829+00:00
"context/split/3/consolidated_xml_filled_in.prompt","This file contains a prompt engineering task designed for an expert LLM Prompt Engineer. The goal is to take a specific `input_prompt` (which describes a Python function called `postprocess` for cleaning up LLM output) and its corresponding generated code, and split it into two distinct prompts: a `sub_prompt` and a `modified_prompt`. The file provides context through two detailed examples (`example_1` and `example_2`) that demonstrate how to extract a specific functionality (like `get_extension` or `construct_output_paths`) into a sub-prompt while updating the main prompt to utilize that extracted function. The user is expected to apply this pattern to the provided `input_prompt`, which details logic for identifying code blocks within text and commenting out non-code sections, specifically extracting the logic for finding code sections into a sub-prompt.",2026-01-03T04:43:20.276380+00:00
"context/split/3/final_postprocess_python.prompt","The file outlines a task for an expert Python engineer to create a function named `postprocess`. This function is designed to take raw string output from a Large Language Model (LLM), which typically contains a mix of explanatory text and code blocks, and convert it into an executable script. The function accepts two inputs: `llm_output` (the raw string) and `language` (the target programming language, e.g., Python or Bash). The core logic involves identifying the correct comment syntax for the specified language, locating the largest code block matching that language within the LLM's output, and commenting out everything else—including the explanatory text and the markdown backticks surrounding the code block. The result is a clean string where only the primary code section remains active, while all surrounding context is preserved but neutralized as comments, ensuring the file can be run directly without syntax errors.",2026-01-03T04:43:26.564361+00:00
"context/split/3/initial_postprocess_python.prompt","The provided text outlines a prompt for an expert Python engineer to create a function named `postprocess`. This function is designed to sanitize the raw string output from a Large Language Model (LLM) so that the embedded code can be executed directly. The function takes two inputs: `llm_output`, a string containing a mix of natural language text and code blocks delimited by triple backticks, and `language`, a string specifying the target programming language (e.g., 'python', 'bash').

The core logic involves a multi-step process: first, identifying the correct comment syntax for the specified language; second, parsing the input string to locate all top-level code sections while handling nested blocks recursively; third, identifying the largest code block that matches the target language; and finally, commenting out all surrounding text and other code blocks, leaving only the primary code section executable. The prompt includes detailed examples of inputs, expected outputs, and specific algorithmic steps for parsing code blocks and applying comments.",2026-01-03T04:43:38.127287+00:00
"context/split/3/postprocess.py","This Python file defines utility functions for processing text output, specifically targeting the extraction and handling of code blocks within a larger text, such as output from a Large Language Model (LLM). The primary function, `postprocess`, takes an LLM's output and a target programming language as input. It first retrieves the appropriate comment syntax for the specified language using an imported `get_comment` helper. It then utilizes the `find_section` function to parse the text for Markdown-style code blocks (delimited by triple backticks), identifying the language and line numbers for each block. The script logic prioritizes finding the largest code block that matches the requested language. Once identified, the script reconstructs the output by preserving the content of this largest matching code block while commenting out all other lines in the file—including text outside code blocks and code blocks in other languages—using the `comment_line` utility. If no matching code block is found, the entire output is commented out. This functionality is likely designed to sanitize LLM responses so that they can be safely executed or saved as valid source code files without extraneous conversational text causing syntax errors.",2026-01-03T04:43:49.638215+00:00
"context/split/3/split_postprocess_find_section.py","The provided file content is a code snippet demonstrating a function call in Python. Specifically, it calls a function named `find_section`. This function appears to be designed for parsing or processing text, as it takes an array of text lines (`lines`) as its primary argument. The function call includes two keyword arguments: `start_index`, which is set to 0, indicating the processing should begin at the first row; and `sub_section`, which is set to `False`, suggesting that the current operation is not looking for a nested section or subsection. The result of this function execution is assigned to a variable named `sections`. The comments within the code provide context for the arguments, clarifying that `lines` represents split lines of text and explaining the default behaviors for the index and subsection flags.",2026-01-03T04:44:02.215638+00:00
"context/split/3/sub_postprocess_python.prompt","The provided text outlines a task for an expert Python engineer to create a function named `find_section`. This function is designed to parse a list of strings derived from an LLM's output and identify top-level code sections. The function takes three arguments: a list of lines, a starting index, and a boolean flag for recursive sub-section calls. It returns a list of tuples, each containing the code language, start line, and end line for every identified section. The text includes an example of how the function might be called and a sample LLM output containing nested code blocks to illustrate the parsing challenge. Finally, the file details a three-step algorithm for the function: initializing a storage list, iterating through lines to detect triple backticks (indicating code blocks), handling nested blocks via recursion, and recording the start and end points of top-level sections while ignoring sub-sections.",2026-01-03T04:44:08.508720+00:00
"context/split/4/construct_paths.py","This Python script provides utility functions for handling file paths and filenames within a command-line interface (CLI) application. The core functionality is encapsulated in two main functions: `generate_output_filename` and `construct_paths`. The `generate_output_filename` function dynamically creates output filenames based on the specific command being executed (e.g., 'generate', 'test', 'fix', 'preprocess'), the base filename, the programming language, and the file extension. The `construct_paths` function orchestrates the file handling process. It first validates and standardizes input file paths, appending extensions if missing. It then reads the contents of these input files into memory. The function attempts to intelligently deduce the programming language and appropriate file extension by parsing the input filename. Subsequently, it constructs absolute output file paths, handling both directory and file inputs. Finally, it includes a safety check that prompts the user for confirmation before overwriting existing files, unless a 'force' flag is active. The script utilizes the `click` library for user interaction and `rich` for formatted console output.",2026-01-03T04:44:20.148137+00:00
"context/split/4/final_construct_paths_python.prompt","The file outlines the requirements for a Python function named `construct_paths`, designed for a tool called `pdd`. This function is responsible for managing file I/O operations, specifically generating and validating input and output file paths and loading input file contents. It utilizes the `rich` library for console output and `Click` for CLI interactions. The function takes inputs such as a dictionary of input file paths, boolean flags for `force` (overwrite) and `quiet` (suppress output), the specific `pdd` command run, and command options. It outputs a dictionary of loaded input strings, a dictionary of determined output file paths, and the target language string. Key logic includes appending default extensions (e.g., '.prompt') if missing, extracting basenames and languages for the 'generate' command, and utilizing helper functions like `get_extension` and `generate_output_filename`. The process involves four main steps: constructing input paths, loading files with error handling, constructing output paths according to specific rules, and handling file overwrite permissions interactively unless forced.",2026-01-03T04:44:32.627200+00:00
"context/split/4/initial_construct_paths_python.prompt","This document outlines the requirements for a Python function named `construct_paths`, designed for a CLI tool called `pdd`. The function's primary role is to manage file I/O operations, specifically generating file paths, checking for their existence, and loading input content. It takes inputs such as a dictionary of input file paths, boolean flags for forcing overwrites and suppressing output (`quiet`), the specific `pdd` command run, and associated command options. The function is expected to handle file extensions intelligently, adding defaults like `.prompt` if missing, and deriving output languages and extensions for the `generate` command. It must utilize the `rich` library for console output and `click` for CLI interactions. The process involves four main steps: constructing input paths, loading file contents into a dictionary (handling errors), constructing output paths based on specific rules (potentially involving a sub-function), and verifying overwrite permissions with the user if files exist and the `force` flag is unset. The final output includes dictionaries of loaded input strings and output file paths, along with the determined language.",2026-01-03T04:44:38.493330+00:00
"context/split/4/split_construct_paths_generate_output_filename.py","The provided code snippet demonstrates a function call to `generate_output_filename`, which is designed to construct a filename based on several specific parameters. The function takes five arguments: `command`, representing a specific operation or directive (e.g., 'generate'); `key`, which serves as an identifier or dictionary key for the output (e.g., 'output', 'output-test'); `basename`, the core name of the file being processed; `language`, indicating the programming or markup language associated with the file; and `file_extension`, specifying the file type suffix. This function likely serves as a utility within a larger system to standardize naming conventions for generated files, ensuring consistency across different commands, languages, and output types.",2026-01-03T04:44:45.536427+00:00
"context/split/4/sub_construct_paths_python.prompt","The provided text outlines a task for an expert Python software engineer to implement a function named `generate_output_filename`. This function is designed to construct specific output filenames based on the context of a command execution within a Python program. It accepts five string parameters: `command` (e.g., 'generate', 'test'), `key` (e.g., 'output'), `basename`, `language`, and `file_extension`. The function must apply a set of six distinct rules to determine the correct filename format. For instance, the 'generate' command simply appends the extension to the basename, while the 'test' command prefixes 'test_' to the basename. More complex logic applies to the 'preprocess' command, which incorporates the language into a '.prompt' file, and the 'fix' command, which varies its output based on whether the key is 'output-test'. A default fallback format is specified for any unrecognized commands. The implementation requires robustness to handle edge cases and ensure the correct string is returned for all scenarios.",2026-01-03T04:44:50.977679+00:00
"context/split/5/cli.py","This Python file defines a Command Line Interface (CLI) tool for Prompt-Driven Development (PDD), utilizing the `click` and `rich` libraries. The application provides a suite of commands to facilitate AI-assisted coding workflows, including generating code from prompts (`generate`), creating examples (`example`), generating unit tests (`test`), and preprocessing prompts (`preprocess`). It features robust error handling and cost tracking functionality via a decorator that logs usage metrics to a CSV file.

The CLI includes advanced features for iterative development, such as an automated loop for fixing code errors based on unit test failures (`fix`), analyzing code crashes (`crash`), and generating regression tests from bug reports (`bug`). It also supports prompt management through commands for splitting complex prompts (`split`), detecting necessary changes (`detect`), resolving conflicts (`conflicts`), and updating prompts based on code modifications (`update`, `change`). Additionally, the tool offers utilities for tracing code back to prompt lines (`trace`) and installing shell completion. Global options allow users to configure AI model parameters like strength and temperature, manage verbosity, and force file overwrites.",2026-01-03T04:45:03.038990+00:00
"context/split/5/cli_python.prompt","This file contains a detailed prompt for an AI to generate a Python command-line interface (CLI) tool named pdd. The tool is to be built using the Python Click library and serves as a utility for managing various code generation and maintenance tasks. The prompt outlines the directory structure, specific naming conventions to avoid conflicts between Click commands and internal function names (e.g., importing 'preprocess' as 'preprocess_func'), and provides a list of required commands. These commands include functionality for generating code, creating unit tests, preprocessing prompts, fixing errors, splitting files, detecting changes, and handling git updates. The file also references numerous external example files that demonstrate how to implement specific internal modules and logic for each command.",2026-01-03T04:45:16.375727+00:00
"context/split/5/modified_cli_python.prompt","The provided text is a detailed prompt specification for an AI to generate a Python command-line interface (CLI) tool named pdd. The tool is to be built using the Python `click` library and serves as a comprehensive utility for managing code generation, testing, and prompt engineering workflows. The specification outlines the directory structure and mandates the use of specific internal modules for various commands.

Key functionalities include generating runnable code from prompts (`generate`), creating examples from code (`example`), generating unit tests (`test`), and preprocessing prompts (`preprocess`). Advanced features include fixing code errors based on unit tests (`fix`), splitting prompts (`split`), updating prompts based on code changes (`update`), and detecting necessary changes across multiple files (`detect`). It also includes debugging tools like tracing execution (`trace`) and converting bugs into unit tests (`bug`). The prompt provides specific instructions on how to import functions to avoid naming conflicts with CLI commands and references external files for context, examples, and implementation details for each specific function.",2026-01-03T04:45:22.457032+00:00
"context/split/5/split_track_cost.py","The provided file content consists solely of the decorator or annotation @track_cost. This suggests that the file is likely a Python source file (or a similar language supporting decorators) intended to define or utilize a mechanism for monitoring resource usage or financial implications associated with specific functions or methods. In a larger context, this decorator would typically be placed above a function definition to wrap its execution with logic that calculates, logs, or reports the cost of running that function, possibly based on execution time, API usage tokens, or cloud resource consumption. However, because the input is extremely minimal and lacks the definition of the decorator itself or any code to which it is applied, the file appears to be incomplete or merely a snippet demonstrating a specific feature tag. It serves as a marker indicating that cost tracking functionality is intended to be implemented or applied at this location in the codebase.",2026-01-03T04:45:35.025089+00:00
"context/split/5/track_cost_python.prompt","The provided text outlines the requirements for creating a Python decorator function named `track_cost` for a command-line tool called pdd built with the Click library. The primary purpose of this decorator is to monitor and log the financial cost associated with executing specific commands. The decorator must wrap a command function, recording the start and end times of its execution. It needs to access the Click context to retrieve command details and check for an output cost file path, either specified directly or via an environment variable (`PDD_OUTPUT_COST_PATH`). The function is responsible for extracting cost and model information from the command's return value (typically a tuple), as well as identifying input and output file paths from the arguments. This data—timestamp, model, command name, cost, and file paths—must then be appended to a CSV file, creating the file with a header if it doesn't already exist. The implementation must be robust, using `functools.wraps` to preserve metadata and handling any logging errors gracefully with Rich's `rprint` without crashing the main program.",2026-01-03T04:45:41.000396+00:00
"context/split/6/cli.py","The provided code defines the main entry point for a Prompt-Driven Development (PDD) Command Line Interface tool, built using the Python `click` library. It serves as an orchestration layer that connects user commands to various backend AI modules for code generation and maintenance.

The CLI defines a global context allowing users to configure AI model parameters like strength and temperature, toggle verbosity, and track API costs. It exposes a comprehensive suite of commands designed to automate the software development lifecycle. Key functionalities include `generate` for creating runnable code from text prompts, `test` for producing unit tests, and `fix` for an iterative, AI-driven debugging loop that resolves errors.

Advanced features include `update` (synchronizing prompts with modified code), `split` (breaking down complex prompts), `detect` (identifying necessary changes), and `trace` (mapping code lines back to prompt instructions). The tool also handles file preprocessing and shell completion installation. Each command wraps specific logic imported from local modules, manages file input/output paths, and utilizes a `@track_cost` decorator to monitor the financial usage of the underlying AI models.",2026-01-03T04:45:52.694721+00:00
"context/split/6/cli_python.prompt","This document provides a comprehensive prompt for generating a Python command-line interface (CLI) tool named pdd using the Click library. The primary goal is to create a `cli.py` file that serves as the entry point for the application. The prompt outlines the directory structure and emphasizes the importance of avoiding naming conflicts between Click commands and imported internal functions (e.g., importing `preprocess` as `preprocess_func`).

The instructions detail numerous commands that the CLI must support, including `generate` (for code generation), `example` (for context generation), `test` (for unit test creation), `preprocess` (with an XML sub-command), `fix` (with a loop sub-command for error correction), `split`, `change` (with a CSV sub-command), `update` (with a Git sub-command), `detect`, `conflicts`, `crash`, `trace`, and `bug`. For each command, the prompt references specific internal modules and provides examples of how to utilize functions like `construct_paths`, `track_cost`, and various generators. Additionally, it includes instructions for an `install_completion` command to handle shell completion setup.",2026-01-03T04:46:24.835276+00:00
"context/split/6/conflicts_main_python.prompt","The provided text is a prompt specification for an AI coding assistant acting as an expert Python engineer. Its objective is to generate a function named `conflicts_main`, which serves as the core logic for the `conflicts` command within a command-line tool called `pdd`. The prompt details the function's inputs (Click context, two prompt file paths, and an optional output path) and expected outputs (a tuple containing analysis results, cost, and model name). It outlines a six-step implementation plan: constructing file paths using internal utilities, loading input files, analyzing conflicts via a `conflicts_in_prompts` function, saving results to CSV, providing user feedback via the Rich library, and implementing robust error handling. Additionally, the prompt references included context files for Python preambles and examples of using the Click library and internal modules like `construct_paths`.",2026-01-03T04:46:31.826449+00:00
"context/split/6/modified_cli_python.prompt","This file contains the specifications and instructions for building `cli.py`, the main entry point for a Python command-line tool named `pdd`. It outlines the use of the Python `Click` library to handle the command-line interface. The file provides a detailed list of commands to implement, including `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, `bug`, and `install_completion`. For each command, it references specific internal modules and functions (e.g., `code_generator`, `context_generator`, `fix_errors_from_unit_tests`) that perform the core logic. It also includes instructions on handling import naming conflicts, using the `track_cost` decorator, constructing file paths, and managing shell completion installation. The document serves as a blueprint for wiring together various backend functionalities into a cohesive CLI application.",2026-01-03T04:46:37.713741+00:00
"context/split/6/split_conflicts.py","The provided code snippet defines a command-line interface (CLI) command named `conflicts` using the `click` library. This command is designed to analyze two input prompt files, specified as arguments `prompt1` and `prompt2`, to identify potential conflicts between them and suggest resolutions. It also accepts an optional `--output` flag to specify a destination for saving the analysis results as a CSV file. The function is decorated with `@track_cost`, implying it tracks resource usage or cost associated with the operation. The implementation of the command is minimal, serving as a wrapper that delegates the core logic to a separate function called `conflicts_main`, passing along the context and arguments.",2026-01-03T04:46:58.912580+00:00
"context/split/7/cli.py","This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` library. It serves as the main entry point for the application, orchestrating various AI-assisted development tasks. The CLI supports global configuration options such as AI model strength, temperature, verbosity, and cost tracking. It implements a wide range of subcommands including `generate` (creating code from prompts), `example` (generating examples from code), `test` (creating unit tests), `preprocess` (formatting prompts), `fix` (iteratively repairing code based on errors), `split` (breaking down complex prompts), `change` (modifying prompts based on instructions), `update` (syncing prompts with code changes), `detect` (identifying necessary prompt changes), `conflicts` (resolving prompt conflicts), `crash` (fixing crashing modules), `bug` (generating tests from bug reports), and `trace` (mapping code lines back to prompt lines). The file integrates with internal modules to perform these tasks, handles file I/O, manages output formatting using the `rich` library, and includes a utility for installing shell completion.",2026-01-03T04:47:09.929413+00:00
"context/split/7/cli_python.prompt","The provided input is a detailed specification for an AI to engineer a Python command-line tool named pdd. Built using the Python `Click` library, this tool is designed to automate various aspects of software development, specifically focusing on code generation, testing, and prompt management.

The specification outlines the project's directory structure and provides strict coding guidelines, such as renaming imported functions (e.g., importing `preprocess` as `preprocess_func`) to avoid namespace conflicts with CLI commands. It details a wide array of commands including `generate` for creating code, `test` for unit testing, `fix` for debugging, and `preprocess` for prompt handling. Advanced features include conflict detection, crash analysis, execution tracing, and git-integrated updates.

Furthermore, the input references numerous external context files and examples for implementing specific functionalities like `track_cost` for metrics and `construct_paths` for file handling. It concludes with instructions for implementing shell completion, effectively serving as a blueprint for a robust developer productivity tool.",2026-01-03T04:47:33.277023+00:00
"context/split/7/modified_cli_python.prompt","This file contains comprehensive instructions and context for building a Python command-line interface (CLI) tool named `pdd` using the `click` library. It outlines the program's directory structure and provides specific implementation details for the `cli.py` file. The instructions emphasize avoiding naming conflicts between Click commands and imported internal functions (e.g., importing `preprocess` as `preprocess_func`). It details the implementation of numerous commands including `generate`, `example`, `test`, `preprocess` (with an `--xml` option), `fix` (with a `--loop` option), `split`, `change` (with a `--csv` option), `update` (with a `--git` option), `detect`, `conflicts`, `crash`, `trace`, and `bug`. For each command, it references specific internal modules and example files demonstrating how to invoke the underlying logic (e.g., `code_generator`, `context_generator`, `fix_errors_from_unit_tests`). Additionally, it includes instructions for a `track_cost` decorator and an `install_completion` command to handle shell completion setup.",2026-01-03T04:47:49.033360+00:00
"context/split/7/split_trace_main.py","The provided code snippet defines a command-line interface (CLI) command named `trace` using the `click` library. This command is designed to analyze the relationship between a prompt file and a specific line of generated code. It accepts three required arguments: `prompt_file` (a path to an existing prompt file), `code_file` (a path to an existing code file), and `code_line` (an integer specifying the line number in the code file). Additionally, it supports an optional `--output` flag to specify a destination for saving the analysis results. The function is decorated with `@track_cost` and `@click.pass_context`. The implementation delegates the core logic to a helper function called `trace_main`, passing along the context and all provided arguments. A comment notes that `trace_main` encapsulates the functionality previously contained within `trace`, suggesting a refactoring to split logic into sub-prompts.",2026-01-03T04:48:06.633802+00:00
"context/split/7/trace_main_python.prompt","This file contains a prompt for an expert Python engineer to generate the `trace_main` function for the `pdd` command-line program. The function is responsible for handling the logic of the `trace` command, which links a specific line of generated code back to its origin in a prompt file. The prompt specifies the function's inputs (Click context, file paths, line number) and expected outputs (prompt line number, cost, model name). It outlines a six-step process for the function: constructing file paths using internal utilities, loading file contents, performing the trace analysis via a `trace` function, saving the results, providing user feedback using the Rich library, and implementing robust error handling. The prompt also includes references to context files for Python preambles, Click library usage, and internal module examples to ensure the generated code matches the existing project style.",2026-01-03T04:48:12.839139+00:00
"context/split/8/change_main.py","The provided code snippet defines a command-line interface (CLI) command named `change` using the `click` library. This command is designed to modify an input prompt file based on a specified change prompt and corresponding input code. The function accepts three optional file path arguments: `input_prompt_file`, `input_code_file`, and `change_prompt_file`. Additionally, it supports two options: `--output` for specifying the destination of the modified prompt file, and `--csv` for providing change prompts via a CSV file instead of a text file. The command is decorated with `@track_cost`, suggesting it tracks resource usage or cost associated with the operation. The implementation of the `change` function is minimal, serving as a wrapper that delegates all logic and functionality to a separate function called `change_main`, passing along the context and all received arguments.",2026-01-03T04:48:18.848599+00:00
"context/split/8/change_main_python.prompt","This document outlines the requirements for generating a Python function named `change_main`, which serves as the core logic for the `change` command within the `pdd` command-line interface. The function is designed to modify prompt files based on input code and change instructions. It accepts several arguments, including the Click context (`ctx`) and file paths for input prompts, code, change prompts, output destinations, and an optional CSV file for batch processing. The function's workflow involves parsing arguments, constructing file paths using helper functions, and performing prompt modifications either individually via `change_func` or in batch via `process_csv_change`. It is responsible for saving results, providing user feedback via `rprint`, and handling errors gracefully. The output of the function is a tuple containing the modified prompt (or status message), the operation cost, and the model name used.",2026-01-03T04:48:24.992382+00:00
"context/split/8/cli.py","This Python script defines the Command Line Interface (CLI) for a Prompt-Driven Development (PDD) tool, utilizing the `click` library to orchestrate AI-assisted software engineering workflows. As the application's entry point, it imports various local modules to handle specific tasks while managing global configurations like AI model `strength`, `temperature`, and cost tracking via a `@track_cost` decorator.

The CLI offers a comprehensive suite of commands covering the entire development lifecycle. Core features include `generate` for creating code from natural language prompts, `test` for automating unit test creation, and `fix`, which performs iterative error correction on code and tests. The tool supports code maintenance through commands like `update` (synchronizing prompts with modified code, potentially via Git), `change` (modifying prompts based on instructions), and `split` (decomposing complex prompts).

Additionally, the script provides debugging and analysis utilities such as `crash` for fixing module errors, `bug` for generating tests based on output discrepancies, `trace` for linking code lines back to prompts, and `detect` for identifying necessary changes. The interface is enhanced with the `rich` library for styled console output and includes a helper to install shell auto-completion.",2026-01-03T04:48:30.881199+00:00
"context/split/8/cli_python.prompt","This file contains the prompt instructions and context for generating a Python command-line interface (CLI) tool named `pdd`. The tool is designed to be built using the Python Click library and serves as a utility for managing prompt-driven development workflows. The file outlines the directory structure, imports necessary internal modules, and provides specific instructions for implementing various commands such as `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`.

It includes detailed examples of how to use internal functions for each command, emphasizing the need to avoid naming conflicts between Click commands and imported functions. The instructions also specify the use of a `track_cost` decorator for monitoring usage. Furthermore, the file details specific requirements for sub-commands like `preprocess --xml`, `fix --loop`, `change --csv`, and `update --git`, as well as a command for installing shell completions. The overall goal is to guide an AI model in writing the `cli.py` file that orchestrates these various development automation tasks.",2026-01-03T04:48:54.257079+00:00
"context/split/8/modified_cli.prompt","The provided text outlines the specifications for a Python command-line interface (CLI) tool named `pdd`, designed to be built using the Click library. It details the directory structure, including folders for prompts, context, and data, and specifies that the main logic resides in `pdd/pdd/cli.py` and associated modules. The document provides instructions for implementing various commands such as `change`, `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`. It emphasizes using internal modules for core logic (e.g., `change_main`, `code_generator`, `fix_errors_from_unit_tests`) and includes examples for integrating these modules. Specific instructions are given for handling shell completion, implementing a `track_cost` decorator for usage tracking, and adding git functionality to the `update` command. The goal is to create a robust CLI tool for managing code generation, testing, and maintenance tasks.",2026-01-03T04:49:01.081208+00:00
"context/split/9/cli.py","This Python file defines the Command Line Interface (CLI) for the PDD (Prompt-Driven Development) tool using the `click` library. It serves as the entry point for the application, orchestrating various sub-commands that facilitate AI-assisted coding tasks. The script initializes the CLI with global options for controlling AI model parameters (strength, temperature), verbosity, and cost tracking. It dynamically determines the package path and includes an auto-update mechanism.

The file registers numerous commands, including `generate` for creating code from prompts, `test` for generating unit tests, `fix` for repairing code based on errors, and `crash` for resolving runtime crashes. Other utility commands include `preprocess` for prompt formatting, `split` for breaking down complex prompts, `detect` for analyzing changes, and `auto_deps` for managing project dependencies. Additionally, it provides a command `install_completion` to set up shell autocompletion for Bash, Zsh, and Fish shells. Each command delegates execution to specific handler functions imported from other modules within the package.",2026-01-03T04:49:13.657766+00:00
"context/split/9/cli_python.prompt","The provided file is a detailed prompt specification for generating a Python command-line interface (CLI) tool named `pdd`. It instructs an AI to act as an expert Python engineer and use the `click` library to build the CLI. The prompt outlines the directory structure and includes references to external files for context, such as a README and prior code examples. It provides specific instructions and code snippets for implementing various commands, including `change`, `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `update`, `detect`, `conflicts`, `crash`, `trace`, `bug`, and `auto-deps`. Each command references a corresponding `_main` function in separate modules. Additionally, the prompt details requirements for an `install_completion` command to handle shell completion setup, including logic for detecting the current shell and managing configuration files. It also specifies handling the `PDD_PATH` environment variable and an auto-update mechanism.",2026-01-03T04:49:27.518152+00:00
"context/split/9/install_completion.py","The provided code snippet defines a single function named `install_completion` within a command-line interface (CLI) context. It utilizes a decorator `@cli.command(name=install_completion)`, which suggests the use of a library like `click` or `typer` to register this function as a specific command available to the user. The purpose of this command is likely to set up or install shell completion scripts, enabling tab-completion features for the CLI tool in the user's terminal environment. Although the function body is empty in the snippet, the naming convention strongly implies it handles the configuration necessary to integrate the CLI's commands with shell autocompletion mechanisms.",2026-01-03T04:49:39.346967+00:00
"context/split/9/modified_cli.prompt","This document serves as a comprehensive prompt and specification for an AI agent to generate the source code for a Python command-line interface (CLI) tool named `pdd`. The tool is to be built using the Python `Click` library and is structured to include a main `cli` function. The prompt outlines the directory structure, which includes source files, prompts, context, and data folders. It provides specific instructions and examples for implementing various commands such as `change`, `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `update`, `detect`, `conflicts`, `crash`, `trace`, `bug`, `auto-deps`, and `install_completion`.

The document heavily relies on included context files (e.g., `context/click_example.py`, `context/auto_update_example.py`) to demonstrate how internal modules and decorators like `track_cost` and `construct_paths` should be utilized. It also specifies logic for automatic updates via `PDD_AUTO_UPDATE` and environment variable handling for `PDD_PATH`. The overall goal is to guide the creation of a consistent, modular CLI tool that integrates various code generation, testing, and maintenance functionalities.",2026-01-03T04:49:45.415501+00:00
"context/split/9/sub_cli.prompt","This file contains a detailed prompt for an AI coding assistant to generate a Python function named `install_completion` for the `pdd` command-line program. The prompt instructs the AI to act as an expert Python engineer and outlines specific requirements for the function, which handles the installation of shell completion scripts. Key steps include detecting the user's current shell (using `ps`, `$0`, or environment variables), locating the appropriate completion script based on the shell type, and updating the user's shell configuration file (RC file) to source this script. The prompt specifies error handling procedures, such as using `click.Abort()` and printing colored error messages with `rprint` for unsupported shells or missing scripts. It also mandates the inclusion of several helper functions: `get_shell_rc_path`, `get_current_shell`, `get_completion_script_extension`, and `get_local_pdd_path`. The prompt includes placeholders for context files like `python_preamble.prompt` and example code snippets.",2026-01-03T04:49:52.454677+00:00
"context/split/simple_math/split.prompt","The provided file content is a concise directive related to software development or code refactoring. It specifically instructs the reader to identify the logic currently responsible for validating input data within a larger codebase or function and move that logic into its own distinct, auxiliary function. This practice, often referred to as 'extract method' or 'extract function,' is a common refactoring technique aimed at improving code readability, maintainability, and reusability. By isolating the validation logic, the main function becomes cleaner and more focused on its primary task, while the validation logic becomes easier to test independently and reuse in other parts of the application if necessary. The instruction implies that the current implementation likely mixes validation concerns with business logic, violating the Single Responsibility Principle, and seeks to correct this by modularizing the code structure.",2026-01-03T04:49:58.880579+00:00
"context/split/simple_math/split_example.py","This file contains a Python script that serves as a usage example for a module named 'split_script'. The script demonstrates how to use the 'add' function from the imported module. It includes two test cases wrapped in try-except blocks to handle potential TypeErrors. The first case attempts to add two integers (10 and 20) and prints the result, demonstrating successful usage. The second case attempts to add an integer (5) and a string ('x'), which is intended to trigger an error, thereby demonstrating the module's error handling capabilities or the script's robustness in catching type mismatches. The file appears to be a placeholder or a test driver for the 'split_script' functionality.",2026-01-03T04:50:05.079068+00:00
"context/split/simple_math/split_script.py","The provided file content is a brief Python script snippet. It begins with a comment indicating it serves as a placeholder for a split command script. The core of the snippet is a single function definition named `add`, which takes two arguments, `a` and `b`. This function is designed to perform arithmetic addition but includes a safety mechanism: it first verifies that both inputs are either integers or floating-point numbers using `isinstance`. If either input fails this type check, the function raises a `TypeError` with a descriptive message. Otherwise, it returns the sum of the two numbers.",2026-01-03T04:50:16.080491+00:00
"context/split_example.py","This Python script serves as a demonstration and test harness for the `split` function within the `pdd.split` module. It begins by importing necessary libraries, including `os` for environment management and `rich.console` for enhanced terminal output. The core logic is encapsulated within a `main` function. Inside `main`, the script first ensures the `PDD_PATH` environment variable is correctly set relative to the script's location. It then defines several input parameters required for the `split` function: a natural language prompt requesting a factorial function, the corresponding Python code implementation, an example usage snippet, and numerical parameters for `strength` and `temperature`. The script executes the `split` function with these inputs and `verbose` mode enabled. It expects a return tuple containing the results, total cost, and model name. Finally, it unpacks the results (specifically a sub-prompt and a modified prompt) and uses the Rich console to print formatted outputs, including the generated prompts, the model used, and the calculated cost. Error handling is implemented to catch and display any exceptions that occur during execution.",2026-01-03T04:50:26.687200+00:00
"context/split_main_example.py","The provided file, `example_split_usage.py`, serves as a demonstration script for utilizing the `split_main` function from the `pdd.split_main` module via a command-line interface (CLI). It employs the `click` library to define a CLI command named `split-cli` nested within a top-level group. The script accepts several required arguments, including paths for an input prompt file, a generated code file, and an example code file, as well as optional arguments for output paths, force overwriting, and quiet execution.

Inside the `split_cli` function, a context object is initialized with custom settings like `strength` and `temperature`. The script then calls `split_main` with the provided arguments, capturing the returned result data, total cost, and model name. If the quiet flag is not set, it prints the contents of the generated sub-prompt and modified prompt, their save locations, the model used, and the calculated cost to the console. The script includes error handling to catch and display exceptions during execution. Finally, it sets up the main entry point to run the CLI group.",2026-01-03T04:50:38.947468+00:00
"context/summarize_directory_example.py","This Python script serves as a demonstration of how to use the `summarize_directory` module from the `pdd` package. The script defines a `main` function that orchestrates the summarization process. It begins by creating a sample string representing an existing CSV file containing outdated summaries for specific Python files. The core functionality is executed by calling `summarize_directory` with parameters specifying a file pattern (`context/c*.py`), a model strength of 0.5, a temperature of 0.0 for deterministic output, and verbose logging enabled. The function also accepts the existing CSV content to potentially update or merge data. Upon execution, the script captures the resulting CSV output, the total cost of the operation, and the name of the model used. It then prints these details to the console. Finally, the script ensures an `output` directory exists and saves the generated CSV content to a file named `output.csv` within that directory. Error handling is implemented to catch and print any exceptions that occur during the process.",2026-01-03T04:50:51.714627+00:00
"context/sync_animation_example.py","This Python script serves as a demonstration and usage example for a module named `sync_animation`, likely part of a larger project called PDD. The script illustrates how to run a terminal-based animation in a separate thread while a main application performs work concurrently. It begins by defining shared state variables—specifically mutable lists for function names, costs, file paths, and box colors, as well as a `threading.Event` object to control execution flow. These variables allow the main thread to communicate updates to the animation thread dynamically.

The core of the script is the `mock_pdd_main_workflow` function, which simulates a series of application states (such as checking, generate, crash, and fix). This function updates the shared variables and sleeps for set intervals to mimic processing time and cost accumulation. The script initializes the animation thread using `threading.Thread`, targeting the `sync_animation` function with the shared state arguments. Finally, it runs the mock workflow, handles graceful shutdown via the stop event, and prints the total elapsed time and cost upon completion. This example effectively shows developers how to integrate visual feedback into long-running command-line processes.",2026-01-03T04:51:04.970769+00:00
"context/sync_determine_operation_example.py","This Python script serves as a demonstration and test harness for the `sync_determine_operation` function within a module named `pdd`. The script simulates a project environment by creating a temporary output directory and manipulating files to represent different development states of a software unit (specifically a calculator function). It defines a helper function, `create_file`, to generate files and return their SHA256 hashes. The `main` function orchestrates four distinct scenarios to verify how the system decides on the next operation:

1.  **New Unit:** Simulates a fresh start with only a prompt file, expecting a decision to generate code.
2.  **Test Failure:** Creates a scenario where a run report indicates failed tests, expecting a decision to fix the code.
3.  **Manual Code Change:** Modifies a code file so its hash mismatches the stored fingerprint, testing detection of manual intervention.
4.  **Unit Synchronized:** Sets up a state where all file hashes match the metadata and tests pass, expecting a decision that the unit is synchronized.

The script prints the decision and reasoning for each scenario to the console before cleaning up by restoring the original working directory.",2026-01-03T04:51:11.961042+00:00
"context/sync_main_example.py","This Python script serves as a programmatic demonstration and test harness for the `sync_main` function within the `pdd` (Project Driven Development) framework. It illustrates how to invoke the core synchronization logic without relying on the full command-line interface or external LLM dependencies.

The script defines a `setup_mock_project` function that creates a temporary directory structure containing prompt files for multiple languages (Python and JavaScript) to simulate a real project environment. The `main` function orchestrates the execution by first initializing a mock `click.Context` object, which mimics the configuration usually passed by the CLI. It then uses `unittest.mock.patch` to replace internal dependencies—specifically `construct_paths` and `sync_orchestration`—with local mock functions. These mocks simulate file path resolution and the outcomes of code generation (success for Python, failure for JavaScript) without making actual API calls. Finally, the script executes `sync_main` with these mocked components and prints the aggregated results, including success status, costs, and detailed JSON output, to the console using the `rich` library for formatting.",2026-01-03T04:51:23.833804+00:00
"context/sync_orchestration_example.py","This Python script serves as a demonstration and usage example for the `sync_orchestration` module within a project likely named `pdd-cli`. The script primarily defines a `setup_example_project` function to initialize a mock directory structure (containing prompts, source code, examples, and tests) and a dummy prompt file to simulate a real project environment. In its main execution block, the script showcases two key workflows: running a full synchronization process using `sync_orchestration` to generate code and tests based on a prompt, and performing a dry-run to analyze the synchronization state without executing changes. It prints the JSON results of these operations to the console, illustrating how the orchestrator manages file generation, budgeting, and logging.",2026-01-03T04:51:30.496346+00:00
"context/test.prompt","The provided file contents outline specific configuration and directory structure requirements for a testing environment. Key constraints include the location of generated tests, which must reside in a `tests` directory separate from the source modules in the `pdd` directory, necessitating absolute references for imports. The source module filenames correspond directly to their function names. Additionally, any files created during the process must be placed in an `output` directory. The instructions explicitly warn against overwriting existing data files (`language_format.csv` and `llm_model.csv`) located in the `data` directory within the path defined by the `PDD_PATH` environment variable. These existing files contain pre-populated data for popular languages and LLM models suitable for testing purposes. Finally, it is noted that the `PDD_PATH` environment variable is pre-configured and available for use.",2026-01-03T04:51:36.331068+00:00
"context/thinking_tokens.md","This document outlines the maximum internal reasoning token limits for various AI models, distinguishing between standard models and those with dedicated chain-of-thought (CoT) capabilities. Standard models like GPT-4.1, GPT-5-nano, Claude 3.5 Haiku, and DeepSeek Coder do not utilize hidden reasoning tokens, relying instead on single-pass generation within their standard context windows. Conversely, specialized reasoning models feature significant budgets for internal deliberation. DeepSeek's R1 series (including distilled variants and the main DeepSeek Reasoner) generally caps reasoning at approximately 32,000 tokens. Anthropic's Claude 3.7 Sonnet supports up to 64,000 tokens for its extended thinking mode. Google's Gemini 2.5 Flash and Pro models are estimated to use tens of thousands of tokens (likely up to ~65k) for internal reasoning. The highest capacities are found in OpenAI's o-series (o4-mini and o3) and xAI's Grok 3 Beta; the former supports up to 100,000 output tokens (inclusive of reasoning), while Grok 3 Beta has no fixed published limit but can utilize thousands of tokens over several minutes of processing. These internal budgets allow models to perform complex, multi-step problem-solving before generating a final response.",2026-01-03T04:51:49.381083+00:00
"context/tiktoken_example.py","The provided file content is a brief Python code snippet demonstrating how to count tokens using the `tiktoken` library, which is commonly used with OpenAI models. The script first imports the `tiktoken` module. It then initializes an encoding object by calling `tiktoken.get_encoding` with the specific encoding scheme named cl100k_base, although the comment suggests other encoding names could be substituted. Finally, the code calculates the number of tokens in a variable named `preprocessed_prompt` by encoding the text and measuring the length of the resulting list using the `len()` function, storing the result in the variable `token_count`.",2026-01-03T04:51:56.185977+00:00
"context/trace_example.py","This Python script serves as a demonstration and entry point for utilizing the `trace` functionality within the `pdd` library. The script imports necessary modules, including `os`, `trace` from `pdd.trace`, and `Console` from `rich.console` for enhanced terminal output. The core of the script is the `main` function, which sets up a test environment to simulate the tracing process. It defines example inputs such as a snippet of Python code (`code_file`), a specific line number to trace (`code_line`), and a corresponding prompt description (`prompt_file`). The script also configures parameters for the underlying Large Language Model (LLM), specifically setting the `strength` to a default value and the `temperature` to 0.0 for deterministic output. Inside a try-except block, the script calls the `trace` function with these parameters and `verbose=True`. Upon successful execution, it prints the results using the `rich` console, displaying the identified corresponding prompt line, the total cost of the operation, and the name of the model used. It also includes robust error handling to catch and display `FileNotFoundError`, `ValueError`, or any unexpected exceptions in a formatted red font.",2026-01-03T04:52:01.606994+00:00
"context/trace_main_example.py","This Python script demonstrates the usage of the `trace_main` function from the `pdd.trace_main` module, likely part of a larger tool for tracing relationships between prompts and code using Large Language Models (LLMs). The script begins by creating two example files in an `output` directory: `calculator.prompt`, which contains a natural language description of a calculator function, and `calculator.py`, which contains the corresponding Python implementation. It then sets up a `click.Context` object to simulate command-line arguments, configuring parameters such as verbosity (`quiet`), file overwriting (`force`), analysis strength, and LLM temperature. The core of the script executes `trace_main` within a try-except block, passing the paths to the created prompt and code files. It specifically requests a trace for line 2 of the code file (the function definition) and specifies an output file for the results. Finally, upon successful execution, the script prints the identified corresponding line number from the prompt file, the total cost of the LLM analysis in USD, and the name of the model used, while handling any potential exceptions that may occur during the process.",2026-01-03T04:52:14.891512+00:00
"context/track_cost_example.py","This Python script defines a command-line interface (CLI) tool named PDD, built using the `click` library, designed for processing prompts and generating outputs with integrated cost tracking capabilities. The core functionality is encapsulated within a `cli` command group that accepts a global option, `--output-cost`, allowing users to specify a path for a CSV file to log usage details. The primary command within this group is `generate`, which requires an input prompt file path (`--prompt-file`) and accepts an optional output file path (`--output`). The `generate` function is decorated with a custom `@track_cost` decorator imported from `pdd.track_cost`, indicating that execution metrics are monitored. Inside the function, the script reads the input file, simulates a generation process (currently a placeholder that prepends text to the input), and calculates a simulated cost and model name (e.g., gpt-4). If an output path is provided, the result is written to that file; otherwise, it is printed to the console using the `rich` library. The script includes a `__main__` block that demonstrates how to invoke the CLI programmatically with sample arguments.",2026-01-03T04:52:27.644307+00:00
"context/unfinished_prompt.txt","This file contains a Python implementation of a command-line interface (CLI) for a Prompt-Driven Development (PDD) tool. Built using the `click` and `rich` libraries, the script defines the main entry point for the application. It sets up a command group named `cli` with global options for controlling file overwrites (`--force`), AI model parameters (`--strength`, `--temperature`), verbosity levels (`--verbose`, `--quiet`), and cost tracking (`--output-cost`). The script includes a helper function `log_cost` to record usage metrics to a CSV file if enabled. It also defines a `generate` command that takes a prompt file as an argument and an optional output path. This command orchestrates the code generation process by utilizing imported helper modules (like `construct_paths` and `code_generator`) to read inputs, generate runnable code via an AI model, and save the result. The file demonstrates a modular architecture, importing various functional components such as context generation, testing, preprocessing, and error fixing, suggesting a larger system designed for AI-assisted software development.",2026-01-03T04:52:39.955623+00:00
"context/unfinished_prompt_example.py","This Python script serves as a usage example for the `unfinished_prompt` function from the `pdd` package. It demonstrates how to programmatically analyze a text prompt to determine if it is semantically complete or unfinished using an underlying Large Language Model (LLM). The script outlines necessary pre-requisites, such as ensuring the `pdd` package is in the Python path and configuring LLM access (e.g., API keys). It then defines an intentionally incomplete sample string about baking sourdough bread and calls the `unfinished_prompt` function with specific parameters for strength, temperature, and verbosity. Finally, the script captures the function's return values—which include the LLM's reasoning, a boolean completion flag, the cost of the operation, and the model name—and displays these results using the `rich` library for formatted output. A commented-out section also illustrates how to call the function with default parameters.",2026-01-03T04:52:46.208462+00:00
"context/unrunnable_raw_llm_output.py","The provided text outlines the implementation of a Python function named `context_generator`. This function is designed to automate the creation of usage examples for Python modules. The process involves several key steps: reading a specified Python source file, preprocessing its content (though the specific preprocessing logic is abstracted in the snippet), and constructing a prompt for an AI model (specifically GPT-4) to generate a concise usage example based on the code. The implementation includes error handling for file operations, such as checking if the input file exists. Additionally, the text provides a brief explanation of the code's logic, highlighting the file reading and preprocessing stages, and concludes with a usage example demonstrating how to call the function with input and output filenames.",2026-01-03T04:52:58.282443+00:00
"context/update/1/final_change_python.prompt","The provided text outlines the requirements for creating a Python function named change, designed to modify an existing input prompt based on specific instructions. Acting as an expert Python Software Engineer, the developer must implement this function to take several inputs: an initial prompt string, the code generated from it, a change instruction string, and model parameters like strength and temperature. The function's primary output is a modified_prompt string, along with the total operational cost and the model name used.

The implementation steps involve loading specific prompt files from a defined path, preprocessing them (handling curly brackets appropriately), and utilizing Langchain LCEL templates. The process requires selecting an LLM model via an llm_selector utility, which also handles token counting and cost estimation. The workflow includes two main model invocations: first to generate a raw modification based on the inputs, and second to extract the final JSON-structured prompt using a secondary template. All console output must be formatted using the Python Rich library, and the code must use relative imports and handle edge cases robustly.",2026-01-03T04:53:03.920904+00:00
"context/update/1/initial_change.py","This Python script defines a function named `change` that orchestrates a multi-step process to modify prompts using a Large Language Model (LLM). The function takes an initial prompt, input code, a description of desired changes, and model parameters (strength and temperature) as inputs. It first loads specific prompt templates from external files (`change_LLM.prompt` and `extract_prompt_change_LLM.prompt`) located via an environment variable. The script then utilizes a helper function, `llm_selector`, to choose an appropriate LLM and cost parameters based on the input strength. The core logic involves two main LLM invocations using LangChain: first, to generate a modified prompt based on the inputs, and second, to extract the specific `modified_prompt` string from the previous step's output using a JSON parser. Throughout the execution, the script calculates and prints token usage and estimated costs for each step using the `rich` library for formatted console output. Finally, it returns the modified prompt, the total calculated cost, and the name of the model used, while including error handling for file access and JSON parsing issues.",2026-01-03T04:53:16.415603+00:00
"context/update/1/initial_change_python.prompt","The file outlines the requirements for a Python function named change, designed to modify an existing input prompt based on specific change instructions and associated code. The function takes five inputs: the original prompt, the generated code, the change instructions, and model parameters for strength and temperature. It is required to use relative imports and the Python Rich library for console output. The process involves loading specific prompt files, preprocessing them, and utilizing Langchain LCEL templates to interact with an LLM. The workflow includes selecting an LLM via an `llm_selector` utility, running the inputs through the model to generate a raw response, and then using a secondary extraction prompt to parse the output into a JSON format containing the modified_prompt. The function must calculate and display token counts and costs at each step. Finally, it returns the modified prompt string, the total cost of operations, and the name of the model used, while handling potential errors gracefully.",2026-01-03T04:53:28.788523+00:00
"context/update/1/modified_change.py","This Python script defines a function named `change` designed to modify input prompts and code using a Large Language Model (LLM). The process begins by loading specific prompt templates from files located via an environment variable. It then preprocesses these templates and selects an appropriate LLM based on specified strength and temperature parameters using a helper function called `llm_selector`. The core logic involves a two-step chain execution using LangChain components: first, it runs a `change_LLM` template to generate modifications based on the input prompt, code, and change instructions; second, it runs an `extract_prompt` template to parse the LLM's output into a structured JSON format containing the final modified prompt. Throughout execution, the script calculates and prints token usage and estimated costs for each step using the `rich` library for formatted console output. Finally, it returns the modified prompt, the total calculated cost, and the name of the model used, while including error handling for file access and JSON parsing issues.",2026-01-03T04:53:35.130023+00:00
"context/update/2/initial_continue_generation.py","This Python script defines a function named `continue_generation` designed to extend and complete code generation tasks using Large Language Models (LLMs) via the LangChain framework. The process begins by loading and preprocessing specific prompt templates from a directory specified by an environment variable. It utilizes an `llm_selector` to choose appropriate models for generation and trimming tasks based on desired strength and temperature settings.

The core logic involves an iterative loop where the script first extracts an initial code block from previous LLM output. It then repeatedly invokes a generation chain to continue the code, calculating costs for token usage at each step. After each generation cycle, the script checks if the output is complete using a helper function `unfinished_prompt`. If the generation is deemed finished, the results are trimmed and finalized; otherwise, the loop continues. The script uses the `rich` library to print progress, costs, and the final Markdown-formatted code block to the console. Ultimately, it returns the fully generated code block, the total calculated cost, and the name of the model used.",2026-01-03T04:53:48.910090+00:00
"context/update/2/initial_continue_generation_python.prompt","The provided text outlines the requirements for creating a Python function named `continue_generation`, designed to complete the generation of a prompt using a Large Language Model (LLM). The function, intended for a Python package using relative imports and the Rich library for output, takes a formatted input prompt, initial LLM output, and model parameters (strength and temperature) as inputs. It returns the final completed LLM output, the total cost of execution, and the model name used. The process involves several steps: loading and preprocessing specific prompt files from an environmental path, setting up Langchain LCEL templates, and selecting an LLM model. The core logic requires trimming the initial output to extract a code block, running a continuation generation loop that checks for completeness using a helper function (`unfinished_prompt`), and iteratively appending content until finished. Finally, the results are trimmed and formatted, and the function calculates and reports the total token cost across all model invocations.",2026-01-03T04:54:00.970384+00:00
"context/update/2/modified_continue_generation.py","This Python script defines a function named `continue_generation` designed to extend and complete code generation tasks using Large Language Models (LLMs) via the LangChain library. The process begins by loading and preprocessing specific prompt templates from a directory specified by an environment variable. It utilizes an `llm_selector` to choose appropriate models based on desired strength and temperature settings. The core logic involves an iterative loop where the script first extracts an initial code block from previous output, then repeatedly invokes an LLM to continue generating code. In each iteration, it checks if the generation is complete using a helper function `unfinished_prompt`. If finished, it trims the final output for cleanliness; if not, it appends the new content and continues the loop. Throughout the execution, the script calculates and prints the cost of token usage for each step (trimming, generation, and completion checks) using the `rich` library for formatted console output. Finally, it returns the fully generated code block, the total cost, and the model name used.",2026-01-03T04:54:12.072008+00:00
"context/update/2/modified_initial_continue_generation.prompt","The provided text outlines the requirements for creating a Python function named `continue_generation`, designed to complete the generation of a prompt using a Large Language Model (LLM). The function, intended for a Python package using relative imports and the Rich library for output, takes a formatted input prompt, existing LLM output, and model parameters (strength, temperature) as inputs. It returns the completed LLM output, total cost, and model name. The process involves loading and preprocessing specific prompt files from an environment path, setting up Langchain LCEL templates, and selecting an LLM model. The core logic iterates through a generation loop: it trims the initial output, continues generation using the LLM, checks for completeness using an `unfinished_prompt` helper, and repeats until the output is deemed complete. Finally, it trims the result, calculates costs based on token usage across different models, and pretty-prints the final output.",2026-01-03T04:54:24.089707+00:00
"context/update_main_example.py","This Python script serves as an example implementation of a Command Line Interface (CLI) using the `click` library to demonstrate the functionality of the `update_main` function from the `pdd` package. The script defines a primary command named `update` which accepts various arguments such as file paths for input prompts, modified code, and original code, as well as configuration flags like `--git`, `--strength`, `--temperature`, and `--verbose`. It illustrates how to pass these CLI options into a context object and subsequently invoke `update_main` to process updates to a prompt based on code modifications. The script handles both file-based inputs and Git history retrieval for original code. Finally, it captures the return values—specifically the updated prompt, total cost, and model name—and displays them to the user using the `rich` library for formatted output. The file concludes by grouping the command under a main CLI entry point for execution.",2026-01-03T04:54:36.237367+00:00
"context/update_model_costs_example.py","This Python script, `example_update_model_costs.py`, serves as an end-to-end demonstration for using the `update_model_costs` helper utility within the `pdd` package. It illustrates how to automatically update pricing and capability data for Large Language Models (LLMs) stored in a CSV file. The script begins by creating a sample CSV file (`sample_llm_model.csv`) containing basic model information for OpenAI's GPT-4o-mini and Anthropic's Claude-3-haiku, intentionally leaving cost and structured output fields blank. It then imports and executes the `update_model_data` function from `pdd.update_model_costs`, which modifies the CSV file in place by fetching and filling in the missing data using LiteLLM. Finally, the script prints the contents of the CSV before and after the update to verify the changes. It also includes commented-out code demonstrating how to achieve the same result using the command-line interface (CLI) via `subprocess`.",2026-01-03T04:54:48.753775+00:00
"context/update_prompt_example.py","This Python script serves as a demonstration and entry point for utilizing the `update_prompt` function from the `pdd` package. The script defines a `main` function that sets up a specific test scenario involving an initial natural language prompt ('Please add two numbers and return the sum'), a corresponding piece of input code (an addition function), and a modified version of that code (a multiplication function). It uses these inputs to invoke `update_prompt`, passing along configuration parameters such as a default strength value and a temperature setting of zero for deterministic behavior. The script includes error handling to catch exceptions during execution and prints the results to the console if successful. Specifically, it outputs the newly generated 'modified prompt' that aligns with the code changes, the total cost of the operation, and the name of the model used. If the update fails or an exception occurs, appropriate error messages are displayed. The script is designed to be run directly as a standalone module.",2026-01-03T04:55:01.787544+00:00
"context/vertex_ai_litellm.py","This Python script demonstrates how to use the `litellm` library to interact with Google's Vertex AI Gemini models, specifically the `gemini-2.5-pro-preview-05-06` version. The script begins by importing necessary modules, including `completion` from `litellm`, `json`, and `os`. It enables debug logging for LiteLLM by setting the `LITELLM_LOG` environment variable. The core functionality revolves around authentication and model invocation. It retrieves the path to a Google Cloud service account credential file from the `VERTEX_CREDENTIALS` environment variable, reads the JSON content of this file, and converts it into a JSON string format required by the library. Finally, the script calls the `completion` function, passing in the specific Vertex AI model name, a list of messages (simulating a conversation with a system prompt and a user greeting), the loaded credentials, the Google Cloud project ID (`meta-plateau-401521`), and the location (`us-central1`). The response from the model is then printed to the console.",2026-01-03T04:55:14.023545+00:00
"context/whitepaper.md","This white paper introduces Prompt-Driven Development (PDD), a software engineering methodology that shifts the primary development artifact from code to high-level prompts. Arguing that traditional and even AI-assisted coding methods are struggling with increasing complexity and technical debt, the paper posits that PDD aligns software more closely with business objectives by focusing on intent rather than implementation details. In PDD, developers craft detailed prompts describing functionality and constraints, which advanced AI models then use to generate code, tests, and documentation. The document outlines the technical workflow, compares PDD favorably against traditional methods regarding productivity and scalability, and details specific CLI commands (e.g., `pdd generate`, `pdd update`, `pdd trace`) designed to mitigate challenges like debugging and code synchronization. Ultimately, the paper presents PDD not merely as a tool but as a necessary paradigm shift that empowers cross-functional collaboration, ensures adherence to security standards, and future-proofs organizations against the rapidly evolving landscape of software development.",2026-01-03T04:55:28.182916+00:00
"context/xml/1/split_LLM.prompt","The provided file contains a prompt template designed for an expert LLM Prompt Engineer. Its primary objective is to instruct the model on how to refactor a single `input_prompt` into two distinct components: a `sub_prompt` and a `modified_prompt`, ensuring no loss of functionality. The context implies a code generation workflow where a larger coding task is being broken down into modular parts. The file includes four specific examples demonstrating this splitting process, referencing file paths for initial prompts, input code, example code, and the resulting split prompts. The prompt template defines the inputs (`input_prompt`, `input_code`, `example_code`) and the expected outputs (`sub_prompt`, `modified_prompt`). Finally, it outlines a four-step instruction set for the model: identifying difficulties in splitting the prompt, proposing solutions to those difficulties, generating the `sub_prompt`, and finally generating the `modified_prompt`.",2026-01-03T04:55:40.233378+00:00
"context/xml/1/split_xml_LLM.prompt","The provided file outlines a prompt engineering task designed for an expert LLM Prompt Engineer. The primary objective is to take an existing `input_prompt` and split it into two distinct components: a `sub_prompt` and a `modified_prompt`, ensuring no loss of functionality in the resulting code generation. The file structure includes a section for `<examples>`, which lists four specific test cases involving Python scripts (e.g., `pdd.py`, `postprocess.py`, `construct_paths.py`) and their corresponding prompt files. A `<context>` section defines the inputs—the original prompt, the generated code, and an example of how the split code should function—and the expected outputs. Finally, the `<instructions>` section provides a step-by-step guide for the LLM, requiring it to first analyze the difficulties of the split, propose solutions, and then generate the actual `sub_prompt` and `modified_prompt` based on the provided inputs.",2026-01-03T04:55:52.687766+00:00
"context/xml/2/xml_convertor_LLM.prompt","The provided text outlines a prompt engineering task designed to enhance the structure and readability of a raw input prompt using XML tags. Acting as an expert Prompt Engineer, the system is instructed to analyze a given `input_raw_prompt` and insert appropriate XML tags—such as `<instructions>`, `<context>`, `<examples>`, and `<formatting>`—without adding new content. The goal is to organize the prompt's components, making it easier for language models to process, especially when dealing with large blocks of text or placeholders. The file includes an example demonstrating how a raw prompt is transformed into a tagged version. It also details a three-step process: analyzing the input to identify components, determining suitable tags, and inserting those tags at the correct locations. The instructions emphasize that XML tags should only be added where necessary to improve organization and clarity, particularly around includes and placeholders.",2026-01-03T04:55:58.783593+00:00
"context/xml/2/xml_convertor_xml_LLM.prompt","The provided file outlines a specific task for an expert Prompt Engineer: to enhance the structure and readability of a raw input prompt by adding appropriate XML tags. The core objective is to organize the prompt's content—such as instructions, context, examples, and formatting guidelines—using XML elements without altering or adding to the original text. The document provides a clear workflow, instructing the engineer to first analyze the raw prompt to identify its components, then determine suitable XML tags, and finally insert these tags to improve the prompt's organization. It includes an example demonstrating how a raw prompt can be transformed into a tagged version. Additionally, the file specifies output instructions, emphasizing the use of tags like <instructions>, <context>, and <examples>, and notes that XML tagging is particularly useful for managing large blocks of text or placeholders. The process is broken down into three distinct steps: analysis, tag selection, and insertion.",2026-01-03T04:56:04.780305+00:00
"context/xml_tagger_example.py","This Python script demonstrates the usage of the `xml_tagger` function from the `pdd.xml_tagger` module. It begins by importing the necessary function and the `rich` library for enhanced console output. The script defines sample input parameters, including a `raw_prompt` (Write a story about a magical forest), a `strength` value of 0.5 (representing the model's capability tier), and a `temperature` setting of 0.7 for controlling generation randomness. Inside a try-except block to handle potential errors, the script calls `xml_tagger` with these parameters. Upon successful execution, it captures three return values: the processed XML-tagged string, the calculated cost of the operation, and the name of the specific Large Language Model (LLM) used. Finally, it utilizes `rich` to print a formatted success message, displaying the resulting tagged prompt, the total financial cost formatted to six decimal places, and the model identifier. If an exception occurs during the process, it catches the error and prints a bold red error message to the console.",2026-01-03T04:56:10.519362+00:00
