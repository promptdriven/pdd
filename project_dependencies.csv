full_path,file_summary,date
"context/DSPy_example.py","This file contains commented-out Python code that demonstrates how to set up and use DSPy, a framework for programming with language models. The code outlines a two-step process for selecting optimal examples from changes using DSPy.

The first step involves setting up the language model configuration. It imports a DSPy example module and configures an OpenAI GPT-3.5-turbo-instruct model with a maximum token limit of 250. The settings are then configured to use this turbo model as the default language model.

The second step defines a custom DSPy module called chainofthought that inherits from dspy.Module. This class implements a Chain of Thought reasoning pattern, which is a prompting technique that encourages the model to break down complex problems into intermediate reasoning steps. The class initializes a ChainOfThought program that maps questions to answers, and includes a forward method that processes questions through this program.

The file ends with an incomplete comment suggesting there would be additional code for compiling and optimizing the module. Overall, this appears to be a work-in-progress template or tutorial for implementing Chain of Thought prompting using the DSPy framework with OpenAI's language models.",2025-12-14T08:28:52.943712+00:00
"context/__init__example.py","This Python script demonstrates the usage of a command registration module for building a command-line interface (CLI) using the Click library. The file serves as an example of how to initialize and configure the main CLI entry point for a tool called PDD.

The script imports necessary modules including Click, sys, and os, and imports a `register_commands` function from the `pdd.commands` package. It defines a main CLI group using Click's `@click.group()` decorator, which creates the root command group that will contain all subcommands.

The core functionality is contained in the `run_example()` function, which performs three main tasks: (1) initializes the CLI group, (2) calls `register_commands()` to attach all subcommands defined in the pdd.commands package to the main CLI group, and (3) simulates running the CLI with the '--help' flag to verify that all commands were registered successfully.

The script uses `standalone_mode=False` when invoking the help command to prevent Click from calling `sys.exit()`, allowing the script to complete gracefully. This example is typically used as a reference for setting up the main entry point (e.g., `cli.py`) of an application that aggregates multiple command-line functionalities into a unified interface.",2025-12-14T08:29:01.960714+00:00
"context/addition_of_time_param.prompt","This file is a prompt template designed for the PDD CLI's `detect` command. Its purpose is to analyze other PDD CLI command prompt files to determine if they need modifications following the introduction of a new global `--time` parameter. The `--time` parameter (ranging from 0.0 to 1.0, defaulting to 0.25) controls the LLM's thinking effort or reasoning allocation when invoking the underlying `llm_invoke` Python function. The prompt explains how the `time` parameter behaves differently based on a model's `reasoning_type`: it's ignored for 'none' types, calculates a token budget for 'budget' types, and maps to qualitative effort levels (low/medium/high) for 'effort' types. The analysis task involves checking target prompts for: conflicting hardcoded reasoning instructions, sensitivity to speed/cost vs. reasoning quality trade-offs, user-facing documentation that may need updating, and whether the prompt can work transparently with the new global parameter. The goal is to identify prompts that either conflict with or are made redundant by the new `--time` functionality, versus those that require no changes because they're generic regarding LLM reasoning behavior.",2025-12-14T08:29:10.378317+00:00
"context/agentic_common_example.py","This Python script serves as a demonstration and usage example for the `pdd.agentic_common` module, specifically illustrating how to integrate and control AI agents programmatically. The script begins by configuring the system path to ensure the `pdd` package is importable.

In the `main` function, the code performs a sequence of operations to execute a headless agentic task. First, it establishes a local working directory (`./output`) for file operations. It then calls `get_available_agents()` to detect installed CLI tools and API keys for providers such as Claude, Gemini, or Codex.

The core functionality is demonstrated by defining a natural language instruction—asking an agent to generate a Python file named `generated_math.py` containing a factorial function. This instruction is passed to `run_agentic_task`, which handles the execution. Finally, the script parses and prints the execution results, including the provider used and estimated cost, and verifies the success of the operation by checking for the existence and content of the generated file.",2025-12-20T23:12:28.196459+00:00
"context/agentic_crash_example.py","This Python script serves as a standalone test and demonstration for the `pdd.agentic_crash` module. It simulates a scenario where an automated agent fixes a crashing program without requiring actual external API calls or LLM access.

The script begins by setting up a temporary environment in an `output` directory, generating four files: a specification for a factorial function, a buggy implementation raising a `NotImplementedError`, a runner script, and a crash log.

To test the `run_agentic_crash` function, the script heavily utilizes the `unittest.mock` library. It patches several dependencies, including `run_agentic_task`, `get_run_command_for_file`, and `subprocess.run`. These mocks simulate the behavior of an AI agent analyzing the crash, generating a fix (calculating the factorial), and verifying the fix by running the code successfully. A side effect is defined to physically update the dummy code file, mimicking the agent's file writing capability.

Finally, the script executes `run_agentic_crash`, captures the results, and prints a summary of the operation, including success status, simulated cost, model usage, and the content of the fixed file to verify the workflow.",2025-12-21T23:17:03.602890+00:00
"context/agentic_fix_example.py","This Python script serves as a demonstration for the `run_agentic_fix` function, illustrating how to programmatically invoke an LLM-based agent to repair broken code. The script sets up a self-contained scenario within a temporary directory, generating a deliberately buggy Python file (a calculator `add` function that performs subtraction), a corresponding unit test that fails, and a prompt file instructing the agent to fix the issue.

The workflow includes a setup phase that executes `pytest` to generate a genuine error log, simulating a real-world failure state. It checks for the presence of required API keys (such as OpenAI, Anthropic, or Google) before proceeding. The `main` function then calls `run_agentic_fix` with the paths to the source code, test file, prompt, and error log. Finally, the script reports the results of the agentic process, including the success status, the specific model used, the estimated cost, and the content of the patched code, providing a complete example of an automated debugging pipeline.",2025-12-22T05:48:00.060078+00:00
"context/agentic_fix_example_iteration_1.py","This Python script serves as a demonstration for the `run_agentic_fix` function, illustrating how to programmatically invoke an LLM-based agent to repair broken code. The script sets up a self-contained scenario within a temporary directory, generating a deliberately buggy Python file (a calculator `add` function that performs subtraction), a corresponding unit test that fails, and a prompt file instructing the agent to fix the issue.

The workflow includes a setup phase that executes `pytest` to generate a genuine error log, simulating a real-world failure state. It checks for the presence of required API keys (such as OpenAI, Anthropic, or Google) before proceeding. The `main` function then calls `run_agentic_fix` with the paths to the source code, test file, prompt, and error log. Finally, the script reports the results of the agentic process, including the success status, the specific model used, the estimated cost, and the content of the patched code, providing a complete example of an automated debugging pipeline.",2025-12-22T05:48:00.420473+00:00
"context/agentic_fix_example_iteration_2.py","This Python script serves as a demonstration for the `run_agentic_fix` function, illustrating how to programmatically invoke an LLM-based agent to repair broken code. The script sets up a self-contained scenario within a temporary directory, generating a deliberately buggy Python file (a calculator `add` function that performs subtraction), a corresponding unit test that fails, and a prompt file instructing the agent to fix the issue.

The workflow includes a setup phase that executes `pytest` to generate a genuine error log, simulating a real-world failure state. It checks for the presence of required API keys (such as OpenAI, Anthropic, or Google) before proceeding. The `main` function then calls `run_agentic_fix` with the paths to the source code, test file, prompt, and error log. Finally, the script reports the results of the agentic process, including the success status, the specific model used, the estimated cost, and the content of the patched code, providing a complete example of an automated debugging pipeline.",2025-12-22T05:48:00.560256+00:00
"context/agentic_fix_example_iteration_3.py","This Python script serves as a demonstration for the `run_agentic_fix` function, illustrating how to programmatically invoke an LLM-based agent to repair broken code. The script sets up a self-contained scenario within a temporary directory, generating a deliberately buggy Python file (a calculator `add` function that performs subtraction), a corresponding unit test that fails, and a prompt file instructing the agent to fix the issue.

The workflow includes a setup phase that executes `pytest` to generate a genuine error log, simulating a real-world failure state. It checks for the presence of required API keys (such as OpenAI, Anthropic, or Google) before proceeding. The `main` function then calls `run_agentic_fix` with the paths to the source code, test file, prompt, and error log. Finally, the script reports the results of the agentic process, including the success status, the specific model used, the estimated cost, and the content of the patched code, providing a complete example of an automated debugging pipeline.",2025-12-22T05:48:00.703090+00:00
"context/agentic_langtest_example.py","This Python module provides utilities for detecting project structures and generating test verification commands across multiple programming languages. The main file `pdd/agentic_langtest.py` contains several key functions: `_which()` checks if command-line tools exist in the system PATH, `_find_project_root()` traverses directories upward to locate project markers like `build.gradle`, `pom.xml`, or `package.json`, and `default_verify_cmd_for()` generates appropriate shell commands to run tests for Python (using pytest), JavaScript/TypeScript (using npm), and Java (supporting Maven and Gradle build systems). The `missing_tool_hints()` function detects missing development tools and provides platform-specific installation instructions for macOS and Ubuntu, covering npm, Java JDK, JUnit, and g++. The accompanying `example.py` demonstrates these utilities by creating temporary mock JavaScript and Python projects, generating verification commands, and checking for missing tools. The module uses the `rich` library for formatted console output with colored panels. This toolset appears designed for an agentic testing framework that needs to automatically detect and execute tests across different language ecosystems.",2025-12-14T08:29:26.961294+00:00
"context/agentic_update_example.py","This Python script serves as a demonstration for the `run_agentic_update` function within the `pdd` package. Its primary purpose is to illustrate how an AI agent can be used to automatically update a documentation or prompt file to match the current state of a codebase. The script begins by configuring the system path to ensure the `pdd` package is importable. It then establishes a local output directory to simulate a development environment. Inside this directory, it generates three files: an outdated prompt file (`math_module.prompt`) that only specifies an addition function, a Python source file (`math_module.py`) that implements both addition and subtraction, and a corresponding test file (`test_math_module.py`). Once the environment is prepared, the script executes `run_agentic_update`. This function triggers an AI agent to analyze the code and tests, then modify the prompt file to reflect the new subtraction feature. Finally, the script reports the execution details, including success status, API costs, the model used, and displays the content of the newly updated prompt file to verify the changes.",2025-12-22T04:24:49.894982+00:00
"context/agentic_verify_example.py","This Python script serves as a demonstration and test harness for the `run_agentic_verify` function within the `pdd` package. It illustrates how an agentic verification workflow operates by simulating the repair of a buggy code file based on a specification.

The script begins by configuring the system path to ensure the `pdd` library is importable. It then defines a `setup_environment` function that generates temporary files: a specification requiring an addition function, a buggy implementation using subtraction, a driver script to test the code, and a log file.

To run without external API dependencies, the script utilizes `unittest.mock` to patch internal functions. Specifically, it replaces the actual agent execution with `mock_agent_behavior`, which simulates an AI agent identifying the error, overwriting `calculator.py` with the correct logic, and returning a structured JSON success response.

In the `main` execution block, the script sets up the files, invokes `run_agentic_verify` using the mocked components, and outputs the results. It concludes by printing the verification metadata (cost, model, success status) and displaying the corrected file content to confirm the simulated agent successfully resolved the issue.",2025-12-22T04:25:21.931133+00:00
"context/anthropic_counter_example.py","This file contains commented-out Python code that demonstrates how to use the Anthropic API client to count tokens in a text string. The code shows the following steps: First, it imports the anthropic library. Then, it initializes an Anthropic client object. A sample text variable is defined with placeholder content (Sample text). The code then calls the client's count_tokens() method, passing in the text to get a token count. Finally, it prints the total number of tokens. Importantly, there is a comment noting that this token counting method is not accurate for version 3.0 and above models of Anthropic's API. The entire code block is commented out (using # symbols), suggesting it may be deprecated, under development, or kept as a reference example. This snippet would be useful for developers who need to estimate token usage before making API calls, though they should be aware of the accuracy limitation mentioned for newer model versions. The code follows standard Python conventions and uses f-string formatting for the output message.",2025-12-14T08:29:34.521936+00:00
"context/anthropic_thinking_test.py","This Python script is a test utility for validating LiteLLM API calls with Anthropic's Claude models, specifically focusing on the thinking or reasoning feature. The script begins by setting up configuration, including defining the project root path, locating the `.env` file for environment variables, and specifying two Claude models: `claude-3-5-haiku` as the main model and `claude-3-7-sonnet` as the thinking model.

It loads environment variables using `dotenv` and checks for the presence of the `ANTHROPIC_API_KEY`, exiting with an error if not found. The script then prepares a LiteLLM completion call with a sample prompt asking to explain recursion using an analogy. The call parameters include enabling the thinking feature with a token budget of 1024 and a maximum output of 50,000 tokens.

After executing the API call, the script attempts to extract any reasoning or thinking content from the response, checking both hidden parameters and the message content. It handles potential errors gracefully, specifically catching `UnsupportedParamsError` to indicate if the thinking parameter isn't supported, and provides informative output using the `rich` library for formatted console printing throughout the process.",2025-12-14T08:29:41.049639+00:00
"context/anthropic_tool_example.py","This Python code demonstrates how to use the Anthropic API to interact with Claude, specifically utilizing a text editor tool capability. The script begins by importing the Anthropic library and initializing a client instance. It then creates a message request to the Claude 3.7 Sonnet model (version dated February 19, 2025) with a maximum token limit of 1024.

The key feature of this code is the integration of a specialized tool called str_replace_editor of type text_editor_20250124. This tool enables Claude to perform text editing operations, which is particularly useful for tasks like fixing errors in documents.

The user message in this example asks Claude for help fixing a syntax error in a file called patent.md. By providing the text editor tool, Claude would be able to view, analyze, and suggest or make corrections to the file's content.

Finally, the script prints the response from the API. This code serves as a basic template for developers looking to leverage Claude's tool-use capabilities for document editing and error correction tasks, showcasing the integration between conversational AI and practical file manipulation functionality.",2025-12-14T08:29:49.494997+00:00
"context/auto_deps_main_example.py","This Python script defines a command-line interface (CLI) tool using the Click library for automated dependency analysis. The tool is named 'auto-deps' and provides several configurable options including: force mode for file overwrites, quiet mode to suppress output, strength and temperature parameters for dependency analysis, paths for input prompt files, directory scanning, CSV dependency files, and output locations, plus a force-scan option to rescan directories.

The main function initializes a Click context with the provided command-line parameters and calls the `auto_deps_main` function from the `pdd.auto_deps_main` module. This function processes a prompt file and analyzes dependencies based on the specified configuration. Upon successful execution, the script displays the modified prompt content, the total cost of the operation (formatted as currency), and the name of the model used for analysis.

Error handling is implemented to catch exceptions during processing, displaying error messages and aborting the Click command gracefully. The script uses a default strength value imported from the pdd module and sets a default temperature of 0.0. The entry point allows the script to be run directly as a standalone program, making it suitable for integration into development workflows or automation pipelines for managing project dependencies.",2025-12-14T08:29:57.189578+00:00
"context/auto_include_example.py","This Python script demonstrates the usage of an `auto_include` function from a custom `pdd` (likely Prompt-Driven Development) module. The main function reads a CSV file containing project dependencies and defines a detailed input prompt for generating unit tests using an LLM (Large Language Model).

The input prompt outlines a multi-step process for a Python function called generate_test that leverages Langchain to: load prompt templates from environment-specified paths, preprocess prompts, create LCEL templates, use an LLM selector for model choice, execute the model with specific parameters (prompt, code, language), track token usage and costs, detect incomplete generations, and handle post-processing.

The script configures parameters including a directory path pattern for context files (`context/c*.py`), default strength and temperature values (0.5), and verbose mode enabled. It then calls `auto_include()` with these parameters, which returns dependencies, CSV output, total cost, and model name.

Finally, the script prints the extracted dependencies, the CSV output, the total cost formatted in dollars per million tokens, and the selected model name. This appears to be part of a larger framework for automated code generation and testing using LLM-powered tools with cost tracking capabilities.",2025-12-14T08:30:04.774270+00:00
"context/auto_update_example.py","This Python script demonstrates the usage of the `auto_update` function from the `pdd.auto_update` module. The code provides a practical example of how to check for and perform package updates in a Python environment.

The `main()` function showcases three different ways to use `auto_update`:

1. **Basic usage**: Calling `auto_update()` without arguments checks for updates to the default 'pdd' package.

2. **Specific package check**: Passing a `package_name` parameter (e.g., requests) allows checking updates for any installed package.

3. **Version comparison**: Providing both `package_name` and `latest_version` parameters enables comparison against a known version number.

The function's workflow involves four steps: checking the currently installed version, comparing it with the latest available version (either from PyPI or a user-specified version), prompting the user if an upgrade is available, and performing the upgrade via pip if the user confirms. If the package is already up to date, no output is produced.

This utility is useful for maintaining package dependencies and ensuring software stays current with the latest releases. The script follows standard Python conventions with a `if __name__ == __main__` guard for proper module execution.",2025-12-14T08:30:13.045028+00:00
"context/autotokenizer_example.py","This Python script demonstrates how to count tokens in a text string using the Hugging Face Transformers library. The code defines a function called `count_tokens` that takes two parameters: the text to be tokenized and an optional model name (defaulting to deepseek-ai/deepseek-coder-7b-instruct-v1.5).

The function works by first loading the appropriate tokenizer for the specified model using `AutoTokenizer.from_pretrained()`. The `trust_remote_code=True` parameter allows the execution of custom code from the model repository, which is sometimes required for certain models. Once the tokenizer is loaded, it processes the input text and converts it into tokens. The function then returns the count of tokens by measuring the length of the 'input_ids' list in the tokenized output.

The script includes an example usage section that demonstrates the function with a sample prompt asking to Write a quick sort algorithm in Python. It calls the `count_tokens` function with this text and prints the resulting token count.

This utility is useful for developers working with large language models who need to track token usage for API rate limiting, cost estimation, or ensuring prompts fit within model context windows.",2025-12-14T08:30:20.403386+00:00
"context/bug_main_example.py","This Python script demonstrates how to use the `bug_main` function from the `pdd` library to automatically generate unit tests based on discrepancies between observed and desired program outputs. The script sets up a Click context with configuration options including force overwrite, output verbosity, model strength, and temperature settings. It then creates several example files in an output directory: a prompt file describing requirements for a function that sums even numbers from a list, a Python code file containing the `sum_even_numbers` function implementation, a main program file that imports and uses this function, and two output files representing the current (buggy) output and the desired correct output. The example illustrates a bug scenario where the function doesn't properly handle empty lists—returning 12 instead of 0. The `bug_main` function is called with all these file paths along with optional parameters for output location and language specification. Finally, the script prints the generated unit test, the total API cost in USD, and the model name used. This serves as a practical example for developers wanting to leverage AI-assisted debugging and test generation capabilities within the pdd framework.",2025-12-14T08:30:27.791361+00:00
"context/bug_to_unit_test_example.py","This Python script demonstrates the usage of a `bug_to_unit_test` function from the `pdd` module. The main purpose is to convert bug information into unit tests using an LLM (Large Language Model).

The script sets up several input parameters including `current_output` and `desired_output`, which contain debug trace logs showing differences between erroneous and expected behavior when matching prompt lines to code. It reads additional inputs from external files: a prompt file describing code generation instructions, the actual code under test from a module file, and a program context file.

Key configuration parameters include `strength` (LLM model strength), `temperature` (set to 0 for deterministic output), and `language` (Python). The script calls `bug_to_unit_test()` with these parameters to generate a unit test that captures the bug scenario.

The function returns three values: the generated unit test, the total API cost, and the model name used. Results are displayed using the Rich console library for formatted output. Error handling is implemented to catch and display any exceptions that occur during execution.

This appears to be part of a Prompt-Driven Development (pdd) toolset that helps developers create unit tests from observed bugs by analyzing discrepancies between actual and expected program outputs.",2025-12-14T08:30:35.754261+00:00
"context/bug_to_unit_test_failure_example.py","This file contains a Python function definition with a notable discrepancy between its name and implementation. The function is named add and accepts two parameters, x and y, suggesting it is intended to perform addition of two values. However, the actual implementation returns x-y, which performs subtraction rather than addition. This appears to be either a bug or an error in the code, as the function name implies addition but the operation performed is subtraction. The function is minimalistic, consisting of only two lines: the function definition line and a return statement. There are no docstrings, comments, type hints, or error handling included in this code snippet. If this function were called with arguments like add(5, 3), it would return 2 (the result of 5-3) instead of the expected 8 (5+3). This type of mismatch between function naming and actual behavior can lead to confusion and bugs in larger codebases, as developers would reasonably expect a function named add to perform addition. The code would benefit from either renaming the function to subtract or correcting the operation to use the addition operator (+) instead of subtraction (-).",2025-12-14T08:30:44.369048+00:00
"context/change/1/change.prompt","The provided text outlines a specific code modification directive aimed at refactoring token counting logic within a software application. It instructs the developer to replace the direct usage of the 'tiktoken' library with a utility named 'token_counter' sourced from the 'llm_selector' module. This change is specifically targeted at the functionality responsible for counting tokens within prompts. The shift implies an architectural decision to abstract the tokenization process, likely to ensure compatibility across different Large Language Models (LLMs) or to centralize dependency management within the 'llm_selector' component. By decoupling the token counting from a hard dependency on 'tiktoken', the system gains flexibility, allowing the 'llm_selector' to determine the appropriate counting method based on the active model context. This refactoring enhances code maintainability and ensures that token estimation remains accurate and consistent with the specific LLM being utilized by the application.",2025-12-20T06:25:41.954895+00:00
"context/change/1/final_code_generator_python.prompt","The provided file outlines the specifications for a Python function named `code_generator`, designed to compile a raw text prompt into a runnable code file using Langchain LCEL. Acting as an expert Python engineer, the developer is tasked with creating a workflow that accepts a prompt, target language, model strength, and temperature as inputs. The function integrates several helper modules to handle preprocessing, model selection, and postprocessing. The execution process consists of seven steps: preprocessing the raw prompt, creating a Langchain template, selecting the appropriate LLM, and running the chain. Throughout the process, the function utilizes the `rich` library to pretty-print console messages, Markdown formatted results, and token usage statistics. A key requirement is the calculation and display of the total financial cost based on input and output tokens. Finally, the function parses the model's mixed text and code output to extract clean, runnable code, returning both the code string and the total cost of the operation.",2025-12-20T06:25:42.443286+00:00
"context/change/1/initial_code_generator.py","This Python script defines a `code_generator` function designed to generate programming code from a text prompt using a Large Language Model (LLM). It utilizes the LangChain framework for orchestration, `tiktoken` for token counting, and the `rich` library for formatted console output. The process begins by preprocessing the input prompt and selecting an appropriate LLM configuration via a custom `llm_selector` based on provided strength and temperature parameters. It also initializes an SQLite cache to optimize repeated queries. During execution, the script calculates and displays the estimated cost for input tokens, invokes the model chain, and renders the output as Markdown. It subsequently calculates output costs and applies a post-processing step to extract the specific runnable code in the target language. The function returns a tuple containing the final executable code and the total cost of the operation, including input, output, and post-processing fees. Error handling is implemented to catch and report issues during the generation process.",2025-12-20T06:25:55.747945+00:00
"context/change/1/initial_code_generator_python.prompt","The provided file outlines the specifications for a Python function named code_generator designed to compile a raw prompt into an executable code file using Langchain. Acting as an expert Python engineer, the developer is tasked with creating a pipeline that accepts a prompt, target language, model strength, and temperature as inputs. The function's primary outputs are the runnable code string and the total cost of the operation. The implementation process is broken down into seven steps: preprocessing the raw prompt, creating a Langchain LCEL template, selecting the appropriate LLM, and running the model. The workflow requires using the rich library to pretty-print status updates, Markdown results, and token usage statistics calculated via tiktoken. Finally, the system must post-process the model's output to extract the code block and calculate the total cost based on input and output tokens. The prompt also references several context files that provide examples for specific sub-tasks like preprocessing, LLM selection, and post-processing.",2025-12-20T06:26:10.963694+00:00
"context/change/10/change.prompt","The provided text outlines a specific code modification requirement for the fix_errors_from_unit_tests function. The primary objective is to update the function's input parameters to include a new argument named error_file. This file serves as a destination for logging specific runtime data. Specifically, the system is required to append the output generated by the initial LCEL (LangChain Expression Language) run to this error_file. To ensure clarity and maintainability of the logs, the instruction emphasizes the necessity of inserting a distinct separator. This separator is intended to demarcate the existing content within the error_file from the newly appended LCEL output. This formatting requirement aims to facilitate easier debugging and analysis by allowing developers to clearly distinguish which parts of the log originated from specific function executions.",2025-12-20T06:26:27.410373+00:00
"context/change/10/final_fix_errors_from_unit_tests_python.prompt","This document outlines the specifications for creating a Python function named `fix_errors_from_unit_tests`, intended to automate the debugging process using Langchain LCEL. The function accepts inputs such as the unit test code, the source code, error details, a log file path, and LLM parameters (strength and temperature). It returns the fixed code, fixed unit test, update status flags, and the total computational cost. The implementation follows a detailed eleven-step workflow. It begins by loading specific prompts (`fix_errors_from_unit_tests_LLM.prompt` and `extract_unit_code_fix_LLM.prompt`) and reading the error log. The core logic involves two LLM invocations: the first generates a solution and logs the process using the `rich` library for pretty-printing and file appending; the second extracts structured data (fixed code and tests) from the first step's output using a JSON parser. The function must track token usage and costs for both steps using a custom `llm_selector`. Finally, it aggregates the costs and returns the parsed results. The specification emphasizes graceful error handling for file I/O operations and LLM interactions throughout the process.",2025-12-20T06:26:40.625536+00:00
"context/change/10/initial_fix_errors_from_unit_tests.py","This Python script implements an automated workflow for resolving unit test failures using Large Language Models (LLMs) and the LangChain framework. The primary function, `fix_errors_from_unit_tests`, takes a unit test, source code, error details, and model configuration parameters as input. It initializes a SQLite cache to optimize resource usage. The logic executes a two-stage process. First, it retrieves prompt templates from a specified directory and uses an LLM to generate a descriptive solution for the errors, calculating and displaying token costs via the `rich` library. Second, it runs a follow-up chain with a JSON output parser to extract specific code and unit test corrections from the previous generation. The function returns a tuple containing boolean flags for updates, the fixed code strings, and the total execution cost, while including error handling to manage exceptions gracefully.",2025-12-20T06:26:58.987378+00:00
"context/change/10/initial_fix_errors_from_unit_tests_python.prompt","The provided file outlines the specifications for creating a Python function named fix_errors_from_unit_tests, designed to resolve unit test failures using Large Language Models (LLMs) via Langchain. The function accepts the unit test, the code under test, the error message, and model parameters (strength and temperature) as inputs. It returns boolean flags indicating updates, the fixed code and test strings, and the total execution cost. The implementation follows a ten-step process utilizing Langchain's Expression Language (LCEL). It begins by loading prompt templates from a path defined by the $PDD_PATH environment variable. The process involves two LLM invocations: the first generates a textual solution to the error, while the second extracts structured data (fixed code and status flags) from that solution using a JSON output parser. The function leverages the rich library for pretty-printing console output, including markdown rendering and token cost tracking. It relies on a helper llm_selector for model instantiation and token counting. Finally, the function aggregates the costs from both steps and returns the structured fixes and total cost, ensuring robust error handling throughout.",2025-12-20T06:27:18.843470+00:00
"context/change/11/change.prompt","The file provides instructions for creating a Python function named change. This function is designed to utilize a specific prompt file located at 'prompts/change_LLM.prompt'. The implementation is required to use LCEL (LangChain Expression Language) and include post-processing steps consistent with the current prompt context. The primary goal of the function is to output a modified prompt. Additionally, the text defines the specific inputs and outputs for the underlying 'change_LLM' prompt. The required inputs are 'input_prompt' (the original prompt string), 'input_code' (the code generated from the original prompt), and 'change_prompt' (instructions on how to modify the original prompt). The expected output is defined as 'modified_prompt', which represents the updated prompt string resulting from the specified changes.",2025-12-20T06:27:36.058201+00:00
"context/change/11/initial_code_generator.py","The provided code implements a Python-based pipeline named `code_generator` designed to generate executable code from natural language prompts using Large Language Models (LLMs) and the LangChain framework. The script integrates external libraries such as `rich` for formatted console output and relies on custom modules for preprocessing, model selection, and postprocessing. The core function, `code_generator`, orchestrates a sequential workflow: it first preprocesses the raw prompt and initializes a LangChain template. It then utilizes an `llm_selector` to choose a model based on desired strength and temperature parameters. The script executes the prompt through the selected model, tracking and displaying token usage and estimated financial costs for both input and output. Following generation, the raw text undergoes postprocessing to extract clean, runnable code. The function ultimately returns the final code and the total cost. The file concludes with a `__main__` block that demonstrates the tool's functionality by generating a Python function to calculate factorials.",2025-12-20T06:27:48.275381+00:00
"context/change/11/initial_code_generator_python.prompt","The provided file outlines the specifications for a Python function named `code_generator`, designed to compile a raw text prompt into a runnable code file using Langchain LCEL. Acting as an expert Python engineer, the developer is tasked with creating a workflow that accepts a prompt, target language, model strength, and temperature as inputs. The function integrates several helper modules to handle preprocessing, model selection, and postprocessing. The execution process consists of seven steps: preprocessing the raw prompt, creating a Langchain template, selecting the appropriate LLM, and running the chain. Throughout the process, the function utilizes the `rich` library to pretty-print console messages, Markdown formatted results, and token usage statistics. A key requirement is the calculation and display of the total financial cost based on input and output tokens. Finally, the function parses the model's mixed text and code output to extract clean, runnable code, returning both the code string and the total cost of the operation.",2025-12-20T06:28:07.472190+00:00
"context/change/11/initial_split_python.prompt","This document outlines the specifications for developing a Python function named `split`, designed to divide a given `input_prompt` into a `sub_prompt` and a `modified_prompt` while maintaining functionality. Acting as an expert Python Software Engineer, the developer must utilize the Langchain LCEL framework and the Python Rich library for formatted console output. The function requires inputs including the original prompt, generated code, example code, and model parameters like strength and temperature. The workflow involves loading and preprocessing specific prompt files (`split_LLM.prompt` and `extract_prompt_split_LLM.prompt`). The function executes a two-step LLM process: first, generating a raw split based on the inputs, and second, extracting the specific prompt components into a JSON format. Throughout execution, the function must utilize an `llm_selector` to manage model selection and calculate token usage and costs. The final output includes the two resulting prompt strings and the total financial cost of the operations. The implementation must also include robust error handling for missing inputs or model failures.",2025-12-20T06:28:21.093634+00:00
"context/change/12/change.prompt","The provided file content consists of a single directive related to prompt engineering and code generation. It instructs the user or system to formulate a specific prompt that will, in turn, create a function named unfinished_prompt. The intended purpose of this function is to handle the execution of a Large Language Model prompt identified as unfinished_prompt_LLM. The text utilizes XML-style tags to reference an external resource, specifically using an include tag to point to the file path prompts/unfinished_prompt_LLM.prompt. This suggests a meta-programming context where the goal is to automate the creation of a handler function for a specific, externally stored prompt.",2025-12-20T06:28:35.789190+00:00
"context/change/12/final_unfinished_prompt_python.prompt","The provided text defines the specifications for developing a Python function named 'unfinished_prompt'. This function is intended to evaluate a given 'prompt_text' to determine if it is syntactically or semantically complete using a Large Language Model (LLM). The function accepts inputs for the prompt text, model strength, and temperature. It operates by loading a specific prompt template from the project directory defined by the '$PDD_PATH' environment variable and constructing a Langchain LCEL chain that outputs JSON. The workflow includes selecting an appropriate LLM using a 'llm_selector' utility, calculating input and output token costs, and invoking the model. The function is required to provide user feedback via the 'rich' library, displaying the reasoning, completion status ('is_finished'), and financial cost of the operation. Ultimately, it returns the structured reasoning, the boolean completion status, and the total cost. The instructions also emphasize the need for graceful error handling and the use of relative imports for internal dependencies.",2025-12-20T06:28:48.334592+00:00
"context/change/12/initial_postprocess.py","This file defines a Python function named `postprocess` designed to extract and format programming code from raw text output generated by a Large Language Model (LLM). It utilizes the LangChain framework and Pydantic for structured data handling to ensure reliable code extraction. The function accepts the raw LLM output, the target programming language, and parameters for model strength and temperature.

The workflow first checks the `strength` parameter; if set to zero, it delegates to a simpler heuristic function (`postprocess_0`). For higher strengths, it loads a specific prompt template from a directory defined by the `PDD_PATH` environment variable. It then selects an appropriate LLM using `llm_selector` and constructs a processing chain to parse the input into an `ExtractedCode` object. The function includes logic to calculate and print token usage and estimated costs using the `rich` library. It also cleans the output by removing markdown backticks before returning the sanitized code string and the total cost. Error handling is implemented to catch exceptions and return a default empty result.",2025-12-20T06:29:02.854082+00:00
"context/change/12/initial_postprocess_python.prompt","The provided text outlines the specifications for developing a Python function named `postprocess`. This function is designed to refine raw string output from a Large Language Model (LLM), which typically contains a mix of conversational text and code blocks, into a clean, executable code string. The function accepts inputs for the raw output, target language, model strength, and temperature. The logic dictates that if the `strength` is set to 0, a zero-cost method named `postprocess_0` is utilized. For other strength values, the function employs a Langchain LCEL pipeline. This involves loading a specific prompt file based on the `$PDD_PATH` environment variable, selecting an appropriate LLM via `llm_selector`, and processing the input to return a JSON object containing the extracted code. Key operational steps include calculating and printing token usage and costs, stripping markdown backticks from the output, and displaying the result using the `rich` library. The function must handle errors gracefully, use relative imports for dependencies, and ultimately return the `extracted_code` string alongside the `total_cost`.",2025-12-20T06:29:19.245149+00:00
"context/change/13/change.prompt","The provided text outlines the specifications for a Python function named 'continue_generation', which is designed to manage and finalize incomplete text outputs from a Large Language Model (LLM). The function accepts a formatted input prompt and the initial output from the LLM as arguments. It relies on a specific prompt template located at 'prompts/continue_generation_LLM.prompt' to facilitate the continuation process. The operational logic involves a helper function called 'unfinished_prompt' that detects whether the model's generation has stopped prematurely. If the output is determined to be incomplete, the function enters a loop where it repeatedly runs the continuation prompt. The resulting text from each iteration is appended to the cumulative 'LLM_OUTPUT'. This cycle continues until the detection logic confirms that the generation is fully complete, ensuring that the final result is a comprehensive and finished response.",2025-12-20T06:29:34.655066+00:00
"context/change/13/initial_split.py","This Python script defines a function named `split` designed to process and split input code using a Large Language Model (LLM) via the LangChain framework. It utilizes the `rich` library for formatted console output and `SQLiteCache` for caching LLM responses to optimize cost and speed. The core functionality involves loading external prompt templates from files, preprocessing them, and constructing a LangChain execution chain. The script dynamically selects an LLM based on provided `strength` and `temperature` parameters using a custom `llm_selector` module. It executes a two-step process: first, generating a raw split response based on input prompts and code; and second, parsing that response into structured JSON to extract a `sub_prompt` and a `modified_prompt`. The function also calculates and reports the estimated cost based on token usage. Error handling is included to manage exceptions during execution. An `if __name__ == __main__:` block is provided to demonstrate example usage of the `split` function with placeholder data.",2025-12-20T06:29:55.190175+00:00
"context/change/13/initial_split_python.prompt","This document outlines the specifications for developing a Python function named `split`, designed to decompose a single `input_prompt` into a `sub_prompt` and a `modified_prompt` while maintaining original functionality. The function operates within a Python package structure, utilizing relative imports and the Rich library for formatted console output. The function requires inputs including the original prompt, generated code, example usage code, and LLM parameters (strength and temperature). It involves a multi-step process using Langchain LCEL. First, it loads and preprocesses specific prompt templates (`split_LLM.prompt` and `extract_prompt_split_LLM.prompt`). It then utilizes an `llm_selector` for model selection and token counting. The workflow executes two main LLM invocations: first to generate the split logic based on the inputs, and second to extract the resulting prompts into a structured JSON format. Throughout the process, the function must calculate and display token counts and financial costs. The final output includes the extracted `sub_prompt`, `modified_prompt`, and the `total_cost` of the operations. The implementation must also handle edge cases and ensure robust error reporting.",2025-12-20T06:30:10.144530+00:00
"context/change/13/modified_initial_split.prompt","The provided file outlines the specifications for a Python function named `continue_generation`, designed to ensure the completeness of Large Language Model (LLM) outputs. Intended for use within a Python package, the function accepts a `formatted_input_prompt` and an initial `llm_output` as inputs. Its workflow involves loading and preprocessing a specific prompt template, configuring a Langchain LCEL pipeline, and utilizing an `llm_selector` for model interaction and token counting. The function implements a loop that detects incomplete generations using an `unfinished_prompt` utility and iteratively appends content until the output is finished. Throughout the process, it uses the `rich` library to pretty-print console outputs, including token counts and cost estimates. The function is required to return the final, complete string and the total cost of the operation, while also handling edge cases such as missing inputs or model errors.",2025-12-20T06:30:34.552812+00:00
"context/change/14/change.prompt","The provided text outlines the specifications for a prompt designed to generate Python code for a function named 'detect_change'. The primary objective of this function is to identify necessary updates within a collection of prompt files and generate a list of these changes. The function is defined to accept two specific inputs: a list of prompt filenames and a description of the desired change. To accomplish its task, the generated code relies on two external prompt dependencies included via XML tags. First, it utilizes the 'detect_change_LLM' prompt (sourced from 'prompts/detect_change_LLM.prompt') to analyze the inputs. Second, it employs the 'extract_detect_change_LLM' prompt (sourced from 'prompts/extract_detect_change_LLM.prompt') to isolate the specific prompts that require modification and to detail the exact nature of those changes. Essentially, this file serves as a meta-prompt or blueprint for creating a utility that automates the detection and extraction of prompt engineering updates based on a high-level change description.",2025-12-20T06:30:52.826099+00:00
"context/change/14/initial_change.py","This Python script defines a function named `change` designed to modify input prompts and code using Large Language Models (LLMs). Leveraging the LangChain framework and the Rich library for console output, the function orchestrates a workflow that involves loading external prompt templates and selecting an appropriate LLM based on provided strength and temperature parameters. The process executes in two main stages: first, a 'change chain' processes the original prompt, code, and a description of the desired change to generate a raw response; second, an 'extract chain' parses this response to isolate the specific modified prompt in JSON format. Throughout the execution, the function calculates and reports token usage and estimated costs for both input and output operations. It includes error handling for file access and parsing issues, ultimately returning a tuple containing the modified prompt string, the total calculated cost, and the name of the model used.",2025-12-20T06:31:09.848804+00:00
"context/change/14/initial_change_python.prompt","The provided file details the specifications for creating a Python function named 'change', intended to modify an input prompt based on a change prompt and associated code. The function requires inputs such as 'input_prompt', 'input_code', 'change_prompt', 'strength', and 'temperature', and returns a 'modified_prompt', 'total_cost', and 'model_name'. The implementation must utilize the Langchain LCEL framework and the Python Rich library for formatted console output. Key steps include loading and preprocessing specific prompt files, selecting an LLM using a custom 'llm_selector', and performing two model invocations: one to generate the change logic and another to extract the result as JSON. The function is responsible for calculating and displaying token counts and costs for both steps. Additionally, it must handle edge cases and errors effectively while using relative imports for dependencies.",2025-12-20T06:31:28.038562+00:00
"context/change/14/modified_initial_change.prompt","The provided text outlines the specifications for a Python function named detect_change, designed to analyze a list of prompt files and a change description to identify necessary modifications. Acting as an expert Python Software Engineer, the developer must implement this function using the Python Rich library for formatted console output and relative imports. The function accepts inputs such as a list of prompt filenames, a change description, and LLM parameters (strength and temperature). It returns a list of JSON objects detailing required changes, the total execution cost, and the model name used. The implementation follows an eight-step process involving loading specific prompt templates, preprocessing them, and utilizing Langchain LCEL for model invocation. It performs a two-stage LLM process: first to analyze the prompts based on the description, and second to extract the results into a structured JSON format. Throughout execution, the function must calculate and display token counts and costs, while also handling edge cases and errors robustly.",2025-12-20T06:31:43.553058+00:00
"context/change/15/README.md","The provided text documents the PDD (Prompt-Driven Development) Command Line Interface, a version 0.1.0 tool designed to accelerate coding workflows using AI models. PDD enables developers to generate runnable code, create usage examples, and produce unit tests directly from structured prompt files. It supports a wide array of programming languages, including Python, Java, and C++, relying on a specific file naming convention (`<basename>_<language>.prompt`).

The CLI offers a suite of commands such as `generate`, `test`, `fix` (which includes an iterative repair loop), `preprocess`, and `split` for managing complex prompts. It also facilitates maintenance through commands like `update`, `change`, `detect`, and `conflicts` to keep prompts aligned with code evolution. Users can fine-tune AI performance using global options for model strength and temperature, while the `--output-cost` feature tracks usage expenses in CSV format.

Notable features include multi-command chaining for efficient pipeline execution, flexible output path configuration via environment variables, and built-in help. The documentation also covers security considerations regarding generated code and data privacy, positioning PDD as a versatile utility for initial development, refactoring, and continuous integration.",2025-12-20T06:31:58.532992+00:00
"context/change/15/change.prompt","The provided text is a concise instruction regarding an update to a prompt. It notes that new commands have been introduced and are documented within a specific README file located at 'context/change/15/README.md'. The directive explicitly requests that these new commands, citing 'detect' as a specific example, be added to the prompt to ensure it reflects the recent changes.",2025-12-20T06:32:15.557236+00:00
"context/change/15/initial_cli.py","The provided file implements a Command Line Interface (CLI) tool called PDD (Prompt-Driven Development) using the `click` and `rich` libraries. It serves as a comprehensive suite for AI-assisted coding tasks, facilitating the generation, testing, and maintenance of code through natural language prompts.

Key functionalities include:
- **Code Generation**: The `generate` command creates runnable code from prompt files.
- **Testing and Debugging**: The `test` command generates unit tests, while `fix` attempts to resolve errors in code and tests, optionally using an iterative loop with a verification program.
- **Prompt Management**: Commands like `preprocess`, `split`, `change`, and `update` allow users to prepare, divide, modify, and synchronize prompts with evolving codebases.
- **Examples**: The `example` command generates reference examples from existing code and prompts.
- **Utilities**: It includes features for cost tracking (logging usage to CSV), shell completion installation, and caching via SQLite to optimize performance and cost.

The tool integrates with various external modules to perform underlying AI operations and manages file paths, user configuration (strength, temperature), and feedback via a polished console interface.",2025-12-20T06:32:27.290651+00:00
"context/change/15/initial_cli_python.prompt","The provided text outlines the specifications for developing a Python command-line interface (CLI) tool named pdd. Designed for an expert Python engineer, the tool utilizes the click library for CLI management and the rich library for formatted console output. The project structure includes specific directories for source code, prompts, context, and data. The CLI supports a variety of commands aimed at automating software development tasks. Key functionalities include generate for creating code from prompts, example for deriving context from code, and test for generating unit tests. It offers prompt manipulation tools like preprocess (with an XML option), split, change, and update. Additionally, the tool includes debugging capabilities via the fix command, which resolves errors based on unit test feedback, and a fix --loop sub-command for iterative fixing. Finally, it provides an install_completion command to configure shell autocompletion. The specification references external example files to guide the implementation of file loading, path construction, and specific logic for each command.",2025-12-20T06:32:37.540236+00:00
"context/change/15/modified_initial_cli.prompt","The provided text outlines the specifications for developing a Python command-line interface (CLI) tool named pdd. This tool utilizes the `click` library for CLI management and the `rich` library for formatted console output. The project structure is defined with specific directories for source code, prompts, context, and data.

The `pdd` tool offers a comprehensive suite of commands designed to automate code generation and maintenance tasks. Key features include generating code from prompts (`generate`), creating examples (`example`), and producing unit tests (`test`). It includes robust debugging capabilities through the `fix` command (with a looping option) and a `crash` handler.

Furthermore, the tool focuses heavily on prompt engineering workflows, providing commands to `preprocess`, `split`, `change`, `update`, and `detect` changes in prompt files, as well as resolving `conflicts` between them. Finally, it includes an `install_completion` command for shell integration. The document relies on referenced example files to guide the implementation of these functions, ensuring consistent handling of file inputs and path construction.",2025-12-20T06:32:53.070301+00:00
"context/change/16/change.prompt","The provided file contents outline a set of instructions for updating a prompt engineering or code correction workflow. The directives focus on replacing specific components related to error handling. Specifically, the instruction mandates changing the 'fix_error_loop' prompt by including content from 'context/change/16/fix_error_loop_python.prompt'. Furthermore, it directs a structural change in the logic: instead of invoking the 'fix_errors_from_units_tests' function, the system should now call 'fix_code_module_errors'. To facilitate this transition, the file points to the relevant example code for the new function ('fix_code_module_errors_example.py') and supplies the associated prompt file ('fix_code_module_errors_python.prompt'). All referenced files are located within the 'context/change/16/' directory, indicating a specific version or iteration of the codebase changes. The overall goal is to refactor the error-fixing mechanism from a unit-test-based approach to a module-error-based approach.",2025-12-20T06:33:08.373612+00:00
"context/change/16/fix_code_module_errors_example.py","This Python script serves as a demonstration and usage example for the `fix_code_module_errors` function from the `pdd` package. The script defines a `main` function that sets up a simulated debugging scenario involving a piece of Python code with a deliberate `TypeError` (attempting to sum a string). It constructs the necessary input variables, including the buggy program source, the original prompt used to generate it, the specific code module, and the corresponding error message traceback. The script then calls `fix_code_module_errors` with these inputs, along with configuration parameters for model strength and temperature. The function returns the fixed program, fixed code module, boolean flags indicating if updates were necessary, the total cost of the operation, and the name of the model used. Finally, the script prints these results to the console to verify the automated code repair process.",2025-12-20T06:33:28.541650+00:00
"context/change/16/fix_code_module_errors_python.prompt","The provided file outlines instructions for an expert Python Software Engineer to generate a function named `fix_code_module_errors`. This function is designed to resolve runtime errors or crashes within a specific code module. The function accepts inputs including the running program, the generating prompt, the problematic code, error logs, and LLM configuration parameters (strength and temperature).

The implementation requires using the LangChain Expression Language (LCEL). The process involves a multi-step workflow: first, loading specific prompt templates (`fix_code_module_errors_LLM.prompt` and `extract_program_code_fix_LLM.prompt`) based on the `$PDD_PATH` environment variable. It then utilizes an internal `llm_selector` module to choose an appropriate model. The workflow executes two distinct LCEL chains. The first chain analyzes the inputs to generate a fix, printing token usage and costs. The second chain processes the output of the first to extract structured JSON data, specifically looking for boolean flags (`update_program`, `update_code`) and the corrected code strings. Finally, the function calculates the total cost of the LLM interactions and returns the fixed components, update status flags, total cost, and the model name used.",2025-12-20T06:33:44.723368+00:00
"context/change/16/fix_error_loop.py","The provided code defines a Python module designed to iteratively fix errors in unit tests and corresponding source code, likely utilizing an AI-based backend. The core functionality is encapsulated in the `fix_error_loop` function, which orchestrates a cycle of testing, error analysis, and code correction.

The process begins by validating inputs and initializing an error log. Inside a loop limited by `max_attempts`, the script executes `pytest` to identify failures. If tests pass, the loop terminates early. Otherwise, it parses the output to quantify errors and creates backup copies of the current files. It then invokes an external fixing function (`fix_errors_from_unit_tests`) to generate corrected versions of the unit test or code file based on the error logs.

Crucially, the script enforces quality control: if the source code is modified, a separate verification program is run. If verification fails, the changes are reverted. The system tracks the best iteration (fewest errors) throughout the process using the `IterationResult` dataclass. Finally, after the loop concludes or the budget is exceeded, it runs a final test. If the final state is worse than a previous iteration, the script restores the files from that best-performing iteration before returning the final status, file contents, and cost metrics.",2025-12-20T06:34:00.810159+00:00
"context/change/16/fix_error_loop_python.prompt","The provided text outlines the specifications for a Python function named fix_error_loop, designed to iteratively repair errors in unit tests and their corresponding source code. Acting as an expert Python Software Engineer, the function utilizes a Large Language Model (LLM) to attempt fixes within a specified budget and iteration limit. The process begins by initializing counters and removing old logs. It enters a loop where it executes the unit test file using pytest. If tests pass, the loop terminates successfully. If failures occur, the function parses the error output, creates versioned backups of the files, and invokes a helper function, fix_errors_from_unit_tests, to generate corrections based on the error logs. The function manages file updates carefully: if the source code is modified, a separate verification program checks for regression. If verification fails, the system restores the previous working state. Throughout the process, the function tracks the best iteration (lowest number of fails/errors). Upon exhaustion of attempts or budget, or upon success, it ensures the best version is restored and returns the final status, file contents, total cost, and attempt count.",2025-12-20T06:34:16.879173+00:00
"context/change/16/fix_errors_from_unit_tests_example.py","This Python script serves as a demonstration and entry point for the `fix_errors_from_unit_tests` function within the `pdd` package. Its primary purpose is to illustrate how the system attempts to resolve discrepancies between code and unit tests using an LLM-based approach. The script defines a `main` function that initializes a specific test case involving a simple arithmetic addition function. It intentionally includes a flawed unit test—asserting that zero plus zero equals one—alongside a simulated `AssertionError` message to trigger the fix logic. The script configures various parameters for the fix process, including the prompt description, a log file path (`error_logs.txt`), and LLM settings like strength (0.7) and temperature (0.5). It then invokes `fix_errors_from_unit_tests` with these inputs. Upon execution, the script captures the returned data, which consists of boolean flags indicating updates, the corrected source code and unit tests, the total financial cost of the operation, and the specific model identifier used. Finally, it utilizes the `rich` library to print a formatted summary of these results to the console, allowing users to verify the automated debugging process.",2025-12-20T06:34:31.759999+00:00
"context/change/16/modified_fix_error_loop_python.prompt","This document outlines the specifications for a Python function named `fix_code_loop`, designed to iteratively repair errors in a code module using a Large Language Model (LLM). Acting as an expert Python Software Engineer, the function accepts inputs such as file paths for the code and verification program, the original generation prompt, model parameters (strength, temperature), a maximum attempt limit, and a financial budget.

The core logic involves a loop that executes the verification program and captures any errors in a log file. If errors occur, the function backs up the current files and invokes a helper function, `fix_code_module_errors`, to generate corrections based on the error log. The function tracks the total cost and attempts, ensuring the budget is not exceeded. If the applied fixes fail subsequent verification, the system restores the previous file versions to maintain stability. The process repeats until the code passes verification, the maximum attempts are reached, or the budget is exhausted. The function ultimately returns the success status, the final content of the code and verification program, the total cost, and the number of attempts made.",2025-12-20T06:34:49.033409+00:00
"context/change/17/change.prompt","The provided file contents serve as a directive for updating a prompt due to changes in the underlying codebase methods. Specifically, it notes that the current method has been replaced by 'llm_invoke' and 'load_prompt_template'. The instruction requires modifying the prompt to align with these new function calls. To assist with this refactoring, the text provides references to specific example files demonstrating the change. It includes a 'before_example' pointing to 'context/change/17/unfinished_prompt_python.prompt' and an 'after_example' pointing to 'context/change/17/updated_unfinished_prompt_python.prompt'. Essentially, this is a migration note intended to guide a developer in updating prompt logic to use the new invocation and template loading mechanisms.",2025-12-20T06:35:04.224145+00:00
"context/change/17/continue_generation.py","This Python script implements a robust mechanism for continuing and refining text generation, specifically targeting code blocks, using Large Language Models (LLMs) within the LangChain ecosystem. The primary function, `continue_generation`, orchestrates an iterative process to extend incomplete LLM outputs. It begins by loading and preprocessing prompt templates from a specified file path (`PDD_PATH`). The script employs Pydantic models (`TrimResultsOutput` and `TrimResultsContinuedOutput`) to structure the parsing of model responses.

Using a dynamic `llm_selector`, the code initializes chains for generation and result trimming. It first extracts the relevant code block from the initial output. Then, it enters a loop where it generates subsequent text segments. After each iteration, it utilizes an external `unfinished_prompt` function to analyze the trailing characters and determine if the generation is complete. If the content is deemed unfinished, the loop continues; otherwise, the script performs a final trim to ensure clean output. Throughout the execution, the script tracks token usage and calculates financial costs, displaying progress via the `rich` console library. Finally, it returns the fully generated code block, the total accumulated cost, and the name of the model used.",2025-12-20T06:35:16.947073+00:00
"context/change/17/continue_generation_python.prompt","The provided file outlines the specifications for a Python function named continue_generation, designed to finalize incomplete text generation from a Large Language Model (LLM). Acting as an expert Python Software Engineer, the function takes a formatted input prompt, partial LLM output, and model parameters (strength and temperature) to produce a complete final_llm_output, along with the total execution cost and model name. The process involves several distinct steps utilizing Langchain LCEL. First, it loads and preprocesses specific prompt files from a directory defined by the $PDD_PATH environment variable. It then configures the LLM using an llm_selector module, tracking token usage and costs throughout execution. The core logic extracts the existing code block from the initial output, then generates continuations. It employs a loop that checks for completeness using an unfinished_prompt helper function. If the output is detected as incomplete, the function iteratively generates and appends text until finished. Once complete, it trims the results to ensure a seamless transition between the original and generated text. Finally, the function prints the result using Rich Markdown and returns the consolidated output, total cost, and model name.",2025-12-20T06:35:34.922455+00:00
"context/change/17/unfinished_prompt_python.prompt","The provided file defines the specifications for a Python function named 'unfinished_prompt', designed to evaluate whether a given text prompt is complete or requires continuation. Acting as an expert Python engineer, the function takes 'prompt_text', 'strength', and 'temperature' as inputs. It relies on the Langchain LCEL framework and a custom 'llm_selector' module to perform the analysis. The implementation involves loading a specific prompt template ('unfinished_prompt_LLM.prompt') using the '$PDD_PATH' environment variable. The function executes the prompt against a selected LLM, calculating and displaying token usage and costs. It is required to parse the LLM's JSON output to extract a 'reasoning' string and an 'is_finished' boolean. Furthermore, the function must utilize the 'rich' library to pretty-print the status, reasoning, and cost details to the console. The final output returns the reasoning, completion status, total cost, and the model name used.",2025-12-20T06:35:51.989750+00:00
"context/change/17/updated_continue_generation_python.prompt","The provided file outlines the specifications for developing a Python function named continue_generation. This function is designed to complete partial outputs generated by a Large Language Model (LLM). Acting as an expert Python Software Engineer, the developer must implement a workflow that accepts a formatted input prompt, existing LLM output, and configuration parameters such as strength and temperature. The process involves loading and preprocessing specific prompt templates, including continue_generation_LLM and trim_results_LLM. The core logic executes a loop that repeatedly invokes the LLM to extend the text, utilizing an unfinished_prompt utility to analyze the last 600 characters for completeness. If the generation is deemed unfinished, the loop persists; otherwise, a final trimming step refines the output. The function ultimately returns the consolidated text string, the total cost of LLM invocations, and the model name, while incorporating error handling and optional verbose logging.",2025-12-20T06:36:05.143578+00:00
"context/change/17/updated_unfinished_prompt_python.prompt","The provided text defines the requirements for an expert Python engineer to implement a function called 'unfinished_prompt'. The primary objective of this function is to evaluate a specific string, 'prompt_text', to determine if the prompt is syntactically or semantically complete or if it requires further continuation. The function signature accepts inputs for the prompt text, model strength (defaulting to 0.5), temperature (defaulting to 0), and a verbose logging flag. It returns a tuple consisting of a reasoning string, a boolean indicating completion ('is_finished'), the total cost, and the model name. Implementation steps specify loading a prompt template named 'unfinished_prompt_LLM' and utilizing an internal 'llm_invoke' mechanism. The input text is passed as a parameter ('PROMPT_TEXT') to the model, which is expected to return a Pydantic object containing the analysis results. The file includes references to external context files for preambles and code examples regarding template loading and LLM invocation to guide the development process.",2025-12-20T06:36:21.550167+00:00
"context/change/18/change.prompt","The provided file contents serve as a directive for updating a prompt due to changes in the underlying codebase methods. Specifically, it notes that the current method has been replaced by 'llm_invoke' and 'load_prompt_template'. The instruction requires modifying the prompt to align with these new function calls. To assist with this refactoring, the text provides references to specific example files demonstrating the change. It includes a 'before_example' pointing to 'context/change/17/unfinished_prompt_python.prompt' and an 'after_example' pointing to 'context/change/17/updated_unfinished_prompt_python.prompt'. Essentially, this is a migration note intended to guide a developer in updating prompt logic to use the new invocation and template loading mechanisms.",2025-12-20T06:36:35.861207+00:00
"context/change/18/code_generator.py","The provided code defines a `code_generator` function that orchestrates the generation of programming code using Large Language Models (LLMs) via the LangChain framework. It utilizes the `rich` library for enhanced console logging and imports several local helper modules for specific tasks. The function's workflow begins by preprocessing the input prompt and selecting an appropriate LLM based on specified strength and temperature parameters. It then executes the prompt, tracking token usage and calculating associated costs. A significant aspect of the logic involves checking if the generated output is complete; if the model cuts off, a continuation process is triggered, whereas complete outputs undergo post-processing to refine the code for the target language. The function handles errors gracefully and returns a tuple containing the final runnable code, the total calculated cost of the operation, and the name of the model used.",2025-12-20T06:36:49.298632+00:00
"context/change/18/code_generator_python.prompt","This file outlines the specifications for an expert Python engineer to develop a function named code_generator. The function's primary purpose is to compile a raw text prompt into a runnable code file in a specified language (e.g., Python, Bash). The function accepts inputs for the prompt, target language, model strength, and temperature, and returns the resulting code, total execution cost, and the model name used. The implementation requires using LangChain Expression Language (LCEL) and follows a specific eight-step workflow. This workflow includes preprocessing the prompt, selecting an appropriate Large Language Model (LLM) via an llm_selector, and executing the prompt while tracking token usage and costs. The process handles output generation robustly by detecting incomplete responses using an unfinished_prompt utility; if incomplete, it triggers a continuation mechanism, otherwise, it applies post-processing. The file provides context through included examples for internal modules like preprocessing, token counting, and post-processing, ensuring the generated code adheres to specific internal standards and error-handling procedures.",2025-12-20T06:37:05.779893+00:00
"context/change/18/unfinished_prompt_python.prompt","The provided file defines the specifications for a Python function named 'unfinished_prompt', designed to evaluate whether a given text prompt is complete or requires continuation. Acting as an expert Python engineer, the function takes 'prompt_text', 'strength', and 'temperature' as inputs. It relies on the Langchain LCEL framework and a custom 'llm_selector' module to perform the analysis. The implementation involves loading a specific prompt template ('unfinished_prompt_LLM.prompt') using the '$PDD_PATH' environment variable. The function executes the prompt against a selected LLM, calculating and displaying token usage and costs. It is required to parse the LLM's JSON output to extract a 'reasoning' string and an 'is_finished' boolean. Furthermore, the function must utilize the 'rich' library to pretty-print the status, reasoning, and cost details to the console. The final output returns the reasoning, completion status, total cost, and the model name used.",2025-12-20T06:37:21.141883+00:00
"context/change/18/updated_code_generator_python.prompt","The provided file outlines the specifications for creating a Python function named `code_generator`, designed to compile a raw text prompt into a runnable code file. Acting as an expert Python engineer, the function accepts inputs including the raw prompt, target programming language, model strength, temperature, and a verbose flag. It returns the generated code, the total cost of the operation, and the name of the LLM used. The execution flow involves several distinct steps: preprocessing the input prompt, loading prompt templates with specific configurations, and invoking the LLM. The function includes logic to detect incomplete code generation by analyzing the last 600 characters of the output; if incomplete, it triggers a continuation process, otherwise, it applies post-processing. It also mandates verbose logging capabilities to track token usage, costs, and display Markdown-formatted results. The implementation relies on specific internal modules for tasks like template loading, LLM invocation, and error handling to ensure robustness.",2025-12-20T06:37:34.661905+00:00
"context/change/18/updated_unfinished_prompt_python.prompt","The provided text defines the requirements for an expert Python engineer to implement a function called 'unfinished_prompt'. The primary objective of this function is to evaluate a specific string, 'prompt_text', to determine if the prompt is syntactically or semantically complete or if it requires further continuation. The function signature accepts inputs for the prompt text, model strength (defaulting to 0.5), temperature (defaulting to 0), and a verbose logging flag. It returns a tuple consisting of a reasoning string, a boolean indicating completion ('is_finished'), the total cost, and the model name. Implementation steps specify loading a prompt template named 'unfinished_prompt_LLM' and utilizing an internal 'llm_invoke' mechanism. The input text is passed as a parameter ('PROMPT_TEXT') to the model, which is expected to return a Pydantic object containing the analysis results. The file includes references to external context files for preambles and code examples regarding template loading and LLM invocation to guide the development process.",2025-12-20T06:37:48.106855+00:00
"context/change/19/change.prompt","The provided text serves as a technical instruction for updating a prompt or code segment to align with recent method changes. It explicitly states that the current implementation has been superseded by the functions 'llm_invoke' and 'load_prompt_template'. Consequently, the user is directed to modify the prompt accordingly. To guide this refactoring process, the text includes a structured example block. This block utilizes XML tags to contrast the previous state (<before_example>) with the desired outcome (<after_example>). Specifically, it references file paths 'context/change/17/unfinished_prompt_python.prompt' and 'context/change/17/updated_unfinished_prompt_python.prompt' to illustrate the specific changes required. The core objective is to facilitate the migration from obsolete methods to the newly introduced 'llm_invoke' and 'load_prompt_template' standards.",2025-12-20T06:38:03.039846+00:00
"context/change/19/context_generator.py","This Python script defines a function called `context_generator` designed to generate concise usage examples for specific code modules using Large Language Models (LLMs). Leveraging the LangChain library, the function orchestrates a workflow that begins by validating the `PDD_PATH` environment variable and loading a specific prompt template. It utilizes a custom `llm_selector` to choose a model based on desired strength and temperature parameters. The process involves preprocessing inputs, calculating token-based costs, and invoking the LLM chain. To ensure robustness, the script checks for incomplete generations and continues the process if necessary, followed by a post-processing step to refine the output. Throughout execution, it uses the `rich` library for console logging and debugging. The function ultimately returns a tuple containing the generated example code, the total financial cost of the API calls, and the name of the model employed.",2025-12-20T06:38:18.600479+00:00
"context/change/19/context_generator_python.prompt","The provided input defines the requirements for an expert Python engineer to write a function named context_generator. This function's purpose is to generate concise usage examples for a specific code_module using LangChain Expression Language (LCEL). The function accepts inputs such as the code module string, the originating prompt, the target language (defaulting to Python), and LLM parameters like strength and temperature. It outputs the generated example code, the total cost, and the model name. The implementation follows a nine-step process involving several internal utility modules. It starts by loading and preprocessing a prompt template from a file path specified by the $PDD_PATH environment variable. It then selects an LLM using llm_selector and invokes the model with the preprocessed inputs. A critical feature is the detection of incomplete generation using unfinished_prompt; if the output is cut off, continue_generation is called, otherwise, postprocess is applied. The function tracks token usage and costs throughout the process, printing status updates and the final total cost before returning the results. The file also includes references to various context files and examples for preprocessing, LLM selection, and post-processing to guide the development.",2025-12-20T06:38:31.819599+00:00
"context/change/19/unfinished_prompt_python.prompt","The provided file defines the specifications for a Python function named 'unfinished_prompt', designed to evaluate whether a given text prompt is complete or requires continuation. Acting as an expert Python engineer, the function takes 'prompt_text', 'strength', and 'temperature' as inputs. It relies on the Langchain LCEL framework and a custom 'llm_selector' module to perform the analysis. The implementation involves loading a specific prompt template ('unfinished_prompt_LLM.prompt') using the '$PDD_PATH' environment variable. The function executes the prompt against a selected LLM, calculating and displaying token usage and costs. It is required to parse the LLM's JSON output to extract a 'reasoning' string and an 'is_finished' boolean. Furthermore, the function must utilize the 'rich' library to pretty-print the status, reasoning, and cost details to the console. The final output returns the reasoning, completion status, total cost, and the model name used.",2025-12-20T06:38:49.344709+00:00
"context/change/19/updated_context_generator_python.prompt","This document outlines the requirements for developing a Python function named context_generator, aimed at creating concise usage examples for code modules. The function is designed to accept inputs including the code module string, the original prompt, the programming language (defaulting to Python), and LLM configuration settings like strength and temperature. It returns a tuple consisting of the generated example code, the total operational cost, and the model name used. The implementation follows a specific four-step workflow: loading the example_generator_LLM prompt template, invoking the LLM, and managing the output. A key requirement is the detection of incomplete text by analyzing the last 600 characters of the response; if the output is unfinished, the function must utilize a continuation mechanism, whereas complete outputs undergo post-processing. The file also lists necessary internal modules for tasks such as loading templates and invoking the LLM to ensure proper integration.",2025-12-20T06:39:02.235411+00:00
"context/change/19/updated_unfinished_prompt_python.prompt","The provided text defines the requirements for an expert Python engineer to implement a function called 'unfinished_prompt'. The primary objective of this function is to evaluate a specific string, 'prompt_text', to determine if the prompt is syntactically or semantically complete or if it requires further continuation. The function signature accepts inputs for the prompt text, model strength (defaulting to 0.5), temperature (defaulting to 0), and a verbose logging flag. It returns a tuple consisting of a reasoning string, a boolean indicating completion ('is_finished'), the total cost, and the model name. Implementation steps specify loading a prompt template named 'unfinished_prompt_LLM' and utilizing an internal 'llm_invoke' mechanism. The input text is passed as a parameter ('PROMPT_TEXT') to the model, which is expected to return a Pydantic object containing the analysis results. The file includes references to external context files for preambles and code examples regarding template loading and LLM invocation to guide the development process.",2025-12-20T06:39:19.235479+00:00
"context/change/2/change.prompt","The provided file content consists of a concise directive regarding a code modification focused on token management within a software application. Specifically, the instruction dictates a shift in the library or utility used to calculate the number of tokens in a text prompt. The developer is required to discontinue the direct use of tiktoken, a popular BPE tokenizer library often associated with OpenAI models, for this specific task. Instead, the code must be updated to utilize a function or object named token_counter, which is sourced from a module or library identified as llm_selector. This architectural change implies a move towards better software design principles, such as abstraction and modularity. By wrapping the token counting logic within llm_selector, the application reduces its direct dependency on tiktoken. This abstraction allows for greater flexibility in the future; for instance, if the application needs to support different Large Language Models (LLMs) that use different tokenizers, the logic can be handled centrally within llm_selector without requiring changes to every call site where tokens are counted. Ultimately, this change streamlines the prompt processing pipeline by standardizing how token limits and usage are calculated across the system.",2025-12-20T06:39:33.443805+00:00
"context/change/2/final_context_generator_python.prompt","The provided text outlines the requirements for a Python function named context_generator, designed to automatically generate concise usage examples for specific code modules. Acting as an expert Python engineer, the developer is tasked with implementing this function using Langchain's LangChain Expression Language (LCEL). The function accepts several inputs, including the code module string, the original prompt, the target language (defaulting to Python), and LLM parameters such as strength and temperature. It outputs the generated example code and the total financial cost of the operation. The implementation process involves seven distinct steps: loading a prompt template from a path defined by the $PDD_PATH environment variable, selecting an LLM, preprocessing the input prompt, and invoking the model. The function must also handle token counting to calculate costs for both input and output. Finally, the workflow requires post-processing the model's raw output to extract clean, runnable code before returning the final result. The instructions reference external context files for specific implementations of preprocessing, postprocessing, and LLM selection.",2025-12-20T06:39:53.228064+00:00
"context/change/2/initial_context_generator.py","The provided code defines a Python function named `context_generator` designed to generate example code using a Large Language Model (LLM) integrated with the LangChain framework. The function accepts parameters for the code module, a descriptive prompt, the target programming language, and model configuration settings like strength and temperature. It begins by loading a prompt template from a file path defined in the `PDD_PATH` environment variable. The function employs a custom `llm_selector` to determine the appropriate model and uses `tiktoken` to calculate and print the input token count and estimated cost prior to execution. The input prompt undergoes preprocessing before being passed through a LangChain LCEL pipeline. After the model generates a response, the output is post-processed to extract the final code. The function concludes by calculating the total financial cost—aggregating input, output, and post-processing expenses—and returns a tuple containing the generated example code and the total cost.",2025-12-20T06:40:09.421271+00:00
"context/change/2/initial_context_generator_python.prompt","The provided file outlines the specifications for creating a Python function named context_generator. This function is designed to generate concise usage examples for specific code modules using Langchain's LCEL framework. It requires inputs such as the code module string, the originating prompt, the programming language (defaulting to Python), and LLM parameters like strength and temperature. The function outputs the generated example code and the total financial cost of the operation. The implementation process involves seven steps: loading a prompt template via the $PDD_PATH environment variable, creating a Langchain template, selecting an LLM, and preprocessing the prompt. The function then invokes the model, calculating and printing token usage and costs using tiktoken. Finally, it post-processes the LLM's output to extract runnable code (separating it from explanatory text) and returns the final code alongside the total calculated cost. The document also references several external example files for handling specific tasks like preprocessing, postprocessing, and LLM selection.",2025-12-20T06:40:27.183882+00:00
"context/change/20/change.prompt","The provided file outlines a requirement to update input prompts due to the replacement of current methods with `llm_invoke` and `load_prompt_template`. It instructs that input prompts must be modified accordingly to align with these new functions. To guide this transition, the file presents a collection of examples structured within XML tags. Specifically, it lists four distinct examples (IDs 1 through 4) that demonstrate the change from an original state to an updated state. These examples reference specific file paths using `<include>` tags, contrasting files such as `unfinished_prompt_python.prompt`, `context_generator_python.prompt`, `code_generator_python.prompt`, and `generate_test_python.prompt` with their corresponding `updated_` versions. The examples cover various scenarios including unfinished prompts, context generation, code generation, and test generation, serving as a reference for implementing the required updates.",2025-12-20T06:40:42.015262+00:00
"context/change/20/generate_test.py","This Python script defines a function named `generate_test` designed to automatically generate unit tests for a given code snippet using Large Language Models (LLMs) via the Langchain library. The function accepts parameters for the original prompt, source code, model strength, temperature, and the target programming language. The process begins with input validation and loading a specific prompt template from a file path defined in environment variables. It utilizes a helper module, `llm_selector`, to choose an appropriate LLM and calculates input token costs. The script executes a Langchain pipeline to generate the test code, displaying progress and results via the `rich` library. A key feature is its handling of output completeness: it analyzes the generated text to determine if the LLM finished its response. If incomplete, it triggers a continuation process; otherwise, it runs a post-processing step. The function tracks costs throughout the lifecycle—covering inputs, outputs, and any additional processing—and returns the final unit test code, the total calculated cost, and the model name. Comprehensive error handling is included to manage missing files or invalid inputs.",2025-12-20T06:40:53.319220+00:00
"context/change/20/generate_test_python.prompt","The provided text serves as a detailed specification for developing a Python function named `generate_test`. Acting as an expert Python Software Engineer, the objective is to utilize LangChain to automate the creation of unit tests based on input code and its generating prompt. The function requires specific inputs: the original prompt, the source code, model parameters (strength, temperature), and the target language. It returns the generated unit test, the total execution cost, and the model name used. The implementation workflow is strictly defined in nine steps. First, it retrieves a prompt template from a path defined by the `$PDD_PATH` environment variable and preprocesses it. It then constructs a LangChain Expression Language (LCEL) pipeline, utilizing an internal `llm_selector` module to choose the appropriate Large Language Model and calculate token costs. During execution, the function invokes the model with specific parameters, provides real-time feedback on token usage and cost, and formats the output using Markdown. Crucially, it includes logic to detect incomplete generations using an `unfinished_prompt` function; if detected, it continues generation, otherwise, it post-processes the result. Finally, the function calculates the total cost across all steps and returns the required outputs.",2025-12-20T06:41:13.586121+00:00
"context/change/20/updated_generate_test_python.prompt","The provided file outlines the specifications for a Python function named `generate_test`, designed to automatically create unit tests for a given code snippet using a Large Language Model (LLM). Acting as an expert Python Software Engineer, the function accepts several inputs, including the original prompt that generated the code, the code itself, model parameters (strength, temperature), the target language, and a verbose flag.

The workflow involves a multi-step process utilizing internal modules for prompt management and LLM interaction. First, it loads and preprocesses the `generate_test_LLM` prompt template. It then invokes the LLM with the preprocessed inputs. The function includes logic to handle verbose logging, displaying token counts, costs, and Markdown-formatted results. A critical step involves detecting incomplete model outputs; if the generation is cut off, the function triggers a continuation process. Otherwise, it applies postprocessing to the result. Finally, the function calculates the total cost and returns a tuple containing the generated unit test string, the total cost, and the model name, while ensuring graceful error handling for missing parameters or model issues.",2025-12-20T06:41:29.577617+00:00
"context/change/21/auto_deps_main_python.prompt","This file contains a prompt template designed to instruct an AI coding assistant to generate a Python function named 'auto_deps_main'. This function is intended to serve as the core logic for the 'auto-deps' command within a CLI tool called 'pdd'. The prompt outlines the specific requirements for the function, including its inputs—such as the Click context, prompt file path, directory path for scanning dependencies, a CSV path for caching, and an output path—and its expected outputs, which include the modified prompt string, operation cost, and model name. Additionally, the file embeds references to several context files, including a Python preamble, examples of using the Click library, and internal utility functions like 'construct_paths' and 'insert_includes'. It also points to a README file to provide the detailed logic and behavior expected for the 'auto-deps' command, ensuring the generated code adheres to the project's architectural standards and functional requirements.",2025-12-20T06:41:43.743709+00:00
"context/change/21/auto_include.py","This Python script defines a utility function named `auto_include` designed to automatically identify and retrieve relevant file dependencies for a given input prompt using Large Language Models (LLMs). The script integrates libraries such as `rich` for console output, `pandas` for data handling, and `pydantic` for structured output validation. The core workflow involves validating inputs, loading prompt templates, and calling a `summarize_directory` function to generate summaries of files within a specified path. These summaries are parsed and presented to an LLM, which determines which files are necessary context for the user's prompt. A subsequent LLM call extracts the specific string of dependencies using the `AutoIncludeOutput` class. The function calculates and returns the dependency string, the CSV summary data, the total cost of the LLM operations, and the model name. The file also includes a `main` function that demonstrates how to execute the `auto_include` function with example parameters.",2025-12-20T06:41:58.446733+00:00
"context/change/21/auto_include_python.prompt","The provided file outlines the specifications for generating a Python function named auto_include. This function is designed to automatically identify and generate the necessary file dependencies (includes) required for a specific input prompt. The function accepts several arguments, including the target input_prompt, a directory_path containing potential dependencies, a csv_file string for caching file summaries, and LLM configuration parameters such as strength and temperature. It returns a tuple consisting of the generated dependencies string, an updated CSV string, the total computational cost, and the name of the model used. The execution logic follows a six-step process. First, it loads specific prompt templates (auto_include_LLM and extract_auto_include_LLM). Next, it utilizes a summarize_directory helper to analyze files and generate a list of available_includes. The function then invokes an LLM to select relevant files based on the input prompt and subsequently extracts the formatted include strings using a second LLM call. Finally, it compiles the results into the output tuple. The file also includes context references for internal modules regarding prompt loading and LLM invocation.",2025-12-20T06:42:13.683287+00:00
"context/change/21/change.prompt","The provided file contains instructions for generating a specific 'wrapper prompt' designed to simplify a main CLI program. The primary objective is to encapsulate complexity so that the CLI can invoke functionality with a single line of code. Specifically, the instructions direct the creation of a '_main' wrapper prompt, explicitly noting that the core input prompt exists in a separate file and should not be recreated. The content includes an example section illustrating the relationship between an initial prompt, source code, and the resulting output wrapper (e.g., 'auto_deps_main_python.prompt'). It also references an external README file to provide context on the CLI command's mechanics. Overall, this file acts as a meta-prompt or template definition used to standardize how the system interfaces with underlying prompts, ensuring modularity and ease of execution within the CLI environment.",2025-12-20T06:42:30.074297+00:00
"context/change/22/change.prompt","The provided file serves as a prompt definition or template designed to instruct an AI system on how to generate a specific wrapper prompt, referred to as a `*_main` prompt. The primary objective of this instruction is to simplify the architecture of a main CLI program, enabling it to invoke complex functionality with a single line of code. The file explicitly constrains the task to creating only the wrapper prompt, noting that the core input prompt resides in a separate file. It includes an `<examples>` section that illustrates the expected input—consisting of an initial prompt and associated source code (such as Python scripts)—and the corresponding output, which is the generated main prompt. Furthermore, the file references an external README (`./README.md`) to provide necessary context regarding the CLI command's operation. Overall, this file acts as a meta-prompt to automate the creation of streamlined interface prompts for software development tools.",2025-12-20T06:42:42.373057+00:00
"context/change/22/preprocess.py","This Python script provides a preprocessing utility for text prompts, designed to handle file inclusions, shell command execution, and string formatting safety. Using the `rich` library for console output, the main `preprocess` function orchestrates several text transformation steps. It scans for specific patterns, such as triple-backtick includes and XML-style tags like `<include>`, replacing them with the contents of the referenced files. It also supports a `<shell>` tag to execute system commands and insert their standard output directly into the text, while `<pdd>` tags are removed. Additionally, the script features a robust mechanism to escape curly brackets (doubling them) to prevent issues with string formatting engines. This escaping logic is context-aware, treating code blocks differently from regular text and allowing specific keys to remain unescaped for variable substitution. The module supports recursive processing of included files and handles file not found errors with warning messages.",2025-12-20T06:42:51.206515+00:00
"context/change/22/preprocess_main_python.prompt","The provided file contains a prompt template designed to instruct an AI model to generate a Python function named 'preprocess_main'. Acting as a CLI wrapper built with the 'click' library, this function is intended to preprocess prompt files. The prompt specifies that the function must accept arguments for the Click context, a prompt file path, an optional output destination, and an XML formatting flag. It is expected to return a tuple containing the preprocessed string, the cost, and the model name. The file provides extensive context through included references, such as a Python preamble, usage examples for internal modules like 'construct_paths', 'preprocess', and 'xml_tagger', and a README file to ensure the generated code aligns with the project's CLI specifications and internal logic.",2025-12-20T06:43:11.027715+00:00
"context/change/22/preprocess_python.prompt","The provided text outlines the specifications for a Python function named 'preprocess_prompt', designed to prepare prompt strings for Large Language Models (LLMs). Acting as an expert Python engineer, the goal is to implement a function that accepts a prompt string, recursion flags, curly bracket settings, and exclusion keys. The function utilizes regular expressions to process specific XML-like tags: 'include' tags are replaced with the contents of the referenced files, 'pdd' tags and their contents are deleted as comments, and 'shell' tags are replaced by the output of the executed shell commands. The system supports recursive file inclusion via both XML tags and angle brackets within triple backticks. Additionally, the function handles the escaping of curly brackets by doubling them, unless the keys are excluded or already doubled, handling nested structures recursively. It also requires a helper function, 'get_file_path', to resolve file paths relative to the current directory, ultimately returning a whitespace-stripped, preprocessed string.",2025-12-20T06:43:24.802236+00:00
"context/change/3/change.prompt","The provided file content consists of a concise directive regarding a code modification focused on token management within a software application. Specifically, the instruction dictates a shift in the library or utility used to calculate the number of tokens in a text prompt. The developer is required to discontinue the direct use of tiktoken, a popular BPE tokenizer library often associated with OpenAI models, for this specific task. Instead, the code must be updated to utilize a function or object named token_counter, which is sourced from a module or library identified as llm_selector. This architectural change implies a move towards better software design principles, such as abstraction and modularity. By wrapping the token counting logic within llm_selector, the application reduces its direct dependency on tiktoken. This abstraction allows for greater flexibility in the future; for instance, if the application needs to support different Large Language Models (LLMs) that use different tokenizers, the logic can be handled centrally within llm_selector without requiring changes to every call site where tokens are counted. Ultimately, this change streamlines the prompt processing pipeline by standardizing how token limits and usage are calculated across the system.",2025-12-20T06:43:42.656933+00:00
"context/change/3/final_test_generator_python.prompt","The provided file outlines the specifications for developing a Python function named `test_generator`, designed to automatically generate unit tests for a given code snippet using a Large Language Model (LLM) via Langchain. Acting as an expert Python Software Engineer, the developer must implement this function to accept inputs including the original prompt, the source code, model strength, temperature, and the target language. The process involves several distinct steps. First, it retrieves a specific prompt template from a path defined by the `$PDD_PATH` environment variable. This template is preprocessed to handle formatting correctly before being converted into a Langchain LCEL object. The function utilizes an `llm_selector` to choose the appropriate model and calculate token costs. During execution, the function must provide real-time feedback to the console using the `rich` library, displaying status updates, token counts, and financial costs for both input and output. Finally, the raw output from the model is post-processed to extract runnable code from Markdown formatting. The function concludes by returning the cleaned unit test code and the total calculated cost of the operation.",2025-12-20T06:44:02.826860+00:00
"context/change/3/initial_test_generator_python.prompt","The provided file outlines the specifications for developing a Python function named test_generator, intended to automate the creation of unit tests using a Large Language Model (LLM). Acting under the persona of an expert Python Software Engineer, the function requires inputs such as the original generation prompt, the source code, model parameters (strength and temperature), and the target language. It outputs the generated unit test code and the total financial cost of the operation. The implementation workflow relies heavily on the Langchain library and its Expression Language (LCEL). Key steps include loading a prompt template from a path defined by the $PDD_PATH environment variable, preprocessing the prompt, and selecting an LLM via a selector utility. During execution, the function must utilize the rich library to pretty-print console output, including markdown formatting and cost analysis based on token counts derived from tiktoken. Finally, the raw output from the model is post-processed to extract valid code, and the function returns the clean unit test alongside the total calculated cost.",2025-12-20T06:44:26.882696+00:00
"context/change/3/intial_test_generator.py","This Python script defines a function named `test_generator` designed to automate the generation of unit tests for provided code snippets using a Large Language Model (LLM). Leveraging the `langchain` library for model orchestration and `rich` for console formatting, the function executes a structured pipeline. It begins by retrieving and preprocessing a specific prompt template located via the `PDD_PATH` environment variable. An appropriate LLM is then instantiated based on user-defined strength and temperature parameters through a custom `llm_selector`. The script calculates and displays estimated costs for input tokens using `tiktoken` before invoking the model chain. Upon receiving the generated response, it calculates output costs, renders the result as Markdown in the console, and applies a post-processing step to finalize the unit test code. The function ultimately returns the generated test and the total calculated cost, incorporating error handling to catch and report exceptions during the process.",2025-12-20T06:44:42.002337+00:00
"context/change/4/change.prompt","The provided file content consists of a concise directive regarding a code modification focused on token management within a software application. Specifically, the instruction dictates a shift in the library or utility used to calculate the number of tokens in a text prompt. The developer is required to discontinue the direct use of tiktoken, a popular BPE tokenizer library often associated with OpenAI models, for this specific task. Instead, the code must be updated to utilize a function or object named token_counter, which is sourced from a module or library identified as llm_selector. This architectural change implies a move towards better software design principles, such as abstraction and modularity. By wrapping the token counting logic within llm_selector, the application reduces its direct dependency on tiktoken. This abstraction allows for greater flexibility in the future; for instance, if the application needs to support different Large Language Models (LLMs) that use different tokenizers, the logic can be handled centrally within llm_selector without requiring changes to every call site where tokens are counted. Ultimately, this change streamlines the prompt processing pipeline by standardizing how token limits and usage are calculated across the system.",2025-12-20T06:44:56.089767+00:00
"context/change/4/final_postprocess_python.prompt","The provided text specifies the requirements for creating a Python function called 'postprocess'. This function is designed to convert raw string output from a Large Language Model (LLM)—which typically mixes text and code blocks—into a clean, runnable code string. The function accepts four inputs: the raw 'llm_output', the target 'language' (e.g., python, bash), a 'strength' parameter, and a 'temperature' setting. The execution logic depends on the strength parameter. If set to '0', the function utilizes a zero-cost utility named 'postprocess_0'. For other values, it initiates a Langchain LCEL workflow. This involves loading a prompt from a specific environment path ($PDD_PATH), selecting an LLM via a helper function, and processing the input to return a JSON object. The function must handle token counting and cost estimation, displaying these details to the user via pretty-printing. It is also responsible for stripping markdown backticks from the result. Finally, the function returns the 'extracted_code' and the 'total_cost', ensuring robust error handling throughout the process.",2025-12-20T06:45:16.322932+00:00
"context/change/4/initial_postprocess.py","This file implements a Python function named 'postprocess' designed to extract and clean code snippets from raw Large Language Model (LLM) outputs. The function's behavior is controlled by a 'strength' parameter. When set to zero, it utilizes a lightweight 'postprocess_0' utility for immediate extraction without incurring extra costs. For higher strength values, it employs a more robust approach using the LangChain framework. This involves loading a specific prompt template ('extract_code_LLM.prompt') from a directory specified by the 'PDD_PATH' environment variable and configuring an LLM chain with a JSON output parser. The script integrates 'tiktoken' for precise token counting and cost estimation, dynamically selecting models via an imported 'llm_selector'. It provides detailed console feedback using the 'rich' library, displaying the estimated input cost, the extracted code rendered in Markdown, and the final total cost based on input and output tokens. The function ultimately returns the extracted code and the calculated cost, ensuring a reliable mechanism for retrieving executable code from LLM responses.",2025-12-20T06:45:32.537529+00:00
"context/change/4/initial_postprocess_python.prompt","The provided file outlines the specifications for developing a Python function named `postprocess`. This function is designed to refine raw string output from a Large Language Model (LLM)—which typically mixes conversational text with code blocks—into a clean, executable code string. The function accepts inputs for the raw LLM output, the target programming language, and model parameters like strength and temperature.

The logic dictates a conditional workflow: if the strength is set to '0', the function defaults to a zero-cost extraction method (`postprocess_0`). For other strength levels, it employs a more complex pipeline using Langchain's LCEL framework. This involves loading a specific prompt from the project directory (`extract_code_LLM.prompt`), selecting an appropriate LLM, and processing the input to return a JSON object.

Key operational steps include calculating and printing token usage and costs using `tiktoken`, stripping markdown code fences (triple backticks), and pretty-printing the final result using the `rich` library. The function must handle errors gracefully and return both the extracted code string and the total financial cost of the operation.",2025-12-20T06:45:53.639609+00:00
"context/change/5/change.prompt","The provided file content consists of a concise directive regarding a code modification focused on token management within a software application. Specifically, the instruction dictates a shift in the library or utility used to calculate the number of tokens in a text prompt. The developer is required to discontinue the direct use of tiktoken, a popular BPE tokenizer library often associated with OpenAI models, for this specific task. Instead, the code must be updated to utilize a function or object named token_counter, which is sourced from a module or library identified as llm_selector. This architectural change implies a move towards better software design principles, such as abstraction and modularity. By wrapping the token counting logic within llm_selector, the application reduces its direct dependency on tiktoken. This abstraction allows for greater flexibility in the future; for instance, if the application needs to support different Large Language Models (LLMs) that use different tokenizers, the logic can be handled centrally within llm_selector without requiring changes to every call site where tokens are counted. Ultimately, this change streamlines the prompt processing pipeline by standardizing how token limits and usage are calculated across the system.",2025-12-20T06:46:09.186755+00:00
"context/change/5/final_split_python.prompt","The provided text outlines the specifications for developing a Python function named `split`, designed to decompose a single `input_prompt` into a `sub_prompt` and a `modified_prompt` while retaining original functionality. Acting as an expert Python Software Engineer, the developer must utilize the Langchain LCEL framework and the Python Rich library for console output. The function requires inputs including the original prompt, associated code, example code, and model parameters (strength, temperature).

The workflow involves loading specific prompt templates (`split_LLM.prompt` and `extract_prompt_split_LLM.prompt`), preprocessing them, and selecting an LLM via a helper function `llm_selector` which also handles token counting and cost estimation. The process executes in two stages: first, generating a raw split response based on the inputs; second, extracting the specific `sub_prompt` and `modified_prompt` strings via a JSON-outputting LCEL template. The function must calculate and display token usage and costs for each step, handle edge cases gracefully, and ultimately return the two resulting prompts alongside the total execution cost.",2025-12-20T06:46:29.450482+00:00
"context/change/5/initial_split.py","The provided file contains the Python implementation of a function named `split`, which utilizes the Langchain library, a custom `llm_selector`, and `tiktoken` to process prompts and code. The function begins by validating the `PDD_PATH` environment variable to locate and load specific prompt templates (`split_xml_LLM.prompt` and `extract_prompt_split_LLM.prompt`). It then selects an appropriate Large Language Model (LLM) based on input strength and temperature parameters. The workflow executes in two main phases: first, it runs a Langchain pipeline to process the input prompt, input code, and example code; second, it employs a parsing chain to extract a `sub_prompt` and `modified_prompt` from the initial LLM output. Throughout the process, the function calculates token usage and estimated costs using `tiktoken`, displaying formatted status updates via the `rich` library. Finally, the function returns the extracted prompts and the total calculated cost.",2025-12-20T06:46:45.393460+00:00
"context/change/5/initial_split_python.prompt","The provided file outlines the specifications for developing a Python function named `split`, designed to decompose a single `input_prompt` into a `sub_prompt` and a `modified_prompt` while preserving original functionality. Acting as an expert Python Software Engineer, the developer must utilize the Langchain LCEL framework for logic and the Python Rich library for formatted console output. The function requires inputs including the original prompt, generated code, example code, and model parameters such as strength and temperature. The workflow involves loading specific prompt templates (`split_LLM.prompt` and `extract_prompt_split_LLM.prompt`), preprocessing them, and selecting an LLM. The execution proceeds in two stages: first generating a raw split string, and then extracting the specific prompt components via a JSON-outputting template. Throughout the process, the function must calculate and display token counts and estimated costs using `tiktoken`. The final return values include the two split prompt strings and the total cost, with mandatory error handling for missing inputs or model failures.",2025-12-20T06:47:00.840792+00:00
"context/change/6/change.prompt","The provided text outlines specific technical modifications required for a software module involving Large Language Model (LLM) operations. The primary objective is to refactor the token counting mechanism by switching dependencies; specifically, the system should utilize the token_counter utility imported from the llm_selector package, replacing the previously used tiktoken library. This change is specifically targeted at the logic responsible for counting tokens within prompts. Additionally, there is a requirement to enhance the functionality of the xml tagger component. This component needs to be updated to calculate and return the total financial cost associated with executing LangChain Expression Language (LCEL) chains. These changes suggest an effort to standardize token counting mechanisms across the application and to improve cost observability and reporting regarding the execution of language model workflows.",2025-12-20T06:47:18.003086+00:00
"context/change/6/final_xml_tagger_python.prompt","The provided file outlines the specifications for developing a Python function named xml_tagger, designed to enhance LLM prompts by adding XML tags for improved structure and readability. The function requires inputs for a raw prompt, model strength, and temperature, and returns the modified prompt string along with the total execution cost. The implementation must utilize the Langchain library's LCEL framework and the rich library for formatted console output.

The workflow involves a multi-step process: first, loading prompt templates using the $PDD_PATH environment variable; second, utilizing a helper function to select the LLM and track token usage; and third, executing a two-stage chain. The first stage converts the raw prompt into an analyzed string, while the second stage extracts the specific XML-tagged content. Throughout the process, the code is required to pretty-print status updates, token counts, and costs, finally displaying the result as Markdown and handling any potential errors gracefully.",2025-12-20T06:47:33.880530+00:00
"context/change/6/initial_xml_tagger.py","The provided Python script implements a utility function named `xml_tagger` designed to transform raw text prompts into XML-tagged formats using the LangChain framework. The function operates by loading specific prompt templates from a directory specified by the `PDD_PATH` environment variable and selecting a Large Language Model (LLM) via a custom `llm_selector` based on strength and temperature parameters. The processing pipeline executes in two stages using LangChain Expression Language (LCEL): first converting the raw prompt into a generated analysis, and then extracting the specific XML-tagged content using a `JsonOutputParser` backed by a Pydantic model (`XMLTaggedOutput`). Throughout the execution, the script utilizes `tiktoken` to calculate and display token counts and associated costs, employing the `rich` library for formatted console output. It also establishes an SQLite cache for LLM responses to optimize performance and includes error handling to catch and report exceptions during the transformation process.",2025-12-20T06:47:52.958354+00:00
"context/change/6/initial_xml_tagger_python.prompt","The provided file outlines the specifications for creating a Python function named 'xml_tagger', intended to enhance Large Language Model (LLM) prompts by adding XML tags for better structure and readability. The function requires inputs for the raw prompt, model strength, and temperature, ultimately returning a string containing the XML-tagged prompt. The implementation must utilize the Langchain LCEL framework and the 'rich' library for console output. The process involves loading specific prompt files via the '$PDD_PATH' environment variable and executing a two-step chain. First, the raw prompt is processed to generate an analysis string; second, this analysis is parsed to extract the final XML content using a JSON-structured output. Crucially, the function must integrate 'tiktoken' to calculate and pretty-print token usage and costs at each step. The final output is displayed as Markdown, and the code must include graceful error handling for missing parameters or model failures.",2025-12-20T06:48:09.541142+00:00
"context/change/7/change.prompt","The provided file contents outline the specifications for a Python code module intended to programmatically resolve errors detected by unit tests, serving as a replacement for a command-line interface program known as 'pdd'. The core functionality is demonstrated through a function named 'fix_errors_from_unit_tests'. This function requires four specific input parameters: the string representation of the failing unit test, the actual source code being tested, the error message generated during execution, and a numerical 'strength' parameter (ranging from 0 to 1) to influence the selection or behavior of the underlying Large Language Model (LLM). The function's output consists of four values: two booleans indicating whether the unit test or the code required modification, and two strings containing the corrected unit test and the fixed source code. The file includes a main execution block example to illustrate how to invoke the function and print the results. Additionally, a specific directive at the end of the file modifies the requirements, stating that the final implementation must be encapsulated within a Python function named 'fix_error_loop' and must accept an additional 'temperature' argument to further control the generation process.",2025-12-20T06:48:26.659399+00:00
"context/change/7/final_fix_errors_python.prompt","This document outlines the specifications for a Python function named fix_error_loop, designed to iteratively repair errors in unit tests and their associated source code. Acting as an automated debugging agent, the function accepts file paths for a unit test, the code under test, and a verification program, alongside LLM configuration parameters like strength, temperature, and a maximum attempt limit. The core logic involves a while loop that executes pytest and captures output to an error log. If tests fail, the function analyzes the error log, creates backup copies of the current files with versioned filenames based on failure counts, and invokes a helper function, fix_errors_from_unit_tests, to generate AI-driven patches. These patches are applied and validated against the verification program; if verification fails, the files are reverted to their backup states. The process repeats until the tests pass or the maximum attempts are reached. The function utilizes the rich library for formatted console output and ensures robust error handling for file I/O and subprocesses. Ultimately, it returns the success status, the final file contents, and the total attempt count.",2025-12-20T06:48:43.421324+00:00
"context/change/7/initial_fix_errors.py","The provided file contains the source code and documentation for a Python script named fix_errors.py, which automates the process of fixing code errors using unit tests and an external tool called PDD. The script operates through an iterative loop designed to resolve test failures. In each iteration, the script executes pytest on a specified unit test file, capturing standard output and errors into a log file. It parses this log to quantify failed tests and errors. Before applying any changes, the script creates backup copies of the code and test files, tagging them with the current iteration and error statistics. It then invokes the PDD tool to attempt repairs based on the error log and a configurable strength parameter. After the repair attempt, a verification program is run to ensure the code remains functional; if verification fails, the script reverts to the backup files. This process repeats until all tests pass or a maximum iteration limit is reached. The file includes the full implementation of the script, including helper functions for file management and process execution, along with a usage guide for command-line arguments.",2025-12-20T06:49:03.918684+00:00
"context/change/7/initial_fix_errors_python.prompt","The provided text outlines the specifications for a Python script named fix_errors.py. This script is intended to automate the process of fixing code errors using a CLI tool called pdd and the rich library for formatted console output. The script requires arguments for a unit test file, a source code file, a verification program, and execution parameters like strength and iteration limits. The defined workflow operates in a loop: it starts by clearing old logs and running pytest. If tests fail, the script parses the error log to count failures, creates versioned backups of the files, and attempts to repair the files using the pdd tool. Subsequently, it runs a verification program to ensure the code remains functional. If the verification succeeds, the process repeats; if it fails, the script reverts to the original files and attempts the fix again. The loop continues until the tests pass or the iteration limit is reached, ending with a final test run.",2025-12-20T06:49:22.458400+00:00
"context/change/8/change.prompt","The provided text outlines a series of technical requirements and refactoring tasks intended to enhance a software module dealing with Large Language Models (LLMs). The first directive involves a dependency change regarding token metrics; specifically, the developer must replace the usage of the external tiktoken library with a specific token_counter utility derived from the internal llm_selector module to handle token counting within prompts. Secondly, the function identified as fix_errors_from_unit_tests requires an update to its signature and internal logic to accept and apply a temperature parameter, allowing for adjustable randomness in model generation during error correction. Lastly, the requirements specify the addition of a financial monitoring feature, necessitating that the system calculates and reports the total cost incurred during the execution of LangChain Expression Language (LCEL) runs. These changes aim to standardize utility usage, increase configuration flexibility, and provide cost transparency.",2025-12-20T06:49:37.385359+00:00
"context/change/8/final_fix_errors_from_unit_tests_python.prompt","The provided text outlines the specifications for developing a Python function titled fix_errors_from_unit_tests. Designed for an expert Python Software Engineer, the function aims to resolve unit test errors within a code file using LangChain's LCEL (LangChain Expression Language). The function accepts inputs including the unit test, the code under test, the error message, and LLM configuration parameters (strength and temperature). The workflow involves a multi-step process utilizing two distinct prompt files loaded from a project directory. First, the function generates a fix strategy using a specified LLM, pretty-printing the token usage, cost, and markdown-formatted result via the rich library. Subsequently, a second LCEL chain extracts the specific code fixes and status flags (indicating if the unit test or code needs updating) into a structured JSON format. The function is required to track and print the financial cost of tokens used across both steps. Ultimately, it returns the fixed code, fixed unit test, update indicators, and the total execution cost, while ensuring graceful error handling throughout the process.",2025-12-20T06:49:53.870897+00:00
"context/change/8/initial_fix_errors_from_unit_tests.py","This Python script implements an automated workflow for fixing code and unit tests based on error messages, utilizing the LangChain framework and OpenAI models. The core function, `fix_errors_from_unit_tests`, operates in two distinct stages. First, it loads external prompt templates and employs a user-specified LLM to analyze the provided code, unit test, and error log to generate a potential fix. It includes functionality to calculate and display token usage and estimated costs using `tiktoken`. In the second stage, the script uses a Pydantic output parser and a secondary LLM call to extract structured data from the initial analysis, resulting in a `FixResult` object. This object contains boolean flags indicating necessary updates and the actual corrected code strings. The script also configures an SQLite cache to optimize LLM calls and uses the `rich` library for enhanced console output. An execution block is provided to demonstrate the function's usage with sample data.",2025-12-20T06:50:09.686671+00:00
"context/change/8/initial_fix_errors_from_unit_tests_python.prompt","The provided file specifies the requirements for a Python function named `fix_errors_from_unit_tests`, designed to resolve unit test errors using Langchain LCEL. The function accepts unit test code, source code, error messages, and an LLM strength parameter, returning fixed code and boolean update flags. The workflow involves a two-step process. First, it loads prompts from a directory specified by the `$PDD_PATH` environment variable and runs an initial LLM chain to generate a fix, while calculating and pretty-printing token usage and costs using `tiktoken` and the `rich` library. Second, it executes a subsequent chain with a JSON output parser to extract specific fields: `update_unit_test`, `update_code`, `fixed_unit_test`, and `fixed_code`. The function utilizes specific helper modules for LLM selection and token counting, ensures all console output is formatted with `rich`, and reports the total financial cost of the LLM usage upon completion.",2025-12-20T06:50:24.973713+00:00
"context/change/9/change.prompt","The file provides instructions for updating the fix_errors_from_unit_tests function and its associated prompt. It requires incorporating a specific usage example that demonstrates inputs like unit test code, source code, error messages, and configuration parameters, while capturing outputs including fixed code and total cost. Furthermore, the instructions specify adding a financial constraint where the function must track cumulative costs and stop processing if a user-defined 'budget' is exceeded. Lastly, the file dictates a selection mechanism for the final output: the function must identify and return the iteration with the fewest 'ERROR's followed by the fewest 'FAILED' tests across all runs, ensuring the most stable code and test combination is preserved.",2025-12-20T06:50:40.306632+00:00
"context/change/9/final_fix_error_loop_python.prompt","The provided text defines the requirements for a Python function named fix_error_loop, intended to automate the debugging process of unit tests and corresponding code files. Acting as an expert Python Software Engineer, the function takes inputs such as file paths, a verification program, and LLM parameters (strength, temperature, budget, and maximum attempts). The workflow initiates by cleaning up logs and initializing trackers for cost and iteration success. It enters a loop that runs pytest, piping output to an error log. If failures occur, the function parses the errors, creates versioned backups of the files, and utilizes a helper function, fix_errors_from_unit_tests, to generate fixes. The process monitors the budget and reverts changes if a secondary verification program fails. It maintains a record of the best iteration based on the lowest error count. Once the loop terminates due to success, budget exhaustion, or maximum attempts, the function performs a final test run, restores the most successful file versions if necessary, and returns a status report containing the success boolean, final file contents, attempt count, and total cost. Output is styled using the rich library.",2025-12-20T06:50:58.874781+00:00
"context/change/9/initial_fix_error_loop.py","The provided code defines a Python function named `fix_error_loop` designed to iteratively repair unit tests and source code files based on test failures. The function operates within a loop constrained by a maximum number of attempts. In each iteration, it executes `pytest` on the specified unit test file and captures the output in an error log. If the tests pass, the loop terminates successfully. If they fail, the script creates backup copies of the current files and invokes an external function, `fix_errors_from_unit_tests`, to generate potential fixes using parameters like strength and temperature. If fixes are generated, they are applied to the files. Subsequently, a separate verification program is executed to ensure the changes did not break critical functionality. If verification fails, the script restores the original files from backups; otherwise, it proceeds with the modified code. The process repeats until the tests pass, no changes are needed, or the attempt limit is reached. Finally, the function performs a last test run and returns a tuple containing the success status, the final contents of the unit test and code files, and the total attempt count. The script utilizes the `rich` library for formatted console output.",2025-12-20T06:51:17.324732+00:00
"context/change/9/initial_fix_error_loop_python.prompt","This document outlines the specifications for a Python function named fix_error_loop, designed to iteratively repair errors in unit tests and their associated source code. Acting as an automated debugging agent, the function accepts file paths for a unit test, the code under test, and a verification program, alongside LLM configuration parameters like strength, temperature, and a maximum attempt limit. The core logic involves a while loop that executes pytest and captures output to an error log. If tests fail, the function analyzes the error log, creates backup copies of the current files with versioned filenames based on failure counts, and invokes a helper function, fix_errors_from_unit_tests, to generate AI-driven patches. These patches are applied and validated against the verification program; if verification fails, the files are reverted to their backup states. The process repeats until the tests pass or the maximum attempts are reached. The function utilizes the rich library for formatted console output and ensures robust error handling for file I/O and subprocesses. Ultimately, it returns the success status, the final file contents, and the total attempt count.",2025-12-20T06:51:33.083807+00:00
"context/change/simple_math/change.prompt","The provided file contents consist of a brief context prompt intended for a software development task involving a regression test for a component named 'simple_math'. The primary instruction directs the user to enhance the code by adding type hints to the function signature and its return value. This task implies a focus on improving code maintainability, readability, and type safety within the codebase. By explicitly defining input and output types, the modification aims to facilitate better static analysis and reduce potential runtime errors. The reference to a 'regression test' suggests that these changes are being introduced to an existing system, requiring verification to ensure that the addition of type annotations does not alter the fundamental logic or break previously working functionality. Overall, the text serves as a specific directive for refactoring a math-related function to adhere to modern typing standards.",2025-12-20T06:51:52.960823+00:00
"context/change_example.py","This Python script demonstrates the usage of a `change` function from the `pdd.change` module, which appears to be a tool for modifying code using a Large Language Model (LLM). The script imports necessary modules including `os` for environment variables and `rich.console` for formatted terminal output.

The `main()` function sets up example inputs including an original prompt describing a factorial function, the corresponding Python code implementation, and a change request to modify the function to return the square root of the factorial result. It also configures LLM parameters: `strength` (set to 0.5) and `temperature` (set to 0.0), both ranging from 0.0 to 1.0.

The script calls the `change` function with these parameters and expects three return values: the modified prompt, the total API cost, and the model name used. Results are displayed using Rich console formatting with bold labels. Error handling is implemented via a try-except block that catches and displays any exceptions in red bold text.

The code includes a commented-out line for setting a `PDD_PATH` environment variable, suggesting the module may require path configuration. The script follows standard Python conventions with a `if __name__ == __main__` guard for direct execution.",2025-12-14T08:30:52.133090+00:00
"context/change_main_example.py","This Python script demonstrates the usage of a `change_main` function from a command-line program called 'pdd'. The script showcases two operational modes: single-change mode and CSV batch-change mode for modifying code prompts using an LLM (Large Language Model).

The script begins by importing necessary libraries including `os`, `click`, `pathlib`, and `rich` for formatted console output. It sets up a Click context with various configuration parameters such as LLM strength (0.8), temperature (0), programming language (Python), file extension (.py), and a budget limit ($10).

In single-change mode, the script creates sample files including a change prompt requesting error handling for division by zero, a simple divide function, and an input prompt. It then calls `change_main` to process these files and outputs the modified prompt along with cost and model information.

For CSV batch-change mode, the script creates multiple code files (addition and subtraction functions) with corresponding prompts, then generates a CSV file specifying batch modifications like handling overflow errors and optimizing for large integers. The `change_main` function processes all changes in batch, outputting results to a specified directory.

The script serves as a practical example for developers learning to use the pdd tool's change functionality programmatically.",2025-12-14T08:30:59.960708+00:00
"context/cli_example.py","This Python script (demo_run_pdd_cli.py) is a minimal demonstration of how to programmatically invoke the PDD (Prompt-Driven Development) command-line interface. The script performs three main tasks: First, it creates a simple prompt file in an output directory that instructs an AI to write a Python function returning Hello, PDD!. Second, it uses Click's CliRunner to invoke the PDD CLI with specific flags including --local (to run without cloud access) and --quiet (to suppress progress bars), along with the generate command that processes the prompt file and outputs generated code. Third, it displays the results including the exit code, CLI output, and the first few lines of the generated Python file if successful. The script imports the CLI entry point from pdd.cli and uses pathlib for file handling. Key files involved are the input prompt (hello_python.prompt) and the output code (hello.py), both stored in the ./output/ directory. This demo is useful for developers who want to integrate PDD's code generation capabilities into their own Python applications or testing workflows without manually running CLI commands.",2025-12-14T08:31:08.608084+00:00
"context/cli_python_preprocessed.prompt","This document is a comprehensive specification for PDD (Prompt-Driven Development), a Python command-line tool designed to automate code generation, testing, and prompt management using AI models. Built with the Click library for CLI handling and Rich for pretty-printed output, PDD follows a specific prompt file naming convention (`<basename>_<language>.prompt`).

The tool offers eight main commands: **generate** (creates code from prompts), **example** (generates usage examples), **test** (produces unit tests), **preprocess** (processes prompt files with optional XML formatting), **fix** (corrects code errors with optional iterative loop mode), **split** (divides large prompts into smaller files), **change** (modifies prompts based on instructions), and **update** (revises prompts based on code changes).

Key features include multi-command chaining for complex workflows, global options for AI model configuration (strength, temperature), cost tracking via CSV output, and flexible output path specification through command options or environment variables. The document provides extensive examples of helper functions like `construct_paths`, `code_generator`, `context_generator`, `generate_test`, `preprocess`, `xml_tagger`, `fix_errors_from_unit_tests`, `fix_error_loop`, `split`, `change`, and `update_prompt` that implement the core functionality. Additional features include tab completion, colorized output, and progress indicators.",2025-12-14T08:31:16.090787+00:00
"context/click_example.py","This Python script implements a command-line image processing pipeline using the Click library and PIL (Pillow). It creates a chainable CLI tool called imagepipe that processes images in a Unix pipe-like fashion, where the output of one command feeds into the next.

The script defines several key components: a main CLI group with chaining enabled, helper decorators (`processor` and `generator`) for creating stream-based command handlers, and a result callback that evaluates the processing chain.

Available commands include:
- **open**: Loads one or multiple images for processing
- **save**: Saves processed images with customizable filename patterns
- **display**: Opens images in the system's image viewer
- **resize**: Resizes images while maintaining aspect ratio
- **crop**: Crops images by a specified border amount
- **transpose**: Rotates (90°, 180°, 270°) or flips (left-right, top-bottom) images
- **blur**: Applies Gaussian blur with configurable radius
- **smoothen**: Applies smoothening filter with multiple iterations
- **emboss**: Applies emboss effect
- **sharpen**: Enhances image sharpness by a factor
- **paste**: Composites one image onto another with offset positioning

Each command processes images as a stream, enabling efficient batch operations. The script demonstrates advanced Click patterns including command chaining, custom parameter callbacks, and decorator-based stream processing.",2025-12-14T08:31:26.658653+00:00
"context/cloud_function_call.py","This Python script demonstrates how to make an authenticated request to a Google Cloud Function using a Firebase authentication token. The code imports the 'requests' library and defines a function called 'call_cloud_function' that takes a Firebase token as its parameter.

The function constructs an HTTP GET request to a specific Cloud Function URL hosted on Google Cloud Platform (us-central1-prompt-driven-development.cloudfunctions.net). It includes the Firebase token in the Authorization header using the Bearer token authentication scheme, which is a standard method for passing authentication credentials in HTTP requests.

The script includes a hardcoded JWT (JSON Web Token) that appears to be a Firebase ID token. This token contains encoded user information including a name (Greg Tanaka), a GitHub profile picture URL, email address, and Firebase authentication details indicating the user signed in via GitHub.

After defining the function, the script calls it with the provided token and prints the JSON response from the Cloud Function. This pattern is commonly used when building applications that need to verify user identity before accessing protected cloud resources. The token-based authentication ensures that only authenticated users can invoke the Cloud Function, providing a security layer for serverless backend operations.",2025-12-14T08:31:35.142547+00:00
"context/cmd_test_main_example.py","test_cli_example.py serves as a demonstration script for integrating the cmd_test_main function into a command-line interface (CLI) using the Python click library. The script defines a top-level CLI group that manages global configuration context, including parameters for verbosity, AI generation strength, temperature, and file overwriting permissions. The core functionality is encapsulated in the test command, which requires a prompt file and a source code file as arguments. It supports various optional flags to customize the test generation process, such as specifying output paths, overriding the programming language, providing coverage reports, and merging new tests with existing ones. Inside the command, the script invokes cmd_test_main with the provided arguments and context. Finally, it outputs the generated test code, the total cost of the operation, and the name of the AI model used. This example illustrates how to build a wrapper around the test generation logic to facilitate automated unit test creation and enhancement via the command line.",2025-12-20T06:52:07.057810+00:00
"context/code_generator_example.py","This Python script demonstrates the usage of a `code_generator` function imported from a `pdd.code_generator` module. The main function serves as an entry point to showcase how to generate code using a language model (LLM).

The script reads a prompt from an external file located at prompts/generate_test_python.prompt rather than using a hardcoded prompt (a commented-out example shows a factorial function request). It then configures several parameters for the code generation process: the target language is set to Python, the LLM strength is set to 0.5 (on a 0.0-1.0 scale), temperature is set to 0.0 for deterministic output, and verbose mode is enabled for detailed logging.

The `code_generator` function is called with these parameters and returns three values: the generated runnable code, the total cost of the API call, and the model name used. These results are then printed to the console, with the cost formatted to six decimal places.

The script includes error handling via a try-except block to catch and display any exceptions that occur during code generation. The standard `if __name__ == __main__` pattern ensures the main function only runs when the script is executed directly.",2025-12-14T08:31:52.827325+00:00
"context/code_generator_main_example.py","This Python script demonstrates how to use the `code_generator_main` function from a PDD (Prompt-Driven Development) CLI package. The script is designed to be self-contained and predictable by using mocked versions of actual code generator functions, avoiding real LLM API calls and API key requirements.

The script includes mock functions for both local and incremental code generation (`mock_local_code_generator_func` and `mock_incremental_code_generator_func`), along with a `MockContext` class to simulate Click CLI context objects. It defines several constants typically used by the PDD package, including default strength, time, and environment variable names for Firebase and GitHub authentication.

The main example function (`main_example`) runs through four scenarios: (1) full local code generation for a simple Java program, (2) incremental code generation with an explicit original prompt for Python code modification, (2b) forced incremental generation even when the model suggests full regeneration, and (3) cloud generation attempt that falls back to local execution due to dummy credentials.

Each scenario creates prompt files, sets up CLI parameters, calls `code_generator_main`, and prints results. The script uses Python's `unittest.mock.patch` decorator to replace actual generator functions with mocks during execution. Output files are written to a designated directory that gets cleaned up at the start of each run.",2025-12-14T08:32:00.310410+00:00
"context/commands/__init___example.py","This Python script serves as a demonstration and verification tool for the `pdd` (Python Development Driver) CLI command registration system. It illustrates how to use the `register_commands` function to attach a wide array of subcommands to a main `click` group. The module defines a primary `cli` function and a `custom_cli` example, showing how to initialize the interface with versioning and descriptions. The script categorizes commands into functional groups such as Generation, Fixing, Modification, Maintenance, Analysis, and Utility. When executed as a standalone script, it inspects the `cli` object to retrieve registered commands and prints a structured report. This report lists commands by category (e.g., `generate`, `fix`, `sync`, `detect-change`) along with their help text summaries. Finally, it provides usage examples, validating that the command registration logic functions correctly and offering a template for setting up the PDD command-line interface.",2025-12-20T06:52:20.635180+00:00
"context/commands/analysis_example.py","This Python script demonstrates how to programmatically invoke various analysis commands from the `pdd` library, specifically `detect_change`, `conflicts`, `bug`, `crash`, and `trace`. Rather than executing these tools via the command line, the script runs them directly as Python functions. It achieves this by setting up a `click.Context` object populated with global configuration settings (such as verbosity and model strength) and pushing it onto the stack to satisfy the library's context requirements.

The `main` function walks through five distinct scenarios. For each command, it generates necessary dummy artifacts—such as prompt files, source code, error logs, and change descriptions—within a local output directory. It then invokes the command's callback function with the appropriate file paths and arguments. This example serves as a reference for integrating `pdd`'s analysis capabilities, such as detecting prompt conflicts, generating bug reproduction tests, fixing crashes, and tracing logic, into larger Python applications or automation workflows.",2025-12-20T06:52:34.330915+00:00
"context/commands/fix_example.py","The provided Python script serves as a comprehensive demonstration for the `pdd fix` command, a tool designed to automatically resolve code and unit test errors using AI. It begins by initializing a testing environment via the `setup_example_files` function, which generates a directory structure containing a buggy calculator module, a corresponding prompt, unit tests, and error logs to simulate a real-world debugging scenario.

The script utilizes the `click` library to wrap the `fix` command, enabling the simulation of various CLI usage patterns. Six distinct example functions illustrate specific features: basic command invocation, iterative loop mode with verification programs and budget constraints, and agentic fallback which employs advanced models like Claude or Gemini for complex errors. Additional examples cover automatic submission of fixes to the PDD Cloud, cost tracking via CSV outputs, and programmatic execution within Python code. The `main` function orchestrates the file setup and provides commented-out calls to these examples, allowing users to selectively test different configurations such as model strength, temperature, and retry limits.",2025-12-20T06:52:44.761518+00:00
"context/commands/generate_example.py","This Python script serves as a comprehensive example guide for using the `pdd.commands.generate` module within the PDD CLI tool. It demonstrates the functionality of three primary Click commands: `generate`, which creates runnable code from prompt files; `example`, which produces compact usage examples; and `test`, which generates or enhances unit tests, including support for coverage-based improvements. The script includes helper functions to set up a demonstration environment by creating output directories, sample prompt files, and a simple calculator code module. It details the specific signatures, arguments, and options for each command, such as handling templates, environment variables, and incremental generation. Additionally, it illustrates how to invoke these commands programmatically using Click's context management, highlighting the configuration of context objects with parameters like model strength, temperature, and local execution flags. The file also notes that all commands are decorated with `@track_cost` to monitor LLM API usage expenses.",2025-12-20T06:52:59.525918+00:00
"context/commands/maintenance_example.py","This Python script serves as a comprehensive demonstration and instructional guide for using the maintenance commands within the `pdd` (Prompt Driven Development) package. It specifically showcases the usage of three primary Click-based commands: `sync`, `auto-deps`, and `setup`.

The script includes utility functions to generate a temporary mock project environment, creating necessary directory structures, sample prompt files, and dummy source code to simulate a real workspace. It then defines specific functions to demonstrate each command: `example_sync_command` illustrates synchronizing prompts with code and tests using parameters like budget and coverage; `example_auto_deps_command` shows how to analyze prompts to inject dependencies; and `example_setup_command` mimics the interactive configuration utility. Additionally, `example_programmatic_usage` demonstrates how to invoke these command callbacks directly via Python code, bypassing the CLI parser while preserving the Click context. Throughout the examples, `unittest.mock` is used to simulate the underlying logic and API calls, ensuring the script runs safely without making actual changes or incurring costs.",2025-12-20T06:53:08.601252+00:00
"context/commands/misc_example.py","This Python script demonstrates the functionality of the `preprocess` command from the `pdd.commands.misc` module, designed to prepare prompt files for Large Language Models (LLMs). The module handles various preprocessing tasks locally—such as resolving `<include>` tags, removing comments, executing shell commands, and scraping web content—without making LLM API calls.

The script initializes by generating sample files in an `./output` directory via `setup_example_files`. It then defines and runs five specific examples using `click.testing.CliRunner` to simulate CLI usage:
1. **Basic Preprocessing**: Shows standard file processing.
2. **XML Delimiters**: Demonstrates the `--xml` flag for inserting structural tags.
3. **Curly Bracket Doubling**: Illustrates the `--double` flag for escaping braces in template systems, utilizing `--exclude` to protect specific keys like `{model}`.
4. **Recursive Processing**: Uses `--recursive` to handle nested file inclusions.
5. **Combined Options**: Merges multiple flags to test complex preprocessing scenarios.

The `__main__` block executes these examples in order, displaying the resulting output and exit codes. This serves both as a usage tutorial and a verification tool for the command's options, including `--output`, `--xml`, `--recursive`, and `--double`.",2025-12-20T06:53:22.297235+00:00
"context/commands/modify_example.py","This Python script demonstrates the usage of the `modify` module within the `pdd` CLI tool, specifically showcasing three commands: `split`, `change`, and `update`. It functions as both a tutorial and a test suite by setting up a local environment in an `./output` directory and generating sample prompt files, source code, and usage examples.

The script executes the following workflows using `Click`'s testing runner:
1. **Split**: Decomposes a large prompt file into smaller sub-prompts by analyzing the relationship between the prompt, the generated code, and example usage.
2. **Change**: Modifies existing prompts based on natural language instructions. This is demonstrated both for single files (e.g., adding logging requirements) and in batch mode using a CSV file to process multiple prompts simultaneously.
3. **Update**: Synchronizes a prompt file to match changes made in the source code, comparing original and modified code versions to infer necessary prompt updates.

Additionally, the script highlights the integration of cost tracking via the `@track_cost` decorator for LLM operations.",2025-12-20T06:53:37.841559+00:00
"context/commands/templates_example.py","This Python script serves as a usage example and documentation for the `pdd.commands.templates` module, demonstrating how to manage PDD templates via the CLI. It utilizes `click.testing.CliRunner` to programmatically invoke commands from the `templates_group`, showcasing three core functionalities: listing, showing, and copying templates.

The script is structured around three specific example functions. `example_list_templates` illustrates how to display available templates in both text and JSON formats, as well as how to filter results by tags. `example_show_template` demonstrates how to retrieve detailed information about a specific template, such as the architecture/architecture_json example, including its metadata, variables, usage patterns, and schemas. `example_copy_template` shows the process of copying a template file to a specified local directory for customization.

A `main` function orchestrates these examples, providing context and descriptive output to the console. This file acts as a practical reference for users or developers needing to understand the capabilities of the PDD template registry system, specifically regarding template discovery, inspection, and retrieval.",2025-12-20T06:53:47.460785+00:00
"context/commands/utility_example.py","This Python script serves as a comprehensive usage example and documentation for the `pdd.commands.utility` module, specifically focusing on the `install_completion_cmd` and `verify` Click commands. The script initializes by creating a local output directory and generating dummy artifacts—including a prompt, a buggy calculator script, and a verification program—to simulate a realistic workflow.

The script explains the `install_completion_cmd`, which configures shell completion for the PDD CLI, though it refrains from execution to avoid modifying the user's environment. The primary focus is on the `verify` command, which utilizes Large Language Models (LLMs) to iteratively fix code based on verification results. The example details the command's input parameters (such as output paths, budget, and attempt limits), illustrates both command-line and programmatic invocation methods, and documents the specific return value structure, which includes success status, code content, operational costs, and the model used.",2025-12-20T06:54:03.217512+00:00
"context/comment_line_example.py","This file documents the `comment_line` function from the `comment_line.py` module, which is designed to comment out lines of code based on specified comment characters. The function accepts two parameters: `code_line`, a string representing the line of code to be commented, and `comment_characters`, which defines how the commenting should be applied. The `comment_characters` parameter supports three modes: (1) a single comment character like # for Python-style comments, (2) a pair of start and end comment characters separated by a space (e.g., <!-- --> for HTML), or (3) the special string del which deletes the line entirely by returning an empty string. The function returns the appropriately commented line or an empty string if deletion is specified. The file includes practical examples demonstrating each use case: commenting a Python print statement with #, wrapping an HTML tag with <!-- -->, and deleting a line using del. This utility function is useful for code processing tools that need to programmatically comment out or remove lines across different programming languages with varying comment syntax conventions.",2025-12-14T08:32:09.907765+00:00
"context/config_example.py","This code snippet demonstrates a Python initialization pattern for configuration management. The file imports the `init_config` function from a utility module located at `utils.config`. Immediately after the import, it calls `init_config()` to initialize the application's configuration settings. The comment Initialize configuration FIRST emphasizes the importance of running this configuration setup before any other code executes, suggesting this is likely placed at the top of a main application file or entry point. This pattern is common in applications that need to load environment variables, set up logging, establish database connections, or configure other global settings before the rest of the application runs. The `utils.config` module path indicates a well-organized project structure with utility functions separated into their own directory. This approach ensures that all subsequent code has access to properly initialized configuration values, preventing errors that could occur if other modules attempted to access configuration before it was set up. The explicit call to `init_config()` rather than relying on import side effects follows Python best practices for explicit initialization.",2025-12-14T08:32:16.741064+00:00
"context/conflicts_in_prompts_example.py","This Python script demonstrates a conflict detection system for AI prompts within the PDD Cloud project. The main function defines two detailed prompts: the first instructs an AI to create an `auth_helpers.py` module for Firebase authentication, including token verification, user extraction, and security best practices; the second prompt requests a `User` dataclass model with fields like uid, email, credits, and contributor tier, along with methods for validation, serialization, Firestore interaction, and permission management.

The script imports a `conflicts_in_prompts` function from the `pdd.conflicts_in_prompts` module and uses the Rich library for formatted console output. It configures LLM parameters including strength (0.89), temperature (0), and verbose mode (True), then calls the conflict detection function with both prompts.

The output displays the model used, total API cost, and any detected conflicts with suggested changes for each prompt. This tool appears designed to identify inconsistencies or contradictions between multiple AI prompts before they are used in code generation, helping maintain coherence across different parts of a software project. The script serves as both a demonstration and testing utility for the prompt conflict detection functionality.",2025-12-14T08:32:23.655288+00:00
"context/conflicts_main_example.py","This Python script demonstrates the usage of a conflict detection system between two AI prompts. It imports the `conflicts_main` function from a module called `pdd.conflicts_main`. The script creates two example prompt files: one defining an AI assistant persona and another defining a helpful chatbot persona, both stored in an output directory.

A `MockContext` class is defined to simulate a Click command-line interface context, containing configuration parameters like 'force', 'quiet', 'strength', and 'temperature'. However, the implementation appears to have a bug where the `obj` dictionary is immediately overwritten with an empty dictionary.

The main execution creates a mock context instance and calls the `conflicts_main` function with the two prompt file paths, an output CSV file path, and verbose mode enabled. The function returns three values: detected conflicts, total processing cost, and the model name used for analysis.

Finally, the script prints the results including the conflicts found, the cost formatted to six decimal places, and the model name. The results are also saved to a CSV file named 'conflicts_output.csv'. This appears to be a testing or demonstration script for a prompt conflict detection tool, likely used to identify inconsistencies or contradictions between different AI system prompts.",2025-12-14T08:32:32.189571+00:00
"context/construct_paths_example.py","This Python script, `demo_construct_paths.py`, serves as a minimal end-to-end demonstration for the `construct_paths` function from the `pdd.construct_paths` module, which is part of a Prompt-Driven Development (PDD) toolkit.

The script follows a five-step workflow: First, it creates a temporary prompt file named Makefile_makefile.prompt containing a simple task description for generating a hello world function. Second, it prepares the arguments that would typically be supplied by a command-line interface, including the input file path, force and quiet flags, the command type (generate), and command options.

Third, the script invokes the `construct_paths` function with these parameters, which returns four values: a resolved configuration, input strings (the file contents that were read), output file paths (where PDD will write results), and the detected programming language for the operation.

Fourth, it prints these returned values to the console in a formatted manner, iterating through dictionaries to display keys and values with proper indentation for multiline content. Finally, the script cleans up by deleting the temporary prompt file to leave the working directory in its original state.

This demo is useful for developers wanting to understand how the PDD path construction mechanism works internally before integrating it into larger applications.",2025-12-14T08:32:40.435943+00:00
"context/context_generator_example.py","This Python script demonstrates the usage of a context generator module from a package called 'pdd'. The script begins by importing necessary dependencies: the 'os' module for environment variable access, a 'context_generator' function from 'pdd.context_generator', and 'print' from the 'rich' library for enhanced console output formatting.

The script first checks for the 'PDD_PATH' environment variable and raises an error if it's not set, ensuring proper configuration before execution. It then defines several input parameters for the context generator: a simple Python function that adds two numbers, a corresponding prompt describing the desired functionality, the target programming language (Python), and configuration values for 'strength' (0.5) and 'temperature' (0.0) which likely control the generation behavior.

The main functionality calls the 'context_generator' function with these parameters and verbose mode enabled, receiving three return values: generated example code, the total cost of the operation (suggesting this may use a paid API service), and the name of the model used for generation.

Finally, the script outputs the results using rich formatting, displaying the generated code, the monetary cost formatted to six decimal places, and the model name. This appears to be a code generation or documentation tool that leverages AI models.",2025-12-14T08:32:48.212554+00:00
"context/context_generator_main_example.py","This Python script demonstrates the usage of a context generator tool, likely part of a prompt-driven development (PDD) framework. The script imports the Click library for command-line interface handling and a custom `context_generator_main` function from the `pdd` module.

The code sets up a Click context object with specific parameters: `force`, `quiet`, and `verbose` flags are all set to False, meaning the tool won't overwrite existing files, will display output messages, but won't show verbose details. Additional AI model configuration is stored in the context object, including a `strength` parameter set to 0.575 and a `temperature` of 0.0, both controlling the behavior of an underlying AI model.

The script defines three file paths: a prompt file located at prompts/get_extension_python.prompt, a code file at pdd/get_extension.py, and an output destination at output/example_code.py. These are passed to the `context_generator_main` function, which processes the inputs and returns generated example code, the total cost of the operation, and the name of the AI model used.

Finally, the script prints the generated code, the monetary cost formatted to six decimal places, and the model name used for generation.",2025-12-14T08:32:56.182460+00:00
"context/continue_generation_example.py","This Python script demonstrates the usage of a `continue_generation` function imported from a `pdd` module. The main purpose is to continue text generation using a language model (LLM) and track associated costs.

The script performs the following operations:

1. **Loading Input Data**: It reads two files - a preprocessed prompt from context/cli_python_preprocessed.prompt and an unfinished LLM output fragment from context/llm_output_fragment.txt.

2. **Configuration Parameters**: Sets up LLM parameters including a strength value of 0.915 and temperature of 0 (deterministic output).

3. **Text Generation**: Calls the `continue_generation` function with the loaded prompt, partial output, and configuration parameters. The function returns the completed text, total cost, and model name used.

4. **Output Handling**: Prints the total cost and model name to the console, then saves the final generated output to context/final_llm_output.py.

5. **Error Handling**: Includes try-except blocks to catch FileNotFoundError and general exceptions, providing appropriate error messages.

The script follows a standard Python entry point pattern with `if __name__ == __main__`. It appears to be part of a larger system for iterative or continued text generation with cost tracking capabilities.",2025-12-14T08:33:04.461704+00:00
"context/core/cli_example.py","This Python script serves as a comprehensive demonstration and documentation for the PDD (Prompt Driven Development) Command Line Interface (CLI) module. It illustrates how to programmatically interact with the main entry point (`cli`) and the custom Click Group class (`PDDCLI`) provided by the `pdd.core.cli` package.

The script defines various functions to showcase specific global options and features. Key demonstrations include invoking the CLI with AI configuration parameters like `--strength` (model power), `--temperature` (creativity), and `--time` (reasoning allocation). It also covers operational flags such as `--force` for overwriting files, `--local` for local execution, and `--quiet` for suppressing verbose output.

Furthermore, the file explains advanced features like cost tracking via `--output-cost`, which exports usage data to CSV, and context management using `--list-contexts` and `--context` to handle `.pddrc` configurations. Debugging capabilities are highlighted through the `--core-dump` flag, which captures execution state for error reporting. The `PDDCLI` class is described as a custom extension that enhances help formatting—specifically grouping Generate Suite commands—and centralizes error handling. The `main` function executes these examples sequentially to verify CLI behavior.",2025-12-20T06:54:17.339938+00:00
"context/core/dump_example.py","This Python script provides a comprehensive demonstration of the `pdd.core.dump` module, which is designed to facilitate debugging and error reporting for the PDD CLI tool. The script defines several example functions that illustrate the module's core capabilities. Specifically, `example_write_core_dump` shows how to serialize execution context, command results, and cost metrics into a JSON core dump file. The `example_github_config` function validates the environment configuration required for GitHub integration. Furthermore, `example_write_replay_script` demonstrates the automatic generation of shell scripts that allow developers to reproduce specific CLI runs using data from core dumps. The script also includes `example_build_issue_markdown`, which constructs formatted GitHub issue bodies containing platform details, error tracebacks, and reproduction instructions, and `example_post_issue_to_github`, which outlines the API for submitting these issues. A main execution block runs these examples in sequence, creating sample outputs in a local directory to verify the functionality of the core dump and replay logic.",2025-12-20T06:54:32.860639+00:00
"context/core/errors_example.py","This Python script serves as a comprehensive demonstration of the pdd.core.errors module, designed for centralized error handling within the PDD CLI. It illustrates how to utilize the module's key components: a styled Rich console, the handle_error function, and error buffer management tools. The script initializes by ensuring a local output directory exists. It then executes a series of examples showing how to print styled messages (using themes like info, warning, and success) and handle various exception types such as FileNotFoundError, ValueError, and RuntimeError. It specifically highlights the handle_error function's ability to process exceptions gracefully without crashing the application, while offering a quiet mode to record errors silently without console output. Furthermore, the code demonstrates the core dump functionality by retrieving collected errors via get_core_dump_errors(), inspecting their details (command name, exception type, message, and traceback), and exporting them to a JSON file named core_dump_errors.json. It concludes by showing how to clear the error buffer using clear_core_dump_errors(), ensuring the system can reset its error state.",2025-12-20T06:54:47.056324+00:00
"context/core/utils_example.py","The file `utils_example.py` serves as a demonstration and documentation script for the `pdd.core.utils` module, which contains helper functions for the PDD CLI. It defines a series of example functions that illustrate the behavior of core utilities used for environment detection and configuration management. Key functions demonstrated include `_first_pending_command`, which parses Click context arguments to identify subcommands; `_api_env_exists`, which verifies the presence of the global API configuration file; and `_completion_installed`, which checks shell configuration files for tab completion markers. The script also showcases `_project_has_local_configuration` for detecting local `.env` or `.pdd` settings, and `_should_show_onboarding_reminder`, which encapsulates the logic for displaying setup prompts to new users. Additionally, it provides informational usage for `_run_setup_utility`, the function responsible for launching the interactive setup wizard. The script utilizes `unittest.mock.MagicMock` to simulate CLI contexts and includes a `main` function that executes all examples, printing diagnostic output to the console to validate the utility logic.",2025-12-20T06:55:07.181921+00:00
"context/crash_main_example.py","This Python script demonstrates the usage of a `crash_main` function from the `pdd.crash_main` module, which is designed to automatically fix errors in code that caused a crash. The script creates a complete test scenario by generating four example files: a prompt file describing the desired factorial function, a buggy factorial code module that lacks negative number validation, a main program that incorrectly calls the factorial function with a negative number, and an error log capturing the resulting RecursionError.

The demonstration sets up a Click context with configuration parameters including strength, temperature, and verbosity settings. It then invokes `crash_main` with iterative fixing enabled, allowing up to 5 attempts to fix the code within a $5.00 budget. The function takes the prompt, code, program, and error files as inputs and produces fixed versions of both the code module and the calling program.

The script outputs the results including whether the fix was successful, the number of attempts required, total cost incurred, the model used, and displays the corrected code. This appears to be part of a prompt-driven development (PDD) toolkit that uses AI to automatically diagnose and repair code crashes by analyzing error logs and the original prompts that generated the faulty code.",2025-12-14T08:33:11.914892+00:00
"context/create_gcp_credential.py","This Python script demonstrates how to retrieve and decode a Google Cloud Platform (GCP) service account credential from Azure Key Vault for use with Vertex AI services. The code imports the `base64` and `json` modules and includes a commented-out placeholder class for `AzureKeyVaultService` that shows the expected structure of a GCP service account JSON (containing fields like project_id, private_key, client_email, etc.).

The main workflow consists of two steps: First, it fetches a base64-encoded service account JSON string from Azure Key Vault using the secret name GCP-VERTEXAI-SERVICE-ACC. Second, it decodes the base64 string and parses it into a Python dictionary using `json.loads()`. The script includes error handling for JSON decoding errors and general exceptions.

The decoded credentials can then be used with Google's `service_account.Credentials.from_service_account_info()` method to authenticate with GCP services, particularly for initializing ChatVertexAI. Currently, the actual Key Vault service call is commented out, leaving the `vertexai_service_account_b64` variable empty, which causes the script to print a failure message. This appears to be a template or example code for integrating Azure Key Vault secret management with GCP authentication in a cross-cloud scenario.",2025-12-14T08:33:19.789438+00:00
"context/ctx_obj_params.prompt","The provided text outlines the standard configuration parameters available within the Click context object (`ctx.obj`) for the PDD Command Line Interface (CLI). This dictionary is populated by the main CLI group and serves as a central repository for runtime settings. Key parameters include boolean flags for verbosity (`verbose`), forcing file overwrites (`force`), and suppressing output (`quiet`). It also manages Large Language Model (LLM) generation behaviors through floating-point values for `strength`, `temperature`, and relative thinking `time`. Additionally, it supports an optional `context` directory path and a `confirm_callback` for Text User Interface (TUI) interactions. The documentation further clarifies the resolution logic for LLM parameters, specifically `strength` and `temperature`. When command-specific functions (`cmd_*`) are invoked, they prioritize explicitly passed arguments over the values stored in `ctx.obj`. This design pattern allows orchestrators to override global CLI context settings with specific values when necessary, ensuring flexible control over the execution environment and generation outputs.",2025-12-20T06:55:24.902309+00:00
"context/detect_change/1/change_python.prompt","The provided file details the specifications for creating a Python function named 'change', intended to modify an input prompt based on a change prompt and associated code. The function requires inputs such as 'input_prompt', 'input_code', 'change_prompt', 'strength', and 'temperature', and returns a 'modified_prompt', 'total_cost', and 'model_name'. The implementation must utilize the Langchain LCEL framework and the Python Rich library for formatted console output. Key steps include loading and preprocessing specific prompt files, selecting an LLM using a custom 'llm_selector', and performing two model invocations: one to generate the change logic and another to extract the result as JSON. The function is responsible for calculating and displaying token counts and costs for both steps. Additionally, it must handle edge cases and errors effectively while using relative imports for dependencies.",2025-12-20T06:55:41.647831+00:00
"context/detect_change/1/code_generator_python.prompt","The provided file contains a prompt specification for an expert Python engineer to develop a function named code_generator. This function is intended to compile a raw prompt into a runnable code file using LangChain Expression Language (LCEL). The function accepts inputs including the raw prompt string, the target programming language, model strength, and temperature. It returns the generated runnable code, the total cost of the operation, and the name of the LLM model used.

The implementation follows a detailed eight-step procedure defined in the file. It begins by preprocessing the prompt and creating a LangChain template. It then utilizes an llm_selector to choose a model and tracks token usage and costs. The process includes a specific mechanism to handle incomplete outputs: it checks the last 600 characters of the generation, calling a continue_generation function if necessary, or a postprocess function if the output is complete. The workflow relies on several included context files and examples for internal modules such as preprocess, unfinished_prompt, and postprocess. Finally, the function calculates the total cost and returns the required data.",2025-12-20T06:55:56.367380+00:00
"context/detect_change/1/detect_change_output.txt","This document outlines a strategy to standardize Python-related instructions across various prompt files by integrating a shared `context/python_preamble.prompt`. The initiative aims to reduce redundancy, improve consistency, and compact the prompts. After analyzing three potential implementation plans—Direct, Conditional, and Reference-based inclusion—Plan C (Reference-based Inclusion) was selected as the optimal solution. This method allows for centralized updates and leverages the existing `<include>` tag functionality within the prompt processing system. The document details specific modifications for `prompts/change_python.prompt` and `prompts/fix_error_loop_python.prompt`. For these files, the plan instructs developers to insert an XML include tag for the Python preamble immediately following the role and goal sections. Additionally, redundant instructions now covered by the preamble—specifically regarding Python package structure, relative imports, and the use of the Python Rich library for console output—must be removed. Any unique instructions not addressed by the preamble are to be retained. The document notes that `context/python_preamble.prompt` and `prompts/code_generator_python.prompt` do not require changes.",2025-12-20T06:56:16.648947+00:00
"context/detect_change/1/fix_error_loop_python.prompt","This document outlines the specifications for a Python function named fix_error_loop, designed to iteratively repair errors in unit tests and corresponding source code. The function requires inputs such as file paths for the unit test, code file, and a verification program, as well as operational parameters like LLM strength, temperature, maximum attempts, and a financial budget. The process operates within a loop that executes pytest, logs errors, and attempts repairs using a helper function called fix_errors_from_unit_tests. Key features include managing file backups with specific naming conventions, tracking the total cost against the budget, and verifying code integrity via a secondary program before accepting changes. The function utilizes the rich library for pretty-printed console output. If the loop terminates due to budget or attempt limits, the system ensures the restoration of the iteration with the fewest errors. The final output includes the success status, the content of the fixed files, the total attempt count, and the accrued cost.",2025-12-20T06:56:35.452630+00:00
"context/detect_change/1/prompt_list.txt","The provided input consists of a collection of prompt definitions aimed at generating Python functions for an automated software engineering workflow. It begins with a preamble prompt establishing coding standards, such as using relative imports and the Rich library for console output. The collection includes a 'change' function prompt designed to modify existing prompts and code using LangChain LCEL, while tracking token usage and costs. A complex 'fix_error_loop' prompt outlines an autonomous debugging process that iteratively runs unit tests, analyzes error logs, and applies fixes within a set budget and attempt limit, ensuring file integrity through backups. Lastly, a 'code_generator' prompt describes a function to compile raw prompts into runnable code, handling preprocessing, incomplete generation detection, and post-processing. Collectively, these prompts define a robust system for AI-assisted code creation, modification, and self-correction.",2025-12-20T06:56:51.986105+00:00
"context/detect_change/1/python_preamble.prompt","The provided text outlines specific technical requirements for developing a function within a Python package context. It specifies that the function must employ relative imports, indicated by a single dot, to reference internal modules, thereby adhering to standard package structure conventions. Regarding user interface and output, the instructions mandate the use of the Python Rich library to ensure that all console text is pretty-printed, offering a polished visual presentation. Additionally, the text places a strong emphasis on code robustness and error management. Developers are instructed to ensure the function anticipates and handles edge cases effectively, specifically citing missing inputs and model errors as examples. To aid debugging and user interaction, the function is required to output clear and descriptive error messages rather than unhandled exceptions. These guidelines collectively aim to ensure the resulting code is modular, user-friendly, and resilient against common runtime issues.",2025-12-20T06:57:11.018478+00:00
"context/detect_change/2/change.prompt","The provided file contents represent a set of instructions and template directives aimed at optimizing prompt engineering workflows. Specifically, the text advocates for the use of a file named 'context/python_preamble.prompt' to achieve more compact prompts. It serves as a meta-instruction, guiding the user or a processing system to incorporate a standardized Python preamble. The text is structured with comments and XML-like tags that suggest a file inclusion mechanism. It defines two main sections: a 'preamble' section that attempts to include the actual content of the python preamble file, and an 'example' section that references 'prompts/code_generator_python.prompt' to demonstrate practical application. This structure implies a modular system where prompt components are reused to maintain consistency and reduce redundancy. The text also notes that this specific preamble might already be present in some contexts, alerting the user to check for duplication. Essentially, this is a documentation or configuration snippet used to standardize the setup for Python-related code generation prompts by dynamically linking to external context files.",2025-12-20T06:57:25.315725+00:00
"context/detect_change/2/change_detect.csv","The provided file contains a list of tasks for refactoring specific prompt files within the context/detect_change/2/ directory. Specifically, it targets three Python-related prompts: change_python.prompt, preprocess_python.prompt, and unfinished_prompt_python.prompt. For each of these files, the instruction is identical. The objective is to standardize the prompts by inserting the contents of a shared preamble file, ./context/python_preamble.prompt, using XML include tags. This insertion is directed to occur immediately following the role and goal statements within each prompt. Furthermore, the instructions mandate a cleanup process. Any instructions in the original files that become redundant due to the inclusion of the preamble—specifically those regarding pretty printing and handling edge cases—must be removed. Despite these deletions, the instructions emphasize the importance of maintaining the logical flow of the prompts and retaining any unique instructions specific to the individual tasks. Essentially, this file outlines a batch update to modularize Python prompts by offloading common instructions to a shared preamble file.",2025-12-20T06:57:44.777018+00:00
"context/detect_change/2/change_python.prompt","The provided text outlines the requirements for a Python function named `change`, designed to programmatically modify an `input_prompt` based on a `change_prompt` and associated `input_code`. Acting as an expert Python Software Engineer, the implementation must utilize the Langchain LCEL framework for logic and the Python Rich library for pretty-printed console output. The function accepts inputs including the original prompt, code, change instructions, and LLM parameters (strength, temperature), returning the modified prompt, total cost, and model name. The workflow involves loading specific prompt templates (`change_LLM.prompt` and `extract_prompt_change_LLM.prompt`), preprocessing them, and selecting an LLM via a `llm_selector` utility. The process executes in two stages: first generating a raw modification response, and then extracting the specific `modified_prompt` string via a JSON-parsing step. Throughout the execution, the function is required to calculate and display token counts and estimated costs for all model invocations. It must also handle edge cases and ensure relative imports are used for dependencies.",2025-12-20T06:57:58.348215+00:00
"context/detect_change/2/detect_change_output.txt","The document outlines a plan to refactor Python-related prompts by integrating a centralized preamble file, `context/python_preamble.prompt`, to ensure consistency and reduce redundancy. This preamble includes standard instructions for relative imports, pretty printing with the Python Rich library, and edge case handling. After evaluating three implementation strategies, the document selects a hybrid approach (Plan C) that inserts the preamble only where it enhances the prompt while retaining specific existing instructions. The plan analyzes four specific prompts. It concludes that `xml_tagger_python.prompt` requires no changes, while `change_python.prompt`, `preprocess_python.prompt`, and `unfinished_prompt_python.prompt` need modification. Detailed instructions are provided for these three files, directing the insertion of the preamble using `<include>` tags immediately after the role and goal statements. Furthermore, the plan requires the removal of any redundant instructions covered by the new preamble to maintain logical flow and prompt integrity.",2025-12-20T06:58:12.512178+00:00
"context/detect_change/2/preprocess_python.prompt","The file outlines requirements for a Python function named 'preprocess_prompt' designed to prepare prompt strings for Large Language Models (LLMs). Acting as an expert Python engineer, the developer must implement this function to handle specific XML-like tags and formatting rules using regular expressions. The function accepts a prompt string, a recursion flag, a curly bracket doubling flag, and an optional list of keys to exclude from doubling. It returns the cleaned, preprocessed string. Key processing tasks include handling three specific tags: 'include' (replaces the tag with file content), 'pdd' (removes comments and the tag itself), and 'shell' (executes shell commands and inserts the output). The function must support recursive file includes, identified either by the 'include' tag or by angle brackets within triple backticks. Additionally, it must manage curly bracket escaping by doubling single brackets unless they enclose keys listed in 'exclude_keys' or are already doubled. File paths should be resolved using the 'PDD_PATH' environment variable via a helper function 'get_file_path'. Finally, the implementation requires using the 'rich' library to pretty-print progress messages to the console.",2025-12-20T06:58:30.638230+00:00
"context/detect_change/2/prompt_list.json","The provided file contains a collection of prompt specifications for generating Python functions designed to manipulate and interact with Large Language Model (LLM) prompts. These prompts guide an expert Python engineer in creating a suite of tools using the LangChain library and the Python Rich library for console output.

The first prompt, change_python.prompt, outlines a function to modify existing prompts based on specific instructions, handling token counting and cost estimation. The second, preprocess_python.prompt, defines a utility for preprocessing prompt strings, specifically handling XML-like tags for file inclusions, comments, and shell commands, as well as managing curly bracket formatting.

The third prompt, unfinished_prompt_python.prompt, describes a function that analyzes a given text to determine if it constitutes a complete prompt, returning structured reasoning and completion status. Finally, xml_tagger_python.prompt details a function that enhances raw prompts by adding XML tags for better structure, utilizing a multi-step LLM process to generate and extract the tagged content.

Collectively, these prompts establish a framework for an LLM-based development environment, focusing on modularity, cost tracking, and robust prompt engineering workflows.",2025-12-20T06:58:45.167327+00:00
"context/detect_change/2/python_preamble.prompt","The provided text outlines specific technical requirements for developing a function within a Python package context. It specifies that the function must employ relative imports, indicated by a single dot, to reference internal modules, thereby adhering to standard package structure conventions. Regarding user interface and output, the instructions mandate the use of the Python Rich library to ensure that all console text is pretty-printed, offering a polished visual presentation. Additionally, the text places a strong emphasis on code robustness and error management. Developers are instructed to ensure the function anticipates and handles edge cases effectively, specifically citing missing inputs and model errors as examples. To aid debugging and user interaction, the function is required to output clear and descriptive error messages rather than unhandled exceptions. These guidelines collectively aim to ensure the resulting code is modular, user-friendly, and resilient against common runtime issues.",2025-12-20T06:58:59.357992+00:00
"context/detect_change/2/unfinished_prompt_python.prompt","The provided text outlines the requirements for developing a Python function named 'unfinished_prompt'. This function is designed to analyze a specific input string, 'prompt_text', to determine if it represents a complete thought or requires continuation. It utilizes a Large Language Model (LLM) via the Langchain LCEL framework. The function accepts inputs for the prompt text, model strength, and temperature. Key implementation steps include loading a specific prompt template from a directory defined by the '$PDD_PATH' environment variable, selecting an LLM using a relative import of 'llm_selector', and executing the model to obtain a JSON output containing reasoning and a boolean completion status. The function is also required to calculate the financial cost of the analysis based on token usage. It must provide user feedback by pretty-printing the status, token counts, costs, and reasoning using the 'rich' library. Finally, the function returns the reasoning, completion status, total cost, and model name, while ensuring graceful error handling for missing parameters or model issues.",2025-12-20T06:59:14.639961+00:00
"context/detect_change/2/xml_tagger_python.prompt","The provided file outlines the requirements for developing a Python function named xml_tagger. This function is designed to enhance raw LLM prompts by adding XML tags to improve structure and readability. It accepts a raw prompt string, along with strength and temperature parameters, and returns the XML-tagged prompt, the total cost of the operation, and the name of the LLM model used. The implementation utilizes LangChain Expression Language (LCEL) and internal modules for model selection and token counting. The workflow involves loading specific prompt templates from a project path, executing a two-step LCEL process to first generate an analysis and then extract the specific XML content via JSON output. Additionally, the function is required to calculate and pretty-print token usage and costs for each step, display the final output using rich Markdown, and return the cumulative cost.",2025-12-20T06:59:34.225120+00:00
"context/detect_change_example.py","This Python script is designed to analyze prompt files for potential changes using a change detection system. It imports the `detect_change` function from a module called `pdd.detect_change` and uses the Rich library's Console for formatted terminal output.

The script defines a list of four prompt files located in context and prompts directories, including files for Python preamble, code changes, error fixing, and code generation. There's also commented-out code that would dynamically gather all `.prompt` files from these directories.

The main purpose is to detect whether these prompt files need modifications based on a specific change description: making prompts more compact by utilizing a Python preamble context file. The script configures two LLM parameters: `strength` (set to 1, controlling model capability/cost) and `temperature` (set to 0, minimizing output randomness).

The `detect_change` function is called with these parameters and returns a list of detected changes, total cost, and the model name used. The script then displays the results using Rich's formatted console output, showing each prompt file name with its corresponding change instructions, along with the total API cost and model information. Error handling is implemented via a try-except block to catch and display any exceptions that occur during execution.",2025-12-14T08:33:28.540139+00:00
"context/detect_change_main_example.py","This Python script serves as a test harness or driver for the `detect_change_main` function from the `pdd.detect_change_main` module. The script simulates a command-line interface (CLI) environment using the Click library without actually running from the command line.

The main function creates a Click context object with model parameters including strength (0.5), temperature (0), and flags for force and quiet modes. It defines a list of four prompt files related to Python code generation and error handling, along with a change description file path.

The script writes a specific change instruction to the change file, directing the system to use a Python preamble context to make prompts more compact. It sets up an output path for results in CSV format and ensures the output directory exists.

The core functionality calls `detect_change_main()` with the configured context, prompt files, change file, and output path. This function returns a list of changes, total cost, and model name. The script then prints the model used, the total cost formatted to six decimal places, and iterates through each change to display the prompt name and change instructions.

Error handling is implemented with a try-except block to catch and display any exceptions that occur during execution. The script runs the main function when executed directly.",2025-12-14T08:33:36.229303+00:00
"context/device_flow.txt","This document explains GitHub's Device Flow authentication mechanism, designed for authorizing users in headless applications like CLI tools or Git Credential Manager. The process involves three main steps: First, the app requests device and user verification codes from GitHub via a POST request to the device/code endpoint, providing the client_id and optional scopes. This returns a device_code, user_code, verification_uri, expiration time (default 900 seconds), and polling interval. Second, the app displays the user_code and prompts the user to enter it at github.com/login/device. Third, the app polls GitHub's access_token endpoint until the user authorizes the device or codes expire, respecting the minimum polling interval to avoid rate limits. Upon successful authorization, the app receives an access token for API calls. The document details response formats (URL-encoded, JSON, or XML), rate limits (50 submissions per hour, with slow_down penalties for excessive polling), and various error codes including authorization_pending, slow_down, expired_token, unsupported_grant_type, incorrect_client_credentials, incorrect_device_code, access_denied, and device_flow_disabled. Device flow must be enabled in app settings before use.",2025-12-14T08:33:43.206937+00:00
"context/edit_file_example.py","This Python script serves as a test harness for demonstrating and verifying an `edit_file` module's functionality. It imports a `run_edit_in_subprocess` function from a separate module and uses it to programmatically edit text files based on natural language instructions.

The script requires the `PDD_PATH` environment variable to be set, pointing to a working directory. It creates a dummy example file with initial content and then applies specific editing instructions: changing original to UPDATED, replacing line 3 entirely, and adding a new line 5.

Key components include:
- **Configuration section**: Sets up paths and imports the editing function with error handling
- **`create_example_files()`**: Creates an output directory and populates a test file with sample content
- **`run_edit_file_test()`**: Executes the edit operation and optionally runs verification checks to confirm expected changes were applied
- **`run_example()`**: Main orchestration function that creates files and runs the test

The verification system checks for specific content changes (presence of UPDATED, replaced line 3, added line 5) and reports detailed pass/fail status with visual indicators. The script appears designed for testing an LLM-powered file editing system that uses MCP (Model Context Protocol) adapters, requiring API keys and external services to function properly.",2025-12-14T08:33:51.812223+00:00
"context/example.prompt","This document provides guidelines for creating code examples and documentation for a Python module system. Key instructions include: 1) Environment setup is pre-configured, so examples should focus on module usage rather than installation. 2) Examples will be placed in a 'pdd' directory and must use absolute imports in the format 'from pdd.module_name import module_name'. 3) All input/output units must be clearly documented (e.g., dollars per million tokens). 4) Click command-line examples should include arguments to enable execution without user interaction. 5) When file operations are involved, example input files should be created with appropriate content. 6) Error handling via try/except blocks should be avoided in examples. 7) All generated output files, including prompts and other artifacts, must be saved to the './output' directory. 8) The PDD_PATH environment variable is pre-set to point to the pdd directory location. These guidelines ensure consistent, runnable, and well-documented code examples across the module system.",2025-12-14T08:34:01.481061+00:00
"context/execute_bug_to_unit_test_failure.py","This file is a simple Python script that demonstrates the use of an imported function. The script imports a function called add from a module located at context.bug_to_unit_test_failure_example. This import path suggests the code is part of a larger project structure, likely related to testing or debugging examples, specifically dealing with unit test failures.

After importing the function, the script calls the add function with two integer arguments (2 and 3) and prints the result to the console. The expected output would be 5, assuming the add function performs standard addition of its two parameters.

The code appears to be a minimal test or demonstration script, possibly used to verify that the add function works correctly or to showcase a specific bug scenario in a unit testing context. The naming convention of the import path (bug_to_unit_test_failure_example) suggests this might be part of educational material or documentation showing how bugs can be identified through unit test failures. The script itself is straightforward with just two lines of executable code - one import statement and one print statement that calls the imported function.",2025-12-14T08:34:08.007223+00:00
"context/final_llm_output.py","This Python file implements a command-line interface (CLI) for PDD (Prompt-Driven Development), a tool that leverages AI to assist with code generation and management. Built using the Click library and Rich console for output formatting, the CLI provides several chainable commands:

**Core Commands:**
- `generate`: Creates runnable code from a prompt file
- `example`: Generates example code from existing code and its prompt
- `test`: Creates unit test files for given code
- `preprocess`: Preprocesses prompt files with optional XML tagging
- `fix`: Repairs errors in code and unit tests, with optional iterative loop mode
- `split`: Divides large prompts into smaller, manageable files
- `change`: Modifies prompts based on change requests
- `update`: Updates original prompts based on code modifications
- `install_completion`: Sets up shell autocompletion for bash, zsh, or fish
- `version`: Displays CLI version

**Global Options:**
The CLI supports flags for force overwrite, AI model strength/temperature settings, verbosity control, and cost tracking via CSV output.

**Architecture:**
Commands use a processor decorator pattern enabling Unix-like piping between subcommands. Each command handles file path construction, calls appropriate generator modules from the pdd package, writes outputs, and reports costs. Error handling with Rich-formatted messages is implemented throughout.",2025-12-14T08:34:15.513743+00:00
"context/find_section_example.py","This file provides documentation and example usage for the `find_section` function from the `pdd.find_section` module. The function is designed to parse text containing code blocks (typically from LLM output) and identify the locations of different code sections within the text.

The example demonstrates how to use the function by first importing it, then providing a sample string containing multiple code blocks in different programming languages (Python and JavaScript). The text is split into lines using `splitlines()`, and then passed to `find_section()` which returns a list of tuples identifying each code block.

The function accepts three parameters: `lines` (a required list of strings representing text lines), `start_index` (an optional integer defaulting to 0 for where to begin searching), and `sub_section` (an optional boolean flag defaulting to False).

The output is a list of tuples, each containing the programming language identifier, the starting line index, and the ending line index of each code block found. The example also shows loading content from an external file (`unrunnable_raw_llm_output.py`) and processing it the same way. This utility is useful for extracting and processing code snippets embedded within larger text outputs.",2025-12-14T08:34:25.165817+00:00
"context/firecrawl_example.py","This Python script demonstrates how to use the Firecrawl library to scrape web content. The code begins with a comment indicating that the firecrawl-py package should be installed using pip. It then imports the FirecrawlApp class from the firecrawl module along with the os module for environment variable access.

The script initializes a FirecrawlApp instance by retrieving an API key from the environment variable 'FIRECRAWL_API_KEY'. If this environment variable is not set, it falls back to a placeholder string 'YOUR_API_KEY', which would need to be replaced with an actual API key for the code to function properly.

The main functionality involves calling the scrape_url method on the app instance, targeting Google's homepage (https://www.google.com). The method is configured to return the scraped content in markdown format by passing ['markdown'] as the formats parameter. The result is stored in the scrape_result variable.

Finally, the script prints the markdown representation of the scraped content by accessing the markdown attribute of the scrape_result object. This is a simple example showcasing Firecrawl's capability to convert web pages into structured markdown format, which can be useful for content extraction, documentation, or data processing tasks.",2025-12-14T08:34:32.867374+00:00
"context/fix_code_loop_example.py","This Python script acts as a demonstration and test harness for the `fix_code_loop` function imported from the `pdd` package. The `main` function sets up a synthetic debugging scenario by creating two files in an `output` directory: `module_to_test.py`, which contains a simple function to calculate averages, and `verify_code.py`, which intentionally calls this function with invalid arguments to trigger an error.

Once the environment is prepared, the script executes `fix_code_loop` using specific parameters, including a maximum of five attempts, a set budget, and a deterministic temperature. This process attempts to iteratively repair the code based on the provided prompt and the verification failure. After the loop completes, the script prints the results, including the success status, total attempts, cost, the model used, and the final corrected code. The script also contains commented-out logic for cleaning up the generated test files.",2025-12-22T05:48:01.420661+00:00
"context/fix_code_module_errors_example.py","This Python script demonstrates the usage of the `fix_code_module_errors` function from the `pdd.fix_code_module_errors` module, which is designed to automatically fix code errors using an AI/LLM-based approach.

The example sets up a scenario with a buggy program that attempts to calculate the sum of a string 123 instead of a list of numbers, resulting in a TypeError. The script defines the original erroneous program, the prompt that generated it, the problematic code module, and the error message received.

The `fix_code_module_errors` function is called with several parameters: the program with the error, the original prompt, the code module, the error message, a model strength setting (0.5), temperature (0 for deterministic output), and a verbose flag for debugging.

The function returns seven values: boolean flags indicating whether the program and code module need updates, the fixed versions of both, the raw LLM output from the fix attempt, the total API cost in USD, and the model name used.

Finally, the script prints all results including whether updates are needed, the corrected code, the raw fix output, the API cost, and which model was utilized. This serves as a practical example for developers wanting to integrate automated code error correction into their workflows.",2025-12-14T08:34:48.261219+00:00
"context/fix_error_loop_example.py","This Python script serves as a demonstration entry point for an automated error-fixing loop system. The main function showcases the usage of a `fix_error_loop` function imported from the `pdd.fix_error_loop` module, which appears to be part of a larger code generation or debugging framework.

The script configures several parameters for the error-fixing process, including paths to a unit test file, a code file, a verification program, and a prompt file. It also sets numerical parameters such as strength (0.85), temperature (1), maximum attempts (5), and a budget limit ($100) for the fixing process.

The `fix_error_loop` function is called with these parameters and returns multiple values: a success flag, the final unit test content, the final code, total attempts made, total cost incurred, and the model name used. The script uses the Rich library to display formatted output panels showing these results.

Error handling is implemented via a try-except block to catch and display any exceptions that occur during execution. The script follows a standard Python pattern with a `if __name__ == __main__` guard to ensure the main function only runs when executed directly. This appears to be part of a prompt-driven development (PDD) workflow for iteratively fixing code errors using AI assistance.",2025-12-14T08:34:56.181068+00:00
"context/fix_errors_from_unit_tests/1/conflicts_in_prompts.py","The provided code implements a Python module designed to identify logical or directive conflicts between two text prompts using Large Language Models (LLMs) and the LangChain framework. It defines Pydantic models, `Conflict` and `ConflictOutput`, to structure the analysis results, ensuring each identified conflict includes a description, explanation, and specific suggestions for modifying both prompts to resolve the issue. The primary function, `conflicts_in_prompts`, orchestrates this process by first loading prompt templates from a directory specified in the `PDD_PATH` environment variable. It utilizes a helper function, `llm_selector`, to configure the model based on strength and temperature parameters. The workflow operates in two stages: initially generating a textual analysis of potential conflicts, and subsequently extracting this analysis into a structured JSON format using a secondary LLM call. Additionally, the code calculates and reports estimated API costs based on token usage and uses the `rich` library for console output. The function returns a list of structured conflicts, the total cost, and the model name used.",2025-12-20T06:59:48.336752+00:00
"context/fix_errors_from_unit_tests/1/conflicts_in_prompts_python.prompt","The provided file outlines the requirements for an expert Python engineer to develop a function named `conflicts_in_prompts`. This function is designed to analyze two input prompts (`prompt1` and `prompt2`) to identify logical or structural conflicts and propose resolutions. The function accepts optional parameters for model strength and temperature. The implementation must utilize the LangChain Expression Language (LCEL) and specific internal modules like `llm_selector` for model selection and token counting. The process involves a multi-step workflow: first, loading external prompt templates (`conflict_LLM.prompt` and `extract_conflict_LLM.prompt`) using the `$PDD_PATH` environment variable. Next, the function executes a preliminary LLM run to generate a textual comparison of the inputs. This is followed by a second, higher-strength LLM run designed to parse the initial output into a structured JSON format containing a list of conflicts, each detailing the issue and providing specific suggestions for modification. Finally, the function returns the list of identified conflicts, the name of the model used, and the total calculated cost of the operations. The code must also include user feedback via pretty-printed messages regarding token usage and costs during execution.",2025-12-20T07:00:04.325446+00:00
"context/fix_errors_from_unit_tests/1/test_conflicts_in_prompts.py","This Python file contains a comprehensive suite of unit tests for the `conflicts_in_prompts` function, part of the `pdd` package. Using the `pytest` framework and `unittest.mock`, the tests isolate the function from external dependencies like the file system and LLM services. Several fixtures are defined to mock the environment (`PDD_PATH`), prompt file reading, the `llm_selector` (simulating LLM outputs, token counting, and costs), and the `JsonOutputParser`. The test cases cover various scenarios including a success path that verifies the function correctly identifies conflicts and calculates costs using mocked inputs. It also includes error handling tests to check for specific exceptions when the environment variable is missing or prompt files are not found. Additionally, the suite tests for resilience against unexpected exceptions and confirms that custom parameters like `strength` and `temperature` are correctly passed to the internal LLM selector. Overall, the file ensures the robustness of the conflict detection logic by validating both successful execution and failure modes.",2025-12-20T07:00:25.067873+00:00
"context/fix_errors_from_unit_tests/2/code_generator.py","This file defines a Python function named `code_generator` designed to generate code snippets based on a user-provided prompt using a Large Language Model (LLM). It utilizes the `rich` library for console output and `langchain` for model interaction. The process begins by preprocessing the input prompt and selecting an appropriate LLM based on a specified strength and temperature via a selector module. It calculates the estimated cost for input tokens before invoking the model. Upon receiving the initial output, the function calculates output costs and displays the result. Crucially, it includes logic to detect if the generated code is incomplete by analyzing the end of the output string. If the generation is deemed unfinished, it triggers a continuation process; otherwise, it runs a post-processing step to refine the output for the target programming language. The function tracks costs throughout the entire pipeline, including input, output, completion checks, and post-processing. Finally, it returns the executable code, the total calculated financial cost of the operation, and the name of the model used, while handling potential errors gracefully with console alerts.",2025-12-20T07:00:43.856313+00:00
"context/fix_errors_from_unit_tests/2/code_generator_python.prompt","This file contains a prompt for an expert Python engineer to design a function called code_generator. The function's purpose is to compile a raw prompt into a runnable code file using the LangChain framework. It requires inputs for the raw prompt, target language, model strength, and temperature, and returns the runnable code, total cost, and model name. The implementation follows a specific eight-step process involving LangChain Expression Language (LCEL). Key steps include preprocessing the prompt, selecting an LLM, and running the model while tracking token usage and cost. The logic includes a mechanism to detect incomplete generations by analyzing the last 600 characters of the output; if incomplete, it continues generation, otherwise, it post-processes the result. The workflow relies on various internal modules for tasks like preprocessing, token counting, and post-processing, examples of which are included via file references. Ultimately, the function aggregates the total cost of inputs, outputs, and processing steps before returning the final results.",2025-12-20T07:01:02.975489+00:00
"context/fix_errors_from_unit_tests/2/test_code_generator.py","This Python file defines a suite of unit tests for the `code_generator` module within the `pdd` package, utilizing the `pytest` framework and `unittest.mock` library. The tests focus on verifying the functionality of the `code_generator` function by mocking various internal dependencies such as `preprocess`, `llm_selector`, `unfinished_prompt`, `continue_generation`, and `postprocess`. The suite includes three primary test cases. The first, `test_code_generator_success`, simulates a standard workflow where the Large Language Model (LLM) successfully generates code. It asserts that the returned runnable code, calculated total cost, and model name match expected mock values. The second test, `test_code_generator_incomplete_generation`, validates the logic when the initial prompt response is deemed incomplete, requiring the `continue_generation` module to finalize the output. Finally, `test_code_generator_exception_handling` ensures robustness by simulating an exception during the preprocessing stage, verifying that the function gracefully handles errors by returning empty strings and a zero cost.",2025-12-20T07:01:21.680865+00:00
"context/fix_errors_from_unit_tests/3/context_generator.py","This file defines a Python function named `context_generator` designed to generate example code using Large Language Models (LLMs) via the LangChain framework. The function accepts inputs including a code module, a user prompt, target programming language, and model parameters like strength and temperature. Its workflow involves several distinct stages: validating the `PDD_PATH` environment variable, loading and preprocessing a specific prompt template, and selecting an appropriate LLM using a custom `llm_selector`. The function executes the generation process and calculates estimated costs based on token usage. Crucially, it includes logic to handle incomplete generations by detecting unfinished outputs and invoking a `continue_generation` routine. Finally, the output is post-processed to extract the relevant code, the total cost is computed (including input, output, and auxiliary steps), and the function returns the generated code, total cost, and model name. Error handling is implemented to catch and report issues during execution.",2025-12-20T07:01:39.069804+00:00
"context/fix_errors_from_unit_tests/3/context_generator_python.prompt","The provided input defines the requirements for an expert Python engineer to write a function named context_generator. This function's purpose is to generate concise usage examples for a specific code_module using LangChain Expression Language (LCEL). The function accepts inputs such as the code module string, the originating prompt, the target language (defaulting to Python), and LLM parameters like strength and temperature. It outputs the generated example code, the total cost, and the model name. The implementation follows a nine-step process involving several internal utility modules. It starts by loading and preprocessing a prompt template from a file path specified by the $PDD_PATH environment variable. It then selects an LLM using llm_selector and invokes the model with the preprocessed inputs. A critical feature is the detection of incomplete generation using unfinished_prompt; if the output is cut off, continue_generation is called, otherwise, postprocess is applied. The function tracks token usage and costs throughout the process, printing status updates and the final total cost before returning the results. The file also includes references to various context files and examples for preprocessing, LLM selection, and post-processing to guide the development.",2025-12-20T07:01:53.325645+00:00
"context/fix_errors_from_unit_tests/3/test_context_generator.py","This file contains a suite of unit tests for the `context_generator` function within the `pdd` package, implemented using the `pytest` framework and `unittest.mock`. The tests are designed to verify the robustness and correctness of the context generation logic, which involves interacting with Large Language Models (LLMs). The suite utilizes several fixtures to mock external dependencies, such as environment variables (`PDD_PATH`), file system operations, and LLM chain executions. Key test cases include `test_context_generator_success`, which validates the complete successful workflow from preprocessing to postprocessing, ensuring the function returns the expected tuple of output text, cost, and model name. Another critical test, `test_context_generator_unfinished_prompt`, checks the handling of scenarios where generation is interrupted and requires continuation. Furthermore, the file includes tests for error handling and edge cases. `test_context_generator_missing_env_variable` confirms that missing configuration raises appropriate errors, while `test_context_generator_file_not_found` and `test_context_generator_exception_handling` ensure the function gracefully handles missing files or unexpected exceptions by returning a default empty result rather than crashing.",2025-12-20T07:02:10.559395+00:00
"context/fix_errors_from_unit_tests/4/detect_change.py","This Python script defines a function named `detect_change` designed to automate the analysis of prompt files using Large Language Models (LLMs) via the LangChain framework. The function takes a list of prompt filenames, a change description, and model parameters (strength and temperature) as input. It proceeds by loading internal prompt templates from a directory specified by an environment variable, preprocessing them, and selecting an appropriate LLM. The script reads the contents of the target files and executes a two-step LLM chain: first to analyze the files against the change description, and second to extract the results into a structured JSON format containing specific change instructions. Throughout the process, the code calculates and logs token usage and estimated costs using the `rich` library for formatted console output. It handles potential errors such as missing files or invalid JSON and returns a tuple containing the list of required changes, the total cost, and the name of the model used.",2025-12-20T07:02:30.822134+00:00
"context/fix_errors_from_unit_tests/4/detect_change_1_0_1.py","This file defines a Python function named `detect_change` designed to analyze a list of prompt files and determine necessary modifications based on a provided change description. Utilizing the LangChain library and a custom `llm_selector`, the function orchestrates a two-step Large Language Model (LLM) workflow. First, it reads the target prompt files and invokes an LLM to evaluate them against the change description using a specific template. Second, it employs a separate extraction step to parse the LLM's analysis into a structured JSON list containing specific change instructions. The code tracks and calculates the token usage and estimated costs for these operations, reporting details via the `rich` console library. It handles potential errors such as file not found or JSON decoding issues and returns a tuple containing the list of changes, the total cost, and the model name used.",2025-12-20T07:02:46.856217+00:00
"context/fix_errors_from_unit_tests/4/detect_change_python.prompt","This document outlines the specifications for a Python function named detect_change, intended for an expert Python Software Engineer context. The function's primary objective is to analyze a provided list of prompt files and a change description to determine which prompts require updates. It accepts inputs including a list of prompt filenames, a description of the desired changes, and LLM parameters like strength and temperature. The function utilizes LangChain Expression Language (LCEL) and internal modules for tasks such as preprocessing, LLM selection, and token counting. The execution flow involves loading specific prompt templates (detect_change_LLM.prompt and extract_detect_change_LLM.prompt), preprocessing them, and invoking an LLM to evaluate the prompt list against the change description. A secondary extraction step parses the LLM's raw output into a structured JSON format. The function ultimately returns a changes_list containing prompt names and modification instructions, along with the total execution cost and the model name used. The file also references various example modules and context files to guide the implementation of preprocessing, postprocessing, and model interaction.",2025-12-20T07:03:00.344802+00:00
"context/fix_errors_from_unit_tests/4/test_detect_change.py","This file contains a comprehensive suite of unit tests for the `detect_change` function within the `pdd` package, utilizing the `pytest` framework. It employs `unittest.mock` to simulate external dependencies and isolate the logic under test. The file defines helper classes such as `MockPromptTemplate`, `MockLLM`, and `MockOutputParser` to mimic the behavior of language model interactions and output parsing without requiring actual API calls. The test suite includes fixtures for setting up mock environments, file contents, LLM selectors, and preprocessing functions. Four primary test cases are implemented: `test_detect_change_success` verifies the correct execution flow, ensuring valid results, costs, and model names are returned when the chain operates as expected. `test_detect_change_file_not_found` checks error handling for missing input files. `test_detect_change_json_decode_error` ensures the system handles malformed JSON responses from the LLM gracefully. Finally, `test_detect_change_unexpected_error` confirms that the function remains robust against unforeseen exceptions, consistently returning empty results rather than crashing. These tests collectively ensure the stability and fault tolerance of the change detection functionality.",2025-12-20T07:03:15.709508+00:00
"context/fix_errors_from_unit_tests/4/test_detect_change_1_0_1.py","This file contains a suite of unit tests for the `detect_change` function within the `pdd` module, utilizing the `pytest` framework. It defines several fixtures to mock external dependencies and environment configurations, ensuring isolated testing. Specifically, `mock_environment` sets up a dummy path, while `mock_file_contents`, `mock_llm_selector`, and `mock_preprocess` simulate file inputs, Language Model (LLM) interactions, and data preprocessing respectively.

The tests cover various scenarios:
1. **Success Case (`test_detect_change_success`):** Verifies that the function correctly processes a prompt file, interacts with the mocked LLM, parses the JSON output, and returns the expected change instructions, cost, and model name.
2. **File Not Found (`test_detect_change_file_not_found`):** Checks the function's resilience when a specified prompt file is missing, ensuring it returns empty results without crashing.
3. **JSON Error (`test_detect_change_json_decode_error`):** Simulates a failure in parsing the LLM's output, verifying that the system handles malformed data gracefully.
4. **Unexpected Error (`test_detect_change_unexpected_error`):** Ensures that generic exceptions raised during execution are caught, resulting in a safe failure state with empty outputs.

Overall, the file ensures the robustness of the change detection logic against valid inputs, missing files, parsing errors, and runtime exceptions.",2025-12-20T07:03:35.282269+00:00
"context/fix_errors_from_unit_tests/5/continue_generation.py","This Python script implements a mechanism for continuing and refining text generation using Large Language Models (LLMs) via the LangChain framework. The primary function, continue_generation, manages the workflow of extending incomplete code blocks or text outputs. It begins by loading and preprocessing specific prompt templates from a directory defined by the PDD_PATH environment variable. The script utilizes an llm_selector to configure models for both generation and content trimming, employing Pydantic models to enforce structured JSON outputs for specific tasks. The core logic operates within a loop that iteratively invokes the LLM to extend the provided content. During each iteration, the script calculates token usage and associated costs, updates the accumulated code block, and utilizes an external unfinished_prompt utility to determine if the generation is complete. Upon completion, a final trimming process cleans the output. The script uses the rich library for console logging to display progress, costs, and the final generated code block, ultimately returning the complete text, total cost, and model name.",2025-12-20T07:03:50.732675+00:00
"context/fix_errors_from_unit_tests/5/continue_generation_2_0_1.py","This Python module implements a `continue_generation` function designed to iteratively generate and complete code blocks using Large Language Models (LLMs) orchestrated by LangChain. It imports necessary libraries for console formatting (`rich`), prompt management, and structured output parsing (`Pydantic`).

The function operates by first loading and preprocessing specific prompt templates from a directory defined by the `PDD_PATH` environment variable. It initializes LLM chains for generating content and trimming results, utilizing a helper `llm_selector` to configure models based on desired strength and temperature.

The core execution flow involves extracting an initial code block from previous output and entering a loop to generate subsequent code segments. Inside the loop, the script tracks token usage and costs, printing updates to the console. It employs an `unfinished_prompt` utility to analyze the tail of the generated text, determining if the generation is complete. If finished, the script trims the final segment, aggregates the total cost, and returns the complete code block along with cost metrics and the model name. If incomplete, it appends the new text and continues the generation cycle.",2025-12-20T07:04:11.051934+00:00
"context/fix_errors_from_unit_tests/5/continue_generation_python.prompt","The provided file outlines the specifications for a Python function named continue_generation, designed to finalize incomplete text generation from a Large Language Model (LLM). Acting as an expert Python Software Engineer, the function takes a formatted input prompt, partial LLM output, and model parameters (strength and temperature) to produce a complete final_llm_output, along with the total execution cost and model name. The process involves several distinct steps utilizing Langchain LCEL. First, it loads and preprocesses specific prompt files from a directory defined by the $PDD_PATH environment variable. It then configures the LLM using an llm_selector module, tracking token usage and costs throughout execution. The core logic extracts the existing code block from the initial output, then generates continuations. It employs a loop that checks for completeness using an unfinished_prompt helper function. If the output is detected as incomplete, the function iteratively generates and appends text until finished. Once complete, it trims the results to ensure a seamless transition between the original and generated text. Finally, the function prints the result using Rich Markdown and returns the consolidated output, total cost, and model name.",2025-12-20T07:04:26.582314+00:00
"context/fix_errors_from_unit_tests/5/error_log.txt","This file contains the console output log generated by a `pytest` execution session. The log details the initialization of a test environment on a Darwin (macOS) operating system. The Python interpreter used is version 3.12.4, located within a specific Anaconda virtual environment path (`/Users/gregtanaka/anaconda3/envs/pdd`). The testing framework, `pytest`, is version 8.3.2, running alongside `pluggy-1.5.0`. Additionally, the environment includes plugins such as `cov-5.0.0` for coverage analysis and `anyio-4.4.0`. The root directory for this test session is identified as `/Users/gregtanaka/Documents/PDD`, with the cache directory located at `.pytest_cache`. The critical information in this log is the failure of the test collection process. The system attempted to collect tests but found zero items (collected 0 items). Consequently, the summary line indicates that no tests ran, and the execution time was effectively instantaneous (0.00s). The log concludes with a fatal error message explaining the reason for the empty session: ERROR: file or directory not found: /path/to/non/existent/file.py. This suggests the user attempted to invoke `pytest` on a specific file path that does not exist on the filesystem, causing the runner to abort immediately before any testing logic could be applied.",2025-12-20T07:04:42.789844+00:00
"context/fix_errors_from_unit_tests/5/test_continue_generation.py","This Python file contains a suite of unit tests for the `continue_generation` function within the `pdd` package, utilizing the `pytest` framework. The tests employ `unittest.mock` to isolate the function from external dependencies such as file systems, environment variables, and Large Language Model (LLM) APIs.

The file defines several fixtures to facilitate testing: `mock_environment` sets required environment variables, `mock_prompts` supplies template strings, and `mock_llm_selector` simulates LLM behavior by returning specific JSON or text responses and tracking token usage. Additionally, `mock_unfinished_prompt` controls the iteration loop by simulating completion status.

The test cases cover various scenarios, including: verifying the successful execution of the generation loop and result aggregation; ensuring proper error handling when the `PDD_PATH` environment variable is missing or files are not found; validating the iterative logic where the generator runs multiple times until finished; and confirming that the total API cost is calculated and returned correctly as a positive float. These tests ensure the robustness of the continuous generation logic and its integration with the `rich` library for console output.",2025-12-20T07:05:05.637073+00:00
"context/fix_errors_from_unit_tests/5/test_continue_generation_2_0_1.py","This Python file contains a suite of unit tests for the `continue_generation` function within the `pdd` package, utilizing the `pytest` framework and `unittest.mock`. The tests are designed to verify the functionality of an LLM-based code generation continuation feature. Key fixtures are defined to mock the runtime environment (`mock_environment`), prompt templates, and the LLM selection process (`mock_llm_selector`), simulating API responses, token counting, and cost calculations. The test suite covers various scenarios including the successful execution path (`test_continue_generation_success`), error handling for missing environment variables or files, and the logic for iterative generation (`test_continue_generation_multiple_iterations`) where the process loops until completion is signaled. Additionally, `test_continue_generation_cost_calculation` ensures that the total financial cost of the LLM usage is aggregated correctly. Overall, the file ensures the robustness of the generation continuation logic, validating environment setup, error management, iterative processing, and resource accounting.",2025-12-20T07:05:24.190136+00:00
"context/fix_errors_from_unit_tests_example.py","This Python script serves as a demonstration for the `fix_errors_from_unit_tests` function from the `pdd` module. The main function sets up example inputs to showcase how the error-fixing utility works. It defines a sample unit test for an `add` function with an intentionally incorrect test case (asserting that `add(0, 0) == 1`), along with the corresponding implementation code that includes a deprecated NumPy function call (`np.asscalar`). The script also includes an error message string containing both an AssertionError and a DeprecationWarning. Configuration parameters include an error log file path, an LLM strength value of 0.85, and a temperature setting of 1.0. The `fix_errors_from_unit_tests` function is called with these inputs and returns multiple values: flags indicating whether the unit test and code were updated, the fixed versions of both, analysis results, total cost, and the model name used. The script uses the `rich` library to print formatted output displaying all these results. This appears to be part of a larger system that uses language models to automatically diagnose and fix errors in code based on failing unit tests.",2025-12-14T08:35:04.044052+00:00
"context/fix_main_example.py","This Python script defines a command-line interface (CLI) tool for automatically fixing errors in code and unit tests. It uses the Click library for argument parsing and Rich for formatted console output.

The main component is a `fix_command` function decorated with Click options that accepts several parameters: paths to a prompt file, code file, unit test file, and error log file; optional output paths for fixed tests, code, and results; a loop flag for iterative fixing; a verification program path; maximum fix attempts (default 3); a budget limit (default $5.00); an auto-submit flag; and an agentic fallback option.

When executed, the command calls `fix_main` from the `pdd.fix_main` module, passing all configuration parameters. The function returns success status, fixed test/code content, attempt count, cost, and model used. Results are displayed with color-coded messages—green for success showing attempts, cost, and model info, or red for failure.

The script's main block sets up a default context with strength, temperature, force, and quiet settings, then configures test arguments pointing to specific prompt, code, test, and verification files in a development environment. This appears to be part of a larger pdd (possibly prompt-driven development) system for AI-assisted code repair.",2025-12-14T08:35:11.924131+00:00
"context/fix_verification_errors_example.py","This Python script demonstrates the usage of the `fix_verification_errors` function from the `pdd` package, which automatically identifies and fixes errors in code modules using LLM (Large Language Model) analysis of verification output logs.

The script sets up a complete example scenario with several key components: a verification program that tests a calculator module, an original prompt describing the intended functionality, a buggy code module containing an intentional error (subtraction instead of addition), and sample verification output showing the failure.

The example uses a `calculator_module` with an `add_numbers` function that incorrectly performs subtraction. When the verification program runs, it compares expected results against actual results and outputs clear success/failure markers that the LLM can interpret.

The `fix_verification_errors` function accepts parameters including the program code, original prompt, buggy code, verification output, LLM strength (model capability), temperature (randomness control), and verbose mode settings. It returns a dictionary containing information about issues found, whether the program or code was updated, the fixed versions, LLM model used, total cost, and explanations of identified issues.

Prerequisites include having the pdd package installed, prompt template files accessible, LLM API keys configured, and proper environment variables set. The script uses the Rich library for formatted console output to display results.",2025-12-14T08:35:20.612650+00:00
"context/fix_verification_errors_loop_example.py","This Python script demonstrates the usage of the `fix_verification_errors_loop` function from the `pdd` module, which is designed to automatically fix bugs in code through iterative verification and correction. The example sets up a simple calculator module scenario with an intentional bug—a subtraction operation instead of addition in the `add_numbers` function.

The script creates three files: a main program (`program.py`) that tests the calculator module with command-line arguments, a buggy code module (`calculator_module.py`) containing the flawed function, and a verification program that asserts expected behavior.

Key configuration parameters include: `strength` (0.5) for LLM model selection, `temperature` (0.0) for deterministic outputs, `max_attempts` (5) for retry limits, and a `budget` ($10) for cost control. The script also specifies output paths for fixed code and logging.

The `fix_verification_errors_loop` function is called with all these parameters, including program arguments [5, 3]. Upon completion, the script prints results including success status, final corrected program and code, total attempts made, total cost incurred, and the model name used. This demonstrates an automated debugging workflow using LLM-powered code correction with verification feedback loops.",2025-12-14T08:35:29.500398+00:00
"context/fix_verification_main_example.py","This Python script demonstrates the usage of a `fix_verification_main` function from a `pdd` package, designed for automated code verification and fixing using LLM (Large Language Model) capabilities. The script sets up a testing environment by creating example files including a prompt file, intentionally buggy calculator code (that subtracts instead of adds), a program to run the code, and a verification program with assertions.

The demonstration showcases two verification modes: Single Pass Mode and Loop Mode. In Single Pass Mode (`loop=False`), the LLM evaluates program output against the original prompt once and may propose fixes. In Loop Mode (`loop=True`), the system iteratively runs the program, uses the LLM to check output, applies fixes to the code/program, and runs verification tests until success is achieved or limits (max attempts or budget) are reached.

The script uses Click for CLI context simulation and includes configurable parameters like temperature, strength, budget limits, and maximum attempts. It creates a dummy Click context to pass parameters as if invoked from a command line. Output files track verification results, corrected code, and corrected programs. The Rich library provides formatted console output. This serves as a practical example for integrating LLM-based automated code verification and repair workflows into development pipelines.",2025-12-14T08:35:37.889808+00:00
"context/gcs_hmac_test.py","This Python script demonstrates how to upload files to Google Cloud Storage (GCS) using the boto3 library with HMAC (Hash-based Message Authentication Code) credentials. The script leverages the S3-compatible API that GCS provides, allowing developers to use familiar AWS SDK tools with Google's storage service.

The code begins by loading environment variables from a .env file using the dotenv library, specifically requiring three credentials: GCS_HMAC_ACCESS_KEY_ID, GCS_HMAC_SECRET_ACCESS_KEY, and GCS_BUCKET_NAME. It configures logging for debugging purposes and validates that all required environment variables are present before proceeding.

The main functionality creates an S3 client configured to point to Google's storage endpoint (https://storage.googleapis.com) instead of AWS. The script includes commented-out configuration options for newer versions of Boto3 (>1.35) that may require specific signature and checksum settings.

Once the client is established, the script uploads a simple test text file (test-hmac-upload.txt) to the specified GCS bucket using the put_object method. Comprehensive error handling is implemented to catch and log various failure scenarios, including missing buckets, authentication failures, and other unexpected errors. Upon successful upload, it provides a direct link to verify the uploaded file in the Google Cloud Console.",2025-12-14T08:35:46.982758+00:00
"context/gemini_may_pro_example.py","This code snippet demonstrates how to use the LiteLLM library to interact with Google's Vertex AI Gemini model. LiteLLM is a Python library that provides a unified interface for calling various Large Language Model (LLM) APIs using a consistent format similar to OpenAI's API structure.

The code imports the `completion` function from the `litellm` package, which is the primary method for generating text completions. It then makes an API call to the Vertex AI platform, specifically targeting the gemini-2.5-pro-preview-05-06 model, which is a preview version of Google's Gemini 2.5 Pro model.

The request follows a standard chat completion format with a messages array containing a single user message. The placeholder text Your prompt here indicates where users should insert their actual query or instruction for the model.

The response from the API call is stored in the `response` variable and then printed to the console. This basic example serves as a template for developers looking to integrate Vertex AI's Gemini models into their applications using LiteLLM's simplified interface, which abstracts away the complexity of directly calling Google Cloud's APIs while maintaining compatibility with OpenAI-style request formatting.",2025-12-14T08:35:55.099868+00:00
"context/generate/1/fix_errors_from_unit_tests.py","This Python script implements an automated workflow for resolving unit test failures using Large Language Models (LLMs) and the LangChain framework. The primary function, `fix_errors_from_unit_tests`, takes a unit test, source code, error details, and model configuration parameters as input. It initializes a SQLite cache to optimize resource usage. The logic executes a two-stage process. First, it retrieves prompt templates from a specified directory and uses an LLM to generate a descriptive solution for the errors, calculating and displaying token costs via the `rich` library. Second, it runs a follow-up chain with a JSON output parser to extract specific code and unit test corrections from the previous generation. The function returns a tuple containing boolean flags for updates, the fixed code strings, and the total execution cost, while including error handling to manage exceptions gracefully.",2025-12-20T07:05:42.442747+00:00
"context/generate/1/fix_errors_from_unit_tests_python.prompt","The provided file outlines the specifications for creating a Python function named fix_errors_from_unit_tests, designed to resolve unit test failures using Large Language Models (LLMs) via Langchain. The function accepts the unit test, the code under test, the error message, and model parameters (strength and temperature) as inputs. It returns boolean flags indicating updates, the fixed code and test strings, and the total execution cost. The implementation follows a ten-step process utilizing Langchain's Expression Language (LCEL). It begins by loading prompt templates from a path defined by the $PDD_PATH environment variable. The process involves two LLM invocations: the first generates a textual solution to the error, while the second extracts structured data (fixed code and status flags) from that solution using a JSON output parser. The function leverages the rich library for pretty-printing console output, including markdown rendering and token cost tracking. It relies on a helper llm_selector for model instantiation and token counting. Finally, the function aggregates the costs from both steps and returns the structured fixes and total cost, ensuring robust error handling throughout.",2025-12-20T07:06:03.331986+00:00
"context/generate/2/cli.py","The provided file implements a Command Line Interface (CLI) tool called PDD (Prompt-Driven Development) using the `click` and `rich` libraries. It serves as a comprehensive suite for AI-assisted coding tasks, facilitating the generation, testing, and maintenance of code through natural language prompts.

Key functionalities include:
- **Code Generation**: The `generate` command creates runnable code from prompt files.
- **Testing and Debugging**: The `test` command generates unit tests, while `fix` attempts to resolve errors in code and tests, optionally using an iterative loop with a verification program.
- **Prompt Management**: Commands like `preprocess`, `split`, `change`, and `update` allow users to prepare, divide, modify, and synchronize prompts with evolving codebases.
- **Examples**: The `example` command generates reference examples from existing code and prompts.
- **Utilities**: It includes features for cost tracking (logging usage to CSV), shell completion installation, and caching via SQLite to optimize performance and cost.

The tool integrates with various external modules to perform underlying AI operations and manages file paths, user configuration (strength, temperature), and feedback via a polished console interface.",2025-12-20T07:06:20.082524+00:00
"context/generate/2/cli_python.prompt","The provided text outlines the specifications for developing a Python command-line interface (CLI) tool named pdd. Designed for an expert Python engineer, the tool utilizes the click library for CLI management and the rich library for formatted console output. The project structure includes specific directories for source code, prompts, context, and data. The CLI supports a variety of commands aimed at automating software development tasks. Key functionalities include generate for creating code from prompts, example for deriving context from code, and test for generating unit tests. It offers prompt manipulation tools like preprocess (with an XML option), split, change, and update. Additionally, the tool includes debugging capabilities via the fix command, which resolves errors based on unit test feedback, and a fix --loop sub-command for iterative fixing. Finally, it provides an install_completion command to configure shell autocompletion. The specification references external example files to guide the implementation of file loading, path construction, and specific logic for each command.",2025-12-20T07:06:30.573417+00:00
"context/generate/3/cli.py","This file defines the main Command Line Interface (CLI) for a Prompt-Driven Development (PDD) application using the `click` library. It establishes a central `cli` group that handles global configuration options such as AI model strength, temperature, verbosity, and cost tracking. A custom decorator, `track_cost`, is implemented to wrap commands, logging execution details and costs to a CSV file.

The CLI exposes a comprehensive set of commands to support AI-assisted development workflows. Key functionalities include `generate` for creating code from prompts, `test` for generating unit tests, and `fix` for iteratively resolving errors in code and tests. Other commands manage prompt engineering tasks: `preprocess` formats prompts, `split` breaks down complex prompts, `change` and `update` modify prompts based on new requirements or code changes, and `detect` identifies necessary prompt updates. It also includes tools for conflict analysis (`conflicts`), crash resolution (`crash`), and generating examples (`example`). Finally, an `install_completion` command is provided to set up shell autocompletion for the tool.",2025-12-20T07:06:46.371364+00:00
"context/generate/3/cli_python_preprocessed.prompt","The provided file outlines the specifications and implementation details for pdd (Prompt-Driven Development), a Python command-line tool designed to streamline software development using AI models. Built with the Click and Rich libraries, the CLI supports multiple programming languages and enforces a specific naming convention for prompt files. Core capabilities include generating runnable code from prompts, creating unit tests, and producing usage examples.

Advanced commands allow users to preprocess prompts, iteratively fix code errors, split complex prompts, and detect or resolve conflicts between prompt files. The tool supports chaining multiple commands for efficient workflows and provides global options to adjust AI model parameters such as strength and temperature. It also features comprehensive cost tracking, exporting usage data to CSV files, and allows configuration via environment variables for output paths. The documentation includes detailed usage examples for internal modules and instructions for handling edge cases, ensuring a robust and automated development environment.",2025-12-20T07:06:59.549308+00:00
"context/generate/4/cli.py","This file defines the Command Line Interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` library. It serves as the main entry point for the application, orchestrating various AI-assisted development workflows. The CLI accepts global configuration options such as AI model strength, temperature, verbosity, and cost tracking.

The file implements numerous subcommands that interface with backend modules to perform specific tasks:
- **Generation**: Creating code (`generate`) and unit tests (`test`) from prompt files.
- **Refinement**: Fixing errors (`fix`, `crash`) and generating bug reproduction tests (`bug`).
- **Prompt Engineering**: Preprocessing (`preprocess`), splitting (`split`), and modifying prompts based on code changes or instructions (`change`, `update`).
- **Analysis**: Detecting necessary changes (`detect`), identifying conflicts between prompts (`conflicts`), and tracing code lines back to prompt definitions (`trace`).
- **Utilities**: Generating examples (`example`) and installing shell completion scripts.

Most commands are decorated with `@track_cost` to monitor API usage, and the `rich` library is used for formatted console output.",2025-12-20T07:07:12.097417+00:00
"context/generate/4/cli_python.prompt","The provided text is a detailed prompt designed to instruct an AI to generate a Python command-line interface (CLI) tool named `pdd` using the `click` library. The prompt establishes the context, directory structure, and specific naming conventions required to avoid conflicts between CLI commands and imported module functions. It outlines the implementation requirements for a wide array of commands, including `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`. For each command, the prompt specifies the corresponding internal functions to be utilized (e.g., `code_generator`, `fix_errors_from_unit_tests`, `trace`) and references example files that demonstrate how to integrate these modules. Additionally, it includes instructions for utility features such as `construct_paths` for file handling, a `track_cost` decorator for monitoring, and an `install_completion` command to manage shell auto-completion settings.",2025-12-20T07:07:27.612778+00:00
"context/generate/5/generate_output_paths.py","The provided code defines a Python function named `generate_output_paths` designed to determine and resolve output file paths for a command-line interface tool. The function accepts arguments for the operation command, specific output locations, a file basename, the programming language, and the file extension. It supports a variety of commands including `generate`, `test`, `fix`, `split`, `detect`, and `crash`. The logic for determining the final path follows a hierarchy: it first checks for paths explicitly provided in the `output_locations` dictionary, then looks for specific environment variables (prefixed with `PDD_`), and finally defaults to a standard naming convention based on the command and basename. The code includes specific handling for commands that produce multiple outputs, such as `fix` (which generates paths for code, tests, and results) and `split`. Additionally, it ensures that if a resolved path is a directory, the appropriate default filename is appended. The function returns a dictionary mapping the relevant output keys to their fully resolved string paths.",2025-12-20T07:07:43.908309+00:00
"context/generate/5/generate_output_paths_python.prompt","The provided text outlines the requirements for implementing a Python function named `generate_output_paths` for the PDD (Prompt-Driven Development) CLI tool. This function is responsible for determining the correct output filenames and paths based on various inputs, including the specific command executed (e.g., `generate`, `test`, `fix`, `split`), user-defined output options, and the file's base name and language.

The input includes detailed documentation for the PDD tool, specifying the hierarchy of logic for determining output paths: explicit command-line arguments take precedence, followed by specific environment variables (such as `PDD_GENERATE_OUTPUT_PATH`), and finally, default naming conventions (e.g., `<basename>_fixed.<ext>`). The instructions guide the developer to analyze these construction methods, list default conventions and environment variables, identify edge cases, and write the code to handle single and multi-file outputs efficiently.",2025-12-20T07:08:06.329749+00:00
"context/generate/6/conflicts_main.py","This Python script defines the `conflicts_main` function, which serves as the core logic for a command-line interface (CLI) tool designed to analyze and resolve conflicts between two input prompts. Utilizing the `click` library for context management, the function accepts paths for two prompt files and an optional output destination. It leverages helper functions to construct file paths and read content before invoking the `conflicts_in_prompts` module to perform the actual analysis based on parameters like strength and temperature. The results, including specific change instructions and prompt identifiers, are mapped back to their original file paths and exported to a CSV file. The script employs the `rich` library to provide formatted console output, displaying the model used, total cost, and a detailed list of detected conflicts. It includes error handling to catch exceptions and exit the program with a status code of 1 if issues arise. Ultimately, it returns a tuple containing the list of conflicts, the operational cost, and the model name.",2025-12-20T07:08:21.432308+00:00
"context/generate/6/conflicts_main_python.prompt","This file provides detailed instructions for an expert Python engineer to implement the 'conflicts_main' function, which serves as the core logic for the 'conflicts' command within the 'pdd' command-line program. The function is responsible for analyzing two prompt files to identify potential conflicts. It takes a Click context object, two input file paths, and an optional output path as arguments. The implementation workflow includes constructing file paths using the 'construct_paths' utility, reading the input files, and invoking the 'conflicts_in_prompts' function with specific parameters like strength and temperature retrieved from the context. The function must process the results to associate prompt names with actual file paths, save the analysis to a CSV file, and return a tuple containing the conflict data, total cost, and model name. The instructions also emphasize the need for user feedback using the Rich library, adherence to verbosity flags, and robust error handling to ensure graceful execution.",2025-12-20T07:08:34.923002+00:00
"context/generate/7/README.md","The provided text documents the PDD (Prompt-Driven Development) Command Line Interface, a tool designed to streamline software development using AI models. PDD facilitates code generation, unit testing, and prompt management through a suite of specific commands. Key functionalities include generating runnable code from prompt files (`generate`), creating examples (`example`), generating and enhancing unit tests (`test`), and fixing code errors iteratively (`fix`, `crash`). It supports various programming languages like Python, JavaScript, and Java, relying on specific naming conventions for prompt files.

The CLI offers extensive configuration options, including API key setup for providers like OpenAI and Anthropic, environment variables for output paths, and auto-update capabilities. Users can control AI model parameters such as strength and temperature, and track operational costs via CSV output. Advanced features include splitting complex prompts (`split`), detecting necessary changes (`detect`, `change`), resolving conflicts (`conflicts`), and managing dependencies (`auto-deps`). The documentation also covers installation, troubleshooting, security considerations regarding code execution and data privacy, and strategies for integrating PDD into workflows like CI/CD pipelines and debugging processes.",2025-12-20T07:08:50.041700+00:00
"context/generate/7/code_generator_main.py","The provided code defines a Python function named code_generator_main, which serves as the core logic for a command-line interface tool designed to generate code from a prompt file. Utilizing the click library for context management and the rich library for terminal output, the function orchestrates the code generation process. It begins by resolving input and output file paths and determining the target language using a helper function named construct_paths. The function then reads the prompt content and calls the code_generator module, passing configuration parameters such as strength and temperature derived from the application context. If an output path is specified, the generated code is written to that file. The function handles user feedback by displaying the operation status, the model used, and the total cost, provided the quiet flag is not set. Additionally, it includes error handling to catch exceptions, print error messages, and exit the program gracefully if the process fails. Ultimately, it returns a tuple containing the generated code, the cost, and the model name.",2025-12-20T07:09:02.632467+00:00
"context/generate/7/code_generator_main_python.prompt","The provided file is a prompt template designed for an AI assistant acting as an expert Python engineer. The specific task defined in the prompt is to develop a Python function named 'code_generator_main', which serves as a CLI wrapper using the 'click' library. This function is responsible for reading a prompt file, generating code via a 'code_generator' function, and handling the output file location. The prompt specifies the function's inputs—a Click context, a prompt file path, and an optional output path—and its outputs, which include the generated code, the cost, and the model name. Furthermore, the file embeds various context references, including a Python preamble, examples of 'click' usage, and internal module demonstrations for path construction and code generation. It also points to a README file for specific command details, ensuring the AI has all necessary context to implement the wrapper correctly.",2025-12-20T07:09:17.215210+00:00
"context/generate/8/llm_invoke.py","The file `llm_invoke.py` acts as a configurable utility for invoking Large Language Models (LLMs) via the LangChain framework, featuring a dynamic model selection system. It reads model metadata—such as provider, cost, and ELO ratings—from a CSV file to instantiate `ModelInfo` objects. The core logic resides in `select_model`, which chooses a model based on a user-defined 'strength' parameter; values below 0.5 prioritize lower costs, while values above 0.5 prioritize higher performance relative to a base model. The script abstracts provider differences (supporting OpenAI, Anthropic, Google, etc.) through `create_llm_instance`. The main function, `llm_invoke`, orchestrates the workflow by setting up SQLite caching, preparing prompt templates, and handling execution. It supports both string and structured Pydantic outputs. Furthermore, a custom `CompletionStatusHandler` callback tracks token usage to calculate and report the specific financial cost of each run. This setup allows for flexible, cost-aware LLM integration into applications requiring varied performance levels and structured data responses.",2025-12-20T07:09:33.408110+00:00
"context/generate/8/llm_invoke_python.prompt","The provided text outlines specifications for developing a Python function named `llm_invoke`, contained within a single file `llm_invoke.py`. Acting as an expert Python engineer, the goal is to implement a Langchain-based solution that utilizes caching to execute LLM prompts. The function accepts parameters including a prompt string, input JSON, a strength float (0-1), temperature, verbosity flag, and an optional Pydantic output schema. It returns the LLM result, execution cost, and the selected model name. A core requirement is a dynamic model selection algorithm based on the `strength` parameter and a CSV dataset of available models. If strength is below 0.5, the system selects a model by interpolating cost between the cheapest and the base model (defaulting to gpt-4o-mini). If above 0.5, it interpolates based on ELO score up to the highest-rated model. Critical implementation details include handling structured outputs via `.with_structured_output()` and `PydanticOutputParser`, managing provider-specific token limits (e.g., Google's `max_output_tokens`), and configuring OpenAI specific settings like `base_url` or `reasoning_effort`. Finally, the function must support verbose logging to display model details, token costs, and results using rich printing.",2025-12-20T07:09:56.190774+00:00
"context/generate_output_paths_example.py","This Python script demonstrates the usage of a `generate_output_paths` function from a module called `pdd.generate_output_paths`. The script begins by setting up the Python path to import the module from a parent directory. It then runs through eight different scenarios to showcase how the function handles various input configurations for generating output file paths.

The scenarios include: (1) using default settings for a 'generate' command, (2) specifying a custom output filename, (3) specifying an output directory, (4) using environment variables for a 'fix' command, (5) mixing user inputs with defaults, (6) handling commands with fixed extension defaults like 'preprocess', (7) handling unknown commands which return empty dictionaries, and (8) handling missing basename inputs.

Each scenario prints the inputs, the actual result from the function, and the expected output for verification. The script creates temporary directories as needed and cleans them up afterward. The function appears to resolve paths to absolute paths, handle both file and directory specifications, support multiple output keys per command (like output_test, output_code, output_results for 'fix'), and fall back to environment variables when user input is not provided. This serves as comprehensive documentation and testing for the path generation utility.",2025-12-14T08:36:02.975783+00:00
"context/generate_test_example.py","This Python script demonstrates the usage of a test generation utility from the 'pdd' package. The code imports necessary modules including 'os', a 'generate_test' function from 'pdd.generate_test', and 'rich' for enhanced console output formatting.

The script sets up parameters for generating unit tests automatically. It defines a sample prompt requesting a factorial function, along with the actual implementation of a recursive factorial function that handles base cases (0 and 1) and recursively calculates factorials for larger numbers.

Additional configuration parameters include a 'strength' value of 0.5, a 'temperature' of 0.0 (for deterministic output), and specifies Python as the target language.

The main functionality is wrapped in a try-except block that calls the 'generate_test' function with all defined parameters. Upon successful execution, it displays the generated unit test, the total cost of the API call (formatted to 6 decimal places), and the name of the AI model used. The output is styled using Rich's formatting capabilities with colored and bold text.

If an error occurs during test generation, the exception is caught and displayed in bold red text. The script also includes a commented-out line for setting a custom PDD_PATH environment variable.",2025-12-14T08:36:11.825113+00:00
"context/get_comment_example.py","This file provides documentation and an example usage for the `get_comment` function from the `pdd.get_comment` module. The function retrieves the comment character(s) associated with a specified programming language. It takes a single input parameter, `language`, which is a case-insensitive string representing the programming language name. The function returns a string containing the comment character(s) for that language (e.g., `#` for Python, `//` for Java). If the `PDD_PATH` environment variable is not set, the language is not found in the CSV data source, or any error occurs, the function returns the default string `'del'`. The example demonstrates calling the function with various languages including Python, Java, JavaScript, and an unknown language. Important prerequisites include setting the `PDD_PATH` environment variable to point to the directory containing `data/language_format.csv`, which must have at least two columns: `language` and `comment`. This utility is useful for applications that need to dynamically determine comment syntax across different programming languages.",2025-12-14T08:36:20.512034+00:00
"context/get_extension_example.py","This Python code snippet demonstrates the usage of a utility function called `get_extension` imported from a module named `pdd.get_extension`. The function appears to be designed to return the appropriate file extension associated with a given programming language or file type name.

The code shows three example calls to the `get_extension` function:

1. When passed Python, it returns .py - the standard file extension for Python source files.
2. When passed Makefile, it returns Makefile - since Makefiles typically don't have an extension and are named exactly Makefile.
3. When passed JavaScript, it returns .js - the standard extension for JavaScript files.

This utility function is likely part of a larger package (pdd) that deals with programming language detection or file type management. It provides a convenient way to map human-readable language names to their corresponding file extensions, which could be useful in various scenarios such as code generation, file creation, syntax highlighting configuration, or build system automation. The function handles both cases where languages have traditional extensions and special cases like Makefile where the filename itself is the identifier.",2025-12-14T08:36:27.325118+00:00
"context/get_jwt_token_example.py","This Python script is a CLI authentication utility for a Prompt Driven Development (PDD) application that handles Firebase authentication using GitHub's Device Flow OAuth. The script begins by loading the Firebase API key from environment variables or appropriate `.env` files based on the target environment (local, staging, or production). It dynamically configures the application name and token variable names based on the `PDD_ENV` environment variable.

The main async function orchestrates the authentication process by calling `get_jwt_token()` with Firebase and GitHub credentials. It includes comprehensive error handling for various failure scenarios including authentication errors, network issues, token problems, rate limiting, and user cancellation.

Upon successful authentication, the script updates the local `.env` file with the obtained JWT token. It writes both an environment-specific token variable (e.g., `JWT_TOKEN_STAGING`) and a generic `JWT_TOKEN` fallback, either replacing existing values or appending new ones.

Key dependencies include the `pdd.get_jwt_token` module which provides the core authentication logic and custom exception classes. The script uses asyncio for asynchronous execution and pathlib for cross-platform file path handling. This utility is designed to streamline developer authentication workflows across different deployment environments.",2025-12-14T08:36:34.875563+00:00
"context/get_language_example.py","This Python script serves as a demonstration module for the `get_language` function imported from the `pdd.get_language` package. The main purpose of the code is to showcase how to use the `get_language` utility to identify programming languages based on file extensions.

The script defines a `main()` function that performs the following operations:

1. **Sets up a test case**: It initializes a sample file extension variable (`.py`) to use as input for the demonstration.

2. **Calls the get_language function**: The function is invoked with the file extension to retrieve the corresponding programming language name.

3. **Handles the output**: The script includes conditional logic to print appropriate messages based on whether a language was successfully identified or not.

4. **Implements error handling**: A try-except block wraps the main logic to gracefully catch and display any exceptions that might occur during execution.

5. **Uses standard Python entry point**: The `if __name__ == __main__:` guard ensures the `main()` function only runs when the script is executed directly, not when imported as a module.

The code follows Python best practices including type hints for variables and a docstring for the main function. It appears to be part of a larger project (pdd) that likely deals with programming language detection or file type identification.",2025-12-14T08:36:42.707634+00:00
"context/get_run_command_example.py","This Python script demonstrates the usage of the `get_run_command` module, which retrieves execution commands for various programming languages based on file extensions. The module reads configuration data from a CSV file located at `$PDD_PATH/data/language_format.csv`, which maps file extensions to their corresponding run command templates (e.g., `.py` maps to `python {file}`).

The script showcases six key examples: (1) retrieving a run command template for Python files using the `.py` extension; (2) demonstrating extension normalization, where the function handles inputs with or without leading dots and is case-insensitive; (3) generating complete executable commands for specific file paths by replacing the `{file}` placeholder with actual paths; (4) handling unknown extensions gracefully by returning empty strings; (5) managing files without extensions like Makefiles; and (6) iterating through multiple language extensions including Python, JavaScript, Ruby, Shell, and Java.

The module requires the `PDD_PATH` environment variable to be set and exposes two main functions: `get_run_command()` for retrieving command templates by extension, and `get_run_command_for_file()` for generating complete commands for specific files. This utility is useful for build systems or development tools that need to dynamically execute files across different programming languages.",2025-12-14T08:36:50.513898+00:00
"context/get_test_command_example.py","This Python script serves as a comprehensive demonstration for the get_test_command module, specifically illustrating the usage of the get_test_command_for_file function. The script explains the function's layered resolution strategy, which prioritizes commands defined in language_format.csv, falls back to smart detection, and returns None if no command is found, signaling a need for agentic intervention. The code includes specific functions to demonstrate various scenarios: resolving commands for standard Python and JavaScript files, using the language parameter to override file extension detection (e.g., for TypeScript), and batch processing multiple files to display results in a formatted table using the rich library. Additionally, it includes a specific example for handling cases where no command is resolved, emphasizing the intended workflow for unknown file types. The main function executes these examples sequentially to provide a visual guide to the module's functionality.",2025-12-20T07:10:16.458352+00:00
"context/git_update_example.py","This Python script serves as a demonstration for using the `git_update` function from the `pdd` package. The script defines a `main` function that sets up necessary parameters, including a specific prompt name ('fix_error_loop'), a target code file path, and configuration settings for model strength and temperature. It reads an initial input prompt from a file located in a `prompts` directory and passes this data to the `git_update` function with verbose logging enabled. Upon successful execution, the script outputs the modified prompt, the total cost of the operation, and the model name used. Furthermore, it overwrites the original prompt file with the newly generated modified prompt. The process is wrapped in error-handling logic to catch and display specific value errors or general exceptions. The script is structured to run as a standalone executable.",2025-12-20T07:10:33.898171+00:00
"context/increase_tests_example.py","This Python script demonstrates the usage of an `increase_tests` function from a `pdd` module, which appears to be a tool for automatically generating additional unit tests to improve code coverage. The script defines an `example_usage()` function that showcases both basic and advanced usage patterns.

The example works with a simple `calculate_average` function that computes the average of a list of numbers. It includes an existing minimal unit test and a coverage report showing 60% code coverage with 2 missed statements.

The script calls `increase_tests()` twice: first with basic parameters (existing tests, coverage report, source code, and the original prompt), and then with advanced parameters including language specification, strength setting (using a default value), temperature for controlling randomness, and verbose mode enabled.

The function returns three values: newly generated tests, the total cost (suggesting this may use an AI/LLM service), and the model name used for generation. Error handling is implemented to catch both validation errors and unexpected exceptions.

This appears to be part of a prompt-driven development (PDD) toolkit that leverages AI to automatically expand test suites based on coverage gaps, helping developers achieve more comprehensive testing with less manual effort.",2025-12-14T08:37:05.574553+00:00
"context/incremental_code_generator_example.py","This Python script demonstrates the usage of an incremental code generator from the `pdd` package. The main function showcases how to update existing code based on prompt changes using language model (LLM) tools.

The script sets up several input parameters including: an original prompt (calculating factorial), a new prompt (adding input validation), the existing code implementation, and various configuration options like language (Python), strength, temperature, time budget, and flags for forcing incremental updates and verbose output.

The core functionality calls `incremental_code_generator()` which analyzes the difference between the original and updated prompts, then decides whether to apply a minimal structured diff (patch) to the existing code or recommend full regeneration. This approach optimizes code updates by avoiding complete rewrites when only small changes are needed.

The output section uses the Rich library for formatted console display, showing whether incremental patching was successful or if full regeneration is recommended. It also displays the updated code (if patching succeeded), the total cost incurred from LLM API calls, and the model name used.

This utility is useful for development workflows where code needs iterative refinement based on evolving requirements, minimizing computational costs and preserving existing code structure when possible.",2025-12-14T08:37:13.430383+00:00
"context/insert/1/dependencies.prompt","The provided file contents serve as a documentation or configuration snippet describing how to utilize internal modules within a software project. Specifically, it outlines procedures for two main tasks: loading prompt templates and executing language model prompts via an 'llm_invoke' mechanism. The content is structured using XML-style tags, defining sections for each example. For loading prompt templates, it references an external Python file located at 'context/load_prompt_template_example.py'. Similarly, for running prompts, it points to 'context/llm_invoke_example.py'. This structure suggests the file is intended to be processed by a tool that includes these external files to provide context or examples to a user or an AI system, effectively acting as an index for best practices regarding prompt management and LLM invocation within the specific codebase.",2025-12-20T07:10:46.634640+00:00
"context/insert/1/prompt_to_update.prompt","The provided text defines the specifications for a Python function named postprocess, intended to extract specific programming code from a raw string output generated by a Large Language Model (LLM). Acting as an expert Python Software Engineer, the developer is instructed to use the rich library for console formatting. The function takes several arguments: llm_output (the source text), language (target language), strength (model strength, default 0.9), temperature, and verbose. It returns a tuple consisting of the extracted_code, total_cost, and model_name. The operational logic requires checking the strength parameter first; if set to 0, it delegates to a postprocess_0 function. Otherwise, it loads a specific prompt template (extract_code_LLM.prompt), invokes an LLM to parse the content, and retrieves the code via a Pydantic object. Post-processing involves stripping markdown syntax, such as triple backticks and language labels, from the extracted string before returning the final results.",2025-12-20T07:11:02.030342+00:00
"context/insert/1/updated_prompt.prompt","This file contains a prompt for an expert Python Software Engineer to create a function named 'postprocess'. The function's purpose is to extract specific code blocks from a string output generated by a Large Language Model (LLM). It takes several inputs: the raw LLM output string, the target programming language, a strength parameter (default 0.9), a temperature setting, and a verbose flag. The function returns a tuple containing the extracted code, the total cost, and the model name. The logic flow specifies that if the strength is 0, a simpler 'postprocess_0' function is used. For other strength values, the function loads a specific prompt template ('extract_code_LLM.prompt') and uses an internal 'llm_invoke' tool to process the text. The output is then cleaned to remove markdown formatting, specifically triple backticks and language identifiers, before returning the final code string. The prompt also includes instructions on using internal modules for loading templates and invoking LLMs, as well as a requirement to use the 'rich' library for console output.",2025-12-20T07:11:18.521133+00:00
"context/insert/2/dependencies.prompt","The provided text represents a documentation or configuration snippet intended to illustrate the usage of internal modules within a project. It specifically focuses on demonstrating how to utilize a component named 'llm_selector'. The content highlights a use case involving the selection of a Langchain Large Language Model (LLM) and the associated functionality for counting tokens. Structurally, the text employs XML-like tags to organize these examples. It includes a specific tag, 'llm_selector_example', which wraps an 'include' directive pointing to a file path: './context/llm_selector_example.py'. This indicates that the detailed code implementation for this example is stored in an external Python file, which is likely meant to be imported or displayed contextually. The snippet serves as a reference point for developers to understand how to interface with the system's internal LLM selection and token management tools.",2025-12-20T07:11:32.436969+00:00
"context/insert/2/prompt_to_update.prompt","This file outlines the specifications for developing a Python function named 'conflicts_in_prompts', designed to identify and resolve conflicts between two input prompts using LangChain Expression Language (LCEL). The function accepts two prompt strings, along with optional parameters for model strength and temperature. The implementation process involves loading specific prompt templates ('conflict_LLM.prompt' and 'extract_conflicts_LLM.prompt') from a directory specified by the PDD_PATH environment variable. It utilizes a relative module named 'llm_selector' to determine the appropriate Large Language Model and calculate token usage and costs. The workflow executes in two stages: first, it generates a markdown analysis of the conflicts between the provided prompts; second, it uses a higher-strength model configuration to parse that analysis into a structured JSON list of recommended changes. The function is required to print status messages regarding token counts and costs during execution and ultimately returns a list of changes, the total cost, and the name of the model used. The file also references context inclusions for a Python preamble and an LCEL example.",2025-12-20T07:11:49.399653+00:00
"context/insert/2/updated_prompt.prompt","The provided file contains a prompt for an expert Python engineer to create a function named `conflicts_in_prompts`. This function is designed to compare two input prompts, identify conflicts, and suggest resolutions using LangChain. The function accepts two prompt strings, along with optional `strength` and `temperature` parameters, and outputs a list of recommended changes, the total cost of the operation, and the name of the model used.

The implementation instructions detail a seven-step process using LangChain Expression Language (LCEL). It involves loading specific prompt templates (`conflict_LLM.prompt` and `extract_conflicts_LLM.prompt`) via the `$PDD_PATH` environment variable and using an internal `llm_selector` module for model selection and token counting. The workflow executes two distinct LLM calls: the first generates a Markdown analysis of the conflicts, and the second parses that output to extract a structured JSON list of changes. The function also handles cost calculation and prints status updates regarding token usage throughout the execution.",2025-12-20T07:12:03.962683+00:00
"context/insert_includes_example.py","This Python script demonstrates example usage of the `insert_includes` module from the `pdd` package, which processes dependencies for code generation tasks. The script imports necessary libraries including `pathlib` for file handling, `rich` for enhanced console output, and the core `insert_includes` function.

The main function `example_usage()` showcases how to set up and execute dependency processing. It initializes a Rich console for formatted output and defines three key input parameters: an input prompt describing a Python function to write (one that reads CSV, processes data, and outputs results), a directory path pattern for context files, and a CSV filename for dependency information.

The script calls `insert_includes()` with custom parameters including a high strength value (0.93) for focused output and zero temperature for consistent results. The function returns four values: a modified prompt with dependencies, CSV output, total processing cost, and the model name used.

Results are displayed using Rich's formatted console output with color-coded sections. The script includes error handling for missing files and general exceptions. Finally, it saves the generated CSV output to a file. The script runs the example function when executed directly via the standard `if __name__ == __main__` pattern.",2025-12-14T08:37:21.946831+00:00
"context/install_completion_example.py","This Python script demonstrates the usage of the PDD (Python Development Directory) shell completion installation module. It showcases two main functions from the pdd package: `get_local_pdd_path()`, which returns the absolute path to the PDD directory, and `install_completion()`, which installs shell completion for the PDD CLI by detecting the current shell, determining the appropriate completion script, and appending a source command to the user's shell RC file.

The script includes a `setup_example_environment()` function that creates a safe, isolated testing environment by setting up dummy directories and files. It forces the shell to bash, creates a dummy home directory (example_home) to avoid modifying the actual ~/.bashrc, establishes a dummy PDD_PATH directory, and generates placeholder completion and RC files.

The `main()` function orchestrates the example by first setting up the environment, then retrieving and displaying the PDD path, and finally calling the installation function. The script uses the Rich library for formatted console output with colored messages.

This example is designed for demonstration purposes, allowing developers to understand how the shell completion installation works without affecting their actual system configuration. Users can run it directly via `python example_install_completion.py`.",2025-12-14T08:37:29.425798+00:00
"context/langchain_lcel_example.py","This Python script demonstrates the use of LangChain, a framework for building applications with large language models (LLMs). It showcases integration with multiple LLM providers including OpenAI (GPT-4o-mini, GPT-3.5-turbo, o1), Google (Gemini via both GenAI and VertexAI), Anthropic (Claude), Azure OpenAI, DeepSeek, Fireworks, Groq, Together.ai, Ollama, MLX (for local Apple Silicon models), and AWS Bedrock.

Key features demonstrated include: setting up SQLite caching to reduce API costs and improve response times; creating prompt templates with variable placeholders; using output parsers (JSON and Pydantic) to structure LLM responses into defined schemas like a Joke class with setup and punchline fields; implementing a custom callback handler to track completion status, finish reasons, and token usage metrics.

The script also illustrates LangChain Expression Language (LCEL) for chaining components using the pipe operator, configurable alternatives for switching between models, fallback mechanisms for reliability, and structured output enforcement. Advanced features like Anthropic's thinking mode for reasoning tasks are shown. Throughout, various chains are invoked with different queries (jokes, company names, code generation) to demonstrate the flexibility of the framework across different use cases and model providers.",2025-12-14T08:37:37.342559+00:00
"context/litellm_bedrock_sonnet.py","This Python code snippet demonstrates how to use the LiteLLM library to interact with Amazon Bedrock's Claude 3.7 Sonnet model. The code imports the `completion` function from the `litellm` package and includes commented-out environment variable configurations for AWS credentials (access key ID, secret access key, and region name), which would need to be set for authentication with AWS services.

The main functionality sends a simple query to the Claude model asking What is the capital of France? The API call specifies the Bedrock model path using the format bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0, indicating it's using the US-hosted Anthropic Claude 3.7 Sonnet model through Bedrock. The `reasoning_effort` parameter is set to low, which likely controls the depth of reasoning the model applies to generate its response.

Finally, the script prints the response object returned by the completion call. This is a basic example of integrating LiteLLM as a unified interface for accessing various LLM providers, in this case specifically for AWS Bedrock-hosted Anthropic models, simplifying the process of making API calls to different AI services through a consistent interface.",2025-12-14T08:37:47.593005+00:00
"context/llm_invoke_example.py","This Python script demonstrates the usage of an `llm_invoke` function for interacting with language models. It imports utilities from a `pdd.llm_invoke` module and uses Pydantic for structured data validation.

The code defines a `Joke` Pydantic model with `setup` and `punchline` fields for structured output parsing. The `calculate_model_ranges` function samples strength values from 0.0 to 1.0 to determine the boundaries where different LLM models are selected, returning a list of dictionaries containing each model's name, start/end range, and midpoint.

The `main` function serves as a demonstration entry point. It first calculates and displays the strength ranges for available models, then iterates through each model, invoking it at its midpoint strength value. For each model, it runs two examples: an unstructured joke generation about programmers and a structured joke generation about data scientists using the Pydantic model to parse JSON output.

The script showcases key features including dynamic model selection based on strength parameters, cost tracking for API calls, temperature control, verbose mode toggling, and both unstructured text and structured Pydantic-validated responses. Error handling is included for the structured output calls. This appears to be a testing or demonstration utility for a multi-model LLM invocation system.",2025-12-14T08:37:55.952595+00:00
"context/llm_output_fragment.txt","This Python file implements a command-line interface (CLI) tool called pdd (Prompt-Driven Development) using the Click library. The program provides several AI-powered code generation and manipulation commands. Key features include: 1) A main CLI group with global options for force overwrite, AI model strength/temperature settings, verbosity controls, and cost tracking. 2) Multiple subcommands: generate creates runnable code from prompt files, example generates example code from existing code and prompts, test creates unit test files, preprocess handles prompt file preprocessing with optional XML tagging, fix repairs code errors with optional iterative loop mode, and split divides complex prompts into smaller files. The tool uses the Rich library for formatted console output and imports various helper modules for path construction, code generation, context generation, test generation, preprocessing, XML tagging, and error fixing. Each command follows a consistent pattern of constructing file paths, calling the appropriate generator function, writing outputs, and displaying cost information. The architecture supports command chaining through a processor decorator pattern similar to Unix pipes.",2025-12-14T08:38:04.936885+00:00
"context/llm_selector_example.py","This Python script demonstrates the usage of an `llm_selector` function imported from a `pdd.llm_selector` module. The main function iterates through different strength values (starting at 0.5 and incrementing by 0.05 until exceeding 1.1) while keeping the temperature fixed at 1.0. For each strength value, it calls the `llm_selector` function, which returns five values: an LLM object, a token counter function, input cost, output cost, and the model name. The script then prints details about the selected model, including its name and the costs per million tokens for both input and output. It also demonstrates the token counting functionality by counting tokens in a sample text string. The code includes error handling for `FileNotFoundError` and `ValueError` exceptions, suggesting the `llm_selector` function may read configuration files or validate input parameters. This appears to be a testing or demonstration utility for exploring how different strength parameters affect LLM model selection, likely used in a larger system where model selection is dynamically determined based on task requirements or resource constraints.",2025-12-14T08:38:13.779267+00:00
"context/load_prompt_template_example.py","This Python script demonstrates a simple utility for loading and displaying prompt templates. The code imports two key components: a custom function called `load_prompt_template` from the `pdd.load_prompt_template` module, and the `print` function from the `rich` library, which enables formatted console output with styling capabilities.

The main function defines a prompt name variable set to generate_test_LLM, which represents the name of a prompt template file (without its file extension). It then calls the `load_prompt_template` function with this name to retrieve the corresponding template content.

If the prompt is successfully loaded (i.e., the returned value is truthy), the script prints a styled header message in blue text reading Loaded Prompt Template: followed by the actual prompt content. The rich library's print function handles the color formatting using bracket notation.

The script follows Python's standard entry point pattern with the `if __name__ == __main__:` guard, ensuring the main function only executes when the script is run directly rather than imported as a module.

Overall, this appears to be a testing or demonstration utility within a larger prompt-driven development (PDD) framework, likely used for verifying that prompt templates are correctly stored and retrievable.",2025-12-14T08:38:20.862437+00:00
"context/logo_animation_example.py","This Python script demonstrates the usage of a PDD (presumably a product or project name) branding animation module. The code imports functions from a custom 'pdd.logo_animation' package to control a terminal-based logo animation.

The main function serves as a demonstration, starting with informational print statements before launching the animation via `start_logo_animation()`, which runs in a background thread. The animation sequence consists of three phases: logo formation (1.5 seconds), logo hold (1.0 seconds), and logo-to-box transition (1.5 seconds), totaling approximately 4 seconds.

The script allows the animation to run for 7 seconds using a simple loop with `time.sleep()` calls, simulating concurrent main program work. An important note in the comments explains that any print statements during this period won't be visible because the animation uses full-screen terminal mode (screen=True), which temporarily takes over the display.

After the viewing duration, the script calls `stop_logo_animation()` to halt the animation, clean up the background thread, and restore normal terminal control. The code follows Python best practices with type hints, docstrings, and a standard `if __name__ == __main__` entry point pattern. This appears to be example or demo code for integrating the PDD branding animation into other applications.",2025-12-14T08:38:28.713395+00:00
"context/no_include_conflicts_in_prompts_python.prompt","This file is a prompt specification for generating a Python function called conflicts_in_prompts that analyzes two prompts for conflicts and suggests resolutions. The function uses LangChain and an LLM to perform the analysis.

**Inputs:** The function accepts two prompts (prompt1, prompt2), a strength parameter (default 0.5), and a temperature parameter (default 0).

**Outputs:** It returns a list of conflict dictionaries (each containing description, explanation, and resolution suggestions for both prompts), total cost, and model name.

**Implementation Steps:**
1. Load prompt templates from files specified via the $PDD_PATH environment variable (conflict_LLM.prompt and extract_conflict_LLM.prompt)
2. Create a LangChain LCEL template from the conflict detection prompt
3. Use an imported llm_selector module to select the model
4. Run the prompts through the model, passing PROMPT1 and PROMPT2 as parameters, while displaying token counts and costs to the user
5. Create a second LCEL chain using the extraction prompt with 0.8 strength, outputting JSON. This processes the LLM output from step 4, calculates tokens/costs, and extracts the conflicts list
6. Return the conflicts list with nested fields, total cost, and model name

The specification includes a reference to an external Python preamble file for context.",2025-12-14T08:38:37.077536+00:00
"context/o4-mini-test.ipynb","This Jupyter Notebook demonstrates how to use the Azure OpenAI API to create a conversational chatbot experience. The code imports the AzureOpenAI client library and configures it with an Azure endpoint, API key, and the o4-mini model deployment. The notebook showcases a multi-turn conversation about Paris travel recommendations. The conversation flow includes: a system prompt establishing the assistant's helpful role, a user asking for Paris sightseeing suggestions, an assistant response listing top attractions (Eiffel Tower, Louvre Museum, Notre-Dame Cathedral), and a follow-up user question asking specifically about the Eiffel Tower. The output displays a detailed response explaining why the Eiffel Tower is remarkable, covering its architectural and engineering significance as a revolutionary 300-meter iron structure from 1889, its status as an iconic global symbol of Paris and French culture, the spectacular 360-degree panoramic views from its three observation levels, the day-to-night experience with its famous hourly light show featuring 20,000+ bulbs, and its role as a cultural hub hosting exhibitions and seasonal events. The notebook runs on Python 3.12.9 and uses the 2024-12-01-preview API version.",2025-12-14T08:38:45.780418+00:00
"context/output/calculate_pi.py","The provided file is a Python 3 script designed to estimate the mathematical constant Pi using the Monte Carlo method. It imports the standard `random` module to facilitate stochastic sampling. The core functionality is encapsulated within the `estimate_pi` function, which accepts an integer representing the number of iterations. Inside this function, the script generates random coordinates (x, y) within a unit square (0 to 1) and determines if they fall within a unit quarter circle using the equation x^2 + y^2 <= 1. The value of Pi is then approximated by multiplying the ratio of points inside the circle to the total number of points by 4. The script includes a main execution block that demonstrates the function with 1,000 iterations, printing the estimated value, the actual value of Pi for comparison, and the calculated error margin to standard output.",2025-12-20T21:05:34.109850+00:00
"context/output/calculator.py","The provided file content consists of a brief Python code snippet defining a single function named 'add'. This function accepts two parameters, 'a' and 'b'. Although the function's name implies an arithmetic addition operation, the implementation logic performs subtraction, returning the result of 'a - b'. This discrepancy represents a logic error, where the code's behavior contradicts its semantic identifier. The error is explicitly noted within the code via an inline comment that states 'Bug: subtraction instead of addition'. The snippet serves as a clear example of a bug, likely intended for debugging exercises or educational demonstrations.",2025-12-22T04:25:40.411554+00:00
"context/output/calculator_prompt.md","The provided file contains a set of instructions addressed to an expert Python engineer for the creation of a basic calculator module. The primary objective is to implement a module capable of performing fundamental arithmetic operations. Specifically, the requirements call for the definition of two distinct functions: `add(a, b)`, which calculates the sum of two numbers, and `subtract(a, b)`, which determines the difference between two numbers. The instructions explicitly state that these functions must accept numeric inputs, specifically integers or floating-point numbers. Furthermore, the return values of these functions are expected to be numeric results that correspond to the types of the inputs provided. The document serves as a concise specification for a coding task focused on building a simple yet strictly typed arithmetic utility in Python.",2025-12-21T22:41:01.051677+00:00
"context/output/crash.log","The provided file content displays a Python traceback message, indicating that a runtime error occurred during the execution of a script named 'run_factorial.py'. The traceback specifies that the program crashed at line 2 of the file. The cause of the termination is a 'NotImplementedError', which was explicitly raised. In Python development, this error is typically used as a placeholder to indicate that a specific function, method, or section of code has been defined but not yet programmed with actual logic. Consequently, this output suggests that the 'run_factorial.py' script is currently incomplete or under development, and the user attempted to execute a code path that has not been fully implemented by the developer.",2025-12-21T23:17:25.200662+00:00
"context/output/driver.py","The provided file contains a concise Python script serving as a fundamental unit test or sanity check for a mathematical utility. The script initiates by importing a function named 'add' from a module identified as 'calculator'. Following this import, the code executes a logical verification step using a conditional statement. It invokes the imported 'add' function with the integer arguments 2 and 2. The script then compares the output of this function call against the expected integer value of 4. If the returned value is not equal to 4, the script triggers an error handling mechanism by raising a generic Exception with the descriptive message 'Math failed'. This behavior indicates that the file is designed to validate the core functionality of the 'calculator' module. It acts as a safeguard to ensure that basic arithmetic operations are performing correctly. If this elementary test fails, the raised exception will likely halt program execution, signaling a critical failure in the underlying logic of the dependency. This type of script is commonly found in test suites or continuous integration pipelines to prevent broken code from being deployed or used.",2025-12-22T04:25:56.935355+00:00
"context/output/factorial_spec.md","The provided file contains a concise programming directive instructing the reader to develop a function responsible for calculating the factorial of a given number. In mathematics, the factorial of a non-negative integer n, denoted by n!, is the product of all positive integers less than or equal to n. This is a fundamental algorithmic problem often used in computer science education to teach concepts such as recursion and iteration. A solution to this prompt would typically involve writing code in a specific programming language that accepts an integer input and returns the calculated factorial value. The implementation might handle edge cases, such as the factorial of zero being defined as one, and ensure the input is a non-negative integer. This type of file content is characteristic of a coding challenge, a homework assignment, or a technical interview question designed to assess a programmer's understanding of basic control structures, mathematical logic, and function definition.",2025-12-21T23:17:40.566566+00:00
"context/output/firebase-debug.log","This log file captures debug and error output from the Firebase Command Line Interface (CLI) during two separate execution attempts on December 20, 2025. The logs reveal a consistent pattern of configuration warnings and authentication failures.

First, the system repeatedly flags a configuration issue within the `firebase.json` file, noting that the `/extensions` object contains an unknown property: `firestore-stripe-payments`. This suggests a potential schema mismatch or deprecated configuration setting.

Following the configuration check, the CLI attempts to execute a command requiring specific Google Cloud and Firebase scopes for the user `glt@promptdriven.ai`. The system detects that the existing authentication tokens are invalid or expired. Automatic attempts to refresh these tokens via the Google OAuth2 API fail, resulting in HTTP 400 (Bad Request) errors.

Ultimately, the process terminates with a critical Authentication Error, indicating that the user's credentials are no longer valid. The logs provide specific instructions to resolve the issue by running `firebase login --reauth` to refresh the session manually or `firebase login:ci` for headless environments.",2025-12-20T23:12:45.308676+00:00
"context/output/fix_attempt.log","The file documents a history of automated code verification and debugging for a Python function named `calculate_average`.

The first attempt resulted in a failure with a `TypeError`. The traceback indicates that the function, located in `module_to_test.py`, crashed because it received a string input (`123`) instead of the expected list of numbers. Consequently, the `sum()` function failed when attempting to add an integer to a string character.

Following the error, an analysis section diagnoses the issue: the calling program (`verify_code.py`) was passing invalid input types. The proposed solution was to update the calling program to convert the string into a list of integers (`[1, 2, 3]`) before invoking the function, rather than modifying the function itself.

The second attempt confirms the resolution of the issue. The verification status shows success (Return Code: 0), and the standard output displays the correct result, Average is: 2.0, demonstrating that the input type mismatch was fixed.",2025-12-22T05:48:42.771474+00:00
"context/output/generated_math.py","This Python module is designed for mathematical calculations, specifically focusing on the computation of factorials. It contains a function named 'calculate_factorial' which accepts a non-negative integer 'n' as its argument. The function implements error handling to ensure data integrity, raising a TypeError if the input is not an integer and a ValueError if the input is negative. The calculation logic uses an iterative approach, returning 1 for inputs of 0 or 1, and computing the product of integers from 2 to 'n' for larger values. Additionally, the file includes a main execution block that runs when the script is executed directly. This block demonstrates the function's usage by calculating the factorial of 5 and printing the result to the console. The code is documented with docstrings explaining the arguments, return values, and potential exceptions.",2025-12-20T23:13:16.886833+00:00
"context/output/math_lib.py","The provided file contains a concise Python script defining a single function named `factorial`. This function is designed to calculate the factorial of a given number, denoted as `n`. The implementation utilizes a recursive approach rather than an iterative loop. Inside the function, a conditional expression (often referred to as a ternary operator) is employed to handle the logic efficiently in a single line. The logic checks if the input number `n` is less than or equal to 1. If this condition is met, representing the base case of the recursion, the function returns 1. This covers the mathematical definition where the factorial of 0 and 1 are both 1. If the input is greater than 1, the function executes the recursive step: it multiplies the current number `n` by the result of calling the `factorial` function again with the argument `n - 1`. This process repeats until the base case is reached, effectively computing the product of all positive integers down to 1. The code demonstrates a standard, functional programming style approach to solving the factorial problem in Python.",2025-12-21T23:17:55.573535+00:00
"context/output/module_to_test.py","The provided file contains a concise Python script that defines a single function named 'calculate_average'. This function serves the specific purpose of calculating the arithmetic mean of a given dataset. It takes one argument, 'numbers', which is expected to be a sequence or iterable object containing numerical values, such as a list of integers or floating-point numbers. Internally, the function utilizes standard Python built-ins to perform the calculation: 'sum(numbers)' aggregates the total value of all elements, and 'len(numbers)' determines the total count of elements in the sequence. The function then returns the result of dividing the sum by the count. It is worth noting that this implementation does not include error handling for empty lists; consequently, passing an empty sequence would trigger a 'ZeroDivisionError' because the length would be zero. This snippet represents a fundamental logic block often found in data processing, statistical analysis, and general-purpose programming where determining the central tendency of a set of values is required.",2025-12-22T05:49:01.578347+00:00
"context/output/module_to_test_1.py","The provided file contains a concise Python script that defines a single function named 'calculate_average'. This function serves the specific purpose of calculating the arithmetic mean of a given dataset. It takes one argument, 'numbers', which is expected to be a sequence or iterable object containing numerical values, such as a list of integers or floating-point numbers. Internally, the function utilizes standard Python built-ins to perform the calculation: 'sum(numbers)' aggregates the total value of all elements, and 'len(numbers)' determines the total count of elements in the sequence. The function then returns the result of dividing the sum by the count. It is worth noting that this implementation does not include error handling for empty lists; consequently, passing an empty sequence would trigger a 'ZeroDivisionError' because the length would be zero. This snippet represents a fundamental logic block often found in data processing, statistical analysis, and general-purpose programming where determining the central tendency of a set of values is required.",2025-12-22T05:49:24.345688+00:00
"context/output/pi_result.txt","The provided file contents represent the output log of a computational program designed to estimate the mathematical constant Pi. The data indicates that the algorithm utilized a specific sample size or duration defined as 1,000 iterations to perform the calculation. Upon completion, the program yielded an estimated value of 3.22. To evaluate the accuracy of this approximation, the output lists the actual, high-precision value of Pi as 3.141592653589793. By comparing the calculated estimate against the true constant, the program derived a margin of error amounting to 0.078407. This discrepancy highlights the limitations of the approximation at this specific iteration count, suggesting that while the result is in the general vicinity of Pi, 1,000 iterations were insufficient to achieve high-precision accuracy. In typical Monte Carlo simulations or series expansions used for such tasks, significantly increasing the number of iterations would be necessary to reduce this error and converge closer to the actual value.",2025-12-20T21:06:21.206659+00:00
"context/output/run_factorial.py","This file contains a concise Python script designed to demonstrate the importation and execution of a mathematical function from an external module. The code begins by importing the 'factorial' function from a module named 'math_lib', indicating a dependency on a separate file or library that defines this mathematical operation. After the import is established, the script calls the 'factorial' function with the integer 5 as its argument. The result of this calculation, which mathematically equates to 120, is then passed to Python's built-in 'print' function to be displayed on the standard output. Overall, the script serves as a basic example of modular programming in Python, illustrating how to utilize functions defined in other modules to perform arithmetic computations.",2025-12-21T23:18:11.297749+00:00
"context/output/spec.md","The provided file contains a concise technical specification titled 'Calculator Spec'. It outlines a specific programming task requiring the creation of a function named 'add'. This function is defined to accept two parameters, 'a' and 'b', and must return the sum of these two values. The document serves as a basic requirement definition for an addition operation within a calculator application.",2025-12-22T04:26:40.359211+00:00
"context/output/test_calculator.py","The provided file is a Python script designed to perform unit testing on a calculator module using the standard `unittest` framework. The script begins by importing the `unittest` library and specific functions—`add` and `subtract`—from a module named `calculator`. It defines a test class named `TestCalculator` that inherits from `unittest.TestCase`. Within this class, two test methods are implemented to verify the correctness of the imported functions. The `test_add` method asserts that the `add` function correctly returns 5 when passed the arguments 2 and 3. Similarly, the `test_subtract` method asserts that the `subtract` function correctly returns 3 when passed the arguments 5 and 2. This code serves as a basic regression test suite to ensure the fundamental arithmetic operations of the calculator application function as expected.",2025-12-21T22:41:33.521732+00:00
"context/output/verification_history.log","The provided file contents document a brief log of two distinct attempts made by a Large Language Model (LLM) to modify or refactor a target text or codebase. In the first attempt, the model focused on correcting indentation, which is a common syntax or formatting fix. The second attempt involved the renaming of variables, indicating a shift toward improving code readability or adhering to specific naming conventions. Together, these entries represent a sequential record of automated edits performed by an AI system.",2025-12-22T04:27:19.362692+00:00
"context/output/verify_code.py","This file contains a short Python script designed to test or demonstrate the functionality of a function named calculate_average. The script begins by importing this specific function from a module named module_to_test. Following the import, the code performs a data preparation step where it takes the string literal 123 and converts it into a list of integers. This is achieved using a list comprehension that iterates over each character in the string, casts it to an integer type, and collects the results, effectively producing the list [1, 2, 3]. This list is assigned to the variable numbers. The script then invokes the calculate_average function, passing the prepared numbers list as its argument. The return value of this function call is captured in a variable named result. Finally, the script prints a formatted message to the console using an f-string, which displays Average is: followed by the calculated result. The code serves as a straightforward example of data conversion, function invocation, and output display within a Python environment.",2025-12-22T05:49:24.518564+00:00
"context/output/verify_code_1.py","This Python source file serves as a brief demonstration or test script focused on the calculate_average function, which is imported from a local module named module_to_test. The primary purpose of this snippet is to illustrate an intentional misuse of the function to provoke an error condition. The script begins by importing the necessary function. It then proceeds to call calculate_average with a single argument: the string literal 123. A comment embedded directly above this line explicitly states that this specific function call is expected to cause an error because a string is being passed instead of the expected numerical input, such as a list of integers or floats. This suggests the script is likely used for testing error handling, type validation, or simply documenting invalid usage patterns for developers. If the function were to execute successfully, the script would print the calculated average to the console using a formatted string. However, based on the inline documentation, the execution flow is anticipated to be interrupted by an exception raised due to the type mismatch of the input argument.",2025-12-22T05:50:01.253765+00:00
"context/pdd_discussiion.txt","This transcript captures an interview-style conversation about Prompt-Driven Development (PDD), a methodology where prompts replace code as the primary development artifact. The interviewee, who created an open-source PDD CLI tool and MCP server, explains that traditional code maintenance accounts for 80-90% of development costs, and PDD addresses this by allowing developers to regenerate clean code from modified prompts rather than continuously patching legacy code.

Key concepts discussed include: the evolution paralleling how HDL languages replaced netlists in chip design; the fundamental PDD unit consisting of four synchronized files (prompt, code, example, and test); token efficiency compared to agentic tools like Cursor; batch processing capabilities that reduce human babysitting; and improved collaboration through human-readable prompts.

The methodology involves creating a PRD, breaking it into module-specific prompts, generating code with examples and tests, then back-propagating learnings to keep all documentation synchronized. Future developments include PDD Cloud—a marketplace for few-shot examples to improve code generation quality—and a VS Code plugin for prompt formatting.

Challenges discussed include the learning curve for writing effective prompts, managing dependencies between modules, and knowing when to use PDD versus quick direct fixes. The interviewee emphasizes that PDD complements rather than replaces tools like Cursor, offering more control and consistency for complex, evolving projects while enabling non-technical stakeholders to participate in development.",2025-12-14T08:38:54.816456+00:00
"context/postprocess_0_example.py","This file provides an example usage guide for the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The function is designed to process output from a language model (LLM) that contains multiple code sections in various programming languages.

The example demonstrates how to use `postprocess_0` by passing it a sample LLM output string containing mixed content—including Python code blocks, Java code, and plain text—along with a specified target language (python). The function processes this input and returns a modified string where only the largest code section in the specified language remains uncommented, while all other text and code sections are commented out using the appropriate comment syntax for that language.

The documentation outlines two input parameters: `llm_output` (the raw LLM string output) and `language` (the target programming language to focus on). The output is a processed string with selective commenting applied.

The file also notes that helper functions `get_comment`, `comment_line`, and `find_section` must be properly implemented and accessible for the `postprocess_0` function to work correctly. This utility appears useful for extracting and isolating relevant code from mixed LLM outputs while preserving context through commenting.",2025-12-14T08:39:05.777701+00:00
"context/postprocess_example.py","This Python script demonstrates the usage of the `postprocess` function from the `pdd.postprocess` module, which is designed to extract code from Large Language Model (LLM) text outputs. The example showcases two distinct extraction scenarios:

**Scenario 1 (Simple Extraction, strength=0):** Uses basic string manipulation to find code blocks enclosed in triple backticks. This method is fast, cost-free, and language-agnostic, returning the first code block found.

**Scenario 2 (LLM-based Extraction, strength>0):** Leverages an LLM for more sophisticated code extraction. This approach is more powerful but incurs API costs and processing time. The script uses `unittest.mock` to simulate the behavior of internal dependencies (`load_prompt_template` and `llm_invoke`), allowing the example to run without actual LLM API calls.

The `postprocess` function accepts parameters including the LLM output text, target programming language, strength (0-1 scale determining extraction method), temperature, time (thinking effort), and verbose flag. It returns a tuple containing the extracted code, total cost, and model name used.

The script requires the `pdd` package and `rich` library for formatted console output. Mock verification ensures correct function calls and parameter passing, making this an educational example for understanding the postprocessing workflow.",2025-12-14T08:39:13.082325+00:00
"context/postprocessed_runnable_llm_output.py","This file contains the implementation of a `context_generator` function in Python, designed to automate the generation of usage examples for Python modules. The function accepts three parameters: `python_filename` (the input Python file to analyze), `output_filename` (where to write the generated output), and an optional `force` boolean flag defaulting to False.

The implementation follows a multi-step workflow: First, it reads the contents of the specified Python file, handling potential FileNotFoundError exceptions gracefully by printing an error message and returning False. Next, it preprocesses the file content (though the preprocessing function itself is referenced but not shown). The function then constructs a prompt for GPT-4, positioning it as an expert Python engineer tasked with generating concise usage examples based on the provided code.

The file includes explanatory comments describing each step of the process, along with usage examples showing how to call the function with appropriate arguments. The code appears to be incomplete or a work-in-progress, as some referenced components like the `preprocess` function and the actual model invocation logic are not fully implemented in the visible code. The function returns a boolean indicating success or failure of the operation.",2025-12-14T08:39:21.928602+00:00
"context/preprocess_example.py","This Python script demonstrates the usage of a preprocessing module from a package called 'pdd'. It imports a 'preprocess' function and uses the Rich library's Console for formatted terminal output.

The script defines a multi-line prompt string containing various XML-like tags including shell commands, PDD comments, web URLs, placeholder variables in curly braces ({test}, {test2}), and a markdown file reference. It also includes multi-line PDD comments that are noted as content that should not appear in the output.

Three configuration variables are set: 'recursive' is set to False, 'double_curly_brackets' is enabled (True), and 'exclude_keys' is defined as a list containing test2 to prevent that specific key from being processed with double curly brackets.

The script then prints debug information showing the excluded keys using Rich's styled console output with bold yellow formatting. Finally, it calls the preprocess function with the prompt and configuration options, then displays the processed result with bold white styling.

This appears to be a test or demonstration script for a prompt preprocessing system that handles special tags, variable substitution, and formatting options for template-like prompt strings.",2025-12-14T08:39:28.990143+00:00
"context/preprocess_main_example.py","This Python file implements a command-line interface (CLI) tool using the Click library for preprocessing prompt files. The CLI provides several configurable options: a required `--prompt-file` path specifying the input file, an optional `--output` path for saving results, and multiple boolean flags including `--xml` for adding XML delimiters, `--recursive` for processing referenced prompt files, `--double` for doubling curly brackets, and `--verbose` for detailed logging. The `--exclude` option allows specifying multiple keys to exempt from bracket doubling.

The main `cli` function initializes a context object with default parameters (strength, temperature, verbose setting) and calls `preprocess_main` from the `pdd.preprocess_main` module. This function handles the actual preprocessing logic and returns three values: the processed prompt content, total processing cost, and model name used.

Upon successful execution, the tool displays the preprocessing results including the processed prompt, cost, and model information. Error handling is implemented to catch and display any exceptions that occur during processing. The script can be run directly as a standalone program, making it a utility for transforming and preparing prompt files with various formatting options for downstream use in prompt-driven development workflows.",2025-12-14T08:39:36.941842+00:00
"context/process_csv_change_example.py","This Python script, `run_demo.py`, serves as a concise usage example demonstrating how to use the `process_csv_change` function from the `pdd.process_csv_change` module. The script walks through a complete workflow in four main steps:

1. **Imports**: It imports necessary modules including `Path` from pathlib and the target function `process_csv_change`.

2. **Workspace Setup**: Creates a demo workspace directory structure containing:
- A sample Python source file (`factorial.py`) with a recursive factorial function
- A prompt file (`factorial_python.prompt`) following a specific naming convention
- A CSV file (`tasks.csv`) containing instructions for modifying the prompt

3. **Function Call**: Invokes `process_csv_change` with various parameters including the CSV file path, strength (0.5), temperature (0.0), code directory, language (python), file extension (.py), and a budget limit (0.25 USD).

4. **Output Inspection**: Displays the returned values including success status, total cost, model name (gpt-5-nano), and a list of JSON objects containing the modified prompt content.

The example output shows the function successfully adding an example test section to the original prompt, demonstrating how the tool can automatically enhance prompts based on CSV-defined instructions using an LLM.",2025-12-14T08:39:44.862789+00:00
"context/prompt_caching.ipynb","This Jupyter notebook demonstrates how to use prompt caching with the Anthropic API to optimize performance and reduce costs when working with large documents. The cookbook uses Pride and Prejudice by Jane Austen (~187,000 tokens) as sample content to illustrate two main use cases.

The first example compares cached versus non-cached API calls for single-turn interactions. Results show dramatic latency improvements—from 21 seconds down to approximately 3 seconds—when leveraging cached prompts. The implementation uses the cache_control attribute with ephemeral type and requires the prompt-caching-2024-07-31 beta header.

The second example demonstrates multi-turn conversations with incremental caching using a ConversationHistory class that manages conversation turns and applies cache breakpoints to the last two user messages. This approach achieves nearly 100% cache hit rates after the initial setup, reducing response times from 24 seconds to 8-9 seconds while maintaining response quality.

Key benefits highlighted include over 2x latency reduction and up to 90% cost savings, making it practical to include detailed instructions and examples in prompts. The notebook provides complete Python code using the anthropic library, BeautifulSoup for content fetching, and demonstrates proper API call structure for enabling prompt caching.",2025-12-14T08:39:52.186970+00:00
"context/pytest_example.py","This Python script implements a custom pytest test runner with result collection capabilities. It defines a `TestResultCollector` class that acts as a pytest plugin to track test outcomes including failures, errors, and warnings. The collector uses pytest hooks (`pytest_runtest_logreport` and `pytest_sessionfinish`) to capture test results during different phases (setup, call, and teardown). It also includes functionality to redirect stdout and stderr to an in-memory StringIO buffer for log capture.

The `run_pytest()` function creates a collector instance, captures logs, and executes pytest with verbose output on a specific test file (`tests/test_get_extension.py`). It returns the counts of failures, errors, warnings, and the captured log output.

The main block includes extensive debug print statements to trace execution flow, displaying the current working directory and status messages at various stages. This appears to be a development or debugging version of a test runner utility, designed to programmatically run pytest tests and collect detailed results for further processing or reporting. The script is useful for integrating pytest into larger automation workflows where programmatic access to test results is needed.",2025-12-14T08:40:02.153924+00:00
"context/pytest_output_example.py","This Python script demonstrates the usage of a custom `pytest_output` module for running and capturing pytest test results. It imports several libraries including argparse, json, pytest, and rich (for console formatting). The script defines a helper function `create_dummy_test_file()` to generate test files with specified content.

The main function `main_example()` showcases six different usage examples:

1. **Using the main function**: Simulates command-line arguments to run pytest on a dummy test file containing passing, failing, error, and warning test cases.

2. **Capturing and saving output**: Demonstrates `run_pytest_and_capture_output()` to capture test results and `save_output_to_json()` to save them to a JSON file.

3. **Non-existent file handling**: Shows error handling when attempting to run tests on a file that doesn't exist.

4. **Non-Python file handling**: Tests behavior when given a non-Python file (`.txt` extension).

5. **Empty test file**: Demonstrates handling of a Python file containing no test functions.

6. **Advanced TestResultCollector usage**: Shows direct usage of the `TestResultCollector` class to capture stdout, stderr, failures, errors, warnings, and passed tests.

The script creates test files in an output directory and serves as both documentation and a functional example for the pytest_output module's capabilities.",2025-12-14T08:40:10.362700+00:00
"context/python_env_detector_example.py","This Python script serves as an example demonstrating the usage of the `python_env_detector` module from the `pdd` package. The script imports four key functions from the module: `detect_host_python_executable`, `get_environment_info`, `is_in_virtual_environment`, and `get_environment_type`.

The main function showcases each of these utilities in sequence. First, it detects and prints the path to the host Python executable. Then, it checks whether the script is running within a virtual environment and displays the boolean result. Next, it identifies and outputs the type of Python environment being used (such as venv, conda, or system Python). Finally, it retrieves comprehensive environment information as a dictionary and iterates through it to display each key-value pair, providing detailed insights about the current Python setup.

The script follows standard Python conventions with a shebang line for direct execution, a docstring explaining its purpose, and the `if __name__ == __main__` guard to ensure the main function only runs when the script is executed directly. This example is useful for developers who need to programmatically detect and understand their Python environment configuration, particularly when working across different virtual environment setups or deployment scenarios.",2025-12-14T08:40:18.593539+00:00
"context/python_preamble.prompt","The provided text outlines specific coding guidelines and structural requirements for a Python software project, intended for an expert Python engineer. It mandates modern Python practices, specifically requiring the inclusion of `from __future__ import annotations` at the beginning of files to handle forward references in type hints. Furthermore, strict adherence to full type hinting for all functions is enforced to ensure code clarity and reliability. For output formatting, the guidelines specify the use of the `rich.console.Console` library instead of standard print statements. Regarding the package structure, the document dictates the use of relative imports (using a single dot) for internal module referencing. It also highlights the existence of a specific initialization file (`./pdd/__init__.py`) which houses global constants such as `EXTRACTION_STRENGTH`, `DEFAULT_STRENGTH`, and `DEFAULT_TIME`, implying these should be imported relatively. Finally, the instructions emphasize robust error handling, requiring the implementation of mechanisms to manage edge cases like missing inputs or model errors, ensuring that the software provides clear and informative error messages to the user.",2025-12-20T21:06:57.899863+00:00
"context/regression_example.sh","This Bash script is a regression testing suite for a tool called PDD (likely a code generation or development tool). The script sets up various path variables and filenames for prompts, scripts, and test files, then executes a comprehensive series of PDD commands to validate the tool's functionality.

The script begins by defining environment variables for staging paths, prompt files, and Python scripts related to file extension handling and code generation. It includes utility functions for logging with timestamps and running PDD commands with error handling.

The regression tests cover multiple PDD operations: generate creates Python scripts from prompts, example produces verification programs, test generates test files, preprocess handles prompt preprocessing (including XML output), and update modifies prompts based on script changes. Additional commands include change for code modifications, fix for correcting failing tests (with a loop option for iterative fixes), split for dividing prompts into sub-components, detect for analyzing multiple prompts, conflicts for identifying issues between prompts, and crash for handling runtime errors.

Throughout execution, the script logs all operations to a regression log file and tracks costs in a CSV file. Upon completion, it calculates and displays the total operational cost. The script exits immediately if any command fails, ensuring reliable regression testing.",2025-12-14T08:40:33.537001+00:00
"context/render_mermaid_example.py","This Python script demonstrates how to use the `render_mermaid.py` module to convert architecture.json files into interactive HTML Mermaid diagrams for visualizing software architecture. The file contains several example functions showcasing different usage patterns.

The script includes a `create_sample_architecture()` function that generates a sample architecture definition with five modules: a FastAPI main application, database configuration, app settings, React frontend components, and an API client. Each module includes metadata like dependencies, priority, file paths, and tags.

Four example functions demonstrate various use cases: (1) basic command-line usage, (2) programmatic usage showing how to generate Mermaid code and HTML output directly, (3) loading and processing existing architecture.json files from common locations, and (4) describing customization features like automatic module categorization into Frontend/Backend/Shared groups, priority indicators, dependency arrows, color-coded subgraphs, interactive tooltips, and responsive HTML output.

The main function orchestrates all examples, providing a comprehensive demonstration of the render_mermaid module's capabilities for creating visual architecture diagrams from JSON configuration files.",2025-12-14T08:40:42.586210+00:00
"context/simple_math.py","The file contains a single comment indicating that it has been intentionally left blank for pdd generate. This suggests the file serves as a placeholder or template that will be populated later by an automated process or tool called pdd (which likely stands for Puzzle Driven Development or a similar development methodology). The file currently holds no substantive content, code, or data—only this explanatory note. Such placeholder files are commonly used in software development projects to establish file structure and organization before actual implementation begins. The pdd generate reference indicates this file is part of a workflow where content will be automatically generated or inserted at a later stage in the development process. This approach helps maintain project structure while allowing for incremental development and automated code generation. The blank state of the file is deliberate and documented, ensuring that other developers or team members understand its purpose and do not mistakenly assume the file is incomplete or missing content due to an error.",2025-12-14T08:40:50.434043+00:00
"context/split/1/final_pdd_python.prompt","The provided text outlines the specifications for pdd, a Python command-line tool designed to compile prompts into code files or generate example code from existing source files. The program utilizes the rich library for console output. Input files follow specific naming conventions like basename_language.prompt. The tool supports flexible output configurations, allowing users to specify filenames, directories, or rely on defaults derived from the input basename. Key command-line options include --force for overwriting files, -o for defining the runnable code output path, and -oe for generating and locating example code. The workflow involves parsing the input filename to extract the basename and language, determining the appropriate file extensions, and resolving output paths based on user arguments. If the input is a prompt file, pdd generates runnable code using a code_generator module. If the input is a code file or the -oe flag is used, it generates an example file using a context_generator module. The logic handles four specific output path scenarios ranging from explicit filenames to default directory behaviors.",2025-12-20T07:12:34.968501+00:00
"context/split/1/initial_pdd_python.prompt","The provided text defines the requirements for pdd, a Python command-line interface (CLI) tool designed to automate the generation of code and code examples. The tool functions by compiling specific prompt files (formatted as basename_language.prompt) into runnable code files or by converting existing code files into example documentation. All console output is formatted using the Python rich library. Key features include flexible output handling via flags: -o defines the destination for runnable code, and -oe triggers and defines the destination for example code. The tool supports four output path behaviors: specific filenames, specific directories, implied filenames within directories, and default locations. A --force flag allows overwriting existing files without user confirmation. The internal logic proceeds by parsing the input filename to identify the target language and basename. If the input is a prompt, the tool uses a code_generator to create the source code. If the input is a code file or if an example is requested, it employs a context_generator to produce the example file. The system automatically handles file extensions appropriate for the target language.",2025-12-20T07:12:51.154698+00:00
"context/split/1/pdd.py","The provided file contains the source code for pdd, a Python command-line utility designed to compile prompt files into runnable code and generate associated example usage files. It utilizes the argparse library to handle command-line arguments and the rich library to provide formatted console output and interactive user prompts. The script relies on two external modules, code_generator and context_generator, to perform the core generation tasks. Key functionality includes parsing input arguments to identify source files, determining appropriate output filenames and extensions (supporting Python, Bash, and Makefiles), and managing file overwrite permissions. The main function orchestrates the workflow: if a .prompt file is input, it generates the target code; subsequently, it attempts to generate an example file. The tool supports flags for specifying custom output paths (-o, -oe) and a --force option to bypass overwrite confirmation. The implementation ensures a structured workflow for converting natural language prompts into executable scripts and their corresponding context examples.",2025-12-20T07:13:07.750023+00:00
"context/split/1/split_get_extension.py","The provided file content comprises a concise, single-line code snippet, appearing to be written in a dynamic programming language like Python. The statement executes a variable assignment, specifically targeting a variable named 'file_extension'. The value assigned to this variable is derived from the execution of a function named 'get_extension', which is invoked with a single parameter, 'language'. From a functional perspective, this snippet implies a logic mechanism designed to resolve file type associations. The variable 'language' likely contains a string identifier for a programming language (e.g., 'Python', 'JavaScript') or a configuration format. The function 'get_extension' is expected to process this identifier and return the standard file suffix associated with it, such as '.py' or '.js'. This kind of logic is frequently found in software development tools, such as code generators, linters, or integrated development environments (IDEs), where the system must automatically handle file creation or recognition based on user input or detected settings. Since this is an isolated fragment, the definitions of the 'get_extension' function and the 'language' variable are assumed to exist in the surrounding scope. The code relies on these external dependencies to function, serving as a specific utility step within a larger file processing workflow.",2025-12-20T07:13:31.723037+00:00
"context/split/1/sub_pdd_python.prompt","The provided file contents outline a detailed specification for a coding task targeted at an expert Python engineer. The central goal of this task is to implement a specific Python function named get_extension. This function serves a utility role, designed to determine and return the correct file extension corresponding to a given programming language name. The specification defines the function's interface clearly: it accepts a single input parameter, language, which is a string (e.g., Python, Bash), and it outputs a string representing the associated extension. To ensure the function operates reliably regardless of how the input is capitalized, the instructions prescribe a three-step logical flow. First, the function must convert the input language string to lowercase, establishing a case-insensitive environment for comparison. Second, the code must perform a lookup operation to identify the specific extension linked to the normalized language name. Finally, the function must return this extension as the output. This structured approach ensures the resulting code is both functional and robust, handling various input cases effectively while adhering to the defined input/output contract.",2025-12-20T07:13:52.437298+00:00
"context/split/2/consolidated.prompt","The file presents a prompt engineering challenge requiring the decomposition of a complex prompt into a `sub_prompt` and a `modified_prompt`. The core subject is a Python CLI tool named pdd designed to compile prompts into code or generate examples. The input data consists of the original prompt specifying pdd's requirements—such as handling file extensions, parsing arguments, and determining output directories—alongside its Python implementation and a target code snippet (`construct_output_paths`).

The task is to extract the logic for determining output file paths into a standalone `sub_prompt` for a function named `construct_output_paths`. Consequently, the `modified_prompt` must be updated to utilize this new function rather than containing the logic inline. The file includes a reference example demonstrating how to extract a helper function (`get_extension`), providing a pattern for the user to follow in isolating the path construction logic while maintaining the tool's overall functionality.",2025-12-20T07:14:15.630822+00:00
"context/split/2/final_pdd_python.prompt","The provided text outlines the specifications for creating pdd, a Python command-line interface (CLI) tool designed to compile prompt files into runnable code or generate example code from existing source files. The tool utilizes the Python rich library for formatted console output. It processes input files following a specific naming convention (basename_language.prompt) and supports command-line arguments such as --force for overwriting files, -o to specify output paths, and -oe to trigger and direct example code generation. The program's logic involves a multi-step process: reading the input filename, extracting the basename and language to determine file extensions, and constructing output paths. Depending on whether the input is a prompt or a code file, the tool employs specific generators (code_generator or context_generator) to produce the required output. The workflow ensures that prompt inputs result in runnable code, while code inputs or specific flags trigger the creation of example files.",2025-12-20T07:14:31.033585+00:00
"context/split/2/initial_pdd_python.prompt","The provided text outlines the specifications for pdd, a Python command-line tool designed to compile prompts into code files or generate example code from existing source files. The program utilizes the rich library for console output. Input files follow specific naming conventions like basename_language.prompt. The tool supports flexible output configurations, allowing users to specify filenames, directories, or rely on defaults derived from the input basename. Key command-line options include --force for overwriting files, -o for defining the runnable code output path, and -oe for generating and locating example code. The workflow involves parsing the input filename to extract the basename and language, determining the appropriate file extensions, and resolving output paths based on user arguments. If the input is a prompt file, pdd generates runnable code using a code_generator module. If the input is a code file or the -oe flag is used, it generates an example file using a context_generator module. The logic handles four specific output path scenarios ranging from explicit filenames to default directory behaviors.",2025-12-20T07:14:48.792798+00:00
"context/split/2/pdd.py","The provided text details the implementation of `pdd`, a Python command-line utility designed to automate code generation tasks. The script functions as a compiler that transforms `.prompt` files into runnable code files and generates example code contexts from existing source files. It relies on external modules named `code_generator`, `context_generator`, and `get_extension` to perform the core logic, while utilizing the `rich` library for enhanced console output. The `pdd.py` script uses `argparse` to handle command-line arguments, including the input file, optional output paths for runnable and example code, and a force flag to overwrite files without confirmation. Key features include automatic file extension handling, language inference based on filenames, and dynamic output path construction. The workflow validates input file existence, determines whether to generate code or examples based on the input type and flags, and safely writes outputs to the disk. The content includes the complete source code for the tool along with a step-by-step explanation of its internal logic, covering argument parsing, path resolution, and conditional execution flows.",2025-12-20T07:15:04.690550+00:00
"context/split/2/split_pdd_construct_output_path.py","The provided code snippet illustrates a Python function call named construct_output_paths, which is responsible for generating two distinct file paths: runnable_output_path and example_output_path. This function takes four arguments that blend file-specific metadata with user-provided command-line inputs. The first two arguments are the basename of the file and its file_extension, which is determined by the programming language. The remaining two arguments, argv_output_path and argv_example_output_path, represent values captured from the command-line flags -o and -oe, respectively. This setup indicates that the code is likely part of a command-line tool or build script where output destinations are dynamically constructed based on the source file's identity and optional user overrides. The function encapsulates the logic required to resolve the final destination paths for a runnable file and an example output file.",2025-12-20T07:15:23.338317+00:00
"context/split/2/sub_pdd_python.prompt","This document outlines the requirements for creating a Python function named construct_output_paths, designed for a command-line tool called pdd. The function's purpose is to generate appropriate file paths for two types of outputs: a runnable file and an example output file. It accepts four inputs: a basename, a file extension, and optional user-provided paths corresponding to the -o and -oe flags. The logic must account for four scenarios regarding user input: filenames without paths, filenames with paths, paths without filenames, and default behavior when nothing is specified. The document provides specific examples of how command-line arguments should translate into file system paths, such as handling directory targets versus specific filenames. Furthermore, it details a four-step implementation plan, which involves defining a helper function to standardize path construction logic, applying this helper to determine both the runnable and example output paths, and finally returning these paths as a tuple.",2025-12-20T07:15:36.489664+00:00
"context/split/3/consolidated_xml_filled_in.prompt","The file provides instructions and examples for an expert LLM Prompt Engineer task involving the decomposition of a complex input prompt into a modular 'sub_prompt' and a 'modified_prompt' without losing functionality. It includes two detailed examples demonstrating how to split a prompt for a Python command-line tool named 'pdd'. In the first example, logic for determining file extensions is extracted; in the second, logic for constructing output paths is separated.

The file concludes with a specific input prompt to be processed: a request to write a Python function named 'postprocess'. This function is designed to sanitize LLM output strings by identifying code blocks (specifically the largest section matching a target language) and commenting out surrounding text to ensure the output is runnable. The provided input includes the prompt description, the corresponding Python implementation (which includes a helper function 'find_section'), and a snippet of example code. The objective is to apply the demonstrated splitting technique to this 'postprocess' prompt, likely extracting the 'find_section' logic.",2025-12-20T07:15:50.097013+00:00
"context/split/3/final_postprocess_python.prompt","The provided file contains a prompt specification for an expert Python engineer to develop a function named `postprocess`. This function is designed to convert raw string output from a Large Language Model (LLM)—which typically mixes conversational text with code blocks—into a directly executable script. The function takes `llm_output` and a target `language` as inputs. Its algorithmic logic involves five specific steps: determining the appropriate comment syntax for the target language, identifying top-level code sections, and selecting the largest code block that matches the requested file type. The function must then modify the string by commenting out all text outside of this primary code block, including the markdown triple backticks, while leaving the code itself active. The file includes references to helper functions for finding sections and commenting lines, along with detailed examples illustrating the input format and the expected, sanitized output string.",2025-12-20T07:16:05.723091+00:00
"context/split/3/initial_postprocess_python.prompt","The provided text outlines the requirements for a Python function named `postprocess`, designed to convert raw Large Language Model (LLM) output into executable code. The function takes two inputs: `llm_output`, a string containing a mix of natural language text and code blocks delimited by triple backticks, and `language`, specifying the target programming language (e.g., python, bash).

The core logic involves five steps. First, it determines the appropriate comment character for the specified language. Second, it parses the input string to identify top-level code sections, distinguishing them from nested blocks via a recursive approach. Third, it identifies the largest code section that matches the requested file type. Finally, the function constructs the output string by commenting out all text and code blocks except for the content of the largest matching section. This ensures that the resulting string is a valid, runnable script where explanatory text and irrelevant code snippets are treated as comments, while the main code logic remains active.",2025-12-20T07:16:23.448416+00:00
"context/split/3/postprocess.py","This Python file implements a post-processing module designed to clean and format output generated by Large Language Models (LLMs), ensuring it contains valid executable code. It relies on helper functions from the 'pdd' package to handle language-specific comment syntax. The script defines two primary functions: 'find_section' and 'postprocess'. The 'find_section' function parses text to locate Markdown-style code blocks delimited by triple backticks, identifying their language and line boundaries. The 'postprocess' function utilizes this parser to analyze the LLM's output. It searches for the longest code block that matches a specified target programming language. Once identified, the function preserves the code within this block while converting all surrounding text—such as explanations, conversational filler, or non-matching code blocks—into comments using the appropriate syntax for that language. If no matching code block is found, the entire output is commented out. This utility effectively isolates the desired code from mixed-content responses, making the output ready for execution or saving to a file.",2025-12-20T07:16:35.941011+00:00
"context/split/3/split_postprocess_find_section.py","The provided file content consists of a Python code snippet illustrating a function call to `find_section`. The result of this execution is stored in a variable named `sections`. The function accepts three specific parameters, each annotated with inline comments explaining their purpose and data types. The first parameter, `lines`, is expected to be an array of strings representing split lines of text. The second parameter, `start_index`, is an integer specifying the starting row for the operation, with a default value set to 0. The third parameter, `sub_section`, is a boolean flag used to determine if the current operation pertains to a sub-section, defaulting to `False`. This snippet appears to be part of a larger text processing or parsing logic, likely used to identify and extract specific structural elements within a document based on line-by-line analysis.",2025-12-20T07:16:50.670660+00:00
"context/split/3/sub_postprocess_python.prompt","The provided text outlines the specifications for a Python function named `find_section`, designed to parse output from a Large Language Model (LLM). The function's primary objective is to identify and extract top-level code sections while correctly handling and ignoring nested code blocks. It accepts a list of text lines, a starting index, and a boolean flag for recursion as inputs, returning a list of tuples that specify the programming language, start line, and end line for each identified section. The document includes a complex example of LLM output containing nested markdown code fences to illustrate the parsing challenges. Additionally, it provides a detailed step-by-step algorithm for implementation. This logic involves iterating through the lines to detect triple backticks, using recursion to manage nested sub-sections, and ensuring that only the outer, top-level code blocks are recorded in the final output list.",2025-12-20T07:17:04.524247+00:00
"context/split/4/construct_paths.py","This Python script defines utility functions for managing file paths and input/output operations within a command-line interface tool. The primary function, `construct_paths`, orchestrates the workflow by first validating and reading input files into memory. It dynamically extracts metadata such as the base filename and programming language from the input path to determine the appropriate file extension using a helper function `get_extension`. A secondary function, `generate_output_filename`, constructs standardized output filenames based on the specific command context (e.g., generate, example, test, preprocess, or fix). The script handles logic for resolving output paths whether the user provides a directory or a specific filename. Furthermore, it implements safety checks to prevent accidental data loss by verifying if output files already exist and prompting the user for overwrite confirmation via the `click` library, unless a force flag is applied. The module relies on `rich` for formatted console logging and returns the loaded input strings, resolved output paths, and detected language.",2025-12-20T07:17:20.068138+00:00
"context/split/4/final_construct_paths_python.prompt","The provided text outlines the requirements for creating a Python function named construct_paths for the pdd command-line tool. This function is designed to manage input and output file operations, utilizing the rich library for formatted console output and click for CLI handling. The function accepts several arguments: a dictionary of input file paths, boolean flags for force (overwrite) and quiet (suppress output), the command name, and command options. Its primary goals are to validate and construct file paths, load input file contents, and determine the target programming language. The process involves five specific steps: constructing input paths (ensuring extensions like .prompt are added if missing), loading file contents into a dictionary with error handling, generating output file paths using a helper function generate_output_filename, managing file overwrites by checking for existing files and prompting the user if the force flag is not set, and finally returning the loaded strings, resolved paths, and language. Console feedback regarding paths and steps is required unless the quiet flag is enabled.",2025-12-20T07:17:36.539578+00:00
"context/split/4/initial_construct_paths_python.prompt","This document outlines the specifications for developing a Python function named `construct_paths`, intended for use within the `pdd` command-line interface tool. The function is responsible for generating and validating input and output file paths, as well as loading input file contents. It utilizes the `rich` library for console output and `click` for CLI management. The function accepts arguments including input file paths, a force overwrite flag, a quiet flag, the specific command being executed, and associated options. It returns a dictionary of loaded input strings, determined output paths, and the target language. Key logic involves appending default extensions (such as `.prompt`) if missing and extracting language details for the 'generate' command. The execution flow consists of five steps: constructing input paths, loading file contents with error handling, determining output paths based on specific rules, checking for existing files (prompting the user for overwrite permission unless forced), and returning the final data structures.",2025-12-20T07:17:57.226350+00:00
"context/split/4/split_construct_paths_generate_output_filename.py","The provided code snippet contains a single Python assignment statement involving a function call to generate_output_filename. The result of this function is assigned to a variable named filename. The function accepts five arguments, each documented with inline comments explaining their purpose. The arguments are: command, which refers to pdd commands such as 'generate'; key, representing an output dictionary key like 'output' or 'output-test'; basename, which is the base name of the file; language, indicating the programming language associated with the file; and file_extension, specifying the file's extension. This snippet appears to be part of a utility for dynamically creating filenames based on configuration parameters and file attributes.",2025-12-20T07:18:11.909506+00:00
"context/split/4/sub_construct_paths_python.prompt","The provided text outlines the specifications for a Python function named `generate_output_filename`, tasked with creating context-aware filenames based on specific program commands. The function requires five input parameters: `command`, `key`, `basename`, `language`, and `file_extension`. It implements distinct naming conventions depending on the `command` argument. For instance, the 'generate' command simply combines the basename and extension, while 'example' and 'test' commands append specific suffixes or prefixes respectively. The 'preprocess' command generates a specific prompt file format incorporating the language, defaulting to 'unknown' if necessary. The 'fix' command applies conditional logic based on the provided key to distinguish between standard and test fixes. Any unrecognized commands default to appending an '_output' suffix. The instructions emphasize the need for robust implementation to efficiently handle these cases and potential edge scenarios.",2025-12-20T07:18:28.908505+00:00
"context/split/5/cli.py","This file implements a Command Line Interface (CLI) for a Prompt-Driven Development (PDD) tool, utilizing the `click` framework for command structure and the `rich` library for formatted console output. The application provides a suite of commands to manage the lifecycle of AI-generated code.

Key functionalities include `generate` for creating code from prompts, `test` for generating unit tests, and `fix` for iteratively resolving errors in code and tests. The tool also offers prompt management capabilities through commands like `preprocess`, `split`, `change`, and `update`, which help maintain and modify prompt files. Diagnostic commands such as `trace`, `crash`, and `bug` assist in debugging by linking code to prompts or generating reproduction tests.

A custom decorator, `track_cost`, is applied to these commands to monitor and log execution metrics, including model usage and costs, to a CSV file. The CLI accepts global options to configure the AI model's parameters (strength, temperature) and output verbosity, and includes a utility to install shell completion scripts.",2025-12-20T07:18:42.376804+00:00
"context/split/5/cli_python.prompt","The provided file contains instructions and context for generating a Python command-line interface (CLI) tool named pdd using the Click library. The prompt outlines the directory structure and specific import conventions to avoid naming conflicts between CLI commands and internal functions. It details a wide range of commands the tool must support, including generate for creating code, test for unit testing, preprocess for prompt handling, and fix for error resolution. Other commands include split, change, update, detect, conflicts, crash, trace, and bug, each associated with specific internal modules and usage examples. The instructions also cover sub-commands like preprocess --xml and fix --loop. Furthermore, the prompt includes requirements for an install_completion command to manage shell completion scripts and logic for tracking output costs by appending details to a CSV file. The file serves as a comprehensive specification for an AI to generate the cli.py entry point for the pdd application, integrating various code generation and maintenance utilities.",2025-12-20T07:18:58.537291+00:00
"context/split/5/modified_cli_python.prompt","The provided text outlines the specifications for developing a Python command-line interface (CLI) tool named pdd using the Click library. The tool is designed to automate various software development tasks, focusing heavily on code generation, testing, and prompt management. It defines a specific directory structure and a suite of commands, including `generate` for creating code, `test` for generating unit tests, and `fix` for resolving errors based on test feedback.

Additional capabilities include preprocessing prompts, splitting files, detecting changes, and handling version updates via Git. The tool also supports debugging through execution tracing (`trace`) and converting bugs into unit tests (`bug`). The instructions provide detailed implementation guidelines, requiring the use of specific internal modules like `construct_paths` for file handling and `track_cost` for monitoring. Developers are explicitly instructed to alias imported functions to prevent namespace conflicts with CLI command names (e.g., importing `preprocess` as `preprocess_func`). Finally, the specification includes a command for installing shell completion to enhance user experience.",2025-12-20T07:19:10.420412+00:00
"context/split/5/split_track_cost.py","The provided file content is extremely minimal, consisting solely of the text @track_cost. Syntactically, in programming languages like Python, the @ symbol combined with an identifier typically denotes a decorator. A decorator is a structural design pattern that allows behavior to be added to an individual object, either statically or dynamically, without affecting the behavior of other objects from the same class. In this specific context, the identifier track_cost suggests that this decorator is likely intended to monitor, calculate, or log the resource consumption or monetary cost associated with the execution of the function or method it decorates. This is a common pattern in applications involving cloud services, API usage (such as Large Language Models), or resource-intensive computational tasks where usage metering is required. However, since the file contains no function definitions, import statements, or implementation logic, it is impossible to determine the specific mechanism or underlying code used to track these costs. The file appears to be a snippet, a placeholder, or an incomplete segment of a larger Python script or configuration file intended to wrap a specific process with cost-monitoring logic.",2025-12-20T07:19:27.283266+00:00
"context/split/5/track_cost_python.prompt","The provided text outlines the specifications for developing a Python decorator function named `track_cost`, designed for a command-line interface (CLI) tool called pdd built with the Click library. The primary objective of this decorator is to monitor and log the financial cost associated with executing specific CLI commands. The decorator is required to wrap command functions, capturing the start and end times of execution. It must interact with the Click context to retrieve command details and identify the output path for cost logging, either via direct options or the `PDD_OUTPUT_COST_PATH` environment variable. Key data points to be extracted include the command name, the cost and model (typically returned by the function), and relevant input/output file paths based on argument naming conventions. This information must be persisted to a CSV file. The logic includes creating the file with headers if it does not exist or appending to it if it does. Crucially, the implementation must be robust: it should use `functools.wraps` to preserve metadata and handle any logging exceptions gracefully using `rich.rprint` without interrupting the main command execution. Finally, the decorator must return the original command's result.",2025-12-20T07:19:41.716270+00:00
"context/split/6/cli.py","This file defines the Command Line Interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` library. It serves as the main entry point for the application, orchestrating various AI-assisted development workflows. The CLI accepts global configuration options such as AI model strength, temperature, verbosity, and cost tracking.

The file implements numerous subcommands that interface with backend modules to perform specific tasks:
- **Generation**: Creating code (`generate`) and unit tests (`test`) from prompt files.
- **Refinement**: Fixing errors (`fix`, `crash`) and generating bug reproduction tests (`bug`).
- **Prompt Engineering**: Preprocessing (`preprocess`), splitting (`split`), and modifying prompts based on code changes or instructions (`change`, `update`).
- **Analysis**: Detecting necessary changes (`detect`), identifying conflicts between prompts (`conflicts`), and tracing code lines back to prompt definitions (`trace`).
- **Utilities**: Generating examples (`example`) and installing shell completion scripts.

Most commands are decorated with `@track_cost` to monitor API usage, and the `rich` library is used for formatted console output.",2025-12-20T07:19:56.486897+00:00
"context/split/6/cli_python.prompt","The provided file contains a detailed prompt for an expert Python engineer to develop a command-line interface (CLI) tool named `pdd` using the Python Click library. The tool is designed to automate various software development tasks centered around prompt engineering and code generation. The prompt specifies the project's directory structure and mandates aliasing imported functions to prevent naming conflicts with Click commands.

It outlines requirements for implementing numerous commands, providing specific examples for each. Key commands include `generate` for creating code, `test` for generating unit tests, `preprocess` for handling prompt files, and `fix` for resolving code errors. Additional functionality covers splitting and changing prompts (`split`, `change`), updating prompts (`update`, with Git support), detecting changes (`detect`), and analyzing conflicts (`conflicts`). The tool also supports debugging and maintenance through commands like `crash` for fixing module errors, `trace` for execution tracing, and `bug` for generating tests from bugs. Finally, instructions are included for an `install_completion` command to configure shell auto-completion.",2025-12-20T07:20:10.437879+00:00
"context/split/6/conflicts_main_python.prompt","The provided file outlines the requirements for developing the conflicts_main function, a core component of the pdd command-line tool designed to analyze conflicts between two prompt files. Acting as an expert Python engineer, the developer is tasked with creating a function that accepts a Click context, two input prompt paths, and an optional output path. The function is expected to return a tuple containing analysis results, operation cost, and the model used. The workflow involves several specific steps: first, utilizing the construct_paths utility to manage file paths and handle existing files based on force/quiet flags; second, reading the contents of the specified prompt files; and third, invoking conflicts_in_prompts to perform the core analysis. The results must then be serialized into a CSV file. The instructions emphasize user experience, requiring the use of the rich library for console output and strict adherence to verbosity settings. Finally, the function must include comprehensive error handling to catch exceptions and exit gracefully using the Click context. The prompt includes references to external examples and preambles to guide the implementation style and dependency usage.",2025-12-20T07:20:22.095114+00:00
"context/split/6/modified_cli_python.prompt","The provided file outlines the specifications for `pdd`, a Python command-line interface (CLI) tool built using the Click library to facilitate prompt-driven development. The tool's entry point is the `cli` function, which orchestrates a variety of commands for generating code, tests, and context. Key features include `generate` for creating code from prompts, `test` for unit test generation, and `example` for context creation. The tool supports robust prompt management through commands like `preprocess` (including XML tagging), `split`, `change` (with CSV support), and `update` (with Git integration). For maintenance and debugging, `pdd` offers `fix` to resolve errors iteratively, `crash` to fix module errors, `trace` for execution tracing, and `bug` to convert bugs into unit tests. It also includes `detect` for identifying necessary changes and `conflicts` for resolving prompt issues. The instructions emphasize careful import management to avoid naming conflicts between Click commands and internal functions (e.g., importing `preprocess` as `preprocess_func`). Helper utilities like `construct_paths` for file handling and `track_cost` for cost tracking are also integrated, alongside an `install_completion` command for shell configuration.",2025-12-20T07:20:36.800081+00:00
"context/split/6/split_conflicts.py","The provided code snippet defines a Python function named `conflicts`, which serves as a command-line interface (CLI) command registered via the `@cli.command()` decorator. This command is designed to analyze two distinct prompt files to identify potential conflicts and propose resolutions. The function utilizes the `click` library to handle command-line arguments. It requires two positional arguments, `prompt1` and `prompt2`, both of which must be valid, existing file paths. Additionally, it accepts an optional `--output` flag, allowing the user to specify a destination path for a CSV file where the conflict analysis results will be saved. The function signature indicates it returns a tuple consisting of a list of dictionaries, a float, and a string. It is also decorated with `@track_cost`, implying that the execution involves cost tracking, likely related to LLM API usage. Internally, the function acts as a wrapper, delegating the core logic to a separate function named `conflicts_main`, passing along the context and arguments. A comment notes that this separation was done to split functionality into a sub-prompt structure.",2025-12-20T07:21:01.089117+00:00
"context/split/7/cli.py","The provided file implements a Command Line Interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` and `rich` libraries. It serves as the main entry point for an application designed to assist developers in generating, testing, and maintaining code through AI-driven prompts.

The CLI defines a main command group that accepts global configuration options such as AI model strength, temperature, verbosity, and cost tracking. It exposes a wide range of subcommands including `generate` for creating code from prompts, `test` for creating unit tests, and `fix` for iteratively resolving code errors. Other commands facilitate prompt management, such as `preprocess` for formatting, `split` for breaking down complex prompts, `change` for applying modifications, and `update` for synchronizing prompts with modified code.

Advanced features include `detect` for identifying necessary changes across files, `conflicts` for resolving prompt discrepancies, `crash` for fixing module errors, and `trace` for mapping code lines back to their originating prompt instructions. The file relies on numerous internal modules to handle the core logic for these operations while managing file I/O and user interaction via the terminal.",2025-12-20T07:21:15.547421+00:00
"context/split/7/cli_python.prompt","The provided file contains a comprehensive prompt for an AI to generate a Python command-line interface (CLI) tool named `pdd`. Designed to facilitate Prompt Driven Development, the tool is to be implemented using the `click` library. The prompt details the project's directory structure, necessary imports, and specific instructions for creating a `cli.py` file that orchestrates various development tasks.

The tool's functionality is divided into several commands, including:
- **Code and Test Generation**: Commands like `generate`, `test`, and `bug` create code from prompts and generate unit tests.
- **Maintenance and Refactoring**: Commands such as `change`, `update`, `split`, and `preprocess` allow users to modify, update, or restructure prompt files, with support for CSV inputs and Git integration.
- **Debugging**: The `fix`, `crash`, and `trace` commands assist in resolving errors and tracing code execution.
- **Analysis**: `detect` and `conflicts` commands help identify necessary changes and resolve discrepancies between prompts.

The file includes specific examples for integrating internal modules (e.g., `code_generator`, `fix_errors_from_unit_tests`) and instructions to handle naming conflicts between CLI commands and imported functions.",2025-12-20T07:21:29.688428+00:00
"context/split/7/modified_cli_python.prompt","The provided text outlines the specifications for building a Python command-line interface (CLI) tool named `pdd` using the `click` library. It defines the project's directory structure and designates `cli.py` as the central file containing the `cli` function. The document enumerates a wide range of commands to be implemented, including `generate`, `test`, `preprocess`, `fix`, `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`. For each command, the text identifies the specific internal module and function responsible for the core logic, such as using `code_generator` for the `generate` command or `trace_main` for `trace`. It emphasizes the importance of renaming imported functions, for example importing `preprocess` as `preprocess_func`, to prevent namespace conflicts with Click command names. Furthermore, it instructs on using shared utilities like the `@track_cost` decorator for monitoring usage and `construct_paths` for handling input/output file paths. Finally, it includes specific details for implementing an `install_completion` command to manage shell auto-completion scripts.",2025-12-20T07:21:39.937768+00:00
"context/split/7/split_trace_main.py","The provided code snippet defines a Python function named `trace`, which serves as a command-line interface (CLI) command registered via the `@cli.command()` decorator. This command is designed to identify the relationship between a specific line of generated code and its originating prompt file. The function accepts three mandatory arguments: `prompt_file` and `code_file`, both validated as existing file paths, and `code_line`, an integer representing the specific line number in the code. Additionally, it accepts an optional `--output` argument to specify a destination for saving trace analysis results. The function is decorated with `@click.pass_context` to access the application context and `@track_cost`, likely used for monitoring resource usage or API costs. Internally, the `trace` function does not contain the core logic itself. Instead, it acts as a wrapper that immediately delegates execution to a separate function named `trace_main`, passing along the context and all received arguments. The return type annotation suggests the operation yields a tuple consisting of a string, a float, and another string. A comment within the code notes that `trace_main` encapsulates the functionality, having been refactored or split for structural reasons.",2025-12-20T07:22:00.278141+00:00
"context/split/7/trace_main_python.prompt","The provided text outlines the specifications for implementing the `trace_main` function, a key component of the `pdd` command-line tool. This function is responsible for executing the logic behind the `trace` command, which maps a specific line in a generated code file back to its corresponding source in a prompt file. The function takes inputs such as the Click context, file paths, and the target line number, returning the identified prompt line, operation cost, and model name. The implementation process involves several steps: constructing file paths using internal helpers, reading input files, performing the trace analysis, saving the results, and providing user feedback via the Rich library. It requires robust error handling to manage exceptions and exit gracefully if necessary. Additionally, the code must adhere to specific stylistic guidelines, including proper type annotations and documentation, while respecting CLI options like `--force` and `--quiet`.",2025-12-20T07:22:20.927037+00:00
"context/split/8/change_main.py","The provided file contents display a Python code snippet defining a CLI command named `change` using the `click` library. This function is decorated with `@cli.command()`, indicating it is part of a larger command-line interface application. The command accepts three optional positional arguments: `input_prompt_file`, `input_code_file`, and `change_prompt_file`, all validated as existing file paths. It also includes two options: `--output`, which specifies the destination path for the modified prompt file, and `--csv`, which allows the user to provide change prompts via a CSV file rather than a text file. The function signature includes type hinting for these parameters. Additionally, the function utilizes the `@click.pass_context` decorator to access the application context and a custom `@track_cost` decorator, likely used for monitoring resource usage or API costs. The docstring explains that the command's purpose is to modify an input prompt file based on a change prompt and input code. Structurally, the `change` function acts as a proxy, immediately delegating all processing to a separate function named `change_main`. A comment within the code clarifies that `change_main` encapsulates the actual functionality, having been split out, possibly to facilitate sub-prompting strategies.",2025-12-20T07:22:34.435436+00:00
"context/split/8/change_main_python.prompt","This file provides the specifications for implementing the `change_main` function, which serves as the core logic for the `change` command within the `pdd` command-line tool. The function is designed to modify prompts based on input code and specific change instructions. It accepts various arguments, including the Click context object, file paths for input prompts, code, change prompts, output destinations, and an optional CSV file for batch processing. The function returns a tuple containing the modified prompt, the total cost of the operation, and the model name used.

The required implementation steps involve parsing arguments to distinguish between single-file operations and CSV batch processing, constructing necessary file paths, and executing the modification logic using internal helper functions like `change_func` or `process_csv_change`. The function must also handle file saving, provide user feedback via `rprint` while respecting verbosity settings, and include robust error handling. The instructions emphasize adherence to the existing project style, including proper type annotations, docstrings, and specific import aliasing to prevent naming conflicts.",2025-12-20T07:22:52.842790+00:00
"context/split/8/cli.py","This file defines the Command Line Interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` library. It serves as the central entry point for the application, orchestrating various AI-driven software development tasks. The CLI exposes a comprehensive suite of commands designed to automate the coding lifecycle based on natural language prompts.

Key functionalities include:
- **Code Generation**: The `generate` command creates runnable code from prompt files, while `example` generates reference implementations.
- **Testing & Debugging**: The `test` command produces unit tests, and `bug` generates tests to reproduce specific bugs based on output discrepancies. The `fix` and `crash` commands automate error resolution in code and tests.
- **Prompt Engineering**: A set of tools (`preprocess`, `split`, `change`, `update`, `detect`) allows users to refine, modify, and synchronize prompts with the codebase, including reverse-updating prompts from modified code.
- **Analysis**: Commands like `conflicts` and `trace` help analyze prompt interactions and map code lines back to their originating prompts.

The script also handles global configurations for the AI model (strength, temperature), output verbosity, and cost tracking.",2025-12-20T07:23:06.844131+00:00
"context/split/8/cli_python.prompt","The provided text outlines the specifications for developing `pdd`, a Python command-line interface (CLI) tool built using the `Click` library. The tool is designed to streamline prompt-driven development workflows through a variety of commands defined in `cli.py`. Key functionalities include generating code from prompts (`generate`), creating unit tests (`test`), preprocessing prompt files (`preprocess`), and iteratively fixing errors (`fix`).

The instructions detail the implementation of additional commands such as `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`. Developers are instructed to import internal logic functions with distinct names (e.g., `preprocess_func`) to avoid conflicts with CLI command names. The tool supports specialized sub-commands like `preprocess --xml` for XML tagging, `fix --loop` for iterative error correction, and `update --git` for leveraging git history. Furthermore, the CLI integrates a `track_cost` decorator to monitor resource usage and includes a utility for installing shell completions. The text provides comprehensive examples for connecting these internal modules and handling file paths via helper functions.",2025-12-20T07:23:19.348501+00:00
"context/split/8/modified_cli.prompt","The provided text is a detailed prompt for generating a Python command-line interface (CLI) tool named `pdd` using the `Click` library. The tool is designed to assist developers with tasks such as code generation, unit testing, prompt preprocessing, and error resolution. The prompt outlines the project's directory structure and provides specific instructions for implementing various commands, including `generate`, `test`, `fix`, `update`, `detect`, `trace`, and `change`. It explicitly references internal modules and helper functions to handle core logic, such as `track_cost` for usage monitoring, `code_generator` for creating code from prompts, and `git_update` for retrieving file history. Additionally, the instructions cover sub-commands and specific flags (e.g., `--git`, `--loop`, `--xml`), detailing how to manage arguments, construct file paths, and install shell completions, ensuring the generated `cli.py` serves as a robust wrapper for these underlying utilities.",2025-12-20T07:23:33.202120+00:00
"context/split/9/cli.py","This file serves as the main entry point for the PDD (Prompt-Driven Development) Command Line Interface. It utilizes the `click` library to define a robust CLI structure with various subcommands and global options. The script begins by dynamically determining the runtime path of the PDD package via environment variables or import resources.

The main `cli` group accepts configuration parameters such as model strength, temperature, verbosity, and cost tracking, storing them in a context object for use by subcommands. It also performs an automatic update check upon initialization.

The file defines numerous commands that map to specific functionalities imported from internal modules. Key commands include:
- `generate`: Creates runnable code from prompt files.
- `test`: Generates and enhances unit tests, optionally using coverage reports.
- `fix` and `crash`: Iteratively repair code and tests based on errors, crashes, or verification programs.
- `preprocess`, `split`, `change`, and `update`: Manage, split, and modify prompt files.
- `detect` and `conflicts`: Analyze prompts for necessary changes or logical conflicts.
- `auto_deps`: Automatically scans and inserts project dependencies into prompts.
- `install_completion`: Sets up shell autocompletion for Bash, Zsh, or Fish.

Overall, this script orchestrates the workflow for a prompt-driven development environment, connecting user inputs to specific AI-driven development tasks.",2025-12-20T07:23:43.504005+00:00
"context/split/9/cli_python.prompt","This file contains a detailed prompt designed to guide an AI in generating the source code for a Python command-line tool named `pdd`. The tool is to be built using the `Click` library and serves as a developer utility with a modular architecture. The prompt defines the project structure and provides specific implementation instructions for a suite of commands including `generate`, `test`, `fix`, `update`, `detect`, `conflicts`, `crash`, `trace`, `bug`, and `auto-deps`. It directs the AI to utilize pre-defined internal modules and decorators (such as `track_cost` and various `*_main` functions) by referencing specific example files. Furthermore, the file includes logic for an `install_completion` command, detailing requirements for shell detection and script installation, as well as instructions for handling environment variables like `PDD_PATH` and `PDD_AUTO_UPDATE`. The overall goal is to produce a consistent, robust CLI entry point (`cli.py`) that orchestrates these various development automation capabilities.",2025-12-20T07:24:00.004076+00:00
"context/split/9/install_completion.py","The provided file content consists of a Python code snippet defining a single function intended for use within a Command Line Interface (CLI) application. The code utilizes a decorator, `@cli.command(name=install_completion)`, to register the associated function, `install_completion`, as an executable command within the CLI's hierarchy. This structure suggests the use of a Python library such as Click or a similar framework for building command-line tools. The function's name clearly indicates its operational purpose: to facilitate the installation of shell completion support. Shell completion is a usability feature that enables the terminal to automatically suggest or complete subcommands, options, and arguments when the user presses the tab key. While the implementation details—such as the specific logic for detecting the user's shell (e.g., Bash, Zsh, Fish) or writing the necessary configuration scripts—are absent from this brief snippet, the definition serves as the entry point for this functionality. By running this command, a user would typically trigger a process that integrates the CLI tool's autocomplete definitions into their shell environment, thereby enhancing the overall user experience and efficiency when interacting with the application.",2025-12-20T07:24:14.024482+00:00
"context/split/9/modified_cli.prompt","The provided file serves as a detailed specification for developing a Python command-line interface (CLI) tool named `pdd`. Designed to be implemented using the Python `click` library, the tool follows a modular structure with specific directories for source code, prompts, context, and data. The specification outlines a wide array of sub-commands that delegate functionality to internal modules, covering tasks such as code generation (`generate`), unit testing (`test`), error fixing (`fix`), and code modification (`change`). Additional features include dependency management (`auto-deps`), conflict detection (`conflicts`), crash analysis (`crash`), and prompt file management (`split`, `update`, `detect`). The document references numerous external example files to guide the implementation of these commands, ensuring correct usage of internal APIs like `code_generator_main` and `cmd_test_main`. It also specifies the handling of environment variables such as `PDD_AUTO_UPDATE` for automatic updates and `PDD_PATH` for resource location, emphasizing consistency in coding standards and documentation throughout the project.",2025-12-20T07:24:28.960083+00:00
"context/split/9/sub_cli.prompt","This file contains a detailed prompt specification for an AI to generate a Python function named `install_completion` within the `pdd` command-line application. The objective is to automate the installation of shell completion scripts. The prompt outlines specific logic requirements, starting with reliable shell detection using various methods (`ps`, `$0`, environment variables) and handling unsupported shells. It instructs the developer to locate the appropriate completion script dynamically and verify its existence. The core functionality involves identifying the user's shell configuration file (RC file), ensuring the file and its directory structure exist, and appending the necessary `source` command if it is not already present. The prompt emphasizes robust error handling using `try-except` blocks for `OSError` and mandates the use of `rprint` for color-coded user feedback (success, warning, or failure). Additionally, it requires the implementation of four specific helper functions: `get_shell_rc_path`, `get_current_shell`, `get_completion_script_extension`, and `get_local_pdd_path`. Finally, the prompt enforces adherence to existing coding styles, including type hinting and docstrings.",2025-12-20T07:24:38.038399+00:00
"context/split/simple_math/split.prompt","The provided text contains a specific instruction related to software development and code refactoring. It directs the developer to identify the existing logic responsible for validating input data and to move that logic out of its current context. The objective is to encapsulate this validation code within a newly created, separate helper function. This approach is a standard best practice in software engineering aimed at improving code modularity, readability, and maintainability. By isolating the validation logic, the main function or component can focus on its primary responsibility, adhering to the Single Responsibility Principle. Furthermore, creating a dedicated helper function for validation often facilitates the reuse of these checks across different parts of the application and simplifies unit testing for the validation rules.",2025-12-20T07:24:56.593060+00:00
"context/split/simple_math/split_example.py","The provided input represents a Python script designed to demonstrate the usage and error handling capabilities of an external module named 'split_script'. The script begins by importing this module. It then proceeds to execute two distinct test cases involving a function named 'add'. The first test case invokes 'split_script.add' with two integer arguments, 10 and 20. This operation is wrapped in a 'try-except' block to catch 'TypeError' exceptions, though with valid integers, it is expected to succeed and print the sum. The second test case intentionally introduces a type mismatch by calling the same function with an integer, 5, and a string, 'x'. This call is also enclosed in a 'try-except' block specifically looking for a 'TypeError'. When the mismatch triggers the error, the script catches the exception and outputs the corresponding error message. Essentially, this file acts as a driver script to verify that the 'split_script' module correctly processes numerical additions and gracefully identifies invalid argument types without causing the program to crash.",2025-12-20T07:25:18.425186+00:00
"context/split/simple_math/split_script.py","The provided file contains a Python script that is currently marked as a placeholder for a split command script, as indicated by the initial comment. The script defines a single function named add which accepts two parameters, a and b. This function includes a docstring explaining that it adds two numbers while performing basic type checking. The implementation utilizes the isinstance function to verify that both arguments are either integers or floating-point numbers. If the first argument a is not a number, the function raises a TypeError with a specific message. Likewise, if the second argument b is not a number, a corresponding TypeError is raised. If both inputs pass these validation checks, the function returns their sum. The code demonstrates a defensive programming style by ensuring inputs are of the correct type before execution.",2025-12-20T07:25:38.379602+00:00
"context/split_example.py","This Python script demonstrates the usage of a `split` function from the `pdd.split` module. The code sets up a main function that configures the environment and calls the split function with specific parameters.

The script begins by importing necessary modules: `os` for environment variable management, `Console` from the `rich` library for formatted terminal output, and the `split` function from `pdd.split`.

In the `main()` function, it first sets the `PDD_PATH` environment variable to the parent directory of the script. It then defines several input parameters: an `input_prompt` requesting a factorial function, sample `input_code` containing a recursive factorial implementation, `example_code` showing how to use the factorial function, and two float parameters (`strength` and `temperature`) both ranging from 0 to 1.

The `split` function is called with these parameters plus a `verbose=True` flag. It returns a tuple containing a result tuple (with `sub_prompt` and `modified_prompt`), a `total_cost` value, and the `model_name` used. The results are then printed to the console using Rich's formatting capabilities, displaying the sub-prompt, modified prompt, model name, and cost. Error handling is implemented to catch and display any exceptions that occur during execution.",2025-12-14T08:40:56.932433+00:00
"context/split_main_example.py","This Python file, `example_split_usage.py`, demonstrates how to create a command-line interface (CLI) using the Click library to interact with a `split_main` function from a module called `pdd.split_main`. The CLI tool is designed to split and process prompt files used in code generation workflows.

The script defines a `split_cli` command with several options: required paths for an input prompt file, an input code file, and an example code file, along with optional paths for saving sub-prompts and modified prompts. Additional flags include `--force` for overwriting existing files and `--quiet` for suppressing output.

The command configures custom settings like strength (controlling how much content is extracted into a sub-prompt) and temperature (controlling randomness) via the Click context object. When executed, it calls `split_main()` which returns a result dictionary containing sub-prompt and modified prompt content, along with the total operation cost and model name used.

The output displays the generated content, file save locations, model information, and cost unless quiet mode is enabled. Error handling is included to catch and report exceptions. The script uses Click's group/command pattern, making it extensible for additional CLI commands.",2025-12-14T08:41:04.701150+00:00
"context/summarize_directory_example.py","This Python script demonstrates the usage of a `summarize_directory` module from the `pdd` package. The main function showcases a workflow for generating file summaries in CSV format. It begins by defining sample existing CSV content containing file paths, summaries, and dates. The script then calls the `summarize_directory` function with several parameters: a wildcard directory path pattern (`context/c*.py`) to locate Python files, a strength value of 0.5 that influences model selection (higher values use more capable but expensive models), a temperature of 0.0 for deterministic output, verbose mode enabled for progress tracking, and the existing CSV content for reference. The function returns three values: the generated CSV output, total processing cost, and the model name used. After processing, the script prints the results including the CSV content, cost in USD, and model information. It then ensures an output directory exists (creating it if necessary) and saves the generated CSV to `output/output.csv`. The entire operation is wrapped in a try-except block for error handling. The script follows standard Python conventions with a `__name__ == __main__` guard for direct execution.",2025-12-14T08:41:12.698067+00:00
"context/sync_animation_example.py","This Python script demonstrates how to use a `sync_animation` module for displaying a terminal-based animation that tracks the progress of a simulated PDD (Prompt-Driven Development) workflow. The script showcases multi-threaded programming with shared state management between a main application thread and an animation thread.

Key components include:
1. **Shared State Variables**: Mutable lists and a `threading.Event` are used to communicate between threads. These include references for the current function name, accumulated cost, file paths (prompt, code, example, tests), and box colors for the animation display.

2. **Static Parameters**: Configuration values like the project basename (handpaint), budget ($3.00), and initial color settings for UI boxes.

3. **Mock Workflow Simulation**: The `mock_pdd_main_workflow()` function simulates various PDD commands (checking, auto-deps, generate, example, crash, verify, test, fix, update) with timed delays, updating the shared state to trigger animation changes.

4. **Thread Management**: The animation runs as a daemon thread, allowing the main workflow to proceed independently. The script properly signals termination via `stop_event.set()` and joins the thread with a timeout.

5. **Cleanup and Reporting**: After completion, the script reports total elapsed time and accumulated cost, demonstrating proper resource management and thread synchronization patterns.",2025-12-14T08:41:20.004164+00:00
"context/sync_determine_operation_example.py","This Python script demonstrates the usage of a `sync_determine_operation` function from a module called `pdd` (likely Prompt-Driven Development). The script simulates a project environment to showcase how the synchronization system determines what operation to perform on code units based on their current state.

The script sets up a simulated project in an `./output` directory and runs through four distinct scenarios:

1. **New Unit**: A fresh prompt file exists with no associated code or history, triggering a new unit creation workflow.

2. **Test Failure**: Simulates a situation where test execution has failed (exit code 1, one test failed), demonstrating how the system detects and responds to test failures.

3. **Manual Code Change**: Shows what happens when a developer manually modifies code, causing the file's hash to differ from the stored fingerprint, indicating the code is out of sync.

4. **Unit Synchronized**: Represents the ideal state where all file hashes (prompt, code, example, tests) match the fingerprint and tests have passed with full coverage.

The script uses SHA256 hashing to track file changes and stores metadata (fingerprints and run reports) as JSON files in a `.pdd/meta` directory. Key data structures include `Fingerprint` (tracking file versions) and `RunReport` (storing test execution results). This appears to be a development tool for maintaining synchronization between prompts, generated code, and tests.",2025-12-14T08:41:29.272238+00:00
"context/sync_main_example.py","This Python script demonstrates and tests the `sync_main` function from the `pdd` (Prompt-Driven Development) framework in a controlled, isolated environment. The code sets up a mock project structure with prompt files for multiple programming languages (Python and JavaScript) and simulates the behavior of the `pdd sync` CLI command.

The script consists of two main functions: `setup_mock_project` creates a temporary project directory structure containing prompt files that instruct code generation for greeting functions in different languages. The `main` function orchestrates the demonstration by: (1) setting up the mock project, (2) creating a simulated Click context object with default configuration options like temperature, strength, and verbosity settings, and (3) patching internal dependencies (`construct_paths` and `sync_orchestration`) with mock implementations.

The mocked `sync_orchestration` function simulates different outcomes—returning success for Python code generation with passing tests, and failure for JavaScript due to a simulated test error. This allows testing the aggregation logic of `sync_main` without requiring actual LLM API calls or test execution.

The script uses the Rich library for formatted console output and demonstrates how the sync command processes multiple language targets sequentially, aggregating results and costs across all processed languages before displaying a final JSON summary.",2025-12-14T08:41:39.496822+00:00
"context/sync_orchestration_example.py","This Python script demonstrates the usage of the `sync_orchestration` module from a PDD (Prompt-Driven Development) CLI tool. The code consists of two main parts:

1. **Project Setup Function (`setup_example_project`)**: This function creates a mock project structure within an output directory, including subdirectories for prompts, source code, examples, and tests. It also generates a sample prompt file for a calculator function and ensures the PDD metadata directory exists.

2. **Main Execution Block**: The script showcases two primary functionalities:
- **Running a Full Sync Process**: Simulates the `pdd sync calculator` command by calling `sync_orchestration()` with various parameters including basename, language, directory paths, and a budget limit of $5.00. The function determines necessary steps (generate, example, test) based on file states and executes them.
- **Viewing Sync Logs**: Demonstrates how to retrieve and display logs from a previous sync operation by setting the `log=True` flag.

The script outputs JSON-formatted results showing the sync operation's success status and any errors encountered. It serves as a practical example for developers learning to integrate the PDD sync orchestration module into their projects, illustrating both execution and logging capabilities.",2025-12-14T08:41:48.419411+00:00
"context/test.prompt","This file contains development guidelines and configuration notes for a project structure involving test generation and module organization. Key points include: 1) Test files are located in a separate 'tests' directory from the main module code in the 'pdd' directory, requiring absolute import references. The naming convention dictates that module file names match their corresponding function names. 2) Any files created during execution should be placed in the 'output' directory. 3) The project includes pre-existing data files (language_format.csv and llm_model.csv) stored in the PDD_PATH/data directory. These files contain information about popular programming languages and LLM models, and developers are instructed not to overwrite them. They can be utilized for testing purposes. 4) The PDD_PATH environment variable is pre-configured and available for use. These guidelines appear to be instructions for developers working on a code generation or testing framework, emphasizing proper file organization, import handling, and preservation of existing data resources.",2025-12-14T08:41:56.645588+00:00
"context/thinking_tokens.md","This document provides a detailed comparison of the maximum internal reasoning tokens (chain-of-thought tokens used for intermediate steps) allowed by various large language models. The models analyzed include offerings from OpenAI (GPT-4.1, o3, o4-mini), Anthropic (Claude 3.5 Haiku, Claude 3.7 Sonnet), Google (Gemini 2.5 Flash and Pro), xAI (Grok 3 Beta), and DeepSeek (DeepSeek Coder, DeepSeek-R1 variants).

Key findings show significant variation across models. OpenAI's o3 and o4-mini support the highest reasoning budgets at approximately 100K tokens. Claude 3.7 Sonnet offers up to 64K reasoning tokens through its extended thinking mode. DeepSeek-R1 models (including distilled variants) cap at around 32K reasoning tokens. Gemini 2.5 models support tens of thousands of reasoning tokens, comparable to their output limits.

Notably, several models lack dedicated reasoning token capabilities: GPT-4.1, GPT-4.1-nano, Claude 3.5 Haiku, and DeepSeek Coder operate as standard single-pass models without hidden chain-of-thought processing. Grok 3 Beta supports extended reasoning but has no published fixed limit.

The document draws from official documentation, model cards, and benchmark reports to provide these estimates, distinguishing internal reasoning budgets from input context and output length limits.",2025-12-14T08:42:03.146311+00:00
"context/tiktoken_example.py","This code snippet demonstrates how to count tokens in a text string using the tiktoken library, which is OpenAI's tokenizer. The code performs three main operations: First, it imports the tiktoken library, which is a fast BPE (Byte Pair Encoding) tokenizer commonly used with OpenAI's language models. Second, it retrieves a specific encoding scheme called cl100k_base using the get_encoding() function. The cl100k_base encoding is the tokenizer used by GPT-4 and GPT-3.5-turbo models. The comment suggests that other encoding names could be used as alternatives depending on the model being targeted. Third, it calculates the token count by encoding a variable called preprocessed_prompt and measuring the length of the resulting token list. This is useful for managing API costs, staying within context window limits, or understanding how text will be processed by language models. Token counting is essential when working with LLMs since they have maximum token limits and pricing is often based on token usage. This simple three-line implementation provides an efficient way to determine exactly how many tokens a given text will consume when sent to an OpenAI model.",2025-12-14T08:42:13.030175+00:00
"context/trace_example.py","This Python script serves as a demonstration for the `trace` function from the `pdd.trace` module. It imports necessary dependencies including `os`, the `trace` function, `Console` from the `rich` library for formatted output, and a `DEFAULT_STRENGTH` constant from the `pdd` package.

The `main()` function sets up example inputs to showcase the tracing functionality. It defines a sample code snippet containing a simple `hello_world()` function that prints a greeting and returns 42. The script specifies line 3 as the target line to trace, along with a prompt file that describes the expected behavior of the code.

Configuration parameters include a default strength value and a temperature of 0.0 for LLM generation. The script calls the `trace()` function with these inputs in verbose mode, which returns three values: the corresponding prompt line, total cost, and model name used.

Results are displayed using Rich console formatting with colored output. The script includes comprehensive error handling for `FileNotFoundError`, `ValueError`, and general exceptions, each displaying appropriate error messages in red. The script follows standard Python conventions with a `if __name__ == __main__` guard to ensure the main function only runs when executed directly.",2025-12-14T08:42:20.323786+00:00
"context/trace_main_example.py","This Python script demonstrates the usage of a tracing utility from the 'pdd' package, specifically the `trace_main` function. The script creates two example files: a prompt file (calculator.prompt) containing instructions for building a simple calculator that adds two numbers, and a Python code file (calculator.py) implementing an `add_numbers` function that takes two floats and returns their sum.

The script uses the Click library to set up a command context with configuration options including: 'quiet' mode (disabled), 'force' mode (enabled to overwrite files), 'strength' parameter (0.7 for LLM analysis intensity), and 'temperature' (0.0 for deterministic LLM output).

The main functionality calls `trace_main()` with the prompt file, code file, a specific line number (2) to trace, and an output file path. This appears to be a tool for tracing relationships between prompts and generated code, likely for debugging or analysis purposes in prompt-driven development workflows.

The function returns three values: the corresponding prompt line number, the total cost of the LLM analysis in USD, and the model name used. Error handling is included to catch and display any exceptions that occur during execution.",2025-12-14T08:42:28.030536+00:00
"context/track_cost_example.py","This Python file implements a command-line interface (CLI) for a tool called PDD using the Click library. The CLI is designed for processing prompts and generating outputs with built-in cost tracking functionality.

The code defines two main components:

1. **Main CLI Group (`cli`)**: The entry point that accepts an optional `--output-cost` parameter to enable cost tracking and export usage details to a CSV file. It uses Click's context object to pass configuration between commands.

2. **Generate Command (`generate`)**: A subcommand decorated with `@track_cost` that processes prompt files. It accepts two parameters:
- `--prompt-file` (required): Path to an input file containing the prompt
- `--output` (optional): Path for the output file; if omitted, results print to console

The generate function reads the prompt file, simulates processing (currently a placeholder), and returns a tuple containing the generated output, execution cost ($0.05 per million tokens), and the model name (gpt-4). The code uses Rich library for enhanced console printing.

The file includes a main block demonstrating example usage with sample arguments. This appears to be a framework or template for building AI/LLM-powered text generation tools with cost monitoring capabilities.",2025-12-14T08:42:35.290899+00:00
"context/unfinished_prompt.txt","This Python file implements a command-line interface (CLI) for PDD (Prompt-Driven Development) using the Click and Rich libraries. The CLI provides tools for AI-assisted code generation and development workflows.

The implementation includes several key components:

1. **Library Imports**: The code imports Click for CLI functionality, Rich for enhanced console output (tables, panels, markdown), and various custom modules for code generation, testing, error fixing, and prompt processing.

2. **Cost Tracking**: Global variables and a callback function enable optional cost tracking, which logs AI model usage to a CSV file with timestamps, model information, commands executed, costs, and input/output files.

3. **Main CLI Group**: The `@click.group(chain=True)` decorator creates a chainable command group with several global options:
- `--force`: Overwrite files without confirmation
- `--strength`: AI model strength (0.0-1.0)
- `--temperature`: AI model temperature setting
- `--verbose`/`--quiet`: Output verbosity controls
- `--output-cost`: Enable cost tracking with CSV output

4. **Generate Command**: The primary command shown creates runnable code from a prompt file. It uses helper functions to construct file paths, generate code via an AI model, and save the output.

The file appears to be part of a larger PDD toolkit for automating code generation through AI-powered prompt processing.",2025-12-14T08:42:42.700442+00:00
"context/unfinished_prompt_example.py","This Python script demonstrates how to use the `unfinished_prompt` function from the `pdd.unfinished_prompt` module. The function analyzes whether a given prompt text is complete or requires continuation, leveraging a Large Language Model (LLM) for assessment.

The script begins with detailed prerequisites: the `pdd` package must be accessible in the Python environment, a prompt template file named unfinished_prompt_LLM must exist, and the LLM invocation must be configured with appropriate API keys (e.g., `OPENAI_API_KEY`).

The example uses an intentionally incomplete prompt about baking sourdough bread to showcase the function's detection capabilities. The `unfinished_prompt` function accepts four parameters: `prompt_text` (required), `strength` (optional, 0.0-1.0), `temperature` (optional, 0.0-1.0), and `verbose` (optional boolean for logging).

The function returns a tuple containing: the LLM's reasoning explanation, a boolean indicating completeness, the estimated API call cost, and the model name used. Results are displayed using the `rich` library for formatted console output.

A commented-out section shows how to call the function with default parameters using a simpler, complete prompt example. This script serves as a practical reference for integrating prompt completeness analysis into applications.",2025-12-14T08:42:51.485759+00:00
"context/unrunnable_raw_llm_output.py","This file contains the implementation of a `context_generator` function in Python, designed to automate the generation of usage examples for Python modules. The function takes three parameters: a Python filename to read, an output filename for writing results, and an optional `force` boolean flag. The implementation demonstrates a workflow that includes: (1) reading a Python source file with error handling for missing files, (2) preprocessing the file contents, and (3) generating a prompt for GPT-4 that requests the model to create concise usage examples based on the provided code. The function returns a boolean indicating success or failure. The code includes try-except blocks for file operations and returns False if the input file doesn't exist. The file also provides documentation explaining each step of the process and includes a usage example showing how to call the function with sample filenames. Note that the implementation appears incomplete, as it references a `processed_content` variable and `preprocess` function that aren't fully defined in the visible code, and the model invocation and output writing steps mentioned in the description are not shown in the provided snippet.",2025-12-14T08:42:59.685127+00:00
"context/update/1/final_change_python.prompt","The provided file details the specifications for creating a Python function named 'change', intended to modify an input prompt based on a change prompt and associated code. The function requires inputs such as 'input_prompt', 'input_code', 'change_prompt', 'strength', and 'temperature', and returns a 'modified_prompt', 'total_cost', and 'model_name'. The implementation must utilize the Langchain LCEL framework and the Python Rich library for formatted console output. Key steps include loading and preprocessing specific prompt files, selecting an LLM using a custom 'llm_selector', and performing two model invocations: one to generate the change logic and another to extract the result as JSON. The function is responsible for calculating and displaying token counts and costs for both steps. Additionally, it must handle edge cases and errors effectively while using relative imports for dependencies.",2025-12-20T07:25:58.689693+00:00
"context/update/1/initial_change.py","The provided code defines a Python module containing a core function named `change`, which automates the process of modifying prompts using Large Language Models (LLMs). Leveraging the `langchain` framework for chain execution and `rich` for enhanced console logging, the function accepts inputs including an initial prompt, associated code, specific change instructions, and model parameters like strength and temperature. The execution flow involves several distinct steps. First, it retrieves necessary prompt templates (`change_LLM.prompt` and `extract_prompt_change_LLM.prompt`) from a directory specified by the `PDD_PATH` environment variable. It then employs a local `llm_selector` utility to instantiate the appropriate LLM and token counter. The logic proceeds in two phases: a generation phase where the LLM proposes changes based on the inputs, and an extraction phase where a secondary chain parses the LLM's output to isolate the final `modified_prompt` using a JSON parser. Crucially, the function tracks financial metrics, calculating the estimated cost for input and output tokens at each stage. It provides real-time feedback via the console and includes robust error handling for file access issues or parsing failures. The function concludes by returning the modified prompt, the total cost incurred, and the identifier of the model utilized.",2025-12-20T07:26:14.569716+00:00
"context/update/1/initial_change_python.prompt","The provided file outlines the requirements for creating a Python function named `change`, designed to modify an `input_prompt` based on a specific `change_prompt` and existing `input_code`. The function requires the use of the Python Rich library for pretty-printed console output and must utilize relative imports. Inputs include the prompts, code, and LLM parameters (`strength`, `temperature`), while outputs consist of the `modified_prompt`, `total_cost`, and `model_name`. The implementation steps involve loading and preprocessing specific prompt files (`change_LLM.prompt` and `extract_prompt_change_LLM.prompt`), creating Langchain LCEL templates, and utilizing an `llm_selector` for model management and token counting. The workflow executes in two main phases: first, invoking the model to process the change request, and second, using a separate template to extract the `modified_prompt` from the model's output in JSON format. Throughout the process, the function must calculate and display token counts and costs. It is also required to handle edge cases and errors gracefully.",2025-12-20T07:26:37.068951+00:00
"context/update/1/modified_change.py","This Python script defines a function named `change` designed to modify input prompts and code using Large Language Models (LLMs). Leveraging the LangChain framework and the Rich library for console output, the function orchestrates a workflow that involves loading external prompt templates and selecting an appropriate LLM based on provided strength and temperature parameters. The process executes in two main stages: first, a 'change chain' processes the original prompt, code, and a description of the desired change to generate a raw response; second, an 'extract chain' parses this response to isolate the specific modified prompt in JSON format. Throughout the execution, the function calculates and reports token usage and estimated costs for both input and output operations. It includes error handling for file access and parsing issues, ultimately returning a tuple containing the modified prompt string, the total calculated cost, and the name of the model used.",2025-12-20T07:26:54.134878+00:00
"context/update/2/initial_continue_generation.py","This Python script defines a function, `continue_generation`, designed to manage the iterative extension and finalization of code blocks using Large Language Models (LLMs) and the LangChain framework. It utilizes Pydantic models to structure output parsing and the `rich` library for console formatting.

The function operates by first loading and preprocessing specific prompt templates from a directory defined by the `PDD_PATH` environment variable. It selects appropriate LLMs based on input parameters for strength and temperature. The process begins by extracting an initial code block from a provided string. It then enters a loop that repeatedly invokes the LLM to continue generation, calculating token costs at each step.

To determine when to stop, the script analyzes the last 200 characters of the output using an `unfinished_prompt` helper. If the generation is deemed incomplete, the loop continues; if complete, the result is trimmed, and the loop terminates. Finally, the function prints the resulting code block and cost statistics to the console before returning the full code, total cost, and model name.",2025-12-20T07:27:12.489960+00:00
"context/update/2/initial_continue_generation_python.prompt","This document outlines the specifications for a Python function called `continue_generation`, designed to finalize incomplete Large Language Model (LLM) outputs. Operating within a package structure using relative imports, the function accepts a formatted input prompt, partial LLM output, and model parameters (strength and temperature). Its primary goal is to produce a complete `final_llm_output` while calculating the `total_cost` and identifying the `model_name`.

The process involves loading and preprocessing specific prompt templates (`continue_generation_LLM`, `trim_results_start_LLM`, and `trim_results_LLM`) from a directory defined by the `$PDD_PATH` environment variable. The function utilizes Langchain LCEL templates and an internal `llm_selector` to manage model interaction and token counting.

Key logic includes extracting an initial code block, generating continuations, and iteratively checking for completeness using an `unfinished_prompt` utility. If the output is incomplete, the function loops to append content; once complete, it trims the results and merges them. Finally, the function utilizes the Rich library for pretty-printing the output and costs before returning the final string, accumulated cost, and model details.",2025-12-20T07:27:29.526268+00:00
"context/update/2/modified_continue_generation.py","This Python script implements a logic flow for continuing and refining LLM-generated code using the LangChain framework. It defines Pydantic models to structure specific outputs related to code trimming. The primary function, continue_generation, orchestrates an iterative generation process. It begins by loading and preprocessing prompt templates from a directory specified by the PDD_PATH environment variable. The function selects appropriate LLMs for generation and trimming tasks based on provided strength and temperature parameters. The core logic involves extracting an initial code block from previous output and entering a loop to generate subsequent code segments. Inside this loop, the script invokes an LLM chain to continue generation, calculates associated token costs, and utilizes an external utility to determine if the code is complete based on the last 200 characters. If incomplete, the loop persists; if complete, the script performs a final trim on the output, aggregates the total cost, and prints the result using the rich library. Finally, it returns the fully generated code block, the total financial cost of the API calls, and the name of the model used.",2025-12-20T07:27:45.040238+00:00
"context/update/2/modified_initial_continue_generation.prompt","The provided text outlines specifications for a Python function named `continue_generation`, intended to be part of a package using relative imports. The function's primary goal is to finalize LLM-generated content by detecting incomplete outputs and generating necessary continuations. It accepts a formatted input prompt, current LLM output, and model parameters (strength, temperature) as inputs. The implementation steps require loading and preprocessing three specific prompt files (`continue_generation_LLM`, `trim_results_start_LLM`, `trim_results_LLM`) from a path defined by the `$PDD_PATH` environment variable. The function must configure Langchain LCEL chains, utilizing `llm_selector` for model selection and cost tracking. The workflow involves extracting an initial code block, running a generation chain, and utilizing a loop that checks for completeness via an `unfinished_prompt` helper. If the output is incomplete, the function iterates; if complete, it trims the result using a specific LCEL chain. Throughout execution, the function must calculate total costs based on token usage and pretty-print outputs using the `rich` library. The final return values include the completed string, total cost, and model name.",2025-12-20T07:28:03.809630+00:00
"context/update_main_example.py","This Python script demonstrates a command-line interface (CLI) example using the Click library to update prompts based on modified code. The script imports an `update_main` function from a `pdd` module and wraps it in a CLI command called update.

The CLI accepts several options including: required paths for an input prompt file and modified code file, an optional original code file path, an output path for the updated prompt, a Git flag to retrieve original code from version history instead of a file, strength and temperature parameters for controlling model behavior, and flags for quiet mode, force overwrite, and verbose logging.

The `update` command function stores configuration parameters in Click's context object and calls `update_main()` with the provided arguments. The function returns three values: the updated prompt text, total cost, and model name used. Unless quiet mode is enabled, results are displayed using Rich's formatted printing, showing a truncated prompt snippet, the total cost, and the model name.

The script uses Click's group decorator to organize commands and includes example usage in the docstring showing how to run the update command with either explicit file paths or Git history integration. This serves as a practical template for building CLI tools that interface with prompt-updating functionality.",2025-12-14T08:43:06.212424+00:00
"context/update_model_costs_example.py","This Python script, `example_update_model_costs.py`, serves as an end-to-end demonstration for using the `update_model_costs.py` helper from the PDD package. The script teaches users how to create or reference an `llm_model.csv` file, invoke the `update_model_data` function either directly from Python or via the command-line interface (CLI), and understand the expected inputs and outputs of the function.

The prerequisites include having the PDD package on the PYTHONPATH, configured environment variables for LiteLLM API keys (e.g., OPENAI_API_KEY), and installed dependencies like `pandas`, `rich`, and `litellm`.

The script outlines the expected CSV schema, which includes fields for provider, model, input/output costs (per million tokens), coding arena ELO ratings, API configuration details, token limits, and structured output flags.

The demonstration workflow involves four steps: (1) creating a minimal sample CSV file with OpenAI and Anthropic model entries where cost fields are intentionally left blank, (2) calling the `update_model_data` function to populate the missing cost and structured output information in-place, (3) displaying the updated CSV results, and (4) optionally invoking the same functionality through the CLI using subprocess. This example provides a practical guide for automating LLM model cost updates.",2025-12-14T08:43:14.763227+00:00
"context/update_prompt_example.py","This Python script serves as a demonstration file for the `update_prompt` function from the `pdd` package. The main purpose is to showcase how to use this function to modify prompts based on code changes.

The script imports necessary modules including `os`, `tabnanny`, and components from the `pdd` package. It defines a `main()` function that sets up example parameters: an initial prompt asking to add two numbers, original code for an addition function, and modified code that changes the function to multiplication.

Key parameters configured include:
- `strength`: Uses a default value from the pdd package
- `temperature`: Set to 0 for deterministic LLM output
- `verbose`: Enabled for detailed output

The script calls `update_prompt()` with these parameters, which returns three values: the modified prompt, the total cost of the API call, and the model name used. Error handling is implemented using a try-except block to catch and display any exceptions that occur during execution.

If successful, the script prints the modified prompt, the cost formatted to six decimal places, and the model name. If the function returns None, it indicates a failure. The script follows standard Python conventions with a `if __name__ == __main__` guard for direct execution.",2025-12-14T08:43:23.189040+00:00
"context/vertex_ai_litellm.py","This Python script demonstrates how to use the LiteLLM library to interact with Google's Vertex AI Gemini model. The code begins by importing necessary modules: `litellm` for API completion calls, `json` for handling JSON data, and `os` for environment variable access. It enables debug logging by setting the `LITELLM_LOG` environment variable to DEBUG.

The script retrieves Vertex AI credentials from a file path stored in the `VERTEX_CREDENTIALS` environment variable. It reads and parses the JSON credentials file, then converts the credentials back to a JSON string format required by the LiteLLM library.

The main functionality calls the `completion` function to send a request to the Vertex AI Gemini 2.5 Pro model (preview version 05-06). The request includes a simple conversation with a system message defining the bot's persona (You are a good bot) and a user greeting (Hello, how are you?). The function is configured with the project ID meta-plateau-401521 and location us-central1 for the Vertex AI endpoint.

Finally, the script prints the model's response. This serves as a basic example of integrating LiteLLM with Google Cloud's Vertex AI for generative AI applications.",2025-12-14T08:43:31.148937+00:00
"context/whitepaper.md","This white paper introduces Prompt-Driven Development (PDD), a new software engineering paradigm that positions high-level prompts as the central development artifact instead of traditional code. PDD addresses limitations of conventional and AI-assisted coding, including cognitive overload, misalignment with business objectives, scalability issues, and technical debt accumulation. The methodology works by having developers craft detailed prompts specifying functional requirements, constraints, and context, which advanced AI models then interpret to generate code, tests, and documentation. Key advantages include enhanced stakeholder collaboration, increased productivity through automation, improved code consistency, and better alignment with business goals. The paper addresses potential challenges such as debugging AI-generated code, making small code changes, and learning curves, offering built-in PDD commands like `pdd trace`, `pdd update`, and `pdd generate` to mitigate these issues. The document provides adoption strategies, including pilot projects, training programs, and workflow integration recommendations. It concludes that PDD represents a fundamental advancement in software engineering, transforming developer roles toward design thinking and strategic oversight while leveraging AI capabilities for code generation.",2025-12-14T08:43:38.801108+00:00
"context/xml/1/split_LLM.prompt","The provided file contains a prompt template designed for an expert LLM Prompt Engineer. Its primary goal is to guide the decomposition of a single 'input_prompt' into two distinct parts: a 'sub_prompt' and a 'modified_prompt', ensuring that the original functionality remains intact. The template includes a section for context, listing four examples that reference file paths for prompts and Python code (such as 'pdd.py' and 'postprocess.py') to illustrate the splitting process. The prompt defines specific inputs, including the original prompt text, the code generated from it, and example code demonstrating the interaction between the split components. The required output consists of the two newly generated prompt strings. Furthermore, the file provides a strict four-step instruction set: identifying the difficulties associated with splitting the specific prompt, explaining the strategy to overcome these challenges, and finally generating the 'sub_prompt' and 'modified_prompt'. This template is essentially a tool for refactoring complex code generation prompts into modular components.",2025-12-20T07:28:20.402962+00:00
"context/xml/1/split_xml_LLM.prompt","The provided file contains a structured prompt template designed to guide an AI model, acting as an expert Prompt Engineer, in the task of refactoring prompts. The specific goal is to split a given `input_prompt` into a `sub_prompt` and a `modified_prompt` without losing functionality, effectively mirroring the process of extracting code into helper functions.

The content is organized into four primary sections. The `<examples>` section lists four few-shot examples (referencing external files like `pdd.py` and `postprocess.py`) to demonstrate the desired input-output mapping. The `<context>` section defines the agent's persona, the input variables (original prompt, existing code, and example usage code), and the output definitions. The `<inputs>` section provides placeholders for the actual data to be processed. Finally, the `<instructions>` section mandates a chain-of-thought approach, requiring the model to analyze difficulties and plan the split before generating the resulting prompts.",2025-12-20T07:28:37.063266+00:00
"context/xml/2/xml_convertor_LLM.prompt","The provided file outlines a task for an expert Prompt Engineer whose goal is to enhance a given raw prompt by adding XML tags for better structure and readability. The instructions specify that the AI should not generate new content but rather identify existing components within the input prompt—such as instructions, context, examples, and formatting guidelines—and wrap them in appropriate XML tags. The process involves three steps: analyzing the input to identify its parts, selecting suitable tags, and inserting those tags to organize the text, particularly around placeholders or included files. The final output must be the original prompt text, now structured with XML elements to improve clarity, without using markdown code blocks for the result.",2025-12-20T07:28:46.754308+00:00
"context/xml/2/xml_convertor_xml_LLM.prompt","The provided file outlines a prompt designed for an AI acting as an expert Prompt Engineer. The primary objective is to restructure a given raw prompt by inserting XML tags to enhance clarity and organization, without altering or adding to the original content. The instructions guide the model to identify distinct components within the input—such as instructions, context, examples, and formatting rules—and wrap them in appropriate XML elements like <instructions>, <context>, and <examples>. The process is defined in three steps: analyzing the input components, selecting suitable tags, and applying them to the existing text. The file also includes specific constraints, noting that the output should not contain markdown code block backticks and acknowledging that placeholders may contain significant amounts of text requiring organization. Essentially, this is a meta-prompt tool used to automate the structural refinement of LLM prompts using XML syntax.",2025-12-20T07:28:59.327384+00:00
"context/xml_tagger_example.py","This Python script demonstrates the usage of an XML tagging utility from a custom module called `pdd.xml_tagger`. The code imports the `xml_tagger` function and the `rich` library's print function for enhanced console output formatting.

The script sets up three key parameters: a raw prompt containing the text Write a story about a magical forest, a strength value of 0.5 (which controls the LLM model selection on a scale where 0 represents the cheapest option and 1 represents the highest ELO rating), and a temperature of 0.7 (controlling randomness in the model's output on a 0.0 to 1.0 scale).

Within a try-except block, the script calls the `xml_tagger` function with these parameters, which returns three values: the XML-tagged version of the prompt, the total cost of the operation, and the name of the model used. Upon successful execution, the script uses rich formatting to display a success message in bold green, followed by the tagged prompt, the cost formatted to six decimal places, and the model name. If an error occurs during execution, it catches the exception and displays the error message in bold red text. This appears to be a demonstration or testing script for an LLM-based XML tagging service.",2025-12-14T08:43:45.450532+00:00
