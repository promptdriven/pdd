full_path,file_summary,date
"context/DSPy_example.py","This code snippet demonstrates the initial setup and implementation of a DSPy-based application focused on example selection. The code begins with importing necessary DSPy components and configuring OpenAI's GPT-3.5-turbo-instruct model as the language model backend, with a maximum token limit of 250. The core of the implementation is a 'chainofthought' class that inherits from dspy.Module. This class is designed to implement a chain-of-thought reasoning process, with a basic structure that includes initialization and a forward method to process questions. The class uses DSPy's ChainOfThought feature to map questions to answers. However, the code appears to be incomplete, as indicated by empty spaces and an unfinished comment about compilation and optimization. The code represents a basic framework for implementing chain-of-thought reasoning in a DSPy environment, though it needs additional implementation details to be fully functional.",2024-12-07T01:53:08.162193+00:00
"context/anthropic_counter_example.py","This Python code demonstrates how to count tokens in a text string using the Anthropic API. The code begins by importing the Anthropic library and initializing an Anthropic client. It then defines a sample text string and uses the client's count_tokens method to calculate the number of tokens in that text. The result is printed to the console. However, there's an important note in the comments indicating that this token counting method is not accurate for Anthropic models version 3.0 and above. The code is straightforward and serves as a basic example of token counting functionality, which is often useful when working with language models to stay within token limits or estimate API costs.",2024-12-07T01:53:08.238576+00:00
"context/auto_include_example.py","This Python script demonstrates the usage of an auto_include function, which appears to be part of a larger system for processing code and generating unit tests. The main components include:

1. A main function that serves as an example implementation
2. File operations to read a CSV file
3. A detailed input prompt that specifies requirements for a 'generate_test' function
4. Configuration parameters including strength (0.7) and temperature (0.5)

The input prompt describes an 8-step process for generating unit tests using Langchain and LLM models, including:
- Loading and preprocessing prompts
- Creating Langchain LCEL templates
- Model selection and execution
- Token counting and cost calculation
- Output processing and completion checking
- Pretty printing of results

The script calls the auto_include function with these parameters and prints the results, including:
- Dependencies
- CSV output
- Total cost (in dollars per million tokens)
- Model name

The code appears to be part of a larger system for automated code processing and test generation, with a focus on LLM integration and cost tracking.",2024-12-07T01:53:08.267261+00:00
"context/autotokenizer_example.py","This Python code defines a function called 'count_tokens' that calculates the number of tokens in a given text using a specified language model's tokenizer. The function uses the Hugging Face transformers library, specifically the AutoTokenizer class. By default, it uses the 'deepseek-ai/deepseek-coder-7b-instruct-v1.5' model, but this can be changed through the model_name parameter. The function works by first loading the appropriate tokenizer, then tokenizing the input text, and finally returning the length of the resulting input_ids. The code includes a simple example that demonstrates how to use the function by counting tokens in the text 'Write a quick sort algorithm in Python.' This utility is particularly useful for working with language models where understanding token count is important for managing context windows and input limitations.",2024-12-07T01:53:08.299472+00:00
"context/bug_to_unit_test_example.py","This Python script demonstrates the usage of a bug_to_unit_test function for automated test generation. The main function serves as the entry point and orchestrates the process of generating unit tests from bug reports. The script reads input from three different files: a prompt file (from 'prompts' directory), a code file (from 'pdd' directory), and a context file (from 'context' directory). It processes two types of outputs (current and desired) that contain debug information and JSON-formatted explanations. The function takes several parameters including the outputs, prompt, code under test, program to run the code, strength (0.9), temperature (0), and language ('Python'). The script uses the Rich library for enhanced console output formatting, displaying the generated unit test, total cost, and model name used. Error handling is implemented to catch and display any exceptions that occur during execution. The code appears to be part of a larger system for automated testing and debugging, possibly using AI/LLM models given the presence of temperature and strength parameters.",2024-12-07T01:53:08.330225+00:00
"context/bug_to_unit_test_failure_example.py","This code snippet contains a Python function named 'add' that takes two parameters 'x' and 'y'. However, there appears to be a logical error in the implementation. Despite the function name suggesting addition, the function actually performs subtraction, returning 'x-y' instead of 'x+y'. This is likely a bug that needs to be fixed to match the intended functionality implied by the function name. The code is very simple, consisting of just two lines: the function definition and the return statement. The indentation suggests this might be part of a larger codebase, but only this single function is shown in the provided content.",2024-12-07T01:53:08.365017+00:00
"context/change_example.py","This Python script demonstrates the usage of a `change` function from the `pdd.change` module. The main function sets up a demonstration of code modification using language models. It includes example inputs for modifying a factorial function: an initial prompt requesting a factorial function implementation, the corresponding code, and a change prompt requesting to modify the function to include a square root calculation of the factorial output. The script uses parameters like strength (0.7) and temperature (0.5) to control the language model's behavior. The code utilizes the Rich library for enhanced console output formatting and includes error handling. The results display the modified prompt, the total cost of the operation, and the model name used. The script is structured with a main function and includes a commented-out environment variable setup for PDD_PATH. Overall, it serves as a practical example of using an AI-powered code modification system.",2024-12-07T01:53:08.422225+00:00
"context/change_main_example.py","This Python script demonstrates the usage of a 'change_main' function from a 'pdd' command-line program, showcasing two different modes of operation: single-change mode and CSV batch-change mode. The script begins by importing necessary modules and setting up a Click context with various parameters including strength, temperature, language, and budget settings. In the single-change mode example, it creates sample input files (a change prompt, input code, and input prompt) and processes them to modify a simple division function. The batch-change mode example demonstrates how to process multiple files using a CSV configuration. It creates sample code files for addition and subtraction functions, along with their corresponding prompt files, and processes them using a CSV file that specifies change instructions for each prompt. Both modes showcase the creation of necessary directories and files, execution of the change_main function with appropriate parameters, and display of results including modified prompts, costs, and model names used. The script is well-documented with comments explaining each step and includes error handling parameters and budget constraints.",2024-12-07T01:53:08.450288+00:00
"context/cli_example.py","This Python script demonstrates the usage of the 'generate' command from the PDD CLI module. The main purpose is to generate runnable code from a prompt file. The script defines a main function that sets up the necessary parameters and executes the code generation process. It uses two key file paths: a prompt file ('prompts/get_extension_python.prompt') and an output file ('get_extension.py'). The generation process is configured with specific parameters including a strength of 0.5 and a temperature of 0, with a force flag to bypass user confirmation. The script uses command-line arguments formatted as a list to control the generation process. Error handling is implemented through a try-except block that catches and prints any exceptions that occur during execution. The script is designed to be run directly and assumes that environment variables are set and required packages are installed. The output includes both the generated code saved to the specified file and console output showing the total cost and model name used for generation.",2024-12-07T01:53:08.480560+00:00
"context/click_example.py","This Python script implements a command-line image processing tool using Click and Pillow (PIL) libraries. It follows a Unix-like pipe pattern where multiple image processing commands can be chained together. The script includes various image manipulation functions such as opening, saving, displaying, resizing, cropping, transposing (rotation and flipping), blurring, smoothening, embossing, sharpening, and pasting images. Each command is implemented as a Click command group with specific options and parameters. The script uses decorator patterns (@processor and @generator) to handle the command chain processing. Key features include:

1. Command chaining functionality similar to Unix pipes
2. Support for multiple input images
3. Various image transformation options
4. Error handling for image operations
5. Flexible file naming for output
6. Preservation of image filenames through transformations

The script is well-documented with docstrings and comments, making it clear how to use each command and what it does. Example usage shows commands can be combined like 'imagepipe open -i example.jpg resize -w 128 display' to process images in sequence.",2024-12-07T01:53:08.509553+00:00
"context/code_generator_example.py","This Python script demonstrates the usage of a code generation system using language models. The main function imports a code_generator function from a 'pdd.code_generator' module and shows how to use it. The script reads a prompt from a file 'prompts/generate_test_python.prompt' and sets various parameters for code generation, including the target programming language (Python), model strength (0.5), temperature (0.5), and verbose output option (True). The code_generator function is called with these parameters and returns three values: the generated code, the total cost of the operation, and the name of the model used. The results are then printed to the console. The script includes error handling to catch and display any exceptions that might occur during execution. This appears to be a practical example of using AI language models for automated code generation, with configurable parameters to control the generation process.",2024-12-07T01:53:08.541379+00:00
"context/comment_line_example.py","This file provides documentation and examples for a Python function called `comment_line` that handles code commenting across different programming languages. The function takes two parameters: `code_line` (the line to be commented) and `comment_characters` (the commenting syntax to use). The function supports three commenting styles: single-character comments (like Python's #), paired comments (like HTML's <!-- -->), and line deletion (using 'del'). The documentation includes the function's definition with detailed parameter descriptions and return value information. The file also contains practical examples demonstrating all three use cases: commenting a Python print statement, commenting an HTML tag, and deleting a line of code. Each example is accompanied by its expected output. The function is designed to be flexible and can accommodate different programming language commenting syntaxes by appropriately formatting the input line based on the specified comment characters.",2024-12-07T01:53:08.574892+00:00
"context/conflicts_in_prompts_example.py","This Python script demonstrates the usage of a conflict detection system for prompts. The main function showcases how to use the 'conflicts_in_prompts' function to analyze potential conflicts between two different prompts. The script contains two example prompts: one for creating an authentication helper module (auth_helpers.py) and another for implementing a User class model. The code includes functionality to detect conflicts between these prompts using a language model with adjustable parameters (strength and temperature). The main function processes these prompts and outputs the results using Rich library's pretty printing, showing any detected conflicts, the model used, and the associated cost. The output includes suggested changes for each prompt if conflicts are found, or a message indicating no conflicts were detected. The script is structured with proper type hints and documentation, following Python best practices. It's designed to be run as a standalone script, with the main execution guard at the bottom.",2024-12-07T01:53:08.606647+00:00
"context/conflicts_main_example.py","This Python script demonstrates the usage of a conflict detection system for AI prompts. The code begins by importing the necessary modules (click and conflicts_main). It creates two example prompt files: 'prompt1.txt' and 'prompt2.txt', containing different AI assistant instructions. The script then implements a MockContext class to simulate Click's context object, with predefined parameters like force, quiet, strength, and temperature. The main functionality uses the conflicts_main function with the mock context and the two prompt files as inputs, specifying 'conflicts_output.csv' as the output file. The script concludes by printing the results, including the detected conflicts, total cost of the analysis, and the AI model used. The results are also saved to the CSV output file. This appears to be a testing or demonstration script for a larger system that analyzes potential conflicts between different AI prompts.",2024-12-07T01:53:08.637043+00:00
"context/construct_paths_example.py","This Python file demonstrates the usage of a construct_paths function from a 'pdd' module. The main function serves as an example implementation, showing how to set up and use the construct_paths function. The code defines two main input parameters: input_file_paths (a dictionary containing paths to code and prompt files) and command_options (a dictionary for additional command settings). The construct_paths function is called with these parameters along with additional boolean flags for force and quiet modes, and a command type. The function returns three values: input_strings (containing file contents), output_file_paths, and language. The code includes error handling through a try-except block and prints the results. The file appears to be part of a larger system that handles file processing and path construction, possibly for code generation or testing purposes. The code is well-documented with comments explaining the purpose of each dictionary and parameter.",2024-12-07T01:53:08.665271+00:00
"context/context_generator_example.py","This Python script demonstrates the usage of a context generator module for code generation. The script first checks for the required environment variable 'PDD_PATH'. It then sets up input parameters including a sample Python function (add), a prompt requesting to write an addition function, the programming language (Python), and model parameters (strength and temperature). The script calls the context_generator function with these parameters to generate example code. The function returns three values: the generated example code, the cost of generation, and the name of the model used. Finally, it prints the results using Rich library's formatted output, displaying the generated code, total cost (formatted to 6 decimal places), and the model name used. The code is structured to demonstrate a practical implementation of automated code generation with cost tracking and model identification.",2024-12-07T01:53:08.693613+00:00
"context/continue_generation_example.py","This Python script demonstrates the usage of a 'continue_generation' function, which appears to be part of a text generation system using a language model. The main function reads two input files: a preprocessed prompt from 'cli_python_preprocessed.prompt' and an existing LLM output fragment from 'llm_output_fragment.txt'. It then calls the continue_generation function with specific parameters including a strength of 0.92 and a temperature of 0. The function generates additional text based on these inputs and returns the final output along with the cost and model name used. The results are then printed, showing the total cost and model name, and the final generated output is saved to a file named 'final_llm_output.py'. The script includes error handling for file operations and general exceptions. This appears to be part of a larger system for managing and extending text generation using language models in a controlled and cost-aware manner.",2024-12-07T01:53:08.721977+00:00
"context/detect_change_example.py","This Python script is designed to analyze and detect changes in prompt files using an LLM (Large Language Model). The script imports necessary modules including a custom 'detect_change' function and Rich console for formatted output. It defines a list of prompt files to analyze from 'prompts' and 'context' directories, though there's a commented-out alternative that would dynamically gather all .prompt files. The script specifies a change description aimed at using 'python_preamble.prompt' to make prompts more compact. It includes configuration for LLM parameters: strength (1.0) and temperature (0.0). The main functionality is wrapped in a try-except block that calls detect_change() with the specified parameters and prints the results using Rich console formatting. For each detected change, it displays the prompt name and change instructions. Finally, it outputs the total cost of the operation and the model name used. If any errors occur during execution, they are caught and displayed with red formatting.",2024-12-07T01:53:08.749981+00:00
"context/find_section_example.py","This file provides documentation and examples for using the `find_section` function from the `find_section.py` module. The function is designed to identify and extract code blocks from text content. The documentation includes a complete example that demonstrates how to use the function with a sample input containing multiple code blocks in different programming languages (Python and JavaScript). The file details both the input parameters (`lines`, `start_index`, and `sub_section`) and the output format (a list of tuples containing language, start line, and end line information). The example shows how to process a string containing code blocks by first splitting it into lines and then using the find_section function to locate the code sections. It also includes a practical example of reading from a file ('unrunnable_raw_llm_output.py') and processing its contents. The documentation concludes with sample output showing how the function identifies code blocks and their locations within the text. The function appears to be particularly useful for parsing and processing text that contains embedded code blocks in various programming languages.",2024-12-07T01:53:08.778520+00:00
"context/fix_code_loop_example.py","This Python script demonstrates the usage of a code fixing utility called 'fix_code_loop'. The main function sets up a test environment by creating two files: a code file containing a function with a potential error (calculate_average) and a verification file that tests the function. The script then uses the fix_code_loop function to automatically detect and fix issues in the code. The fix_code_loop takes several parameters including the code file path, original prompt, verification program, model strength, temperature, maximum attempts, budget limit, and error log file location. After running the fix attempt, it prints the results including whether the fix was successful, number of attempts made, total cost incurred, model used, and the final corrected code. The script also includes functionality to read prompts from files and handle module-specific paths. While there's code to clean up the created files at the end, it's currently commented out. The script is designed to work with a larger system that likely involves machine learning models for code correction.",2024-12-07T01:53:08.807027+00:00
"context/fix_code_module_errors_example.py","This Python script demonstrates the usage of a function called 'fix_code_module_errors' from the 'pdd' module to fix errors in code. The main function contains an example of a program with a type error where it attempts to sum a string ('123') instead of a list of numbers. The script includes several key components: the original program with the error, the initial prompt that generated the code ('Write a function that calculates the sum of numbers'), the original code module, and the error message received when running the program. The fix_code_module_errors function is called with parameters including the problematic program, original prompt, code module, error message, model strength (0.7), and temperature (0). The function returns multiple values including whether updates are needed for the program and code module, the fixed versions of both, the total cost of API calls, and the model name used for fixing. The script concludes by printing all these results. This appears to be part of a larger system for automated code error detection and correction.",2024-12-07T01:53:08.835913+00:00
"context/fix_error_loop_example.py","This Python script demonstrates the implementation of an error-fixing system using the 'fix_error_loop' function. The main function sets up various parameters and executes the error-fixing process. Key parameters include paths to unit test files, code files, and prompt files, along with control parameters like strength, temperature, maximum attempts, and budget. The script reads a prompt from a file and uses it along with other parameters to attempt fixing errors in code. The function returns multiple values including success status, final versions of unit tests and code, number of attempts, total cost, and the model name used. Results are displayed using Rich library's Panel for better formatting. The script includes error handling through a try-except block to catch and display any exceptions that might occur during execution. The code is structured to run only when executed directly (not when imported as a module) through the __name__ == '__main__' check. The implementation suggests this is part of a larger system for automated code error detection and correction.",2024-12-07T01:53:08.864596+00:00
"context/fix_errors_from_unit_tests_example.py","This Python script demonstrates the usage of a function called 'fix_errors_from_unit_tests' which is designed to automatically fix errors in code and unit tests. The main function sets up example inputs including a simple unit test for an addition function, the corresponding code implementation, and various parameters. The unit test deliberately includes an incorrect test case (asserting 0 == 1), which triggers an error. The script uses several parameters including a prompt describing the desired functionality, an error message, a file for logging fixes, and LLM (Language Learning Model) parameters like strength and temperature. The function returns multiple values including updated versions of both the unit test and code, fixed versions of both, the total cost of the operation, and the name of the model used. The results are printed using Rich library's pretty printing functionality. The script is structured as a demonstration tool, showing how the error-fixing functionality can be used in practice.",2024-12-07T01:53:08.893156+00:00
"context/generate_output_paths_example.py","This Python script demonstrates the usage of the 'generate_output_paths' function from the 'pdd' module. The script shows three different scenarios for generating output paths: a basic example, an example using an environment variable, and an example for the 'fix' command with multiple outputs. In the first example, it calls the function with a 'generate' command, empty output locations, and basic parameters like basename, language, and file extension. The second example demonstrates how the function handles an environment variable 'PDD_GENERATE_OUTPUT_PATH' to determine the output location. The third example shows how the function processes multiple output paths for a 'fix' command, generating separate paths for test files, code files, and results. The script includes print statements to display the generated paths for each scenario. The code appears to be part of a larger system for managing file paths in a development environment.",2024-12-07T01:53:08.934832+00:00
"context/generate_test_example.py","This Python script demonstrates the usage of a test generation system, likely part of a package called 'pdd'. The script is designed to automatically generate unit tests for a given piece of code. The example shows how to generate a test for a factorial function implementation. The main components include importing necessary modules (os and a custom generate_test function), setting up input parameters (including a prompt describing the desired functionality, the actual code implementation of a factorial function, and parameters like strength and temperature for test generation), and executing the test generation process. The script uses error handling to catch potential exceptions during execution and employs the 'rich' library for formatted console output. The generated output includes the unit test code, the total cost of generation (suggesting an API-based service), and the name of the model used. The code also includes a commented-out section for setting an environment variable 'PDD_PATH', which might be necessary for the package's configuration.",2024-12-07T01:53:08.962789+00:00
"context/get_comment_example.py","This file provides documentation and usage examples for the `get_comment` function from the `get_comment.py` module. The function retrieves comment characters for different programming languages. The documentation includes a practical example showing how to import and use the function, with sample calls for languages like Python (#), Java (//), and JavaScript (// or /* */). The function takes a single case-insensitive string parameter 'language' and returns the corresponding comment character(s) as a string. If there's an error (missing environment variable PDD_PATH, unknown language, or other issues), it returns 'del'. The function relies on a CSV file (language_format.csv) that should be located in a directory specified by the PDD_PATH environment variable. This CSV file must contain at least two columns: 'language' and 'comment' that map programming languages to their respective comment characters.",2024-12-07T01:53:08.992494+00:00
"context/get_extension_example.py","This code snippet demonstrates the usage of a function called 'get_extension' imported from a module 'pdd.get_extension'. The code shows three example function calls that illustrate how get_extension() converts programming language names into their corresponding file extensions. The examples include converting 'Python' to '.py', 'Makefile' to 'Makefile' (which remains unchanged), and 'JavaScript' to '.js'. The code appears to be a simple demonstration or test of the get_extension function's functionality, showing how it handles different programming language inputs and returns their standard file extensions. While the actual implementation of the get_extension function is not shown in this snippet, the examples suggest it's designed to map programming language names to their conventional file extensions.",2024-12-07T01:53:09.020819+00:00
"context/get_language_example.py","This Python script demonstrates the usage of a 'get_language' function that determines the programming language associated with a file extension. The code consists of a main function that serves as an example implementation. Inside the main function, it defines a sample file extension '.py' and attempts to get the corresponding programming language using the imported get_language function. The code includes error handling through a try-except block to catch any potential exceptions during execution. If a language is successfully identified, it prints the file extension and its associated programming language. If no language is found, it displays a message indicating that no programming language was found for the given extension. The script uses type hints for better code readability and follows Python's standard if __name__ == '__main__' pattern for script execution. The code is well-structured and includes proper documentation through docstrings.",2024-12-07T01:53:09.050133+00:00
"context/git_update_example.py","This Python script demonstrates the usage of a git_update function for code modification using Language Learning Models (LLMs). The main function reads an input prompt from a file named '{name}_python.prompt' in the prompts directory, where 'name' is set to 'insert_includes'. It then attempts to update code in a file located at 'pdd/{name}.py' using the git_update function with specified parameters: strength (0.93) and temperature (0). The function returns a modified prompt, total cost, and model name if successful. The script handles errors through try-except blocks, catching both ValueError and general exceptions. The modified prompt is saved back to the original prompt file. The script is designed to be run as a standalone program with error handling for input validation and unexpected issues. It imports necessary modules including 'os' and a custom 'git_update' function from 'pdd.git_update'.",2024-12-07T21:38:03.179958+00:00
"context/increase_tests_example.py","This Python file demonstrates the usage of an 'increase_tests' function, which is designed to generate additional unit tests for existing code. The file contains an 'example_usage' function that showcases both basic and advanced implementations of the test generation process. The example uses a simple 'calculate_average' function as the target code, along with an existing basic unit test and a coverage report showing 60% code coverage. The function demonstrates two ways to call 'increase_tests': a basic usage with default parameters and an advanced usage with custom parameters including language specification, strength, temperature, and verbosity settings. The code includes proper error handling using try-except blocks to catch both ValueError for input validation errors and general exceptions. The example is structured to run when the script is executed directly. The code serves as a practical demonstration of how to use the test generation functionality while handling potential errors and displaying relevant output information such as the generated tests, associated costs, and the model used for generation.",2024-12-07T01:53:09.111951+00:00
"context/insert_includes_example.py","This Python file demonstrates the usage of an insert_includes module for processing dependencies. The main function, example_usage(), showcases how to set up and execute dependency processing with error handling. The code initializes a Rich console for formatted output and defines input parameters including a prompt for a Python function that handles CSV files, a directory path for example files, and a CSV filename for dependency information. The insert_includes function is called with specific parameters like strength and temperature to control the output. The code then displays the results including the original prompt, modified prompt with dependencies, CSV output, model name, and total cost. The processed CSV output is saved to a file. Error handling is implemented for common issues like missing files or processing errors. The code uses the Rich library for enhanced console output formatting and the pathlib library for file operations. This appears to be part of a larger system for managing and processing project dependencies with AI model integration.",2024-12-07T22:11:27.202537+00:00
"context/langchain_lcel_example.py","This Python code demonstrates various implementations and uses of Language Models (LLMs) using the LangChain framework. The file includes examples of working with different LLM providers including Google's Gemini, OpenAI, Azure, Fireworks, Anthropic's Claude, Groq, Together.ai, and Ollama. It showcases several key features:

1. Setting up a SQLite cache for LLM responses
2. Implementation of a custom CompletionStatusHandler for tracking completion status and token usage
3. Various prompt templates and chain configurations
4. Different output parsing methods including JSON and Pydantic models
5. Structured output handling with a custom Joke class
6. Examples of fallback configurations and model alternatives
7. Integration with multiple API endpoints and services
8. Usage of both chat and completion models
9. Implementation of prompt templates with variable substitution

The code serves as a comprehensive example of how to integrate and use different LLM providers within the LangChain ecosystem, including proper error handling, response parsing, and output formatting. It also demonstrates modern best practices like using invoke() instead of deprecated run() methods.",2024-12-07T01:53:09.175039+00:00
"context/llm_invoke_example.py","This Python script demonstrates the usage of an LLM (Language Learning Model) invocation system through a function called `llm_invoke`. The code includes a Pydantic model definition for structured joke output and a main function that tests the LLM with varying strength parameters. The script performs two types of tests: unstructured and structured outputs for generating jokes. In the unstructured test, it simply generates jokes about programmers, while the structured test uses a Pydantic model to format jokes about data scientists with specific setup and punchline fields. The script iterates through different strength values (0.0 to 1.0 in 0.02 increments) and tracks which models are used at different strength ranges. For each iteration, it prints the generated joke, associated costs, and the model used. The code also includes error handling and model tracking functionality, storing the strength ranges where each model is active. At the end, it provides a summary of which models were used across different strength ranges. The script is well-structured with clear documentation and demonstrates both basic and advanced usage of the LLM invocation system.",2024-12-07T01:53:09.215831+00:00
"context/llm_selector_example.py","This Python code demonstrates the usage of a language model selector function (llm_selector) through a main function. The code iteratively tests different strength values ranging from 0.5 to 1.1 in increments of 0.05, while maintaining a fixed temperature of 1.0. For each iteration, it calls the llm_selector function, which returns various parameters including the selected LLM model, token counter function, input and output costs per million tokens, and the model name. The code includes error handling for FileNotFoundError and ValueError exceptions. Additionally, it demonstrates the usage of the token counter function with a sample text. The main function is structured to showcase how to interact with different language models based on varying strength parameters, making it useful for testing and comparing different LLM configurations. The code is organized with type hints and proper error handling, following good programming practices.",2024-12-07T01:53:09.245457+00:00
"context/llm_token_counter_example.py","This Python script demonstrates different methods of counting tokens in text using various tokenization libraries. The code includes three main approaches: tiktoken, anthropic, and autotokenizer. The main function contains three try-except blocks, each showcasing a different tokenization method. For tiktoken, it uses the 'o200k_base' encoding. For anthropic, it utilizes the 'claude-3-sonnet-20240229' model. For autotokenizer, it employs the 'deepseek-ai/deepseek-coder-7b-instruct-v1.5' model. Each method processes a sample text string and prints the token count. The script imports necessary libraries including tiktoken, anthropic, and transformers.AutoTokenizer, and uses a custom token counter function 'llm_token_counter' from a module called 'pdd'. The code is structured with error handling to gracefully manage potential issues with each tokenization method. The script is designed to be run as a standalone program, with the main function being called only if the script is the primary program being executed.",2024-12-07T01:53:09.273521+00:00
"context/load_prompt_template_example.py","This Python script demonstrates a simple program that loads and displays a prompt template. The code imports two functions: 'load_prompt_template' from a custom module 'pdd.load_prompt_template' and 'print' from the 'rich' library, which provides rich text formatting capabilities. The main function defines a variable 'prompt_name' set to 'generate_test_LLM' (presumably the name of a prompt template file without its extension), loads the template using the imported function, and then displays it with blue-colored formatting if the template is successfully loaded. The script uses a standard Python idiom with the '__name__ == __main__' check to ensure the main function only runs when the script is executed directly. This appears to be part of a larger system dealing with prompt templates, possibly for working with Large Language Models (LLMs).",2024-12-07T01:53:09.302428+00:00
"context/postprocess_0_example.py","This file provides a comprehensive example of how to use the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The example demonstrates processing output from a language model (LLM) that contains multiple code sections in different programming languages. The main example shows how to extract and process Python code specifically, though the function can work with other languages. The code includes a sample LLM output containing Python and Java code blocks, along with regular text. The example demonstrates how to use the function to process this mixed content, focusing on extracting Python code while commenting out other content. The documentation section clearly outlines the input parameters (`llm_output` and `language`) and explains that the function returns a string with only the largest section of the specified language's code uncommented, while other content is commented out. The file concludes with a note about ensuring proper implementation of supporting functions (`get_comment`, `comment_line`, and `find_section`) for the example to work correctly.",2024-12-07T01:53:09.330350+00:00
"context/postprocess_example.py","This Python script demonstrates the usage of a postprocessing function designed to extract code from mixed text and code output typically generated by Large Language Models (LLMs). The main function contains an example LLM output that includes a factorial function implementation along with usage examples, all embedded within markdown-style code blocks. The script uses the 'postprocess' function from a 'pdd.postprocess' module to extract the pure code from this mixed output. The script also utilizes the 'rich' library for enhanced console output formatting. The postprocessing function takes several parameters including the target programming language (python), model strength (0.7), and temperature (0.2). The script outputs the extracted code, the total cost of the operation, and the model name used. The output is formatted with color coding using the rich console library's formatting capabilities. This appears to be a demonstration or utility script that might be part of a larger toolkit for handling LLM-generated code outputs.",2024-12-07T01:53:09.359769+00:00
"context/preprocess_example.py","This Python script demonstrates the usage of a preprocessing functionality, likely part of a larger system. The code performs several key operations:

1. It imports a preprocess function from 'pdd.preprocess' and sets up a Rich console for formatted output.

2. It processes multiple different prompts:
- An initial XML-style prompt containing various tags (include, shell, pdd)
- A prompt loaded from 'prompts/xml/change_LLM.prompt'
- A prompt from 'prompts/example_generator_LLM.prompt'
- A simple mock database prompt

3. For each prompt processing operation, it:
- Calls the preprocess function with specific parameters (recursive and double_curly_brackets flags)
- Prints the processed results using the Rich console
- In some cases, writes the processed output to test files

The script appears to be part of a prompt processing system, possibly for a language model interface, with functionality to handle different types of prompts and formatting options. It includes file I/O operations and uses the Rich library for enhanced console output formatting.",2024-12-07T01:53:09.387848+00:00
"context/process_csv_change_example.py","This Python code demonstrates the usage of a function called 'process_csv_change' from a module named 'pdd.process_csv_change'. The script is designed to process changes in code files based on CSV input. It initializes several parameters including the CSV file path, strength (0.8), temperature (0), code directory path, programming language (Python), file extension (.py), and a maximum budget of $10.00. The main function call is wrapped in a try-except block for error handling. If successful, it prints the total cost of changes, the AI model used, and displays modified prompts for each affected file. The code includes error handling for file not found scenarios and unexpected errors. The function appears to be part of a larger system that automates code modifications, possibly using AI models, while maintaining budget constraints and tracking costs.",2024-12-07T01:53:09.415833+00:00
"context/split_example.py","This Python script demonstrates the usage of a 'split' function from a 'pdd.split' module. The code is structured around a main function that showcases how to process and split programming-related prompts. The script begins by importing necessary modules, including 'rich.console' for enhanced console output formatting. It sets up environment variables by configuring the PDD_PATH variable to point to the parent directory. The main function includes example inputs: a prompt requesting a factorial function implementation, sample code for the factorial function, and example usage code. It also defines parameters like strength (0.7) and temperature (0.5) for controlling the split operation. The script calls the split function with these inputs and prints three outputs: a sub-prompt, a modified prompt, and the total cost of the operation. Error handling is implemented using a try-except block, and any errors are displayed with red formatting using the Rich console. The script is designed to be run as a standalone program, with the main function being called only if the script is the primary program being executed.",2024-12-07T01:53:09.443529+00:00
"context/summarize_directory_example.py","This Python script demonstrates the usage of the 'summarize_directory' module from the 'pdd' package. The main function showcases how to summarize Python files in a directory while handling existing summaries. It includes an example of existing CSV content and demonstrates calling the summarize_directory function with various parameters including directory path (with wildcard), strength (model capability), temperature (output determinism), verbosity, and existing CSV content. The script processes the files, generates summaries, and outputs the results including the generated CSV content, total cost in USD, and the model used. The results are both printed to console and saved to 'output.csv'. The code includes error handling and is structured as a standalone executable script with a main() function guard.",2024-12-07T01:53:09.471711+00:00
"context/tiktoken_example.py","This code snippet demonstrates the use of the tiktoken library for token counting in text processing. The code first imports the tiktoken library, then creates an encoding object using the 'cl100k_base' encoding scheme (which is commonly used with newer GPT models). Finally, it counts the number of tokens in a variable called 'preprocessed_prompt' by encoding the text and measuring the length of the resulting encoded sequence. The code is particularly useful for applications that need to track token usage when working with language models, as it provides a way to count tokens before sending text to an API. The 'cl100k_base' encoding is specifically mentioned, though the comment indicates other encoding options are available. This is a common pattern when working with OpenAI's GPT models, where understanding token count is important for both cost management and staying within model context limits.",2024-12-07T01:53:09.500027+00:00
"context/trace_example.py","This Python script demonstrates the usage of a `trace` function from the `pdd.trace` module. The main function sets up a demonstration environment with example inputs for code tracing. The script includes imports for required modules, including `os` and `rich.console` for enhanced console output. The main components include:

1. Example code that defines a simple 'hello_world' function
2. Configuration parameters such as code line number, strength (0.7), and temperature (0.2)
3. A prompt file containing instructions for the code implementation
4. Error handling for various potential exceptions (FileNotFoundError, ValueError, and general exceptions)

The script attempts to trace a specific line of code (line 3) using the provided parameters and outputs the results including the corresponding prompt line, total cost, and model name used. The output is formatted using the Rich console library for better readability with color-coded messages. The script is structured to run the main function when executed directly rather than being imported as a module.",2024-12-07T01:53:09.527136+00:00
"context/trace_main_example.py","This Python script implements a command-line interface (CLI) using the Click library for a code tracing tool. The script contains example content for both a prompt file (which requests implementation of a factorial function) and a code file (which contains the actual factorial implementation). The main CLI function 'cli' is decorated with various Click options including prompt-file path, code-file path, code line number, output file path, and several flags and parameters for controlling the trace analysis. The options include force overwrite, quiet mode, model strength, and temperature settings for LLM (Language Learning Model) generation. The script writes example files to disk and then processes them using a trace_main function imported from pdd.trace_main. When executed, it outputs the prompt line, total cost, and model name used for the analysis, unless quiet mode is enabled. The factorial function implementation included in the example handles negative numbers and uses recursion for the calculation.",2024-12-07T01:53:09.556510+00:00
"context/track_cost_example.py","This Python file implements a Command-Line Interface (CLI) tool called PDD using the Click library. The tool is designed for processing prompts and generating outputs with cost tracking functionality. The code consists of two main components: a base CLI group and a 'generate' command. The CLI accepts an optional '--output-cost' parameter to track costs and save usage details to a CSV file. The 'generate' command requires a prompt file input and has an optional output file parameter. It's decorated with a 'track_cost' decorator for cost monitoring. The command reads a prompt from the input file, processes it (currently using placeholder logic), and either writes the output to a specified file or prints it to the console. The function returns a tuple containing the generated output, the cost of execution (simulated at $0.05 per million tokens), and the model name (set to 'gpt-4'). The file includes proper type hints, documentation, and error handling through Click's built-in functionality. The main execution block demonstrates example usage of the CLI with specific command-line arguments.",2024-12-07T01:53:09.597187+00:00
"context/unfinished_prompt_example.py","This Python file demonstrates the usage of an 'unfinished_prompt' function from a 'pdd' module. The code consists of a main function that analyzes the completeness of a given text prompt using a Language Learning Model (LLM). The example uses a simple fairy tale beginning ('Once upon a time...') as the test prompt. The function takes several parameters including the prompt text, model strength (0.5), and temperature (0.0). The unfinished_prompt function returns four values: reasoning, whether the prompt is finished, the total cost of the analysis, and the model name used. The code includes error handling through a try-except block and is structured to run the main function only when the file is executed directly. This appears to be a demonstration or testing script for a larger prompt analysis system, likely used to determine if a given text prompt is complete or needs additional content.",2024-12-07T01:53:09.626009+00:00
"context/update_prompt_example.py","This Python script demonstrates the usage of an 'update_prompt' function through a main function implementation. The code imports necessary modules and defines a main function that showcases how to use the update_prompt function with specific input parameters. The example uses a simple prompt about adding numbers and includes sample code snippets for addition and multiplication functions. The script includes error handling and takes parameters such as strength (0.9) and temperature (0) for the language model. When executed, it attempts to update a prompt using these inputs and prints the modified prompt, total cost, and model name if successful. If the operation fails, it provides appropriate error messages. The code is structured with a standard Python main guard pattern and includes proper type hinting and documentation. The script appears to be part of a larger system that involves prompt engineering or code transformation using language models.",2024-12-07T01:53:09.653811+00:00
"context/xml_tagger_example.py","This Python script demonstrates the usage of an XML tagging functionality using a custom module 'pdd.xml_tagger'. The code imports the 'xml_tagger' function and the 'rich' library for enhanced console printing. The script takes a raw text prompt ('Write a story about a magical forest') and processes it with two parameters: 'strength' (set to 0.5, representing base model performance) and 'temperature' (set to 0.7, controlling output randomness). The code is wrapped in a try-except block for error handling. When successful, it outputs the XML-tagged version of the prompt, the total cost of processing, and the name of the model used. The output is formatted using rich's printing capabilities with color coding (green for success, red for errors) and bold text formatting. This appears to be a demonstration or example implementation of an AI-powered text processing tool that adds XML markup to input text.",2024-12-07T01:53:09.681130+00:00
