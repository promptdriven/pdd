full_path,file_summary,content_hash
context/cli_example.py,"This Python script serves as a comprehensive demonstration and documentation of the PDD (Prompt-Driven Development) CLI module. It illustrates how to programmatically interact with the main CLI entry point and its custom `PDDCLI` Click Group class. The script defines various example functions that showcase key features: displaying help and version information, listing available configuration contexts, and invoking the CLI with global options such as AI model strength, temperature, and verbosity settings. It also demonstrates specific flags like `--quiet` for suppressing output, `--output-cost` for tracking usage costs in a CSV file, `--context` for overriding default settings, and `--core-dump` for generating debug bundles. Through these examples, the file explains the CLI's architecture, including its custom error handling, help formatting, and the integration of global flags that control the behavior of underlying AI models and file operations.",04e5e7b7e841716190a924f327fe35c2e920202e6dd4176f6a0686141dd3a5ae
context/xml_tagger_example.py,"This Python script demonstrates the usage of the `xml_tagger` function from the `pdd.xml_tagger` module. It begins by importing the necessary function and the `rich` library for enhanced console output. The script defines a sample raw prompt, ""Write a story about a magical forest,"" along with configuration parameters for the language model, specifically `strength` (set to 0.5) and `temperature` (set to 0.7). 

The core logic is encapsulated within a `try-except` block to handle potential errors gracefully. Inside the `try` block, the `xml_tagger` function is called with the defined prompt and parameters. This function returns three values: the processed XML-tagged prompt, the total cost of the operation, and the name of the model used. Upon successful execution, the script uses `rich` to print a success message in bold green, followed by the tagged prompt, the calculated cost formatted to six decimal places, and the model name. If an exception occurs during the process, the script catches it and prints a bold red error message detailing the issue.",3b4606a0ac11cb728a5aa657d457312c83f2d710f53f4ac94d35675f7e911f60
context/insert_includes_example.py,"This Python script serves as a demonstration for using the `insert_includes` module, specifically designed to process project dependencies and enhance prompts with relevant context. The core functionality is encapsulated within the `example_usage` function, which sets up necessary parameters such as an initial input prompt requesting a CSV processing function, a directory path pattern for context files, and a target CSV filename for dependency information.

The script executes the `insert_includes` function with specific configuration settings, including a high strength value of 0.93 and a temperature of 0 to ensure focused and consistent results. It handles the returned values—the modified prompt, CSV output, cost calculation, and model name—and displays them using the `rich` library for formatted console output. Additionally, the script includes error handling to catch `FileNotFoundError` or other exceptions during execution and saves the resulting CSV data to a file. The code is structured to run as a standalone script via the standard `if __name__ == ""__main__"":` block.",eed5d9f7c51e9c8531a0cbd524f0e4df710136ca4cc1b0b7555258d5786116ea
context/unrunnable_raw_llm_output.py,"The provided text outlines the implementation of a Python function named `context_generator`. This function is designed to automate the creation of usage examples for Python modules. The process involves several key steps: first, the function reads the contents of a specified Python source file. It handles potential errors, such as the file not being found. Next, the code is preprocessed (though the specific preprocessing logic is abstracted in the snippet). A prompt is then constructed for an AI model, specifically GPT-4, instructing it to act as an expert Python engineer and generate a concise usage example based on the processed code. Finally, the function is intended to write this generated example to an output file. The text includes a code snippet demonstrating the file reading and prompt generation logic, along with an explanation of these steps and a usage example for calling the function.",435a9e40a86fa84e438a492350e839e0e1cbcd09c3b420b8dc83aa98bcd2db22
context/code_generator_main_example.py,"This Python script serves as a standalone example and test harness for the `code_generator_main` function from the `pdd` package. It demonstrates how to programmatically invoke the PDD CLI's code generation logic without making actual calls to Large Language Models (LLMs). The script achieves this by using `unittest.mock.patch` to replace the internal generator functions (`local_code_generator_func` and `incremental_code_generator_func`) with mock implementations. These mocks return predictable, hardcoded strings for Java and Python code, ensuring the example runs quickly and without requiring API keys.

The `main_example` function orchestrates three primary scenarios: full code generation (local execution), incremental code generation (updating existing code based on a new prompt), and a forced incremental generation scenario. It also attempts a cloud generation scenario that falls back to local execution due to mocked credentials. The script handles file system operations, such as creating input prompt files and cleaning up output directories, and includes a helper class `MockContext` to simulate `click` CLI contexts. Finally, it prints detailed results for each scenario, including the generated code, cost, and model usage.",4f81dbeae92ff120fdcdf23040cabc80152384bce754ed84b65cb0ec81a77dee
context/agentic_bug_example.py,"This file serves as an example script demonstrating how to utilize the `run_agentic_bug` function for investigating bugs based on GitHub issues. The script sets up the environment by adding the project root to the system path and imports the necessary module. To avoid making actual API calls or executing complex logic during the demonstration, it employs `unittest.mock.patch` to mock the `run_agentic_bug_orchestrator`. The mock is configured to simulate a successful eight-step workflow, returning a predefined success status, completion message, cost, model provider, and a list of changed files. The `main` function executes the `run_agentic_bug` function with a dummy GitHub issue URL and verbose settings, then prints a formatted summary of the results, including success status, cost, and any files modified during the simulated process.",96e35df4e8425a9173ff6a30281892c2bd8b81361cff72b732e488172ddf52f5
context/simple_math.py,"The provided file content is extremely minimal, consisting of a single line of text that serves as a placeholder or marker. Specifically, the file contains a comment indicating that it has been ""Intentionally left blank for pdd generate."" This suggests that the file is likely a template, a stub, or a configuration file intended to be populated or processed by a tool or script named ""pdd"" at a later stage in a development or build workflow. There is no actual code, data, or narrative content within the file beyond this instructional comment. Consequently, the file currently serves no functional purpose other than to reserve a location or filename within the directory structure, awaiting automated generation or manual entry as part of a specific software generation process.",23798297717fdc449fb01b557103f0e3b7aa46b535e91953cbca5fcf2aae0ecb
context/sync_orchestration_example.py,"This Python script serves as a demonstration and usage example for the `pdd-cli` tool's synchronization orchestration module. It primarily illustrates how to programmatically invoke the `sync_orchestration` function to manage a Prompt-Driven Development (PDD) workflow. The script begins by defining a helper function, `setup_example_project`, which creates a mock directory structure (prompts, src, examples, tests) and initializes a dummy prompt file for a calculator project within an `./output` directory. 

The main execution block showcases two key scenarios. First, it runs a full synchronization process for a ""calculator"" project using the Python language. This step simulates the core functionality of the CLI command `pdd sync`, executing necessary generation and testing steps within a specified budget while suppressing detailed logs via a quiet flag. Second, it demonstrates a ""dry-run"" scenario, simulating the `pdd sync --dry-run` command. This allows users to view the current state of synchronization and analyze pending actions without executing any actual file modifications or API calls. Both examples output their results as formatted JSON to the console, providing clear visibility into the orchestration's success status, errors, and operational analysis.",cc1313a948946026d418c85a07e5bf0eedeb5915c44d630440e489979755e1f7
context/python_env_detector_example.py,"This file serves as an example script demonstrating the usage of the `pdd.python_env_detector` module. It imports four key functions from the module: `detect_host_python_executable`, `get_environment_info`, `is_in_virtual_environment`, and `get_environment_type`. The script defines a `main` function that sequentially calls these functions to showcase their capabilities. Specifically, it prints the path of the host Python executable, checks whether the script is currently running inside a virtual environment, identifies the specific type of environment (e.g., venv, conda), and finally iterates through and displays a dictionary of detailed environment information. The script concludes with a standard `if __name__ == ""__main__"":` block to execute the `main` function when run directly.",65619b3e8bad0509f1ccf54d81b5c3760f423b6fc1925c932fc7968da78a172e
context/update_model_costs_example.py,"This Python script, `example_update_model_costs.py`, serves as an end-to-end demonstration for using the `update_model_costs` helper utility within the `pdd` package. It illustrates how to automatically update pricing and capability data for Large Language Models (LLMs) stored in a CSV file. The script begins by creating a sample CSV file (`sample_llm_model.csv`) containing basic model information for OpenAI's GPT-4o-mini and Anthropic's Claude-3-Haiku, intentionally leaving cost and structured output fields blank. It then imports and executes the `update_model_data` function from the `pdd` library, which modifies the CSV file in place by fetching and filling in the missing data using LiteLLM. Finally, the script prints the contents of the CSV before and after the update to verify the changes. It also includes commented-out code demonstrating how to achieve the same result using the command-line interface (CLI) via `subprocess`.",90c9e0bab64d7495f55ba5876f8beab75ef40037508b5e9800ebc6afb5b55f5e
context/autotokenizer_example.py,"This Python script provides a utility for counting tokens in a text string using the Hugging Face `transformers` library. It defines a function named `count_tokens` which accepts a text input and an optional model name, defaulting to ""deepseek-ai/deepseek-coder-7b-instruct-v1.5"". Inside the function, an `AutoTokenizer` is instantiated for the specified model with remote code trust enabled. The text is then processed by the tokenizer to generate token IDs, and the function returns the total count of these IDs. The script concludes with an example usage block where it calculates and prints the token count for the phrase ""Write a quick sort algorithm in Python.""",5e37a08f8ffede0d105cf1d5096f895f0a1a87663010e7b29c942d160c24883d
context/bug_to_unit_test_failure_example.py,"The provided file content defines a single Python function named ""add"". This function takes two arguments, ""x"" and ""y"". However, despite the function name suggesting an addition operation, the implementation actually performs subtraction, returning the result of ""x - y"". This indicates a logical error or a deliberate misnaming within the code, as the behavior (subtraction) directly contradicts the identifier (add). The function is syntactically simple, consisting of a definition line and a single return statement.",278742918a1e8806c703370429db53a4c178825af16be11405374a161e1d0198
context/sync_order_example.py,"This Python script serves as an example demonstration for the `pdd.sync_order` module. It illustrates a workflow for managing dependencies between prompt files. The script first creates a temporary directory containing mock prompt files (`base_utils`, `logger`, `database`, and `api`) with defined `<include>` dependencies. It then builds a dependency graph from these files and performs a topological sort to determine the correct synchronization order. The script also demonstrates how to identify affected modules when a base dependency (like `base_utils`) is modified. Finally, it generates a shell script (`sync_all.sh`) that would execute the synchronization commands in the correct order. The process is visualized using the `rich` library for console output, and the script cleans up its temporary files upon completion.",d33f05562f47811283ef8ebb0a799d3bbde1580c31e5a23f8292562faad5301e
context/anthropic_thinking_test.py,"This Python script demonstrates how to use the LiteLLM library to interact with Anthropic's Claude models, specifically focusing on enabling and handling ""thinking"" or reasoning capabilities. The script begins by setting up the environment, locating the project root, and loading API keys from a `.env` file using `python-dotenv`. It defines configuration variables for the model to test (Claude 3.7 Sonnet) and specific parameters for the thinking process, such as a token budget. The core functionality involves preparing a message payload asking for an explanation of recursion and constructing a dictionary of arguments for `litellm.completion`, which includes the experimental `thinking` parameter. The script then executes the API call within a try-except block to handle potential errors, specifically catching `UnsupportedParamsError`. Upon a successful response, it prints the full output using the `rich` library for formatting and attempts to extract and display any reasoning content returned by the model, checking both hidden parameters and standard message fields. This code serves as a testbed for verifying the integration of advanced reasoning features in LLM interactions.",a442745e51983bb051292e5f260efdadc57d046504e50a7194af8e1872ce0c84
context/trace_example.py,"This Python script serves as a demonstration and entry point for utilizing the `trace` function from the `pdd.trace` module. The script imports necessary components, including `Console` from the `rich` library for enhanced terminal output and `DEFAULT_STRENGTH` from the `pdd` package. The core functionality is encapsulated within the `main` function, which sets up a test environment by defining example inputs: a string representing a Python code snippet (`code_file`), a specific line number to trace within that code, a string representing a prompt file (`prompt_file`), and configuration parameters for model strength and temperature.

The script then attempts to execute the `trace` function with these parameters, enabling verbose mode to capture detailed output. Upon successful execution, it prints the results to the console using rich formatting, displaying the corresponding line number in the prompt file, the total cost of the operation, and the name of the model used. The script includes robust error handling to catch and display specific exceptions such as `FileNotFoundError` and `ValueError`, as well as a general catch-all for unexpected errors, ensuring that issues are reported clearly to the user.",de4d5d1ca347ea7ef9acd11dc80e86188917f61355e68db9c397ca6fd5108f99
context/sync_animation_example.py,"This Python script serves as a demonstration and test harness for the `sync_animation` module, illustrating how to integrate a threaded terminal animation into a main application workflow. It sets up shared state variables—such as the current function name, accumulated cost, file paths, and UI box colors—using mutable lists to allow dynamic updates across threads. The script initializes a `threading.Event` to control the animation's lifecycle and defines static parameters like the project basename and budget. A mock workflow function simulates various stages of a ""PDD"" process (e.g., ""checking"", ""generate"", ""test"", ""fix""), updating the shared state and sleeping to mimic processing time. The animation runs in a separate daemon thread, visualizing these state changes in real-time, while the main thread executes the mock workflow. Finally, the script handles graceful shutdown by signaling the stop event and joining the animation thread.",5b6a8af940fe44241889a279c1a8d3c978fe0c6ec3afc648510d8e33ffa7b18c
context/pytest_isolation_example.py,"This file serves as a reference guide for implementing proper test isolation patterns in Python, specifically designed to prevent test pollution in generated tests. It demonstrates best practices using `pytest` fixtures and `unittest.mock` to ensure tests remain independent. Key patterns include using `monkeypatch` for safe environment variable manipulation and function mocking, employing the `tmp_path` fixture for isolated file system operations, and creating custom fixtures with `yield` to ensure robust cleanup of resources and `sys.modules`.

The file addresses advanced scenarios such as mocking module-level dependencies before import (critical for testing code with decorators) and safely capturing or restoring `sys.stdout` and `sys.stderr` streams to prevent output corruption. It also explicitly warns against common anti-patterns, such as failing to stop patchers or incorrectly patching bound names, and provides a decision tree for choosing between module-level and fixture-based mocking strategies. The provided code examples emphasize the importance of restoring global state immediately after tests or imports to avoid affecting subsequent test execution.",c5b726f330c9d105978d51affffd38849f9685cde739816beded2bd34a1aba39
context/get_comment_example.py,"This file provides documentation and usage examples for the `get_comment` function within the `pdd.get_comment` module. The primary purpose of this function is to retrieve the specific comment syntax characters associated with a given programming language. The documentation outlines that the function accepts a single string argument, `language`, which is case-insensitive. It returns the comment character string for valid languages (e.g., `#` for Python, `//` for Java) or returns `'del'` if the language is unknown, an error occurs, or the required environment variable is missing. The file includes a Python code snippet demonstrating how to import and call the function for various languages. Additionally, it notes a critical dependency: the `PDD_PATH` environment variable must be set to point to the directory containing a `data/language_format.csv` file, which serves as the data source mapping languages to their comment styles.",3293b4631db7c783d87745abf36a79996d5c182b74e8bee885a2e522550db851
context/job_manager_example.py,"This Python file implements a `JobManager` system designed for a PDD Server to handle asynchronous job execution. It defines a `Job` data model and a `JobStatus` enumeration to track the lifecycle of tasks (queued, running, completed, failed, cancelled). The core `JobManager` class facilitates submitting jobs, managing concurrency via semaphores, and tracking execution history. It includes features for cancelling active jobs, cleaning up old records, and shutting down gracefully. Additionally, a `JobCallbacks` class is provided to support event-driven integration, such as WebSocket streaming for job progress and output. The file concludes with an asynchronous `main` function demonstrating how to initialize the manager, submit mock commands (like ""sync"" and ""generate""), and monitor their status until completion.",e421698fdf16c5dfebf97eb3f2cb7210878efa1869804f871100489e35102543
context/conflicts_in_prompts_example.py,"This Python script serves as a demonstration and entry point for a utility designed to detect conflicts between two different LLM prompts. The script defines a `main` function that sets up two detailed example prompts: one for creating an authentication helper module (`auth_helpers.py`) and another for defining a user data model class (`User`). It then configures parameters such as `strength` (0.89), `temperature` (0), and `verbose` mode before invoking the core function `conflicts_in_prompts` from the `pdd` package. This function analyzes the two prompts to identify potential contradictions or inconsistencies. Finally, the script uses the `rich` library to print the results of the analysis to the console, including the model name used, the total cost of the operation, and any suggested changes to resolve detected conflicts. The code is structured to run as a standalone script via the `if __name__ == ""__main__"":` block.",d286f38ca6e43216d62197230f6c53ec665734db782d965e5448f8e9f8bbdfb2
context/agentic_fix_example.py,"This Python script serves as a demonstration and test harness for an ""agentic fix"" workflow, likely part of a larger package named `pdd`. The script illustrates how to automatically repair buggy code using Large Language Model (LLM) agents. It defines a scenario with a deliberately incorrect Python function (`add` implemented as subtraction), a corresponding failing unit test, and a prompt describing the issue. 

The script includes utility functions to check for available AI agents (supporting Claude, Gemini, and OpenAI via CLI or API keys) and to set up a temporary environment containing the necessary code, test, and error log files. The `main` function orchestrates the workflow: it verifies prerequisites, creates the temporary scenario, generates an initial error log by running `pytest`, and then invokes `run_agentic_fix`. Finally, it reports the success or failure of the agent's attempt, including the model used, estimated cost, and the content of the corrected code, before cleaning up the temporary directory.",01ddb1127fbb8bbbdc6774635abace80cef43ccacb8aa525a8546da1f0654f5d
context/fastapi_example.py,"This Python file provides a complete example of a FastAPI application designed to serve as a backend for a ""PDD"" web frontend. It implements a REST server using the app factory pattern, incorporating dependency injection for application state management and Pydantic models for structured request and response validation. The application includes three main routers: a status router for server health and version information, a files router for reading content and metadata from the local filesystem (with path traversal security), and a commands router for queuing execution jobs. It features a lifespan context manager for startup/shutdown events and configures CORS middleware to allow connections from common local development ports. The file concludes with a `main` function that demonstrates how to initialize the app with the current working directory as the project root and run it using the Uvicorn ASGI server.",4b3d64175b8f46b48a4387e9cf98ed4024fd74a57413bd2f8a55c5d9c3dd0108
context/agentic_e2e_fix_orchestrator_example.py,"This script serves as an example usage and simulation of the `agentic_e2e_fix_orchestrator` module within the `pdd` project. It demonstrates how to invoke the `run_agentic_e2e_fix_orchestrator` function without requiring actual LLM calls or a live GitHub issue. The script sets up a mock environment by patching internal dependencies, specifically `load_prompt_template` and `run_agentic_task`, using Python's `unittest.mock`. 

The simulation defines a scenario where end-to-end (e2e) tests fail due to a bug in a payment processing module. It mocks the responses for a 9-step workflow, simulating actions such as running unit tests, analyzing root causes, identifying failing development units, creating new unit tests, and applying fixes via `pdd fix`. The mock logic includes conditional responses to simulate multiple cycles, where tests fail initially and pass in a subsequent cycle. The `main` function initializes dummy issue data, executes the patched orchestrator, and prints the final results, including success status, total cost, and a list of changed files. This allows developers to verify the orchestrator's logic flow and integration without incurring API costs.",e5e9f717c1241b192b4594ad98233fd79f17af654f02e49cf80de7a8017e5161
context/increase_tests_example.py,"This file provides a practical demonstration of how to use the `increase_tests` function from the `pdd` library. It defines a main function, `example_usage`, which sets up a mock scenario involving a simple Python function for calculating averages, an initial unit test, and a coverage report indicating partial code coverage. The script then executes `increase_tests` twice to showcase different usage patterns. The first call demonstrates basic usage with default parameters, passing in the existing code, tests, coverage report, and the original prompt. The second call illustrates advanced usage by specifying optional parameters such as the programming language, test generation strength (using `DEFAULT_STRENGTH`), temperature setting for the model, and enabling verbose output. The script includes error handling to catch `ValueError` for input validation issues and general exceptions, printing the results—including the generated tests, cost, and model name—to the console. Finally, the script checks for the main execution context to run the example.",f53be429e9a9e6a523c770ed6d0d4498e2201b22d5a8e4ef28ac64e8ff172c91
context/test.prompt,"This document outlines critical guidelines and best practices for writing isolated, non-polluting tests using Pytest, specifically within a project structure involving a `pdd` module and a `tests` directory. It emphasizes that generated tests must reside in `tests`, reference absolute paths, and output to an `output` directory without overwriting existing data files in `pdd/data`. 

The core of the document details twelve strict rules to prevent test pollution. Key mandates include using `monkeypatch` for environment variables instead of `os.environ`, employing context managers or `monkeypatch` for mocking external dependencies, and ensuring all fixtures use `yield` for proper cleanup. It strictly warns against direct manipulation of `sys.modules` or `sys.stdout/stderr` without immediate restoration mechanisms. 

Furthermore, the guide advises using the `tmp_path` fixture for file operations and warns against dangerous patterns like starting patchers at the module level without stopping them. It distinguishes between top-level and deferred imports, explaining how to patch bound names correctly. Finally, it highlights the necessity of in-place restoration for mutable containers (like dictionaries or lists) to ensure that shared references across modules are correctly reset, thereby preventing flaky tests and state leakage.",3198b780ac51f45c121fc1171f85862a4437ab1450eed39829b5c5b78d4a4828
context/sync_main_example.py,"This Python script serves as a programmatic example and test harness for the `sync_main` function within the `pdd` (Project Driven Development) framework. It demonstrates how to invoke the core synchronization logic without relying on the actual command-line interface or external LLM services. 

The script defines a `setup_mock_project` function that creates a temporary directory structure containing mock prompt files for Python and JavaScript (e.g., `greeting_python.prompt`). The `main` function then constructs a mock `click.Context` object to simulate CLI parameters like temperature and verbosity. Crucially, it uses `unittest.mock.patch` to replace the internal `construct_paths` and `sync_orchestration` dependencies with local mock functions. These mocks simulate the file path resolution and the outcomes of code generation (success for Python, failure for JavaScript) without making real API calls. Finally, the script executes `sync_main` using these mocked components and prints the aggregated results, costs, and success status to the console using the `rich` library, effectively showcasing how the tool handles multi-language synchronization tasks.",3eb17c0e79c0402489782f7e758d7063dfbd820428c5e5b695415290ebf2a7f8
context/command_logging.prompt,"This document outlines the guidelines for operation logging within CLI commands for the PDD system. It mandates the use of the `@log_operation` decorator from `pdd.operation_log` for any command that modifies PDD state, ensuring unified tracking for both manual and sync-initiated actions. The guide specifies the correct order for decorator stacking, noting that `@log_operation` should be applied before `@track_cost` due to bottom-up execution. It details key parameters for the decorator: `clears_run_report` for operations invalidating test results, `updates_fingerprint` for code or example changes, and `updates_run_report` for operations producing test results. Additionally, it explains that the decorator attempts to infer module identity (basename and language) from the `prompt_file` argument, silently skipping logging if this identity cannot be determined while allowing command execution to proceed.",8f0a0935821fa5fe835fb105251a72eb4198faaf4bcd1cb5fbda5f578b13d8e6
context/preprocess_example.py,"This Python script demonstrates the usage of a `preprocess` function from the `pdd.preprocess` module. It begins by importing necessary libraries, including `rich.console` for formatted output. A multi-line string variable named `prompt` is defined, containing XML-like tags (e.g., `<shell>`, `<pdd>`, `<web>`) and placeholders like `{test}` and `{test2}`. The script sets configuration variables: `recursive` is set to `False`, `double_curly_brackets` is set to `True`, and an `exclude_keys` list is defined containing ""test2"". Debug information regarding the excluded keys is printed to the console. The core logic involves calling the `preprocess` function with the defined prompt and configuration settings. Finally, the script prints the resulting processed prompt to the console using `rich` for styling, showcasing how the preprocessor handles the input string based on the specified parameters.",19151ac1cae6b41c190a9f6dd48b9f9c06ea8e9f7a87bf751909711fd6391298
context/render_mermaid_example.py,"This Python script serves as a demonstration and usage guide for the `render_mermaid` module, specifically illustrating how to convert `architecture.json` files into interactive HTML Mermaid diagrams. It defines a sample architecture structure containing backend (FastAPI, SQLAlchemy) and frontend (React) components to simulate real-world data. The script includes four main example functions: `example_basic_usage` showing command-line syntax; `example_programmatic_usage` demonstrating how to generate Mermaid code and HTML programmatically using a sample dataset; `example_with_real_architecture_file` which attempts to locate and process existing architecture files from common paths; and `example_customization` which highlights features like automatic categorization and color-coding. The `main` function orchestrates the execution of these examples, providing a comprehensive overview of the tool's capabilities for visualizing software architecture.",193a57bc4191595f252881c016ba7168c27c27c01ff192128d98cf76ce5f842b
context/construct_paths_example.py,"This file, `demo_construct_paths.py`, serves as a concise end-to-end demonstration of the `pdd.construct_paths.construct_paths` function within the PDD (Prompt-Driven Development) library. It illustrates how the library processes input arguments typically provided via a Command Line Interface (CLI). The script begins by creating a temporary toy prompt file named `Makefile_makefile.prompt` containing a simple task description. It then sets up the necessary arguments—such as input file paths, force flags, quiet mode settings, and the specific command type (set to ""generate"")—to mimic a real CLI invocation. The core of the script calls `construct_paths` with these parameters and captures the returned values: the resolved configuration, the content of the input strings read from the file, the calculated output file paths where results would be written, and the detected programming language. Finally, the script prints these returned structures to the console for inspection and cleans up by deleting the temporary prompt file, ensuring the directory remains tidy.",9fb88745a2a0e2834f9da30c2128a122fef4d5a74212ceacc5f9c3d9ddb9a8ca
context/update_prompt_example.py,"This Python script serves as a demonstration and entry point for utilizing the `update_prompt` function from the `pdd` package. The script defines a `main` function that sets up specific test parameters, including an initial natural language prompt asking to add two numbers, the corresponding original Python code for addition, and a modified version of the code that performs multiplication instead. It also initializes configuration settings such as a default strength value and a temperature of zero for the underlying Large Language Model (LLM).

The core logic involves a try-except block where the `update_prompt` function is invoked with these parameters and a verbose flag set to True. The script captures the function's return values—specifically the modified prompt, the total cost of the operation, and the model name used. If the operation is successful, it prints these details to the console; otherwise, it reports a failure. The script concludes with a standard conditional block to execute the `main` function when the file is run directly, ensuring it acts as a standalone executable example for testing prompt updates based on code modifications.",56b9d8c7aa823c2996a3dc5741a47844e223bfd088366d74e48b6a099b5a11c4
context/unfinished_prompt.txt,"This file contains a Python implementation of a command-line interface (CLI) for a Prompt-Driven Development (PDD) tool. Built using the `click` and `rich` libraries, the script defines a main command group `cli` with global options for controlling file overwrites (`--force`), AI model parameters (`--strength`, `--temperature`), verbosity levels, and cost tracking. It includes a helper function `log_cost` to record usage metrics to a CSV file. The script also defines a `generate` command that takes a prompt file as input, utilizes imported modules like `construct_paths` and `code_generator` to produce runnable code, and saves the output to a specified location. The code imports numerous other utility functions (e.g., `context_generator`, `generate_test`, `fix_errors`) indicating it serves as the central entry point for a suite of AI-assisted coding tools.",620cc099421924160d5ee9f62467dfa37877e969ec536d1df0ba91cfc8f45682
context/agentic_crash_example.py,"This Python script serves as a test harness or demonstration for the `run_agentic_crash` function within the `pdd` package. It sets up a simulated environment to test how an AI agent handles a program crash. The script defines a `setup_test_files` function that creates a temporary directory containing a dummy specification (`factorial_spec.md`), a buggy implementation (`math_lib.py` raising `NotImplementedError`), a runner script (`run_factorial.py`), and a simulated crash log. The `main` function mocks several external dependencies—including the agentic task runner, command generation, prompt loading, and subprocess execution—to simulate a successful AI-driven fix without incurring real API costs. It uses `unittest.mock.patch` to inject these mocks and includes a side effect that updates the buggy code file to a working factorial implementation. Finally, it executes `run_agentic_crash` with the test files, prints the results (success status, cost, model used), and verifies that the code file was correctly updated.",4a79b152db522063ccf099eca0419b419a656f168839d197f85781ae4fe1b5f6
context/unfinished_prompt_example.py,"This Python script serves as a usage example for the `unfinished_prompt` function from the `pdd` package. It demonstrates how to programmatically analyze a text prompt to determine if it is semantically complete or unfinished using an LLM. The script outlines necessary pre-requisites, such as ensuring the `pdd` package is in the Python path and that required prompt templates and API keys are configured. 

The core functionality is illustrated by defining an intentionally incomplete string about baking sourdough bread. The script then calls `unfinished_prompt` with specific parameters, including `strength`, `temperature`, and a `verbose` flag to enable detailed logging. It captures the function's return values—a tuple containing the LLM's reasoning, a boolean completion flag, the cost of the operation, and the model name used. Finally, it uses the `rich` library to print a formatted report of these results to the console. A commented-out section at the end provides a secondary example of how to invoke the function using default parameters for a simpler use case.",5d4d74064e5b845391f035fb8631903ec2f39a54a90713f9c13dc24d2eb73c4f
context/llm_invoke_example.py,"This Python script serves as a demonstration suite for the `pdd.llm_invoke` library, showcasing various methods for interacting with Large Language Models (LLMs). It defines four distinct example functions, each illustrating a specific capability of the `llm_invoke` function. The first example, `example_simple_text`, demonstrates basic text generation using a prompt template and input variables, targeting a balanced model strength. The second, `example_structured_output`, shows how to enforce a specific output schema using Pydantic models, specifically generating a structured movie review. The third example, `example_batch_processing`, illustrates how to process a list of inputs efficiently using batch mode with a lower-cost model. Finally, `example_reasoning` highlights advanced capabilities by requesting explicit thinking time or reasoning steps from high-strength models for solving riddles. The script uses the `rich` library for formatted console output and includes a main execution block that runs all examples sequentially, handling potential exceptions.",88e1071cf088837c1adb6738f0a1f8e5134efd4a44370758dd19d0719475ac48
context/get_extension_example.py,"The provided file is a short Python script that demonstrates the usage of a function named `get_extension`. This function is imported from a module called `pdd.get_extension`. The script primarily consists of three example calls to this function, passing different programming language names or file types as string arguments: ""Python"", ""Makefile"", and ""JavaScript"". The results of these function calls are printed to the console. Comments included in the code indicate the expected return values for each call, suggesting that the function maps language names to their corresponding file extensions (e.g., ""Python"" to "".py"", ""JavaScript"" to "".js"") or standard filenames (e.g., ""Makefile"" remains ""Makefile"").",def9c34c1416259a0a031a996b96aeff5e73a7012adff6af1cc557e74ba97cc8
context/fix_errors_from_unit_tests_example.py,"This file contains a Python script designed to demonstrate the functionality of the `fix_errors_from_unit_tests` function from the `pdd` package. The script defines a `main` function that sets up a mock scenario involving a buggy unit test and a corresponding piece of Python code. Specifically, it provides a unit test for an `add` function with an intentional assertion error (asserting 0 equals 1) and an implementation of `add` that includes a deprecated NumPy function call (`np.asscalar`). 

The script defines input variables including the source code, the unit test string, a prompt description, and a simulated error log string containing both an `AssertionError` and a `DeprecationWarning`. It then calls `fix_errors_from_unit_tests` with these inputs, along with configuration parameters for the LLM such as strength and temperature. Finally, the script uses the `rich` library to pretty-print the results returned by the function, which include the updated unit test, updated code, analysis results, total cost, and the model name used for the fix. The script is executed via a standard `if __name__ == ""__main__"":` block.",df6c93b3ae585af13827cf3edb398e7c0da75648b34ff0477044b659df85a117
context/cloud_example.py,"This Python script serves as a usage example and demonstration for the `pdd.core.cloud` module within the PDD CLI project. It illustrates how to interact with the centralized cloud configuration system. The script defines several functions to showcase specific capabilities: `example_get_urls` demonstrates retrieving base URLs and specific endpoint URLs, highlighting how environment variables can override default settings for local testing. `example_cloud_enabled_check` shows how the system verifies if cloud features are active based on the presence of required API keys (Firebase and GitHub). `example_authentication_flow` simulates the authentication process, covering both token injection via environment variables (useful for CI/CD) and a mocked interactive device flow for user login. Finally, `example_endpoint_listing` iterates through and displays all available cloud endpoints defined in the configuration. The script uses `rich.console` for formatted output and `unittest.mock` to simulate various environment states without affecting the actual system configuration.",92418beb7a1e7f25682bf2abce5b19013b95e7eed95256c930b83c1dd6dbfb0a
context/create_gcp_credential.py,"This Python script demonstrates the process of retrieving and decoding a Google Cloud Platform (GCP) service account key stored as a secret within an Azure Key Vault. The code includes a placeholder class, `AzureKeyVaultService`, which simulates the retrieval of a secret named ""GCP-VERTEXAI-SERVICE-ACC"" by returning a dummy base64-encoded JSON string. The main logic of the script attempts to fetch this secret (though the actual call is currently commented out and the variable initialized to an empty string). If the secret is successfully retrieved, the script proceeds to decode the base64 string into a UTF-8 string and then parses it into a JSON object using the `json` library. It includes error handling for JSON decoding issues and general exceptions. The comments suggest that the parsed credentials are intended for use with the `google.oauth2` library to authenticate services like Vertex AI.",22babc43e3ee7f5bdef36d1e4830b4ad65e4e3658f874f6914f88f6b7e0df2e0
context/cli_python_preprocessed.prompt,"The provided file describes the functionality and implementation details of ""pdd"" (Prompt-Driven Development), a Python command-line interface (CLI) tool designed to streamline software development using AI models. PDD facilitates the generation of code, unit tests, and usage examples directly from structured prompt files. Key features include a robust set of commands: `generate` for creating code, `example` for usage demonstrations, `test` for unit test creation, `preprocess` for handling prompt files, and `fix` for iteratively resolving errors in code and tests. It also supports advanced workflows like splitting large prompts (`split`), modifying prompts based on requirements (`change`), and updating prompts to reflect code changes (`update`). The tool emphasizes flexibility through global options for AI model strength and temperature, cost tracking via CSV output, and multi-command chaining for complex automation. The CLI is built using the Python `click` library and utilizes `rich` for formatted console output, supporting environment variables for configuration and offering shell completion.",bfc5835c208c47e02891a93a87064948059cca62a4a9b295b2b09f667ed75d4f
context/postprocess_example.py,"This Python script serves as a demonstration and usage example for the `postprocess` function within the `pdd.postprocess` module. It illustrates two distinct methods for extracting code blocks from raw Large Language Model (LLM) text outputs. The first scenario demonstrates a simple, cost-free extraction method (strength = 0) that relies on basic string manipulation to identify content within triple backticks. The second scenario showcases a more advanced, LLM-based extraction method (strength > 0) designed for robustness, which incurs a computational cost. 

The script utilizes the `rich` library for formatted console output and `unittest.mock` to simulate external dependencies like `load_prompt_template` and `llm_invoke`. This mocking ensures the example runs without requiring actual LLM API keys or specific prompt files. The `main` function sets up sample LLM output containing mixed text and code, then executes both extraction scenarios, printing the resulting code, associated costs, and model names to the console. It concludes with verification steps to ensure the mocked functions were called with the correct parameters.",b98362e6ffb84ae7826dad27d0854e30706f3993266314439859986b32ecd0bd
context/anthropic_tool_example.py,"The provided file content is a Python script demonstrating how to interact with the Anthropic API using the `anthropic` Python client library. Specifically, it initializes an `Anthropic` client instance and sends a request to the `claude-3-7-sonnet-20250219` model. The script configures the API call with a maximum token limit of 1024 and enables a specific tool capability defined as `text_editor_20250124` with the name `str_replace_editor`. The core interaction involves sending a user message asking for assistance with a syntax error in a file named `patent.md`. Finally, the script captures the model's response in a variable named `response` and prints it to the console. This snippet serves as a basic example of setting up an agentic workflow where the AI model is equipped with text editing tools to modify files based on user prompts.",6afb1637b9c794cad39f12d72969c50029b9e5cac80175cb3cc65666ce89e9e1
context/range_validator_example.py,"This Python script serves as a usage example and demonstration for a custom module named `range_validator`. It begins by dynamically adjusting the system path to locate and import the module from a relative directory structure. The core of the script is the `main` function, which systematically showcases four key utility functions provided by the library. First, it demonstrates `is_in_range` for checking if values fall within inclusive or exclusive bounds, including error handling for invalid ranges. Second, it illustrates `normalize_to_range` by mapping values between different scales, such as converting sensor data to percentages or Celsius to Fahrenheit. Third, it presents `wrap_to_range`, using angular data to show how values can be wrapped within a modular limit (e.g., 0-360 degrees). Finally, it exhibits `quantize`, showing how to snap floating-point numbers to a specific grid step using rounding, flooring, and ceiling modes. The script is designed to be executed directly to verify the module's functionality.",ff9ea438b40abece583b8182d77340ccb0e45318a50374b187a4e2dbdaaf929c
context/postprocessed_runnable_llm_output.py,"The provided file contains a Python implementation and explanation for a function named `context_generator`. This function is designed to automate the creation of usage examples for Python modules. It takes a Python filename and an output filename as arguments. The implementation details include reading the source Python file (with error handling for missing files), preprocessing the content (though the specific preprocessing logic is abstracted), and constructing a prompt for an AI model (specifically GPT-4) to generate a concise usage example based on the code. The file also includes markdown-style documentation explaining the code's steps, such as file reading and prompt generation, and provides a usage example for calling the function. The text appears to be part of a larger tutorial or documentation snippet, as indicated by the repetition of the introductory paragraph at the end.",ca0e49021f9ba3ff162c1a8a98ef4639e49b1d4a0c22d143e2dc808e385325f2
context/git_update_example.py,"This Python script serves as a demonstration and entry point for using the `git_update` function from the `pdd` package. The script defines a `main` function that orchestrates the process of updating a prompt based on code modifications. It sets up necessary parameters such as the prompt name (`fix_error_loop`), the path to the prompt file, and the target code file (`pdd/fix_error_loop.py`). It also configures LLM parameters like `strength` (using a default constant) and `temperature`.

The core logic involves reading the initial prompt content from a file and then invoking `git_update`. The call is configured with `simple=False` and includes the `prompt_file` path, signaling an intent to use an agentic update path if available, rather than the legacy method. The script handles the result of this operation, printing the modified prompt, the total cost of the LLM usage, and the model name if successful. It also includes error handling for value errors and unexpected exceptions. Finally, although the agentic path might update the file directly, the script includes a fallback step to write the modified prompt back to the file system.",811304cee2cb890879ee68a816aed49dff2e7b50cfdca3125e16e0ae99819574
context/logo_animation_example.py,"This Python script serves as a demonstration for the `pdd` package's branding animation functionality. It imports `start_logo_animation` and `stop_logo_animation` from the `pdd.logo_animation` module. The `main` function initiates the animation, which runs in a background thread and takes over the terminal screen. The script calculates a specific duration (approximately 7 seconds) to allow the full animation sequence—comprising formation, holding, and expansion phases—to complete and hold its final state. During this period, the main thread simulates ongoing work using `time.sleep`. Finally, the script calls `stop_logo_animation` to terminate the visual effects and restore standard terminal control to the main program, printing confirmation messages upon completion.",f32a5cc330235c3b61fb16ac62d3ae7531c7c6dbc11af077fc79054736f0944d
context/auto_deps_main_example.py,"This file defines a command-line interface (CLI) tool named `auto-deps` using the Python `click` library. The script serves as an entry point for an automated dependency analysis workflow. It imports necessary modules, including `auto_deps_main` from the `pdd` package and `Path` from `pathlib`. The core functionality is encapsulated in the `main` function, which is decorated with various `click` options to handle user inputs such as force overwrites, output suppression, analysis strength, temperature settings, and file paths for prompts, directories, and CSV outputs. Inside the `main` function, the script initializes the Click context with the provided parameters and invokes the `auto_deps_main` function to perform the actual processing. It captures the results—specifically the modified prompt, total cost, and model name—and prints them to the console. The script also includes error handling to catch exceptions and abort execution gracefully if issues arise. Finally, a standard `if __name__ == ""__main__"":` block ensures the CLI runs when the script is executed directly.",099fb138baed69b975f9cc571ad4855905e13f9ef5a855ac223815fc0de95c1c
context/get_jwt_token_example.py,"This Python script serves as a command-line interface (CLI) utility for authenticating a user via GitHub Device Flow to obtain a Firebase ID token. It is designed to support multiple environments (local, staging, production) by dynamically loading the appropriate Firebase API key and GitHub Client ID based on the `PDD_ENV` environment variable. The script defines a helper function, `_load_firebase_api_key`, which attempts to retrieve the API key from environment variables or by parsing specific `.env` files located in the project structure.

The main asynchronous execution flow initializes the authentication process using the `get_jwt_token` function. It handles various potential errors, including authentication failures, network issues, token errors, and rate limiting, providing user-friendly feedback for each scenario. Upon successful authentication, the script updates a local `.env` file with the retrieved JWT token. It writes both an environment-specific variable (e.g., `JWT_TOKEN_STAGING`) and a generic `JWT_TOKEN` variable to ensure the credentials are available for subsequent application use.",ae8e98e0b795bd1358ffc6a3e05da6112ae03dc9d4b58e4eb86e6926cbadff81
context/incremental_code_generator_example.py,"This Python script serves as a demonstration and usage example for the `incremental_code_generator` function from the `pdd` package. The script illustrates how to programmatically update existing code based on a revised prompt without necessarily rewriting the entire codebase from scratch. It defines a `main` function that sets up specific input parameters, including an original prompt (calculating a factorial), a new prompt (adding input validation), the existing Python code, and configuration settings such as strength, temperature, and time budget.

The script executes the `incremental_code_generator` with these parameters to determine whether an incremental patch is feasible or if a full regeneration is required. It utilizes the `rich` library to display formatted output to the console, showing the resulting updated code if patching was successful, or a recommendation for full regeneration otherwise. Additionally, the script reports metadata about the execution, including the total financial cost of the LLM usage and the specific model name employed.",f6cd628ddc2c10853a15c449b32451a1290d2457942d642989678720bc0cea82
context/context_generator_example.py,"This Python script demonstrates the usage of the `context_generator` function from the `pdd.context_generator` module. It begins by verifying that the required `PDD_PATH` environment variable is set, raising a `ValueError` if it is missing. The script then initializes several input parameters, including a sample code module (a simple addition function), a descriptive prompt, the programming language (Python), and configuration settings for strength and temperature. These parameters are passed to the `context_generator` function, which is called with verbose output enabled. Finally, the script uses the `rich` library to print the returned results, displaying the generated example code, the total cost of the operation formatted to six decimal places, and the name of the model used for generation.",277b54ee71ffc836d9db9ad4d4fc0b3fee365ce6f5691222acad11b58461923b
context/process_csv_change_example.py,"This file, `run_demo.py`, serves as a concise usage example for the `process_csv_change` function from the `pdd.process_csv_change` module. The script demonstrates how to programmatically modify prompt files based on instructions provided in a CSV file. 

The demo begins by setting up a temporary workspace on disk, creating a `code` directory, and populating it with a sample Python source file (`factorial.py`) and a corresponding prompt file (`factorial_python.prompt`). It also generates a CSV file (`tasks.csv`) containing instructions to modify the prompt, specifically requesting the addition of an example section.

The core of the script involves calling `process_csv_change` with various parameters, including the path to the CSV, the target code directory, model settings like strength and temperature, and a budget limit. Finally, the script prints the results of the operation, including a success flag, the total cost incurred, the model name used (e.g., `gpt-5-nano`), and a list of JSON objects showing the modifications made to the prompt file. The output confirms that the prompt was successfully updated with a Python test example.",c7cf31cd55d5312b5398d3d518eddd33354fa184d0e92c6cf926a3c71d018227
context/bug_to_unit_test_example.py,"This file is a Python script designed to demonstrate the functionality of the `bug_to_unit_test` function from the `pdd` package. It serves as a driver or example script that sets up a specific test case involving a debugging scenario. The script initializes variables representing a `current_output` (showing a failure or error trace) and a `desired_output` (showing the expected successful trace), specifically focusing on a module named ""trace"".

The script reads necessary context from external files, including a prompt file (`prompts/trace_python.prompt`), the source code under test (`pdd/trace.py`), and an example program used to run that code (`context/trace_example.py`). It configures parameters such as the LLM strength, temperature, and programming language.

The core logic resides in the `main` function, which calls `bug_to_unit_test` with these inputs to generate a unit test that reproduces the bug or verifies the fix. Finally, it uses the `rich` library to print the generated unit test, the total cost of the LLM operation, and the model name to the console, handling any exceptions that occur during execution.",ab1d665923937f6599f78c0cac82f5d7c04a7afa092e7767d67e417e067a8817
context/llm_selector_example.py,"This Python script serves as a demonstration and test harness for the `llm_selector` function, which is imported from the `pdd.llm_selector` module. The core functionality is encapsulated within a `main` function that iterates through a range of `strength` values to showcase how the selector behaves under different configurations. 

Starting with a base `strength` of 0.5 and a fixed `temperature` of 1.0, the script enters a `while` loop that continues as long as the strength does not exceed 1.1. inside this loop, it calls `llm_selector` to retrieve a specific Large Language Model (LLM) configuration, including the model object itself, a token counting utility, pricing information (input and output costs per million tokens), and the model's name. 

For each iteration, the script prints the current strength setting, the selected model's name, and its associated costs. It also demonstrates the utility of the returned `token_counter` by calculating and printing the token count for a sample string. The loop increments the strength by 0.05 in each pass. The execution is wrapped in a try-except block to handle potential `FileNotFoundError` or `ValueError` exceptions gracefully.",0a8259580406d3c7ba68ee57230cacc49a5446e488433bf85c44958ab0b07541
context/crash_main_example.py,"This Python script serves as a demonstration for using the `crash_main` function from the `pdd` package to automatically fix a crashed program. The script defines a function, `run_crash_fix_example`, which sets up a simulated environment containing a buggy calculator module, a main application that triggers a crash by passing incorrect types (strings instead of integers), and a log file capturing the resulting traceback. It then constructs a mock Click context with configuration settings such as verbosity and local execution mode. The core of the script invokes `crash_main` with paths to these dummy files, attempting a single-pass fix with specific parameters for model strength and budget. Finally, it checks the success of the operation, printing the fixed code, the cost of the operation, and the model used if successful, or an error message if the fix fails. The script is designed to be run directly to showcase the automated debugging capabilities.",5c9b909107f74f773ccf7013d556a5812643d2a3b086e79fe07da7ea7877458c
context/gemini_may_pro_example.py,"The provided file content is a concise Python code snippet demonstrating how to utilize the `litellm` library to interact with a specific Vertex AI model. It begins by importing the `completion` function from the `litellm` package. The core of the script involves calling this `completion` function with two primary arguments: the `model` identifier, specified here as ""vertex_ai/gemini-2.5-pro-preview-05-06"", and a `messages` list containing a single user role dictionary with a placeholder prompt. Finally, the script captures the output of this API call in a variable named `response` and prints it to the console. This snippet serves as a basic usage example for developers looking to integrate Gemini 2.5 Pro models via the LiteLLM abstraction layer.",50c14eac723918c8a752ccc66d12eac4220d5ac62620bbf0c5e21bd821f08734
context/agentic_test_orchestrator_example.py,"This Python script serves as an example and simulation of the `agentic_test_orchestrator` module. It demonstrates how to invoke the `run_agentic_test_orchestrator` function without making actual Large Language Model (LLM) calls or interacting with a live GitHub repository. The script achieves this by using `unittest.mock` to patch internal dependencies, specifically `load_prompt_template` and `run_agentic_task`. It defines mock implementations for these functions that simulate a successful 9-step workflow for generating UI tests for a login page, including steps like duplicate checking, codebase review, test planning, generation, execution, fixing, and PR submission. The `main` function sets up dummy issue data, applies the patches, runs the orchestrator, and prints the simulated results, including success status, cost, and changed files.",3551a350a211d81d0c4f109fa102428bdfd9bb7c76b1ad09375a6479a3bf573e
context/click_example.py,"This Python script implements a command-line interface (CLI) tool named `imagepipe` for processing images using a Unix-style pipeline approach. Built with the `click` library and `Pillow` (PIL), the tool allows users to chain multiple image manipulation commands together, where the output of one command feeds into the next. 

The script defines a main command group `cli` that utilizes a result callback to manage the processing stream. It includes custom decorators, `@processor` and `@generator`, to handle the flow of image data between functions. 

The tool supports a variety of operations, including opening images (from files or stdin), saving processed results, and displaying images in a viewer. Image manipulation capabilities include resizing (maintaining aspect ratio), cropping borders, transposing (rotating and flipping), blurring (Gaussian), smoothening, embossing, sharpening, and pasting one image onto another. Each command processes an iterable stream of images, applies the specific transformation, and yields the result for the next step in the chain, enabling complex workflows like opening a file, resizing it, blurring it, and saving the result in a single command line execution.",07653c30b63c213102cf00f7c3380a5005330014e00a26342921cd85d20273c1
context/comment_line_example.py,"The provided file documents the usage and implementation of a Python utility function named `comment_line`, which is designed to programmatically comment out lines of code. The function accepts two string arguments: `code_line`, representing the code to be modified, and `comment_characters`, which dictates the commenting style. The documentation details three distinct behaviors based on the `comment_characters` input: if the input is 'del', the function returns an empty string, effectively deleting the line; if the input contains a space (e.g., '<!-- -->'), it treats the parts as start and end delimiters to wrap the code; otherwise, it prepends the input as a single comment character (e.g., '#'). The file includes the source code for the function, comprehensive docstrings explaining parameters and return values, and practical examples demonstrating its application for Python-style comments, HTML-style block comments, and line deletion.",debc424d539b65aef1267888bb2eb77cf4de16745407bb5df5a18b8e2f231095
context/agentic_langtest_example.py,"The provided code defines a Python module, `pdd/agentic_langtest.py`, designed to automate the execution of unit tests across different programming languages. Its primary functionality revolves around the `default_verify_cmd_for` function, which generates appropriate shell commands to compile and run tests based on the specified language (Python, JavaScript/TypeScript, or Java) and the project's structure.

To achieve this, the module includes helper functions like `_find_project_root`, which traverses directory trees to locate project markers such as `package.json`, `pom.xml`, or `build.gradle`. It also features `missing_tool_hints`, a utility that checks the system environment for required tools (e.g., `npm`, `javac`, `g++`) and provides installation suggestions for macOS and Ubuntu if dependencies are absent.

The input also contains an appended example script (`example.py`). This script utilizes the `rich` library to demonstrate the module's capabilities by creating temporary mock projects for JavaScript and Python. It showcases how the module identifies project roots, generates test commands, and reports missing toolchains in a user-friendly format.",668f3abf1c2a4bc683ca02f137cb37ba38974839d612fc57ad1a46e03a5af6f8
context/generate_output_paths_example.py,"This Python script serves as a comprehensive demonstration and test suite for the `generate_output_paths` function, which is imported from a module named `pdd.generate_output_paths`. The script first sets up the Python path to locate the parent directory where the module is assumed to reside. It then executes a series of eight distinct scenarios to validate the function's behavior under various conditions. These scenarios cover cases such as using default settings, specifying custom output filenames or directories, leveraging environment variables to define paths, handling mixed inputs (user-defined vs. defaults), and dealing with specific command types like 'preprocess' that enforce fixed file extensions. Additionally, the script tests error handling for unknown commands and missing required arguments like the basename. For each scenario, the script prints the inputs, the actual result returned by the function, and the expected output path, allowing for verification of the path generation logic.",0f8a6384ebfe9dc3ea448a4df7b4f5ebf9aa9b714f67a4322437638700a4307f
context/execute_bug_to_unit_test_failure.py,"The provided file content is a very short Python script that demonstrates a basic usage of an imported function. It begins by importing a function named 'add' from a specific module path: 'context.bug_to_unit_test_failure_example'. Following the import statement, the script executes a single print command. This command calls the imported 'add' function with the integer arguments 2 and 3. The result of this addition operation is then printed to the standard output. Essentially, this script serves as a simple driver or test case to verify that the 'add' function from the specified context works as expected, likely outputting the integer 5 when run.",cdc899979df1de48bba42fd026d5f980dc7381fdf224f8ab4db76554b69ee91e
context/config_example.py,"This file contains a minimal Python script that serves as an entry point or initialization module. Its primary and sole function is to set up the application's configuration before any other operations occur. It imports the `init_config` function from a local `utils.config` module and immediately executes it. The comment explicitly emphasizes that this initialization step must happen 'FIRST', indicating that the configuration loading is a prerequisite for the rest of the application's runtime environment.",9440de90fa1ba1ed48f0bf1a080357ef15080d4406bc6f90c794f1eb539c8c40
context/detect_change_main_example.py,"This Python script serves as a driver or entry point for executing a change detection process, likely within a larger system for managing prompt engineering or code generation workflows. It imports the `detect_change_main` function from a local module (`pdd.detect_change_main`) and the `click` library to simulate command-line interface (CLI) context. The `main` function initializes a `click.Context` object with specific model parameters such as strength and temperature. It defines a list of prompt files (e.g., `python_preamble.prompt`, `change_python.prompt`) and creates a change description file containing instructions to make prompts more compact. The script ensures the output directory exists before invoking `detect_change_main` with the configured context, file paths, and output destination. Upon successful execution, it prints the model name used, the total cost of the operation, and iterates through the returned list of changes to display specific instructions for each prompt file. Error handling is included to catch and print exceptions during the process.",ddc494709d127398004f53de58bede388692b2839a97b26668c55575369d9edd
context/pdd_discussiion.txt,"This transcript records a conversation between a user and ChatGPT discussing the concept and methodology of Prompt-Driven Development (PDD). The user argues that traditional software development, which focuses on code as the primary artifact, suffers from high maintenance costs and ""spaghetti code"" over time. In contrast, PDD proposes treating the prompt as the main artifact, similar to how High-Level Description Languages (HDLs) replaced netlists in chip design. By maintaining prompts, developers can regenerate clean code, examples, and tests, ensuring synchronization across documentation and implementation. 

The user introduces a tool called PDD-CLI and a concept called PDD-MCP (Model Context Protocol) to facilitate this workflow, emphasizing that PDD allows for batch processing rather than the interactive ""babysitting"" required by current AI coding assistants like Cursor or Cloud Code. The conversation covers the benefits of PDD, such as token efficiency, easier onboarding, and better scalability for complex systems. It also addresses potential challenges, including the learning curve for writing effective prompts and the need for rigorous testing to manage dependencies. The user outlines a specific workflow involving prompts, generated code, usage examples, and unit tests, all kept in constant sync via back-propagation of changes.",9a627a93e31f408e5d5611413b72957f1137f9efc1b2fbcfeae16cca9197874c
context/split_main_example.py,"The file `example_split_usage.py` provides a command-line interface (CLI) example demonstrating how to utilize the `split_main` function from the `pdd.split_main` module. It uses the `click` library to define a command named `split-cli` that accepts several arguments, including paths for an input prompt file, a generated code file, and an example code file, as well as optional output paths and flags for forcing overwrites or suppressing output. Inside the function, a context object is configured with parameters like strength and temperature before invoking `split_main`. The script handles the execution flow by capturing the returned result data, total cost, and model name. Finally, it includes error handling and logic to display the results—such as the content of the sub-prompt and modified prompt, file save locations, and operation costs—to the console if the quiet flag is not set. The script is structured with a top-level CLI group to facilitate execution as a standalone program.",ced192fcbfbfda15b35bd2dad5bbf4b4888acb091084cf58326832d224a77661
context/regression_example.sh,"This file is a Bash shell script designed to automate a comprehensive regression testing suite for a tool named `pdd`. The script begins by defining various environment variables for paths, filenames, and logging configurations, including staging directories, prompt locations, and output logs like `regression.log` and `regression_cost.csv`. It includes helper functions for verbose logging and timestamping entries to track execution progress.

The core functionality involves executing a series of `pdd` commands to test different features of the tool. These features include generating code from prompts, creating verification examples, running tests, preprocessing prompts (including XML output), and updating existing prompts. The script also tests more complex operations such as `change` (modifying code based on context), `fix` (debugging code based on pytest output or error logs, including a loop mode), `split` (refactoring prompts), `detect` (analyzing prompts), `conflicts` (checking for prompt conflicts), and `crash` (handling runtime errors).

Finally, the script calculates and displays the total cost of operations by parsing the generated cost CSV file, ensuring that the financial impact of the regression test is tracked alongside technical success.",e4df58d36270ab7c005dfa8a111159c78bf149378510ae3423fb83efeb8f622a
context/get_run_command_example.py,"This Python script serves as a comprehensive usage example for the `get_run_command` module within the `pdd` package. It demonstrates how to retrieve execution commands for various programming languages based on file extensions, utilizing a CSV configuration file located via the `PDD_PATH` environment variable. The script defines a `main` function that walks through six distinct scenarios: retrieving a command template for a standard Python extension, demonstrating extension normalization (handling missing dots or uppercase letters), generating a complete run command for a specific file path, handling unknown extensions gracefully, managing files without extensions, and iterating through a list of common language extensions like `.js`, `.rb`, and `.java`. Through these examples, the code illustrates the functionality of two key functions: `get_run_command`, which returns a template string with a `{file}` placeholder, and `get_run_command_for_file`, which returns a fully interpolated command string ready for execution. The script is designed to be executed directly to verify the module's behavior.",4d187e844adb255c2738274556f4828728d0d73d84ebf8bda885d51d4be9414f
context/change_main_example.py,"This Python script serves as a demonstration and test harness for the `change_main` function from the `pdd` command-line program. It illustrates how to programmatically invoke the tool's functionality in two distinct modes: single-change mode and CSV batch-change mode. 

The script begins by setting up a `click.Context` object populated with configuration parameters such as LLM strength, temperature, and budget. It then creates necessary output directories and sample input files (prompts and Python code snippets) to simulate a real-world environment. 

In the single-change mode section, the script generates a specific change prompt and input code, calls `change_main` to modify the prompt based on the code, and prints the resulting modified prompt and cost. In the batch-change mode section, it sets up multiple code and prompt files along with a CSV file defining specific instructions for each. It then invokes `change_main` with `use_csv=True` to process these changes in bulk, displaying the final status and total cost. The script uses the `rich` library for formatted console output.",990b2dfc2b9ef2f1871cb4fe1b7a01d5d6f4f4f54f9a6a95fc3661c679802d42
context/architecture_sync_example.py,"This file serves as a comprehensive usage guide and demonstration for the `architecture_sync` module, which manages bidirectional synchronization between a central `architecture.json` file and individual prompt files using PDD (Prompt Driven Development) metadata tags. Through nine distinct examples, the code illustrates core functionalities such as parsing XML-style tags (reason, interface, dependencies) from prompt content, validating dependency existence, and verifying JSON interface structures.

The script demonstrates how to update the architecture registry based on prompt content (both for single files and in batch operations) and performs the reverse operation: generating metadata tags from architecture entries. It also includes utility workflows for checking if tags already exist to prevent overwriting and a complete example of injecting generated tags into new prompts. The file concludes with a main execution block that runs a selection of these examples, showcasing how the module maintains consistency between architectural definitions and implementation prompts.",95624e3241bf1d837528b897cf8014c0946c8b65bca2c174f3bd5cc54c7ee3ff
context/dict_utils_example.py,"This Python script serves as a demonstration and usage guide for a custom utility module named `dict_utils`. The script begins by dynamically adjusting the system path to locate and import the `dict_utils` module from a relative directory structure, specifically targeting an `experiments` folder. It defines helper functions for printing formatted headers and JSON data. The `main` function showcases five key functionalities provided by the utility library: `deep_merge`, which recursively combines nested dictionaries; `flatten_dict`, which converts nested structures into single-level dictionaries using dot notation or custom separators; `unflatten_dict`, which reverses the flattening process to reconstruct nested objects; `filter_keys`, which allows for including or excluding specific keys from a dictionary; and `get_nested`, which safely retrieves values from deep within a data structure using path strings, offering default values for missing keys. Each section includes concrete examples with sample data and prints the results to the console to verify the behavior of the utility functions.",4c123047dc113ec65a2fccf858e047a7379382ac66d09029037dd7489b6f113f
context/fix_code_module_errors_example.py,"This file is a Python script designed to demonstrate the functionality of the `fix_code_module_errors` function from the `pdd.fix_code_module_errors` module. The script sets up a test scenario involving a deliberately flawed Python program that attempts to calculate the sum of a string, which triggers a `TypeError`. It defines the original erroneous program, the prompt used to generate it, the specific code module containing the logic, and the resulting error message trace. 

The core of the script calls `fix_code_module_errors` with these inputs, along with configuration parameters for model strength, temperature, and verbosity. This function is intended to automatically debug and repair the code using an LLM. The script captures the return values, which include boolean flags indicating if updates were needed, the corrected program and code module strings, the raw fix output from the LLM, the total API cost, and the model name used. Finally, the script prints these results to the console, allowing users to verify the automated debugging process and see the corrected code alongside the operational metrics.",f35244d6e920711044fee92ade984f3e820a0fbfe766fd3d49718cbcfe793e60
context/get_test_command_example.py,"This Python script serves as a demonstration and documentation for the `get_test_command` module, specifically the `get_test_command_for_file` function. It illustrates how the system resolves test commands through a layered strategy: checking a configuration CSV (`language_format.csv`), attempting smart detection via `default_verify_cmd_for`, or returning `None` to signal that an agentic fallback is required.

The script includes several specific use cases using the `rich` library for formatted console output. It demonstrates resolving commands for standard Python and JavaScript test files, showing how the system handles common extensions. It also highlights the ability to override file extension detection by explicitly passing a `language` parameter (e.g., for TypeScript). Furthermore, the script provides examples of batch processing multiple files, displaying the results in a table, and explicitly handling scenarios where no command is found (unknown file types), emphasizing the intended workflow for agentic intervention. The `main` function orchestrates these examples to provide a comprehensive overview of the module's capabilities.",e41c5ad25fb7e9ab01baf376e17226508365ab4269808f935de4d69b0bbdd4eb
context/edit_file_example.py,"This Python script serves as a test harness and demonstration for the `pdd.edit_file` module, specifically targeting the `run_edit_in_subprocess` function. It begins by configuring the runtime environment, ensuring the critical `PDD_PATH` environment variable is set and dynamically adjusting the system path to import the necessary module from a parent directory.

The script includes a helper function, `create_example_files`, which generates a dummy text file within an `output` subdirectory to serve as a test subject. It defines specific editing instructions, such as replacing words, overwriting entire lines, and appending new text. The core logic is encapsulated in `run_edit_file_test`, which executes the editing process and prints the results.

Crucially, the script implements a verification mechanism that reads the modified file after execution. It performs specific boolean checks to confirm that the requested changes (e.g., changing ""original"" to ""UPDATED"") were applied correctly. The output provides a detailed report, including the initial file content, execution status, error messages, and a final pass/fail verification status, effectively validating the functionality of the MCP-based text editing tool.",64c4d8db9ecbb99f14083e10c9e5788e4b04fc6f9dd0942ab571ffea7a6dd1ca
context/update_main_example.py,"This Python script serves as an example implementation demonstrating how to utilize the `update_main` function from the `pdd.update_main` module within a command-line interface (CLI) built using the `click` library. The script defines a CLI command named `update` that accepts numerous arguments, including paths for input prompts, modified code, and original code, as well as configuration flags for Git integration, model strength, temperature, verbosity, and output handling. It showcases how to pass these user-provided options into a context object and subsequently invoke the core `update_main` logic to update a prompt based on code modifications. Additionally, the script handles the display of results using the `rich` library, printing the updated prompt snippet, total cost, and the model name used for the operation. The file includes a main execution block that initializes the CLI group and adds the `update` command, making it directly executable for testing or demonstration purposes.",b7817f30fab788c21b4b0fb42846cbb5fe82b91e8dea265cd742252e036407e2
context/prompt_caching.ipynb,"This Jupyter Notebook serves as a cookbook for implementing prompt caching using the Anthropic API. It demonstrates how caching context within prompts can significantly reduce latency and costs, particularly for tasks involving large amounts of text. The notebook begins by setting up the environment and fetching the full text of Jane Austen's ""Pride and Prejudice"" (~187,000 tokens) to serve as a large context example. It then presents two main examples: a single-turn comparison between cached and non-cached API calls, showing a dramatic reduction in processing time (from ~21 seconds to ~3 seconds), and a multi-turn conversation simulation. The multi-turn example illustrates how to maintain cache breakpoints across a dialogue, achieving near-instant processing of input tokens for subsequent user questions while maintaining high-quality responses. The code includes a helper class for managing conversation history and dynamically adjusting cache control headers.",c9d723779341cfa01e6f78abd1128aae8ecdf66ba2c4cada31880df0b3668ff4
context/split_example.py,"This Python script serves as a demonstration and entry point for utilizing the `split` function from the `pdd.split` module. It begins by importing necessary libraries, including `os` for environment management and `rich.console` for enhanced terminal output. The core logic is encapsulated within a `main` function. Inside `main`, the script first ensures the `PDD_PATH` environment variable is correctly set relative to the script's location. It then defines several input parameters required for the `split` function: a descriptive `input_prompt`, a block of `input_code` (a factorial function), `example_code` demonstrating usage, and numerical parameters for `strength` and `temperature`. The script calls the `split` function with these arguments, enabling verbose mode. Upon execution, it unpacks the returned tuple into a result (containing a sub-prompt and modified prompt), the total cost of the operation, and the model name used. Finally, it uses the Rich console to pretty-print these outputs—displaying the generated prompts, the specific model employed, and the calculated cost—while also including error handling to catch and display any exceptions that occur during execution.",c36a6c3e5ddac660ef95d72dabc89f5453a3ebcd47f5f82ddc5fec77806154d9
context/agentic_change_orchestrator_example.py,"This Python script serves as an example and simulation for the `agentic_change_orchestrator` module within a project likely named `pdd`. It demonstrates how to invoke the `run_agentic_change_orchestrator` function without performing actual Large Language Model (LLM) calls or interacting with a live GitHub repository. 

The script sets up a mock environment by patching internal dependencies such as `load_prompt_template` and `run_agentic_task`. It defines a `mock_run_agentic_task` function that simulates the outputs for a 13-step workflow, guiding the orchestrator through a scenario where a user requests an email validation feature. The mock responses cover steps ranging from duplicate issue checking and documentation review to identifying development units, modifying prompts, and creating a pull request. 

The `main` function initializes a temporary directory and dummy issue data, then executes the orchestrator within a context where git operations and state management are also mocked. Finally, it prints the simulation results, including success status, total simulated cost, and a list of changed files, illustrating the expected behavior of the orchestration tool.",26a09cdbb0b010106128bf51f6eca9e59fa8d15692a73dea94e8b6d77496f6aa
context/agentic_verify_example.py,"This Python script serves as a demonstration and test harness for the `run_agentic_verify` function from the `pdd` package. It sets up a mock environment to simulate an automated code verification workflow without requiring actual external API calls or LLM access. The script first configures the Python path to ensure the `pdd` package is importable. It then defines helper functions to create dummy files, including a specification file, a buggy implementation of a calculator (using subtraction instead of addition), a driver program to test the code, and a verification log. A `mock_agent_behavior` function is defined to simulate an AI agent identifying and fixing the bug in the calculator code. The `main` execution block uses `unittest.mock.patch` to intercept calls to internal `pdd` functions, replacing them with the mock behavior. Finally, it runs the verification process, prints the execution results (success status, cost, model used), and displays the corrected content of the calculator file to verify the fix.",24022f00007d707a593280b1dd2151e05c33680a65a67aaf034243cac5a9bf6c
context/cmd_test_main_example.py,"This Python script serves as a usage example for the `cmd_test_main` function from the `pdd` package, demonstrating how to programmatically generate unit tests. The script sets up a mock environment by creating a temporary output directory and generating dummy input files: a prompt file describing a calculator task and a corresponding Python source file implementing simple addition and subtraction. It then constructs a mock `click.Context` object to simulate command-line arguments such as verbosity and force-overwrite flags. The core of the script executes `cmd_test_main` with these inputs, specifying parameters like the target language (Python), model strength, and temperature. Finally, it displays the results of the execution using the `rich` library, printing the model used, the estimated cost, the location of the generated test file, and a preview of the generated code. The script ensures necessary environment variables are set before running the example.",906f51b918543e5d7a9a2d30e33096dd7ff81df3c4673e0d20053155fbf4d841
context/remote_session_example.py,"This Python script serves as a comprehensive usage example for the `pdd.remote_session` module, specifically demonstrating the `RemoteSessionManager` class. It illustrates the lifecycle of a remote development session within the `pdd` ecosystem. The script begins by setting up the environment to import the local package and mocking the `CloudConfig` to bypass real cloud credentials. Inside an asynchronous `main` function, it initializes a `RemoteSessionManager` with a mock JWT token and project path, setting it as the global active manager. The example then proceeds to register a new session, mocking the HTTP response to simulate a successful cloud registration that returns a session ID and cloud URL. It demonstrates starting a background heartbeat task to maintain session connectivity. Furthermore, the script shows how to list active sessions using the static `list_sessions` method, parsing and displaying mock session data. Finally, it covers the cleanup process by deregistering the session and stopping the heartbeat, ensuring the manager's internal state is reset. Throughout the code, `unittest.mock.patch` is used extensively to simulate network interactions with the cloud API.",83e17416100132467ee86c4e574cb2c52721d5c4aaf096d149fdd6f3df984faf
context/DSPy_example.py,"The provided file content is a Python code snippet demonstrating the initial setup and module definition for a DSPy application. The code begins by importing a local module named `DSPy_example` and configuring the language model settings to use OpenAI's `gpt-3.5-turbo-instruct` with a token limit of 250. Following the setup, the script defines a custom class named `chainofthought` that inherits from `dspy.Module`. Inside this class, the `__init__` method initializes a `dspy.ChainOfThought` program designed to map questions to answers. A `forward` method is also defined to execute this program when the module is called with a question. The snippet concludes abruptly with a comment indicating the next steps would involve compiling and optimizing the module, but the code for these actions is missing.",403910f924da848e2016f2a19cd9283759b6eac67f969f4d0bd5c573e39784f7
context/agentic_update_example.py,"This Python script serves as a demonstration and utility for using the `run_agentic_update` function from the `pdd` package. Its primary purpose is to showcase how an AI agent can automatically update a prompt file to match the current state of a codebase. The script begins by setting up the environment, including adding the project root to the system path. It then creates a simulated development scenario within a local `./output` directory by generating three files: an outdated prompt file (`math_module.prompt`), an updated Python code file (`math_module.py` containing both add and subtract functions), and a corresponding test file (`test_math_module.py`). 

The core of the script executes the `run_agentic_update` function, passing in the paths to the prompt and code files. This function triggers an AI agent to analyze the code and tests, then rewrite the prompt file to reflect the new code features (specifically the addition of the `subtract` function). Finally, the script prints the results of the operation, including success status, cost, the model used, and the content of the updated prompt file to verify the changes.",019bc86f2c262b279dd0c7ad89223fbbc3ffd9d5229ae06ccbd07ef1c954d26c
context/websocket_example.py,"This Python file provides a comprehensive example of implementing WebSocket handling for a PDD Server using FastAPI and Pydantic. It defines a structured protocol for bidirectional communication, including specific message types for standard output streaming, progress updates, input requests, and job completion status. Key components include a `ConnectionManager` for tracking and broadcasting to multiple clients (grouped by job ID), an `OutputStreamer` class designed to capture and stream subprocess output while handling ANSI codes, and a `WebSocketInputHandler` for managing interactive user input over the socket. The file also includes FastAPI route definitions for job streaming and file watching, along with a main execution block that sets up a test server to simulate job execution and output streaming.",6f50399fd41177e47da3f170a1cc84380f8f266036e8f510f7807a916265ab0f
context/get_language_example.py,"This file contains a Python script designed to demonstrate the functionality of a `get_language` function imported from a local module named `pdd.get_language`. The script defines a `main` function which serves as the entry point. Inside this function, a variable `file_extension` is initialized with the string value '.py'. The script then attempts to call `get_language` with this extension to retrieve the associated programming language name. It includes error handling using a try-except block to catch and print any exceptions that might occur during execution. If the function successfully returns a language, it prints a formatted string confirming the mapping; otherwise, it informs the user that no language was found. The script concludes with a standard `if __name__ == ""__main__"":` block to execute the `main` function when the file is run directly.",8b09af25b96fa49e2dcf5ff307bbeb0bc83e84b64618971d393397772a2fafab
context/detect_change_example.py,"This Python script demonstrates how to use the `detect_change` function from the `pdd` package to analyze a set of prompt files against a specific change description. The script begins by importing necessary modules, including `detect_change`, `rich.console` for formatted output, and `pathlib`. It defines a list of specific prompt files to be examined, such as `python_preamble.prompt` and `change_python.prompt`, although a commented-out section suggests an alternative method to dynamically list all prompt files in specific directories. A `change_description` is provided, instructing the system to make prompts more compact using a specific preamble file. The script also sets configuration parameters for the Large Language Model (LLM), specifically `strength` and `temperature`. The core logic is wrapped in a try-except block where `detect_change` is called with the file list, description, and model parameters. Upon success, the script iterates through the returned list of changes, printing the prompt name and specific instructions for each change using the `rich` console. It also displays the total cost of the operation and the name of the model used. If an error occurs during execution, the exception is caught and printed in red.",44d0c9172db01bd645d48a11a7afa527a7e509f5aac911fa41ed091d3f982096
context/trace_main_example.py,"This Python script demonstrates the usage of the `trace_main` function from the `pdd.trace_main` module, likely part of a larger tool for tracing relationships between code and prompts using Large Language Models (LLMs). The script begins by creating two sample files in an `output` directory: `calculator.prompt`, which contains a natural language requirement for a calculator function, and `calculator.py`, which contains the corresponding Python implementation. It then sets up a `click.Context` object to simulate command-line arguments, configuring parameters such as verbosity (`quiet`), overwrite permission (`force`), analysis strength, and LLM temperature. The core of the script invokes `trace_main` with these configurations, passing the paths to the prompt and code files, a specific line number in the code to analyze (line 2), and an output path for results. Finally, it prints the results of the analysis, including the identified corresponding line number in the prompt file, the total cost of the LLM operation, and the model name used, while handling any potential exceptions.",c1f43ebf55c05af801640f5f8331ee7a674aef9e3f383b0e5ffee7abc7246fe1
context/find_section_example.py,"The provided file serves as a usage guide and documentation for the `find_section` function from the `pdd.find_section` module. It includes a practical Python code example demonstrating how to import the function, prepare input data by splitting a multi-line string containing Markdown code blocks into a list of lines, and invoke the function to identify code sections. The example iterates through the returned results to print the programming language, start line index, and end line index for each detected block. Additionally, the file provides detailed documentation on the function's input parameters—specifically `lines`, `start_index`, and `sub_section`—and describes the output format as a list of tuples containing the language and line indices. Finally, it illustrates the expected output for the given sample input, showing how the function correctly identifies Python and JavaScript blocks within the text.",681043403aa02b9eb0d265a95560fce565507d0656d1693f79650f8eb5f0683a
context/fix_verification_errors_loop_example.py,"This Python script serves as a demonstration for a function named `fix_verification_errors_loop` from the `pdd` package. It sets up a temporary demo environment containing a buggy Python file (`calculator.py` which incorrectly subtracts instead of adds), a verification script (`verify_calc.py` which asserts the correct addition behavior), and a prompt file describing the intended functionality. The script then executes the `fix_verification_errors_loop` function, passing in parameters such as the code file, verification program, budget, and temperature settings. This loop is designed to use an LLM to iteratively fix the code until the verification passes. Finally, the script reports the success or failure of the operation, printing the fixed code, total attempts, and cost to the console using the `rich` library for formatting.",0212e6010a7c163c476c3df095f6ca99414829f73df34c60651afae83633d307
context/path_resolution_example.py,"This Python script demonstrates the usage of a path resolution utility from the `pdd.path_resolution` module. The script defines a `main` function that initializes a default resolver using `get_default_resolver`. It then attempts to resolve and print the absolute paths for three specific resources: an include file named `context/python_preamble.prompt`, a prompt template named `load_prompt_template_python`, and the project's root directory. Additionally, the script includes a try-except block to attempt resolving a data file located at `data/language_format.csv`. If the resolution fails (likely due to the `PDD_PATH` environment variable not being set), it catches a `ValueError` and prints an error message indicating that the path configuration is missing. The script concludes by executing the `main` function if run as the entry point.",15ba84fd11c61af8fa12dcb19d63d42f0cb09b747542bf20e69650b8ff4ac137
context/thinking_tokens.md,"This document provides a technical breakdown of the maximum internal ""reasoning"" token budgets for a specific list of AI models. These tokens represent the hidden Chain-of-Thought (CoT) steps models take before generating a final response.

The models fall into three primary categories regarding reasoning capabilities. First, several standard models—including GPT-4.1 (Base and Nano), Claude-3.5 Haiku, and DeepSeek Coder—possess no dedicated internal reasoning tokens, relying instead on single-pass generation within their standard context windows.

Second, the dedicated ""reasoning"" models show distinct tiers of capacity. OpenAI’s o3 and o4-mini lead with the highest capacity, allowing up to ~100,000 tokens for internal thought. Claude-3.7 Sonnet and Google’s Gemini 2.5 series (Flash and Pro) occupy a high-capacity middle tier, supporting approximately 64,000 reasoning tokens. Finally, the DeepSeek R1 family (including the main model and distilled Llama/Qwen variants) consistently caps reasoning at roughly 32,000 tokens.

Additionally, Grok 3 Beta is noted for having a flexible, undefined limit that can scale into the thousands. The summary highlights that while some models treat reasoning tokens as part of the total output quota (like OpenAI), others (like DeepSeek) separate them from input context limits.",47d0635d14672ad9b6aa133166058fa6eaea36029ea3f332bc9f647a01a25f6c
context/tiktoken_example.py,"The provided file content is a brief Python code snippet demonstrating how to count tokens using the `tiktoken` library, which is commonly used with OpenAI's language models. The script first imports the `tiktoken` module. It then initializes an encoding object by calling `tiktoken.get_encoding` with the specific encoding scheme ""cl100k_base"", which is the tokenizer used by models like GPT-4 and GPT-3.5 Turbo. Finally, the code calculates the number of tokens in a variable named `preprocessed_prompt` by encoding the text and measuring the length of the resulting list using the `len()` function, storing the result in the variable `token_count`. This snippet serves as a utility for developers needing to manage context window limits or estimate costs associated with API usage.",0cc5e74006d0d30b399158127d9258105ce16be1815c012a08bafd3435f78f36
context/resolve_effective_config_example.py,"This file provides example usage for the `resolve_effective_config` function from the `pdd.config_resolution` module. It demonstrates how to resolve configuration parameters such as strength, temperature, and time by prioritizing command-line interface (CLI) arguments, followed by `.pddrc` configuration files, and finally default values. The examples illustrate two primary scenarios: calling the function with just the context and resolved configuration to rely on standard priority logic, and calling it with an optional `param_overrides` dictionary to explicitly inject command-specific parameters. The code snippets show how to extract the resulting effective configuration values for use in downstream processes.",8bb73c48821e07f533909f368d5cc1e029b12690830ceb9918c4f1bd7611a293
context/summarize_directory_example.py,"This Python script serves as a demonstration and usage example for the `summarize_directory` module within the `pdd` package. The script defines a `main` function that orchestrates the summarization process for Python files matching a specific pattern (in this case, `context/c*.py`). It begins by defining a sample string representing an existing CSV file, which includes file paths, old summaries, and content hashes, simulating a cache mechanism for invalidation. 

The core functionality is executed by calling the `summarize_directory` function with specific parameters: a directory path with a wildcard, a model strength setting of 0.5, a temperature of 0.0 for deterministic output, and verbose logging enabled. It also passes the existing CSV content to demonstrate how the tool handles prior data. Upon execution, the script captures the resulting CSV output, the total cost of the operation, and the name of the model used. Finally, it prints these details to the console and saves the generated CSV content to a file named `output.csv` within an `output` directory, ensuring the directory exists before writing. Error handling is implemented to catch and print any exceptions that occur during execution.",a1fb936c45b2dc3b9995d22aed05e75af20b171d8990c4a483b1cf8a54dc6df3
context/__init__example.py,"This Python script serves as an example and verification tool for the command registration module within the 'pdd' application. It demonstrates the initialization of the main Click CLI group and the dynamic registration of subcommands. The script defines a main entry point function, `main_cli`, decorated with `@click.group()`, which acts as the root for the command-line interface. The core functionality is encapsulated in the `run_example` function, which first prints a status message indicating the start of the initialization process. It then calls `register_commands(main_cli)`, importing this function from the `pdd.commands` package to attach all defined subcommands to the main group in-place. To verify that the registration was successful, the script programmatically invokes the CLI with the `--help` argument using `main_cli.main()`. It sets `standalone_mode=False` to prevent the script from exiting immediately, allowing it to print the help output—which lists the newly registered commands—bounded by separator lines. Finally, the script prints a completion message. This file is typically used to test that the command aggregation logic works correctly before deployment.",84208a445d03a336e465709ec0687eb969c8d865e6739bf8b79f2c8a8aad351a
context/track_cost_example.py,"This Python script defines a command-line interface (CLI) tool named PDD, built using the `click` library, designed for processing prompts and generating outputs with integrated cost tracking. The main entry point is a `cli` group that accepts an optional `--output-cost` argument to specify a CSV file for logging usage details. The core functionality is encapsulated in the `generate` command, which requires an input prompt file path and accepts an optional output file path. This command is decorated with a custom `@track_cost` decorator, indicating that execution metrics are monitored. Inside the `generate` function, the script reads the input file, simulates a generation process (currently a placeholder that prepends text to the input), and calculates a simulated cost and model name (e.g., ""gpt-4""). If an output path is provided, the result is written to that file; otherwise, it is printed to the console using the `rich` library. The script concludes with a standard `if __name__ == '__main__':` block that demonstrates how to invoke the CLI programmatically with sample arguments.",30c33d5c1c945e0e8f0c0f9861f8b952d1eb4f22e6cbc8c05089b160b1a4c8eb
context/agentic_test_example.py,"This file is an example script demonstrating the usage of the `run_agentic_test` function for generating tests based on GitHub issues. It sets up a mock environment to simulate the agentic workflow without making actual API calls or requiring external dependencies like the GitHub CLI. The script mocks several internal components, including the orchestrator, issue data fetching, and repository context checks. It defines a `main` function that executes `run_agentic_test` with a dummy issue URL and specific parameters. The mock orchestrator returns a successful result, simulating a scenario where tests are generated and a pull request is created, along with associated costs and file changes. Finally, the script prints a summary of the execution results, including success status, the model used, total cost, changed files, and the output message.",1d8b490a2b0ff53a8b786c6c95faeecc4efd0becf837b35273508bae19fac3ab
context/auto_update_example.py,"This file provides a Python script demonstrating the usage of the `auto_update` function from the `pdd.auto_update` module. The script defines a `main` function that illustrates three different scenarios for checking and performing package updates. First, it shows the basic usage to check for updates to the 'pdd' package itself. Second, it demonstrates how to check for updates for a specific third-party package, using 'requests' as an example. Finally, it shows how to check a package against a specific known version number, using 'pandas' and version '2.0.0'. The accompanying docstring explains the underlying logic of the `auto_update` function, which involves verifying the installed version, comparing it against the latest available version (either from PyPI or a provided argument), prompting the user for confirmation if a newer version is found, and executing the upgrade via pip.",4fc8ab0da96c0e8abe793d195dbc64b6b9771631be818eecde059769e5914fc9
context/addition_of_time_param.prompt,"This file contains a system prompt designed for the `pdd detect` command. Its primary purpose is to analyze other PDD CLI command prompt files to determine if they require modification following the introduction of a new global `--time` parameter. The prompt outlines the background of this new feature, explaining that `--time` is a float value (0.0 to 1.0, default 0.25) controlling the ""thinking effort"" or reasoning allocation of the underlying LLM via the `llm_invoke` function. It details how this parameter translates into specific behaviors depending on the model's reasoning type (e.g., calculating token budgets for 'budget' types or mapping to qualitative levels like low/medium/high for 'effort' types). The task assigned to the AI is to review a target prompt and decide if changes are needed based on specific criteria: checking for conflicting hardcoded instructions regarding reasoning effort, assessing parameter relevance, and identifying outdated documentation within the prompt. The goal is to ensure existing prompts are compatible with the new global reasoning control mechanism.",77ea69875b7c21bd5b6989620c5dd67a0a6e4b740fba1b726c4aa6df93d6cb27
context/cloud_function_call.py,"This Python script demonstrates how to invoke a specific Google Cloud Function secured by Firebase Authentication. It begins by importing the `requests` library, which is essential for making HTTP requests. The core functionality is encapsulated within the `call_cloud_function` function, which accepts a `firebase_token` as an argument. Inside this function, a constant `CLOUD_FUNCTION_URL` is defined, pointing to a specific endpoint hosted on `us-central1-prompt-driven-development.cloudfunctions.net`. The script constructs an HTTP GET request to this URL, including an `Authorization` header that carries the provided Firebase token formatted as a Bearer token. This header is crucial for authenticating the request against the Cloud Function. The function then returns the JSON content of the response received from the server. Finally, the script includes a placeholder variable `firebase_token` intended to be replaced with a valid user token, calls the `call_cloud_function` with this token, and prints the resulting output to the console.",62ac7724ce2e3eddbba2b743bd7aa5790dbed34eae01e77e0252c4cc6358c95e
context/install_completion_example.py,"This Python script serves as an example demonstration for the PDD shell completion installation module. It illustrates how to programmatically install shell completion scripts for the PDD CLI tool using functions imported from the `pdd.install_completion` package. The script primarily showcases two functions: `get_local_pdd_path()`, which retrieves the absolute path to the PDD directory, and `install_completion()`, which automates the process of detecting the current shell, locating the appropriate configuration file (like `.bashrc`), and appending the necessary source command.

To ensure safety and prevent modification of the user's actual environment, the script includes a `setup_example_environment()` function. This function creates a sandboxed environment by setting temporary environment variables (forcing the shell to `/bin/bash` and redirecting `HOME` and `PDD_PATH` to local dummy directories). It also generates dummy completion scripts and RC files. The `main()` function orchestrates the setup, prints the detected paths using the `rich` library for formatted output, and executes the installation process, providing a clear, risk-free example of how the underlying library functions operate.",999be17c0ce476845d7048a1a909ad425e60c298dbc7c74f51b596ef134a8aec
context/auto_include_example.py,"This Python script serves as an example usage demonstration for the `auto_include` function from the `pdd` library. The script begins by importing necessary modules, including `os`, `pandas`, and specific components from the `pdd` package. The core logic resides in the `main` function, which first reads the contents of a local file named ""project_dependencies.csv"". It then defines a detailed, multi-line string variable `input_prompt` that simulates a request for an AI agent to generate a Python unit test function named ""generate_test"". This prompt outlines specific inputs, outputs, and a step-by-step process involving Langchain, prompt preprocessing, LLM selection, and cost calculation. The script sets up additional parameters such as a directory path pattern (`context/c*.py`), a default strength value, a temperature setting of 0.5, and a verbose flag. Finally, it calls the `auto_include` function with these arguments to process the prompt and dependencies. The script concludes by printing the returned dependencies, the updated CSV output, the total cost of the operation, and the name of the model used, executing the `main` function only if run as the primary script.",2bae7ee830c9bcf7f5c45ce639a6b49337c0d55294fd91b51cd0717dcde11c78
context/bug_main_example.py,"This Python script serves as a demonstration or example file illustrating how to utilize the `bug_main` function from the `pdd` package. The script's primary purpose is to show how to automatically generate unit tests by comparing observed program outputs against desired outputs. 

The `main` function sets up a mock environment by creating a `click.Context` object configured with parameters like force overwrite, quiet mode, model strength, and temperature. It then programmatically generates several sample files in an `output` directory: a prompt file describing a function requirement (summing even numbers), a Python code file implementing that logic, a main program file executing the code, and two text files representing the current (buggy) output and the desired (correct) output.

Finally, the script calls `bug_main` with paths to these generated files to produce a unit test that captures the discrepancy between the current and desired behavior. It concludes by printing the generated unit test code, the cost of the operation, and the name of the AI model used, utilizing the `rich` library for formatted output.",662ecde39d05a5345533c4ae74c6b1031f016916ec2ff4764d34a26f7ec26172
context/anthropic_counter_example.py,"The provided file content consists of a Python script that demonstrates how to use the Anthropic API client to count tokens for a given text string. The code is currently entirely commented out. It begins by importing the `anthropic` library and initializing an instance of the `Anthropic` client. A sample string variable named `text` is defined with the value ""Sample text"". The script then attempts to calculate the token count for this text using the `client.count_tokens(text)` method. A comment within the code explicitly notes a limitation, stating that this specific method is ""not accurate for 3.0 and above models,"" suggesting it may be a legacy function or deprecated for newer model versions like Claude 3. Finally, the script includes a print statement intended to output the total number of tokens calculated. The entire block serves as a basic usage example for token counting with the Anthropic Python SDK, albeit with a warning regarding its applicability to newer models.",50d2554102428b7961b67ea6a5161d7c1099a9f4f5090e9eb124181a6e5d90ff
context/litellm_bedrock_sonnet.py,"This file contains a Python script demonstrating how to use the `litellm` library to interact with the AWS Bedrock API. Specifically, it imports the `completion` function and sets up a request to the `anthropic.claude-3-7-sonnet-20250219-v1:0` model hosted on Bedrock. The script includes commented-out lines for configuring necessary AWS environment variables such as access keys and region. It constructs a message payload asking ""What is the capital of France?"" and specifies a `reasoning_effort` parameter set to ""low"". Finally, the script prints the response received from the model completion call.",3a0bb8269d3a765b728a09be9fca29f7b932f2860111dde2a79f370192eefea4
context/load_prompt_template_example.py,"This Python script demonstrates how to load and display a specific prompt template using a custom utility. It imports the `load_prompt_template` function from the `pdd.load_prompt_template` module and the `print` function from the `rich` library for formatted output. The script defines a `main` function where it specifies a target prompt name, ""generate_test_LLM"". It then attempts to load this template. If the loading process is successful and a prompt is returned, the script prints a blue-styled header followed by the content of the loaded prompt to the console. Finally, the script includes a standard conditional block to execute the `main` function when the file is run directly.",a1cd6619182c6c951f5856dda4070e202875a5884bbfab9cc191d24de2f4951f
context/postprocess_0_example.py,"This file provides a concise usage example and documentation for the `postprocess_0` function from the `staging.pdd.postprocess_0` module. The example demonstrates how to import and utilize the function within a Python script (`example_usage.py`). It sets up a mock `llm_output` string containing mixed content—including text, multiple Python code blocks, and a Java code block—to simulate a typical Large Language Model response. The script then calls `postprocess_0` with the target language set to ""python"" and prints the result. The accompanying documentation details the input parameters: `llm_output` (the raw model response) and `language` (the target programming language). It explains that the function's output is a string where only the largest code section matching the specified language remains active, while all other text and code blocks are commented out. Finally, a note advises users to ensure that dependency functions like `get_comment`, `comment_line`, and `find_section` are available in the environment.",2ff3f65a972fbd46519fefbb1467687c4d9f2b269e14e3810eb1a8427b1b396e
context/fix_main_example.py,"This Python script serves as a demonstration and test harness for the `fix_main` function within the `pdd` package. It illustrates how to programmatically invoke the automated code-fixing functionality of the library. The script begins by setting up the environment, ensuring the project root is in the system path, and creating a directory of mock files. These mock files include a buggy Python calculator script (which subtracts instead of adds), a failing unit test, a prompt file describing the intended functionality, a verification script, and an error log.

The `main` function simulates a Click CLI context with specific configuration parameters like verbosity, LLM strength, and temperature. It then executes two distinct examples of the `fix_main` function. The first example runs in ""Loop Mode,"" demonstrating an iterative fixing process where the system attempts to repair the code up to three times using a verification program. The second example runs in ""Single-Pass Mode,"" performing a one-time fix attempt based on a pre-existing error log. The script concludes by printing the success status, number of attempts, cost, and the resulting fixed code for each scenario to the console.",29f88d8bfcc2256ebebe6efcb611f9487eb5ca6d8781286a6dd98689f92bca7a
context/fix_verification_main_example.py,"This Python script serves as a demonstration and test harness for a module named `fix_verification_main`. It sets up a local environment to simulate a code verification workflow. The script defines a helper function, `create_dummy_files`, which generates a temporary directory containing a prompt file, a Python code file with an intentional bug (a subtraction operation instead of addition), and a verification program designed to fail against that bug. The `main` function initializes a mock Click context with specific configuration parameters (such as verbosity, temperature, and local execution flags) to simulate command-line arguments. It then invokes the `fix_verification_main` function, passing in the paths to the dummy files and defining output locations for results. Finally, the script prints the execution results using the `rich` library, displaying whether the verification succeeded, the number of attempts made, the cost incurred, and the content of the fixed code if successful.",6490f398aca6df47b1a95e46a4ecfc0aabe299b5022de5c31ebc436e1bc3e459
context/generate_test_example.py,"This Python script demonstrates how to utilize the `generate_test` function from the `pdd` library to automatically create unit tests for a given piece of code. The script begins by importing necessary modules, including `os`, the `generate_test` function itself, and the `print` function from the `rich` library for formatted console output. It sets up several input parameters required for test generation: a natural language prompt describing the desired functionality (""Write a function that calculates the factorial of a number""), the actual source code implementation of a factorial function, a `strength` parameter set to 0.5, a `temperature` setting of 0.0 for deterministic output, and the target programming language specified as ""python"". The core logic is wrapped in a try-except block where it calls `generate_test` with these parameters. Upon success, it prints the generated unit test code, the estimated cost of the API call, and the name of the model used, utilizing `rich` tags for colored output. If an exception occurs during the process, it catches the error and prints a formatted error message in red.",f73533a9cf63f2e2f4818ee923a8500e612d155e230017dbdb35aec945e41998
context/continue_generation_example.py,"This Python script serves as a demonstration and entry point for utilizing the `continue_generation` function from the `pdd.continue_generation` module. The script defines a `main()` function that orchestrates the process of extending text generation using a language model. It begins by loading necessary context data from local files: a preprocessed prompt is read from `context/cli_python_preprocessed.prompt`, and an initial fragment of LLM output is read from `context/llm_output_fragment.txt`. 

The script sets specific configuration parameters for the generation process, including a `strength` of 0.915 and a `temperature` of 0. It then invokes the `continue_generation` function with these inputs and parameters, enabling verbose mode. Upon successful execution, the script captures the final generated text, the total cost of the operation, and the model name used. It prints the cost and model name to the console and saves the completed text output to a file named `context/final_llm_output.py`. The code includes error handling to manage potential `FileNotFoundError` or other general exceptions during execution.",29c33a967bcdf4fa0037e1c7b0ec699c00027606dbe9d93fa4d1434f9c16c19e
context/langchain_lcel_example.py,"This Python script demonstrates the implementation of various Large Language Models (LLMs) and chat models using the LangChain framework. It showcases how to integrate multiple providers, including OpenAI (GPT-4o, o1, o4-mini), Google (Gemini via VertexAI and GenerativeAI), Anthropic (Claude), DeepSeek, Groq, Together AI, Ollama, AWS Bedrock, and MLX. 

The code highlights key LangChain functionalities such as creating prompt templates, chaining components using the RunnableSequence syntax (pipe operator `|`), and utilizing caching with SQLite to optimize performance. It also features a custom `CompletionStatusHandler` callback to track token usage and completion reasons. 

A significant portion of the script focuses on structured output parsing. It defines Pydantic models (specifically a `Joke` class) to enforce specific JSON schemas on LLM responses, demonstrating both `JsonOutputParser` and `PydanticOutputParser`, as well as the model-specific `.with_structured_output()` method. Additionally, the script includes examples of configuring fallbacks between different models and setting up local inference pipelines using MLX and Ollama.",cffe5f77babeb7ac27dc42383a99a03f5a9f55ae1d6f2472e5f9863df0a84daf
context/ctx_obj_params.prompt,"This file documents the standard parameters available within the `ctx.obj` dictionary for PDD CLI commands. It details the keys populated by the main CLI group, including configuration options such as 'verbose' for debugging, 'force' for overwriting files, 'quiet' for suppressing output, and 'local' for execution mode. It also lists LLM-specific parameters like 'strength', 'temperature', and 'time' (relative thinking time). Additionally, the file explains the precedence logic for `strength` and `temperature` parameters: explicit function arguments override the values found in `ctx.obj`, allowing orchestrators to customize behavior dynamically.",829e2ab327e37b75afcd0dcf3893dcaec9012a89a1f6e5198ec7cfc0395d2782
context/example.prompt,"This file outlines specific guidelines for creating usage examples for a Python module, likely within a project named 'pdd'. It emphasizes that the environment is pre-configured with necessary variables and packages, so the focus should remain strictly on demonstrating module usage. Key instructions include using absolute imports (e.g., 'from pdd.module_name import module_name') because the example script runs from a different directory than the module itself. The guidelines specify that input and output units must be clearly documented. For Click-based CLI examples, arguments must be passed directly to ensure non-interactive execution. Additionally, the instructions mandate that any file operations—such as reading required input files or writing output files—must target a specific './output' directory, and the content for any necessary input files must be created within the example. Finally, the use of try/except blocks is discouraged to ensure errors are visible.",cf2eb7f018902c0ae9fbd70722cbd512236fd0de2aa15990b29cb65762fae321
context/python_preamble.prompt,"The file is a set of development instructions for implementing Python code in a package. It specifies that the module must begin with `from __future__ import annotations`, and that every function should include complete type hints. For console output, it mandates using `rich.console.Console` rather than standard `print`. Structurally, the code is expected to live inside a Python package and to use relative imports (single-dot form) when importing internal modules (e.g., `from .module_name import module_name`). It also notes that package-level constants such as `EXTRACTION_STRENGTH`, `DEFAULT_STRENGTH`, `DEFAULT_TIME`, and other global configuration values are defined in `./pdd/__init__.py` and should be imported from there (e.g., `from . import DEFAULT_STRENGTH`). Finally, it includes error-handling requirements: functions should gracefully handle edge cases like missing inputs or model-related failures and should emit clear, actionable error messages. Overall, the content serves as style, structure, and robustness guidelines for writing expert-level, maintainable Python package code.",f2d47592d2ca895fe3bc1071293dbbbfcb10dc922c2b762369f60ae827bc7e05
context/list_stats_example.py,"This Python script serves as a demonstration and usage example for a custom module named `list_stats`. The script begins by dynamically adjusting the system path to ensure the `list_stats` module can be imported from a specific relative directory structure (`../experiments/cloud_vs_local_fewshot_v2/src`). It includes error handling to warn the user if the import fails. The core functionality is encapsulated within a `main()` function, which defines a sample dataset of floating-point numbers and an empty list to showcase edge case handling. The script systematically executes and prints the results of four key statistical functions provided by the module: `safe_mean`, which calculates the average with a fallback for empty lists; `median`, which determines the middle value of the dataset; `variance`, demonstrating both population and sample variance calculations; and `percentile`, which computes specific statistical percentiles (e.g., 90th and 25th). Each section includes print statements to display the calculated results formatted to two decimal places, providing a clear verification of the module's capabilities.",46ae10331f07afe5a953030db2cb03cc02109518f4fa95b0b49dcfb784e85462
context/o4-mini-test.ipynb,"The provided file is a Jupyter Notebook containing a single Python code cell that demonstrates how to interact with the Azure OpenAI API. The script initializes an `AzureOpenAI` client using specific credentials, including an endpoint URL, a subscription key, and a deployment name ('o4-mini'). It constructs a chat completion request with a sequence of messages simulating a conversation about tourist attractions in Paris. Specifically, the conversation history includes a system prompt, a user query about what to see in Paris, an assistant response listing top sites (Eiffel Tower, Louvre, Notre-Dame), and a follow-up user question asking for details about the Eiffel Tower. The code then sends this request to the API and prints the content of the assistant's response. The notebook's output displays the generated text, which provides a detailed explanation of why the Eiffel Tower is a significant landmark, highlighting its architectural importance, iconic status, spectacular views, day-to-night experiences, and role as a cultural hub.",ce3de59ca52b7ca7623749f0a07887bd1dba25bc21da0b32a2a1c51c61b995f9
context/preprocess_main_example.py,"This Python script defines a command-line interface (CLI) tool using the `click` library to preprocess prompt files. The script imports necessary modules, including `preprocess_main` from a local `pdd` package. The core functionality is encapsulated in the `cli` function, which accepts several options: a required input prompt file path, an optional output path, flags for XML structuring, recursive processing, double curly bracket escaping, exclusion lists for escaping, PDD metadata injection, and verbose logging. 

Inside the `cli` function, the script initializes a context object with default settings for strength and temperature. It then invokes the `preprocess_main` function with the provided arguments to perform the actual processing logic. Upon successful execution, the script prints the preprocessed prompt, the total cost of the operation, and the model name used. It also includes error handling to catch and display exceptions that occur during the process. The script is designed to be executed directly as a standalone program.",dc4cf0483361ef94467d8936ceef30b54b62e61d658c916663407a8f142ec851
context/agentic_common_example.py,"This Python script serves as a demonstration and usage guide for the `pdd.agentic_common` module within the project. It illustrates how to programmatically interact with various AI agent providers (such as Claude, Gemini, or Codex) to perform headless tasks. The script begins by setting up the environment, specifically adding the project root to the system path to ensure correct imports. It then defines a `main` function that executes a workflow: first, it discovers available agents using `get_available_agents()`; second, it sets up a local output directory; and third, it defines a natural language instruction to create a Python file named `generated_math.py` containing a factorial function.

The core of the demonstration is the `run_agentic_task` function call, which executes the instruction in the specified working directory. The script captures and prints the execution results, including success status, the specific provider used, the estimated cost, and the agent's output message. Furthermore, it verifies the side effects of the task by checking if the requested file was actually created and printing its content. Finally, the script briefly highlights how to handle timeouts for complex, multi-step workflows using the `STEP_TIMEOUTS` dictionary, addressing specific orchestration needs.",5c2a5da7c0281a5277e775bda0e64e1de379f546b59eef359179052fafe7dea9
context/llm_output_fragment.txt,"The provided file, `pdd.py`, serves as the main entry point for a Python command-line interface (CLI) tool called ""pdd"" (Prompt-Driven Development). Built using the `click` library for command handling and `rich` for formatted console output, the script orchestrates various AI-driven development tasks. It defines a main command group `cli` that accepts global configuration options such as AI model strength, temperature, verbosity, and cost tracking.

The file implements several subcommands including `generate` (creating code from prompts), `example` (generating examples from code/prompts), `test` (creating unit tests), `preprocess` (handling prompt file formatting and XML tagging), `fix` (iteratively repairing code based on error logs), and `split` (breaking down complex prompts). Each command utilizes helper functions imported from other modules within the `pdd` package (e.g., `construct_paths`, `code_generator`, `fix_error_loop`) to perform specific logic. The script handles file I/O, manages execution costs, and provides user feedback via the console, effectively acting as the central controller for an AI-assisted coding workflow.",dc07a5094cc7e87b5536a2e3dc7e7e688c3f03b00c8b48dc422f97e28d7f9a59
context/fix_error_loop_example.py,"This Python script serves as an example demonstrating the usage of the `pdd.fix_error_loop` module. It illustrates an automated workflow for repairing buggy code using unit tests and an LLM-based repair process. The script first sets up a temporary environment by creating mock files in an output directory: a buggy `calculator.py` file (where an addition function incorrectly performs subtraction), a failing unit test file `test_calculator.py`, a prompt file describing the desired functionality, and a verification script. It then configures parameters such as LLM strength, temperature, maximum attempts, and budget before invoking the `fix_error_loop` function. This function iteratively runs tests, detects failures, consults an LLM for fixes, and applies them until the tests pass or the attempt limit is reached. Finally, the script prints the results of the operation, including success status, the number of attempts used, the total cost, and the final fixed code.",09aeef3c02612c54ba643feca7e2e692adaa1cec9fa1217a61507fec031929cc
context/conflicts_main_example.py,"This Python script demonstrates the usage of the `conflicts_main` function from the `pdd.conflicts_main` module, likely designed to analyze or compare prompts for conflicts. The script begins by creating two sample prompt files, `prompt1_LLM.prompt` and `prompt2_LLM.prompt`, each containing a simple instruction for an AI assistant. It then defines a `MockContext` class to simulate a Click command-line interface context, initializing it with an empty object attribute to mimic the environment expected by the main function. The core of the script executes `conflicts_main`, passing in the mock context, the paths to the two generated prompt files, and a destination path for the output CSV file. The function call is set to be verbose. Finally, the script prints the results returned by the function, specifically the identified conflicts, the total cost of the operation formatted to six decimal places, and the name of the model used. The output indicates that detailed results are saved to `outputconflicts_output.csv`.",36e9f73fd888af26dc73f35ee765715e99e4325881bc0ffdc5d70106eda7e239
context/fix_code_loop_example.py,"This Python script serves as a demonstration and test harness for a module named `fix_code_loop`. Its primary purpose is to illustrate how an automated code repair loop functions by setting up a controlled environment with a known bug. The script defines a `create_dummy_files` function that generates a directory containing a buggy Python script (`calculator.py` which fails to cast string inputs to integers), a corresponding verification script (`verify_calculator.py` which tests the addition logic), and a prompt file. The `main` function configures execution parameters—such as maximum attempts, budget, and LLM settings (temperature, strength)—and invokes the `fix_code_loop` function to attempt an automated repair of the buggy code. Finally, it reports the results of the operation to the console using the `rich` library, displaying success status, total attempts, cost, and the final fixed code.",c512f0f317474550a9ed74b5211ae0eab62c3f04115db14af67a88033aacfa87
context/fix_verification_errors_example.py,"This file is a Python example script demonstrating the usage of the `fix_verification_errors` function from the `pdd` package. The script illustrates an automated workflow for identifying and repairing bugs in code modules using Large Language Models (LLMs) based on verification logs. It sets up a simulated scenario involving a buggy `calculator_module` (which subtracts instead of adds) and a `program_code` script that verifies the module's output. The example defines necessary inputs such as the original prompt, the buggy code, the verification failure output, and LLM configuration parameters like strength and temperature. It then executes `fix_verification_errors`, which analyzes the failure logs to propose fixes. Finally, the script prints the results, including whether issues were found, if the code or program was updated, the specific fixes applied, the LLM model used, and the associated cost.",087d89544db864fe4c8025124ce2fd3fdf9f621c988bd415ba9a0ba942d2f497
context/operation_log_example.py,"This Python script serves as a comprehensive usage example for the `operation_log` module, which provides logging infrastructure for tracking PDD (Prompt Driven Development) operations. The file demonstrates how to implement shared logging for both manual CLI invocations and automated sync-initiated tasks. It imports key functions from `pdd.operation_log` and defines three primary demonstration functions: `demonstrate_log_operations`, which covers creating, updating, and appending log entries for different invocation modes; `demonstrate_module_identity`, which shows how to infer module names and languages from file paths; and `demonstrate_state_management`, which illustrates saving fingerprints and run reports to track file states and test results. Additionally, the script includes examples of Click CLI commands decorated with `@log_operation`. This decorator automates the logging lifecycle—inferring identity, clearing stale reports, creating log entries, executing the function, and updating records upon success—showcasing how to integrate logging seamlessly into command-line tools.",6cd7962ab255d6080aeb97897175250f987ce936cf7333e7601f10834405a940
context/code_generator_example.py,"This Python script serves as a demonstration and entry point for utilizing a `code_generator` function imported from the `pdd.code_generator` module. The script defines a `main()` function that orchestrates the process of generating code using a Large Language Model (LLM). It begins by reading a specific prompt from a file located at `prompts/generate_test_python.prompt`. It then configures several parameters for the generation process, including setting the target programming language to Python, defining a model strength of 0.5, setting the temperature to 0.0 for deterministic output, and enabling verbose logging.

Inside a try-except block to handle potential runtime errors, the script calls the `code_generator` function with these parameters. Upon successful execution, it captures three return values: the generated runnable code, the total cost of the API call, and the name of the model used. Finally, the script prints these results to the console, displaying the generated code block, the cost formatted to six decimal places, and the model identifier. The script follows standard Python practices by using the `if __name__ == ""__main__"":` idiom to ensure the `main` function runs only when the script is executed directly.",4e66ed9866b05a2de791376d4881ae5d1d52ac4a6ea99176dce8febdf9a14355
context/agentic_change_example.py,"This Python script serves as an example demonstration of the `run_agentic_change` workflow within the `pdd` project. It illustrates how to programmatically trigger an agentic change process based on a GitHub issue URL. To avoid making actual API calls or executing complex logic during the demonstration, the script uses `unittest.mock.patch` to mock the internal `run_agentic_change_orchestrator`. It simulates a successful execution scenario, returning predefined values for success status, completion messages, cost, the model used (Anthropic), and a list of modified files. The `main` function sets up the environment by adding the project root to the system path, executes the mocked workflow, and then prints a structured summary of the results, including the simulated cost and changed files, concluding with advice on the next steps.",be4da4499b5007ee7e8e7b464dc9f0cb5c9a04aaf5de192ae9cbb0133db06d85
context/pytest_example.py,"This Python script defines a custom test runner mechanism using `pytest` to programmatically execute tests while capturing detailed results and logs. It introduces a `TestResultCollector` class, which acts as a plugin to hook into pytest's execution lifecycle. This collector tracks the number of test failures, errors (occurring during setup, call, or teardown phases), and warnings. It also includes methods to redirect standard output and standard error to an in-memory string buffer, allowing the script to capture all console output generated during the test run.

The script includes a `run_pytest` function that instantiates the collector, initiates log capturing, and runs `pytest.main` specifically targeting the test file `tests/test_get_extension.py`. It ensures that standard output streams are reset even if errors occur. Finally, the `if __name__ == ""__main__"":` block serves as an entry point, printing the current working directory and executing the test runner. It outputs the final counts of failures, errors, and warnings, along with the full captured logs, to the console for verification.",f13c02d911d386ac5bab8e3d4aae64a768e4a7e960803c80fcac14e9ba5975cb
context/pytest_output_example.py,"This Python script serves as a demonstration and usage guide for a module named `pdd.pytest_output`. It defines a `main_example` function that systematically showcases different ways to interact with the module. The script begins by creating a dummy Python test file containing passing, failing, error-raising, and warning-triggering tests. It then proceeds through six distinct examples: invoking the module's main function via simulated command-line arguments, programmatically running tests and capturing output to JSON, handling non-existent files, handling non-Python files, processing empty test files, and advanced usage involving direct interaction with a `TestResultCollector` class to capture logs and test statuses. The script utilizes the `rich` library for formatted console output to clearly distinguish between these examples.",6f0cd68e3c4440081267c716651caec4fbdd7d9c4516f967517f02ae2a853920
context/change_example.py,"This Python script serves as a demonstration and entry point for utilizing the `change` function from the `pdd.change` module. The script begins by importing necessary libraries, including `os`, the `change` function itself, and `Console` from the `rich` library for enhanced terminal output. A `main` function is defined to orchestrate the execution flow. Within this function, the script sets up example parameters required for the `change` operation: an initial `input_prompt` requesting a factorial function, a corresponding snippet of `input_code` implementing that factorial logic, and a `change_prompt` instructing a modification to calculate the square root of the result. It also defines configuration parameters such as `strength` and `temperature` to control the behavior of the underlying Large Language Model (LLM). The core logic is wrapped in a try-except block where the `change` function is invoked with these inputs. Upon successful execution, the script uses the `rich` console to print the returned `modified_prompt`, the total cost of the operation, and the name of the model used. If an exception occurs during the process, an error message is displayed in bold red text.",cc5f850c8e41a8c5b82c94b6667975d1811dff1f51888bb8caf4ff69815f8352
context/final_llm_output.py,"This file defines a command-line interface (CLI) application for ""PDD"" (Prompt-Driven Development) using the `click` and `rich` libraries. The CLI provides a suite of tools to facilitate AI-assisted coding workflows, including generating code from prompts (`generate`), creating examples (`example`), generating unit tests (`test`), preprocessing prompts (`preprocess`), fixing code errors (`fix`), splitting complex prompts (`split`), modifying prompts (`change`), and updating prompts based on code changes (`update`). It also includes utility commands for installing shell completion and checking the version. The application supports global options for controlling AI model parameters (strength, temperature), verbosity, and cost tracking. The code relies on various imported modules from the `pdd` package to handle the core logic for each command, such as `code_generator`, `context_generator`, and `fix_error_loop`.",69322ebbb5b23b8d0f7c29f9f25fe28aff81a55cbed4fbd8be30652aa894cf47
context/agentic_e2e_fix_example.py,"This Python script serves as an example usage guide for the `agentic_e2e_fix` module, specifically demonstrating how to invoke the `run_agentic_e2e_fix` function. The script illustrates the entry point for an automated end-to-end fix workflow that parses a GitHub issue URL and orchestrates a multi-step repair process. It highlights cross-machine capabilities, noting that the tool can automatically locate worktrees on the same machine or detect branch mismatches on different machines, aborting unless forced. The `main` function sets up a sample call with parameters such as `issue_url`, `timeout_adder`, `max_cycles`, and flags for resuming state or forcing execution. Finally, it includes logic to handle the execution results, printing success metrics like total cost and changed files, or reporting failure messages for common issues like branch mismatches or missing dependencies.",9d78fd50a5bd3510f4a13caf19214e3d0e07f6149e59e666d35e6c2d0e0f4b1d
context/vertex_ai_litellm.py,"This Python script demonstrates how to use the `litellm` library to interact with Google's Vertex AI Gemini models, specifically the `gemini-2.5-pro-preview-05-06` version. The script begins by importing necessary modules, including `completion` from `litellm`, `json`, and `os`. It enables debug logging for the library by setting the `LITELLM_LOG` environment variable. The code then retrieves the path to a Vertex AI credentials file from the `VERTEX_CREDENTIALS` environment variable, reads the JSON content from this file, and converts it into a JSON string format required for authentication. Finally, it invokes the `completion` function to send a chat completion request. This request includes the model identifier, a sequence of messages (a system prompt and a user greeting), the serialized credentials, the Google Cloud project ID (`meta-plateau-401521`), and the location (`us-central1`). The script concludes by printing the response received from the Vertex AI service.",ffbbe9dcdec01683ffe96cb1f027dbf1eb095f4f93158ffc2a47a443f2bbe699
context/auth_service_example.py,"This Python module, intended for use within the `pdd.auth_service` package, provides a suite of helper functions for managing authentication in a CLI environment. It primarily handles the storage, retrieval, and validation of authentication tokens, specifically JSON Web Tokens (JWTs) and refresh tokens. The module defines constants for token storage locations, utilizing a local cache file for JWTs and the system keyring for sensitive refresh tokens. 

Key functionalities include checking for the existence and validity of cached JWTs and stored refresh tokens, clearing these tokens to facilitate logout operations, and retrieving the current authentication status. It includes functions like `get_jwt_cache_info`, `has_refresh_token`, `logout`, and `get_cached_jwt`. Additionally, it offers an asynchronous `verify_auth` function for deep validation, which attempts to refresh expired tokens to ensure a valid session. The file concludes with extensive commented-out examples demonstrating how to integrate these functions into CLI commands for status checks, logging out, making API calls with bearer tokens, and performing deep authentication verification.",9dc49bee224f626f94777a252ef3571d6d313feaf1262fae3a795b7a5d0d2c7c
context/sync_determine_operation_example.py,"This Python script serves as a demonstration and test harness for the `sync_determine_operation` function within the `pdd` module. It simulates a project environment by creating a temporary output directory and generating necessary file structures (prompts, code, tests, and metadata). The script walks through four distinct scenarios to showcase how the system decides on the next operation: a new unit creation, handling test failures, detecting manual code changes (drift), and verifying a fully synchronized state. For each scenario, it sets up the specific file conditions (including calculating SHA256 hashes and creating JSON metadata), calls the decision logic, and prints the resulting operation and reasoning to the console.",851e0759a0d08e1a567879a98c9e6fd14b310c324a8c11e03bdfdaf562a543e7
context/gcs_hmac_test.py,"This Python script demonstrates how to upload a text file to a Google Cloud Storage (GCS) bucket using the `boto3` library and GCS HMAC keys. It begins by loading necessary configuration settings—specifically the GCS Access Key ID, Secret Access Key, and Bucket Name—from environment variables using `dotenv`. The script configures logging to track execution flow and errors. It then initializes an S3 client via `boto3`, pointing the endpoint URL to `https://storage.googleapis.com` to interface with GCS instead of AWS S3. The core functionality involves attempting to upload a simple string content as a file named `test-hmac-upload.txt` to the specified bucket. The code includes robust error handling, catching `ClientError` exceptions to provide specific feedback on issues like missing buckets or authentication failures, as well as general exceptions. Finally, it prints a verification URL upon success or logs detailed error messages upon failure.",965d3bb183e7e3898690b121a76ae0c57eb96425e7b670cecc50b829ea003bab
context/device_flow.txt,"This document outlines the GitHub Device Flow, an authorization method designed for headless applications like CLI tools or the Git Credential Manager. Before implementation, the flow must be enabled in the application's settings. The process involves three main steps. First, the application requests device and user verification codes from GitHub via a POST request to `https://github.com/login/device/code`, providing a client ID and scope. This returns a device code, a user code, a verification URI, an expiration time (default 15 minutes), and a polling interval.

Second, the application prompts the user to visit the verification URI and enter the provided user code. Third, the application polls GitHub at `https://github.com/login/oauth/access_token` using the device code and a specific grant type to check for authorization. Polling must adhere to the specified interval to avoid rate limits. Once the user authorizes the device, the API returns an access token. The document also details response formats (JSON, XML), rate limits (including a `slow_down` error that increases the polling interval), and various error codes such as `authorization_pending`, `expired_token`, and `access_denied`.",ae0cab6a414e4d7189c2025ef677fb7e6d1b800325f62bbb6de68a1505a5cdac
context/click_executor_example.py,"This Python file demonstrates how to programmatically execute Click commands within a server environment, specifically designed for a ""PDD Server"" context. It provides a robust framework for running CLI commands in isolation, capturing their output, and handling errors without relying on a standard terminal interface.

The core component is the `ClickCommandExecutor` class, which manages the execution lifecycle. It uses a `create_isolated_context` helper to manually construct `click.Context` objects, allowing for the injection of shared state (via `ctx.obj`) and the mocking of parameter sources. To manage command output, the script implements `OutputCapture` and `StreamingWriter` classes. These utilities intercept `stdout` and `stderr`, buffering the content for the final result while simultaneously supporting real-time streaming through callback functions.

The implementation encapsulates execution results in a `CapturedOutput` dataclass, which stores output streams, exit codes, and exceptions. It includes specific error handling for `click.ClickException` and `click.Abort` to ensure graceful failures. The file concludes with a `main` function that provides practical examples of executing commands with parameters, handling errors, and simulating real-time output streaming.",322d322bcbe2672e9adb1d52744018f38ae6ae3209711441816f923727760b8e
context/no_include_conflicts_in_prompts_python.prompt,"The file outlines a specification for an expert Python engineer to create a function named ""conflicts_in_prompts"". This function is designed to analyze two input prompts (""prompt1"" and ""prompt2"") to identify conflicts and propose resolutions. It utilizes Langchain and a custom ""llm_selector"" module to process the prompts through a multi-step workflow. The process involves loading specific prompt templates from a project directory, running an initial LLM pass to generate a conflict analysis, and then performing a second LLM pass to extract structured JSON data containing descriptions, explanations, and resolution suggestions for each conflict. The function also calculates and reports the token usage and total cost of the operations, returning a list of conflict dictionaries along with the cost and model name.",09137b6ae364af16795a3066a06d28d6ebfdc7d42d83fad4c068e75bfbad77ab
context/whitepaper.md,"This white paper introduces Prompt-Driven Development (PDD), a transformative methodology that shifts the central artifact of software engineering from source code to high-level prompts. Arguing that traditional and current AI-assisted coding practices struggle with escalating complexity and business misalignment, the paper proposes PDD as a solution where developers define ""what"" software should do via detailed prompts, while advanced AI handles the implementation details.

The document outlines the PDD workflow, emphasizing that prompts encapsulate intent, requirements, and constraints, thereby bridging the gap between technical and non-technical stakeholders. Key advantages highlighted include increased productivity, reduced technical debt, improved code consistency, and better scalability. Crucially, the paper addresses potential adoption challenges—such as debugging AI-generated code or managing manual edits—by detailing specific PDD command-line tools (e.g., ""pdd trace"", ""pdd update"") that maintain synchronization between prompts and code. It concludes that PDD is not merely an incremental improvement but a necessary paradigm shift, empowering organizations to deliver high-quality, adaptable software that aligns closely with business objectives in a rapidly evolving landscape.",e8bd279d08e21590bf548de02f5cc9486c868b534efbba4051ed3f324bcff975
context/firecrawl_example.py,"This file contains a Python script demonstrating the basic usage of the `firecrawl-py` library to scrape a website. The script begins by importing the `FirecrawlApp` class from the `firecrawl` module and the standard `os` module. It initializes an instance of `FirecrawlApp`, attempting to retrieve the API key from an environment variable named `FIRECRAWL_API_KEY`, with a fallback placeholder provided. The core functionality is demonstrated by calling the `scrape_url` method on the app instance, targeting 'https://www.google.com' and specifying 'markdown' as the desired output format. Finally, the script prints the resulting markdown content from the scrape operation to the console. A comment at the top of the file provides the necessary pip installation command for the library.",9dbfc798c1670a244e5c2d8822e065ce2800c2d861dcd4e4e2a02874e9de854f
context/agentic_bug_orchestrator_example.py,"This Python script serves as an example usage demonstration for the `agentic_bug_orchestrator` module. It simulates a bug investigation workflow without requiring actual LLM calls or a live GitHub repository by mocking internal dependencies. The script sets up a scenario involving a ""ZeroDivisionError"" in a calculator application and defines mock functions for `load_prompt_template` and `run_agentic_task`. These mocks simulate the outputs of a 9-step agentic process, including checking for duplicates, reproducing the bug, identifying the root cause, generating tests, and creating a pull request. The `main` function initializes dummy issue data, patches the dependencies using `unittest.mock`, executes the `run_agentic_bug_orchestrator` function, and prints the final results, including success status, total simulated cost, and changed files.",99ed69d9088094db35843f1a69364195459d089b9f8f522481cce8da244b58ec
context/context_generator_main_example.py,"This Python script serves as a demonstration and test harness for the `context_generator_main` function from the `pdd` package. The script defines a `run_example` function that simulates the workflow of generating example code based on a prompt and an existing source file. It begins by setting up a local output directory and creating dummy input files: a prompt file describing a math utility task and a corresponding Python source file containing a simple addition function. The script then mocks a `click.Context` object to simulate command-line arguments, configuring parameters such as model strength, temperature, and verbosity. It subsequently calls `context_generator_main` with these inputs to generate example usage code. Finally, the script prints the results, including the model used, the calculated cost, the output file path, and a snippet of the generated code. The main execution block also ensures necessary environment variables are set before running the example.",f6c8f39d0177c122a3bc49594922c16aee750f5eb0f56dc5c48c231578617de9
context/split/9/cli_python.prompt,"This file contains a detailed prompt for an LLM to generate a Python command-line interface (CLI) tool named `pdd`. The tool is designed to be an expert-level engineering utility built using the Python `Click` library. The prompt outlines the structure of the program, including directory organization and specific internal modules. It provides extensive instructions and examples for implementing various commands, such as `change`, `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `update`, `detect`, `conflicts`, `crash`, `trace`, `bug`, and `auto-deps`. 

The prompt includes references to external context files (via `<include>` tags) for specific logic implementations like `auto_update`, `track_cost`, and `construct_paths`. It also specifies requirements for an `install_completion` command, detailing logic for shell detection and configuration file management. Finally, it mandates the use of `importlib.resources` for path handling if environment variables are missing and emphasizes consistency and coding standards across all generated commands.",e587e1338016f4702249d76538ccab4778670edde1a3000a983cc9fcfbed3909
context/split/9/install_completion.py,"The provided code snippet defines a single Python function named `install_completion`. This function is decorated with `@cli.command(name=""install_completion"")`, indicating that it is likely part of a command-line interface (CLI) application, possibly built using a library like `click` or a similar framework. The purpose of this command appears to be the installation of shell completion scripts, allowing users to use tab-completion for the CLI tool's commands in their terminal environment. The function body itself is empty in the provided snippet, serving only as the command definition entry point.",1eac80184769c28eccb35e1baba1eeded8b464cd01de7129c386d9a28ca50cd1
context/split/9/sub_cli.prompt,"This document outlines the requirements for an AI coding assistant to generate a Python function named `install_completion` for the `pdd` command-line tool. The function's primary purpose is to automate the installation of shell completion scripts for the user's current shell environment. The prompt details specific steps the function must execute: detecting the current shell (using `ps`, `$0`, or environment variables), locating the appropriate completion script based on the shell type, and updating the user's shell configuration file (RC file) to source this script. It specifies error handling procedures, such as using `click.Abort()` and printing colored error messages with `rprint` if the shell is unsupported or the script is missing. Additionally, the prompt requires the inclusion of several helper functions (`get_shell_rc_path`, `get_current_shell`, `get_completion_script_extension`, and `get_local_pdd_path`) to support the main logic.",278c01b32695237e99ecac7be5d17556c458277bea94722c2a3c6f231f512d70
context/split/9/cli.py,"This file, `cli.py`, serves as the main entry point for the PDD (Prompt-Driven Development) command-line interface. It utilizes the `click` library to define a robust CLI structure with global options for controlling AI model behavior (strength, temperature), verbosity, and cost tracking. The script dynamically determines the package path at runtime and handles shell completion installation for Bash, Zsh, and Fish.

The file aggregates functionality from various sub-modules, exposing them as distinct commands. Key commands include `generate` for creating code from prompts, `test` for generating unit tests, `fix` for iteratively repairing code based on errors, and `crash` for resolving runtime crashes. Other utilities include `preprocess` for prompt formatting, `split` for breaking down complex prompts, `update` for synchronizing prompts with code changes, and `auto_deps` for managing project dependencies. The CLI also includes features for conflict detection, change analysis, and bug reproduction via unit test generation. A `track_cost` decorator is applied to most commands to monitor API usage, and an auto-update mechanism checks for new versions upon execution.",d4f1f5bb29900f56ef676ca2cf14f17f69603696455ed1a692e25540a3e778b8
context/split/9/modified_cli.prompt,"This file contains a detailed specification and prompt for an AI to generate a Python command-line interface (CLI) tool named `pdd`. The tool is to be built using the `click` library and is structured around a main `cli` function. The document outlines the directory structure, including folders for prompts, context, and data. It provides specific instructions and examples for implementing various sub-commands such as `change`, `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `update`, `detect`, `conflicts`, `crash`, `trace`, `bug`, `auto-deps`, and `install_completion`. Each command references internal modules and functions (e.g., `change_main`, `code_generator_main`, `track_cost`) that handle the core logic, often illustrating usage with included example files. Additionally, it mentions utility features like automatic updates via `auto_update` and path construction, ensuring the generated code follows consistent standards and correctly handles environment variables like `PDD_PATH`.",8b8c7363455b4cb630ef90f7faf248227f5a7a2e5ef74d03f8c4e35814bf3086
context/split/7/modified_cli_python.prompt,"This document provides a comprehensive prompt for an AI expert Python engineer to generate a command-line interface (CLI) tool named `pdd`. The tool is to be built using the Python `click` library and is structured to include a main `cli.py` file along with specific directories for prompts, context, and data. The prompt details the implementation of numerous commands, including `generate` (for code generation), `example` (for context generation), `test` (for unit tests), `preprocess` (for prompt processing), `fix` (for error correction), `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`. It provides specific instructions on importing internal modules to avoid naming conflicts, using the `@track_cost` decorator, and handling file paths via a `construct_paths` helper. Additionally, it includes instructions for an `install_completion` command to manage shell auto-completion setup.",4bd93aa7ba1298dfaab089c32e39fe5082d1c3e4166456c54692b6f16e74a71e
context/split/7/cli_python.prompt,"This document provides a comprehensive specification for an AI to generate a Python command-line interface (CLI) tool named ""pdd"". The tool is to be built using the Python `click` library and serves as a developer utility for managing code generation, testing, and prompt engineering workflows. The specification outlines the directory structure, imports, and specific functionality for various commands including `generate` (code generation), `test` (unit test creation), `preprocess` (prompt handling), `fix` (error correction loops), `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`. It includes detailed instructions on how to interface with internal modules (e.g., `code_generator`, `track_cost`, `construct_paths`) and provides examples for implementing each command, ensuring proper naming conventions to avoid conflicts between CLI commands and imported function names.",198890ab03025d5e8585b77375cd396d29d6961c758bfd96627c94c83c5aec07
context/split/7/trace_main_python.prompt,"This file contains a prompt specification for an expert Python engineer to generate the `trace_main` function for the `pdd` command-line program. The function is designed to handle the logic for a `trace` command, which links a specific line of generated code back to its origin in a prompt file. The prompt details the function's inputs (Click context, file paths, line number) and expected outputs (prompt line number, cost, model name). It outlines a six-step process: constructing file paths using internal utilities, loading file contents, performing the trace analysis via a `trace` function, saving results, providing user feedback with the Rich library, and implementing robust error handling. The prompt also references included context files for Python preambles and examples of using the Click library and internal modules like `construct_paths` and `trace`.",dfd02a104fd1e4f96aeb0feacd9bf17091e54924544d748b09ce03ce3039fee5
context/split/7/cli.py,"This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` library. It serves as the main entry point for the application, orchestrating various AI-driven development tasks. The CLI provides a suite of commands including `generate` (creating code from prompts), `example` (generating examples from code), `test` (creating unit tests), `preprocess` (formatting prompts), `fix` (iteratively repairing code based on errors), `split` (breaking down complex prompts), `change` (modifying prompts based on new requirements), `update` (syncing prompts with code changes), `detect` (identifying necessary changes), `conflicts` (resolving prompt conflicts), `crash` (fixing crashing modules), `bug` (generating tests from bug reports), and `trace` (mapping code lines back to prompt lines). It also includes a utility command to install shell completion. The script handles global options for AI model configuration (strength, temperature), verbosity, and cost tracking. Each command wraps specific backend logic imported from other modules, manages file I/O, tracks API costs via a decorator, and uses the `rich` library for formatted console output.",d62918ef5903ffb5fb0b194a9321cf8742f2d51ace5e2ee3bec9e9e0e8d266bd
context/split/7/split_trace_main.py,"The provided code snippet defines a command-line interface (CLI) command named `trace` using the `click` library. This command is designed to analyze the relationship between a prompt file and a specific line of generated code. It accepts three required arguments: `prompt_file` (a path to an existing file), `code_file` (a path to an existing file), and `code_line` (an integer representing the line number). Additionally, it supports an optional `--output` argument to specify where the analysis results should be saved. The function is decorated with `@track_cost` and `@click.pass_context`. The implementation of the `trace` command delegates its core functionality to a separate function called `trace_main`, passing along the context and all provided arguments. A comment in the code notes that `trace_main` encapsulates the logic previously contained within `trace`, suggesting a refactoring to split functionality.",d9a7d733a1fd74b5d3d0e25c298bef38b6951880f0f86f645cfe07cde092905f
context/split/6/modified_cli_python.prompt,"This file contains a detailed prompt for an AI to generate a Python command-line interface (CLI) tool named `pdd`. The tool is designed to be built using the `click` library and serves as a utility for managing code generation, testing, and prompt engineering workflows. The prompt outlines the directory structure, imports, and specific instructions for implementing various commands such as `generate` (for code generation), `example` (for context generation), `test` (for unit test creation), `preprocess` (for prompt processing), `fix` (for error correction), and others like `split`, `change`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`. It provides explicit mapping between CLI commands and internal function names to avoid namespace conflicts and references numerous external example files for implementation details.",11221fcb359e67d91ba305ba8c1c428def99b81ba9422d7fe5954171d2962e85
context/split/6/cli_python.prompt,"This file contains a prompt for generating a Python command-line interface (CLI) tool named `pdd` using the `click` library. It outlines the program's directory structure, specific import strategies to avoid naming conflicts between internal functions and CLI commands, and provides detailed instructions and examples for implementing various commands. These commands include functionality for code generation (`generate`), creating examples (`example`), unit testing (`test`), preprocessing prompts (`preprocess`), fixing errors (`fix`), splitting prompts (`split`), changing prompts (`change`), updating prompts (`update`), detecting changes (`detect`), resolving conflicts (`conflicts`), handling crashes (`crash`), tracing execution (`trace`), and converting bugs to tests (`bug`). It also includes instructions for a shell completion installation command.",af6414f4512dc36571632bacff6d084783df93bd88319774470b5abaeda55870
context/split/6/split_conflicts.py,"The provided code snippet defines a command-line interface (CLI) command named `conflicts` using the `click` library. This command is designed to analyze two input prompt files, specified as arguments `prompt1` and `prompt2`, to identify potential conflicts between them and suggest resolutions. It also accepts an optional `--output` argument to specify a destination for saving the analysis results as a CSV file. The function is decorated with `@track_cost` and `@click.pass_context`, indicating it likely tracks resource usage and accesses the CLI context. The implementation delegates the core logic to a separate function called `conflicts_main`, passing along the context and arguments, which suggests a refactoring or separation of concerns where the main processing logic is decoupled from the CLI command definition.",ed275ba6d2c4742df3bab8eb78f6cb38b855677364244b0fc205625d73c579ee
context/split/6/conflicts_main_python.prompt,"This file contains a prompt for an expert Python engineer to generate the `conflicts_main` function for a command-line tool called `pdd`. The function is designed to handle the logic for a `conflicts` command, which analyzes potential conflicts between two prompt files. The prompt specifies the function's inputs (Click context, two prompt file paths, and an optional output path) and expected outputs (a list of conflict results, total cost, and model name). It outlines a six-step process for the function: constructing file paths using `construct_paths`, loading the input files, analyzing conflicts via `conflicts_in_prompts`, saving the results to a CSV file, providing user feedback using the Rich library, and implementing robust error handling. The prompt also references included context files for Python preambles and examples of using the Click library and internal modules.",3a99cdbb40bf5d080247c2417133cb5d811219d3283f5d7c816149c1ed69388a
context/split/6/cli.py,"This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the Python `click` library. It serves as the main entry point for an application designed to assist developers in generating, testing, fixing, and managing code through AI-driven prompts. The CLI exposes a variety of commands including `generate` (creating code from prompts), `test` (creating unit tests), `fix` (iteratively repairing code based on errors), `update` (modifying prompts based on code changes), and `detect` (analyzing necessary changes). 

The script handles global configuration options such as AI model strength, temperature, verbosity, and cost tracking. It integrates with various backend modules to perform specific tasks like preprocessing prompts, splitting complex files, tracing code lineage, and generating bug reproduction tests. Additionally, it includes utility commands for installing shell completion and managing file paths. The application emphasizes developer workflow automation by linking natural language prompts directly to executable code and test suites, while providing feedback on API costs and model usage.",aeaaceab84a98e9cb777d691f1a9de4aa3538569ce0fa0b732228c8dfbae51cf
context/split/1/sub_pdd_python.prompt,"The file outlines a task for an expert Python engineer to create a function named ""get_extension"". This function is designed to accept a programming language name (e.g., Bash, Makefile, Python) as a string input and return the corresponding file extension. The specified logic for the function involves three main steps: first, converting the input language string to lowercase to ensure case-insensitive comparison; second, performing a lookup to find the appropriate file extension associated with that language; and third, returning the identified extension as a string.",45146709fd9f1032380de677025bc37b7cdfb5c6e9a4269ff7ddaf0d0e6be562
context/split/1/split_get_extension.py,"The provided file content consists of a single line of code, likely written in Python or a similar high-level programming language. This line performs an assignment operation where a variable named 'file_extension' is being set to the return value of a function call. Specifically, the function 'get_extension' is invoked with a single argument, 'language'. This suggests that the purpose of this code snippet is to determine the appropriate file extension (e.g., '.py', '.js', '.txt') associated with a specific programming language or locale provided as input. The context implies a utility or helper function scenario where dynamic file handling or generation is taking place based on language parameters. Due to the extreme brevity of the content, no further logic, error handling, or context is visible.",9e3fc213b536b755722c8c4c407d92e88d22e64ab0d491b0c5e1d22b746f2500
context/split/1/final_pdd_python.prompt,"The provided text outlines the specifications for a Python command-line utility named ""pdd"". This tool is designed to compile prompt files into runnable code or generate example code from existing source files. It utilizes the Python `rich` library for formatted console output. The program accepts input files following a specific naming convention (`basename_language.prompt`) and supports flexible output configurations, allowing users to specify filenames, paths, or rely on defaults derived from the input basename. Key features include handling file overwrites via a `--force` flag, and specific flags (`-o`, `-oe`) to control the generation and location of runnable code and example files. The workflow involves parsing the input filename to determine the language and extension, resolving output paths based on user arguments, and then invoking internal generators (`code_generator` or `context_generator`) to produce the final files.",4337784a5517d6e80fac11df769749c916c003261b27b36d6d5e6c7b580af78c
context/split/1/pdd.py,"The provided file contains the Python implementation for a command-line interface (CLI) tool named `pdd`. This tool is designed to automate the generation of runnable code from prompt files and subsequently create example usage code. It leverages the `argparse` library for parsing command-line arguments and the `rich` library for enhanced console output and user interaction.

The script's core functionality revolves around the `main` function, which processes input files (either `.prompt` files or code files). It determines appropriate output paths and file extensions based on the input language (supporting Python, Bash, and Makefile). The tool integrates two external modules, `code_generator` and `context_generator`, to handle the actual logic of converting prompts to code and generating context examples, respectively. It includes safety features such as checking for existing files and prompting the user for confirmation before overwriting, unless a `--force` flag is used. The file also includes helper functions for path manipulation and extension retrieval, along with usage instructions and dependency requirements.",e748fdf06a3f0bfed6c18bdcb1d7c8a7a78c18256dfb51adfd99086c3a14f2cc
context/split/1/initial_pdd_python.prompt,"This document outlines the specifications for a Python command-line tool named ""pdd"". The tool is designed to compile prompt files into runnable code files or generate example code files from existing source code. It utilizes the Python ""rich"" library for formatted console output. The input files follow a specific naming convention (basename_language.prompt), allowing the tool to automatically determine the target language and appropriate file extensions (e.g., .py for Python, .sh for Bash). 

The program supports flexible output options, allowing users to specify filenames, paths, or rely on defaults relative to the input file. Key command-line arguments include ""--force"" to overwrite existing files without confirmation, ""-o"" to define the output path for runnable code, and ""-oe"" to trigger the generation of example code. 

The workflow involves reading the input filename, parsing the basename and language, determining output paths based on user arguments, and then executing the generation process. If the input is a prompt file, it generates runnable code using a ""code_generator"". If the input is a code file or the ""-oe"" flag is used, it generates an example file using a ""context_generator"".",0900e3d8d148d17b913e323855ca7cf60e70d2c1bf9600183c5f3ae2e3dc1dba
context/split/8/cli_python.prompt,"The provided text outlines the specifications for generating a Python command-line interface (CLI) tool named `pdd`. This tool is designed to be built using the Python `Click` library and serves as a comprehensive utility for managing code generation, testing, and prompt engineering workflows. The instructions detail the directory structure and require the implementation of a `cli` function within `cli.py`. 

The document provides extensive examples and instructions for implementing various commands, including `generate` (creating code from prompts), `example` (generating context), `test` (creating unit tests), `preprocess` (handling prompt files), `fix` (resolving errors), `split` (dividing prompts), `change` (modifying prompts), `update` (refreshing prompts, optionally via git), `detect` (identifying necessary changes), `conflicts` (resolving prompt conflicts), `crash` (fixing crash errors), `trace` (tracing execution), and `bug` (converting bugs to tests). It emphasizes using specific internal modules for logic (e.g., `code_generator`, `xml_tagger`, `track_cost`) and handling imports carefully to avoid naming conflicts. Additionally, it includes instructions for an `install_completion` command to manage shell auto-completion.",f2f50720c4bef8c93dad2b7d42677eeaffae49f17052f40a0232f621296179bf
context/split/8/change_main_python.prompt,"This document provides a detailed specification for implementing the `change_main` function, a core component of the `pdd` command-line interface. The function is responsible for handling the logic of the `change` command, which modifies prompt files based on input code and change instructions. The specification outlines the required inputs, including the Click context and various file paths (input prompt, code, change prompt, output, and CSV). It defines the expected output as a tuple containing the modified prompt string, operation cost, and model name. The document details a step-by-step process for the function: parsing arguments (handling both single-file and CSV batch modes), constructing file paths using helper functions, performing the prompt modification via `change_func` or `process_csv_change`, saving results, and providing user feedback via `rprint`. It also emphasizes adherence to existing coding styles, proper error handling, and includes references to context examples for using the Click library and internal modules.",fb4558a1149c68efd12212af23e22c3d1a93e744acc07f7c4b7699e71f5146cf
context/split/8/cli.py,"This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` library. It serves as the main entry point for the application, orchestrating various AI-assisted development tasks. The CLI exposes a suite of commands designed to automate the software development lifecycle using Large Language Models (LLMs). Key commands include `generate` for creating code from prompts, `example` for generating usage examples, `test` for creating unit tests, and `fix` for iteratively resolving code errors and test failures. Other utilities include `preprocess` for optimizing prompts, `split` for breaking down complex prompts, `change` and `update` for modifying existing code and prompts, and `detect` for identifying necessary changes based on instructions. The tool also features debugging aids like `crash` for fixing module errors, `bug` for generating reproduction tests based on output discrepancies, and `trace` for mapping code lines back to prompt requirements. The CLI supports global configuration for AI model strength, temperature, and cost tracking, integrating with various backend modules to execute these tasks.",8950b0a13caf9936d281dde770fb572bec99060800e2c0b1d68a5ca7fa631f3a
context/split/8/change_main.py,"The provided code snippet defines a command-line interface (CLI) command named `change` using the `click` library in Python. This command is designed to modify an input prompt file based on a change prompt and corresponding input code. It accepts three optional file path arguments: `input_prompt_file`, `input_code_file`, and `change_prompt_file`, all of which must exist if provided. Additionally, it supports two options: `--output` to specify a destination for the modified prompt file, and `--csv` to utilize a CSV file for change prompts instead of a text file. The function is decorated with `@track_cost` and `@click.pass_context`, indicating it likely tracks resource usage and requires access to the click context. The body of the function delegates the core logic to a separate helper function called `change_main`, passing along all received arguments and the context.",549e436acc4baf223c73decdf493d11dd9ab063bb938dea05b001efac351a13a
context/split/8/modified_cli.prompt,"This file contains a detailed prompt for an LLM to generate a Python command-line interface (CLI) tool named `pdd`. The tool is designed to be built using the Python `Click` library and serves as a utility for managing and generating code from prompts. The prompt outlines the directory structure, imports, and specific instructions for implementing various commands such as `change`, `generate`, `test`, `preprocess`, `fix`, `split`, `update`, `detect`, `conflicts`, `crash`, `trace`, and `bug`. It provides examples of how to utilize internal modules for each command, including specific function calls like `track_cost`, `construct_paths`, and `code_generator`. Additionally, it includes instructions for handling shell completion installation and specific logic for commands like `update` (supporting a `--git` flag) and `trace` (wrapping a main logic function).",34a0b2ec2995bf377ad36ee35316bd38828b4147eefd7100da401d7c81149e5e
context/split/simple_math/split.prompt,"The provided file content consists of a concise directive aimed at refactoring code. Specifically, it instructs the developer to isolate the existing input validation logic and move it into a distinct, dedicated helper function. This change is likely intended to improve code modularity, readability, and reusability by separating validation concerns from the main execution flow.",db775c89b5841e554d0e476e211514248525612ada2d78fd1f9ab2329c864944
context/split/simple_math/split_script.py,"The provided file content is a short Python script that defines a single function named ""add"". This function takes two arguments, ""a"" and ""b"", and is designed to return their sum. Before performing the addition, the function implements type checking to ensure that both inputs are either integers or floating-point numbers. If either argument fails this check, the function raises a TypeError with a descriptive message. The file also includes a comment at the top indicating it is a placeholder for a split command script.",74e08259f6febb7c93e191d0687e3fb5c30358b920e108d6798146fb9141c0e3
context/split/simple_math/split_example.py,"The provided file content is a Python script designed to demonstrate and test the functionality of an imported module named 'split_script'. The script begins by importing this module. It then proceeds to execute two distinct test cases wrapped in try-except blocks to handle potential type errors gracefully. In the first test case, the script calls the 'add' function from the 'split_script' module with two integer arguments (10 and 20), printing the resulting sum if successful. The second test case attempts to call the same 'add' function with mixed data types—an integer (5) and a string ('x')—which is expected to raise a TypeError. This error is caught by the except block, which then prints the corresponding error message. Overall, the script serves as a basic usage example and error-handling demonstration for the 'add' function within the 'split_script' module.",68347c89811c07ed8503dd519f44e6ddd88e0c937829b865180c94664dbcd701
context/split/4/construct_paths.py,"This Python file defines utility functions for handling file paths and naming conventions within a command-line interface (CLI) application. The primary function, `construct_paths`, orchestrates the loading of input files and the determination of output file paths based on user commands (such as 'generate', 'test', or 'fix'). It handles logic for inferring file extensions and languages from filenames, specifically parsing basenames and utilizing a helper function `get_extension`. Additionally, the code includes a `generate_output_filename` function that standardizes output naming patterns according to the specific operation being performed. The module also manages user interaction for file overwrites using `click` and provides status feedback via `rich` printing, ensuring that file operations are safe and transparent.",c227dc44f90465b64f94000958c11eb8885854466becf403a7f19e61519b50d2
context/split/4/final_construct_paths_python.prompt,"This document outlines the requirements for a Python function named `construct_paths`, designed for a CLI tool called `pdd`. The function is responsible for generating, validating, and loading input and output file paths based on specific command parameters. It takes inputs such as a dictionary of file paths, a force overwrite flag, a quiet mode flag, the command name, and command options. The function's workflow involves five key steps: constructing input paths (adding extensions if missing), loading file contents into a dictionary while handling errors, generating output file paths using a helper function `generate_output_filename`, checking for existing files to prompt for overwrite permission (unless forced), and finally returning the loaded strings, paths, and language. The implementation must utilize the `rich` library for console output and `Click` for CLI interactions, specifically handling logic for the 'generate' command to extract basenames and languages.",4035ae675b3bb8b6620db80e43c284abf0036ceb8df132741038424897aa9338
context/split/4/sub_construct_paths_python.prompt,"The file outlines a task for an expert Python software engineer to implement a function named `generate_output_filename`. This function is designed to construct specific output filenames based on the execution context of a Python program. It accepts five parameters: `command`, `key`, `basename`, `language`, and `file_extension`. The core logic requires handling distinct naming conventions for various commands: 'generate' produces a standard filename; 'example' appends an '_example' suffix; 'test' adds a 'test_' prefix; 'preprocess' creates a prompt file including the language; and 'fix' modifies the filename with '_fixed', adjusting for test outputs specifically. A default fallback pattern is provided for unrecognized commands. The implementation must be robust, efficiently handling all specified cases and potential edge cases like missing language information.",5cfc774bcf11370f270d03d4c7336e41ea519f9915c60c76a47ce8b4bd25ceee
context/split/4/initial_construct_paths_python.prompt,"The file outlines the requirements for a Python function named `construct_paths`, designed for a CLI tool called `pdd`. This function is responsible for generating and validating input and output file paths, as well as loading the content of input files. It utilizes the `rich` library for pretty-printed console output and `click` for CLI interactions. The function takes inputs such as file paths, a force overwrite flag, a quiet mode flag, the specific command run, and command options. It outputs a dictionary of loaded input strings, a dictionary of determined output file paths, and the target language. Key logic includes handling file extensions (appending defaults like `.prompt` if missing), extracting language and basenames for the `generate` command, and implementing an interactive overwrite check unless forced. The process involves four main steps: constructing input paths, loading file contents with error handling, determining output paths based on specific rules, and managing file overwrite permissions.",5c0e63adb81aa695a26d555aa6bedc5eb632bcaca2dd5fa8d701d7ed8bcab88f
context/split/4/split_construct_paths_generate_output_filename.py,"The provided code snippet demonstrates a function call to `generate_output_filename`. This function is designed to construct a filename based on several parameters: a command type (e.g., 'generate'), a specific key from an output dictionary (e.g., 'output' or 'output-test'), the base name of the file, the programming language involved, and the file's extension. The arguments passed to the function suggest it is part of a larger system for managing file generation or processing, likely within a tool that handles multiple languages and command types, such as a documentation generator or a code scaffolding utility.",2c39060071cc48ca5baab6467c4c4cbcfa5e7cb8eb9e1d05d2b0a892f4f5185a
context/split/3/split_postprocess_find_section.py,"The provided file content consists of a single Python function call named `find_section`. This function appears to be designed for parsing or processing text data, specifically structured into sections. It accepts three arguments: `lines`, which is described in the comments as an array of text lines (likely the result of a `splitlines()` operation); `start_index`, an integer defaulting to 0 that specifies the starting row for the search; and `sub_section`, a boolean flag defaulting to `False` that indicates whether the search is for a nested or sub-section. The function call assigns its return value to a variable named `sections`. The code snippet suggests a utility for navigating or extracting specific segments from a larger body of text based on line indices and hierarchical structure.",31bd47832cd2fb2ad1787a9d7b4064af1d57dc3dfba26b64dbd7d15a78cc571f
context/split/3/postprocess.py,"This Python file defines utility functions for processing text output from a Large Language Model (LLM), specifically focusing on extracting and isolating code blocks of a target programming language. The core functionality is encapsulated in two main functions: `find_section` and `postprocess`. The `find_section` function parses a list of strings to identify markdown-style code blocks (delimited by triple backticks), returning metadata about their language, start, and end lines. The `postprocess` function orchestrates the cleanup of LLM output. It first retrieves the appropriate comment syntax for a specified language using an imported helper. It then scans the output to find all code sections and identifies the largest block matching the target language. Finally, it reconstructs the output string by preserving the content of the largest matching code block while commenting out all other lines—including text outside code blocks and code blocks in other languages. This ensures that the resulting output is a valid, executable file in the target language, with non-code explanations or hallucinations safely commented out.",7bd1d2cf74436558a0fa14db3d7fc31abf6fcced57faa1d875a70a5566be23ae
context/split/3/initial_postprocess_python.prompt,"The file outlines a task for an expert Python engineer to create a function named `postprocess`. This function is designed to sanitize the raw string output from a Large Language Model (LLM), which typically contains a mix of conversational text and code blocks, into a directly executable script. The function takes two inputs: the raw `llm_output` string and a target `language` (e.g., python, bash). Its primary objective is to identify the largest code block matching the target language and comment out all surrounding text and other code blocks using the appropriate comment syntax for that language. The process involves several steps: determining the correct comment character, recursively parsing the string to identify top-level code sections, selecting the largest relevant code block, and applying comments to all other lines, including the markdown backticks of the selected block. The final output is a string containing the main code block ready for execution, with all explanatory text safely commented out.",5d4baddf9f4ee69a26f8db6480e32fdedc5959289f44e6eedc81e5cdec57f3a8
context/split/3/sub_postprocess_python.prompt,"The provided text outlines the specifications for a Python function named `find_section`, designed to parse Large Language Model (LLM) outputs and identify top-level code sections. The function takes a list of strings (lines from the LLM output), a starting index, and a boolean flag for recursive calls as inputs. Its primary goal is to return a list of tuples, each containing the code language, start line index, and end line index for every identified code block. The document includes a usage example demonstrating how to split an LLM output string into lines and call the function. It also provides a sample LLM output containing mixed text and Python code blocks (including nested code blocks within strings) to illustrate the data structure the function must handle. Finally, the text details a three-step algorithm for the implementation: initializing a storage list, iterating through lines to detect triple backtick delimiters, handling nested blocks via recursion, and recording the start and end points of top-level sections while ignoring sub-sections.",58b5cc265fdc219c18e64ef1fa8d5dea44b0ce126592ce26a0384f2fe8d1f8a3
context/split/3/consolidated_xml_filled_in.prompt,"The file outlines a prompt engineering task designed to split a complex `input_prompt` into two distinct components: a `sub_prompt` and a `modified_prompt`, ensuring no loss of functionality. The process involves extracting a specific sub-task (like a helper function) into its own prompt (`sub_prompt`) and updating the original prompt (`modified_prompt`) to reference this new sub-task. The file provides detailed examples (Example 1 and Example 2) demonstrating this workflow. In Example 1, a prompt for a command-line program (`pdd`) is split to extract a `get_extension` function. In Example 2, the same `pdd` prompt is split to extract a `construct_output_paths` function. The file concludes with a new `input_prompt` describing a Python `postprocess` function intended for cleaning LLM outputs, along with its corresponding generated code and example usage. The user is instructed to apply the splitting methodology to this new input, generating the appropriate `sub_prompt` and `modified_prompt`.",e8a713f21a088b0afa73e7ab777af562a95fd787e4aed99e13b2d0200ac189df
context/split/3/final_postprocess_python.prompt,"The file outlines a task for an expert Python engineer to create a function named `postprocess`. This function is designed to take a raw string output from a Large Language Model (LLM), which typically contains a mix of explanatory text and code blocks, and convert it into an executable script. The function accepts two inputs: the `llm_output` string and a `language` identifier (e.g., 'python' or 'bash'). Its primary objective is to identify the largest code block matching the specified language within the LLM's response. It then comments out all surrounding text, other code blocks, and the markdown backticks themselves, leaving only the main code section active and runnable. The process involves looking up the correct comment syntax for the language, locating code sections, selecting the largest relevant block, and applying comments to all other lines to ensure the final output is a valid script file.",b27cde9185b170d15761b830ad784af4ecd2694c2aa3f3660d58264015855051
context/split/2/consolidated.prompt,"The file contains a prompt engineering task designed for an expert LLM Prompt Engineer. The objective is to take a complex `input_prompt` and split it into two distinct parts: a `sub_prompt` (a smaller, self-contained task extracted from the original) and a `modified_prompt` (the original prompt updated to utilize the output of the sub-prompt). The file provides detailed definitions for the inputs (`input_prompt`, `input_code`, `example_code`) and outputs (`sub_prompt`, `modified_prompt`). It includes a comprehensive example demonstrating this process, where a Python command-line tool prompt (`pdd`) is split to extract a helper function (`get_extension`) into a sub-prompt. Finally, the file presents a specific `input_prompt` describing a Python CLI tool named ""pdd"" (which compiles prompts to code or generates examples) and asks the user to perform the split based on provided `input_code` and `example_code`.",7c9ea2e83d38c5df64e79fa279b2df03daf4033150914b3812c2e8bc320a0a7d
context/split/2/sub_pdd_python.prompt,"The provided text outlines the requirements for a Python function named ""construct_output_paths,"" designed to generate file paths for runnable code and example outputs within a command-line tool called ""pdd."" The function takes four inputs: the file's basename, the file extension, and optional output paths provided via command-line flags ('-o' and '-oe'). It returns a tuple containing the paths for the runnable output and the example output. The logic must handle four scenarios for specifying output locations: a filename without a path, a filename with a path, a path without a filename, and a default case where nothing is specified. The prompt includes examples of how command-line arguments should translate into file paths, noting specific naming conventions like ""basename_language.prompt."" The implementation plan involves defining a helper function to standardize path construction and then applying it to determine both the runnable and example output paths before returning them as a tuple.",bbc23abd6c529f9d0e2240973d74c08d91f24132321447a1bbcf96c0a4736e8a
context/split/2/split_pdd_construct_output_path.py,"The provided code snippet demonstrates a single Python function call to `construct_output_paths`. This function is designed to generate two distinct file paths: `runnable_output_path` and `example_output_path`. It accepts four arguments: `basename`, which represents the core name of the file; `file_extension`, which is determined by the programming language being used; `argv_output_path`, a string derived from a command-line argument flag `-o`; and `argv_example_output_path`, a string derived from a command-line argument flag `-oe`. The purpose of this call is likely part of a larger script that processes files and handles output configuration based on user-provided command-line arguments.",0f2ab7543853ba418ad7d35b3fd113a5312e5ecc6117ad906aa77931e6190215
context/split/2/final_pdd_python.prompt,"The provided text outlines the specifications for a Python command-line utility named ""pdd"". This tool is designed to facilitate code generation and documentation by converting prompt files into runnable code or by creating example files from existing code. The program utilizes the Python `rich` library for formatted console output. It accepts input files following a specific naming convention (e.g., `basename_language.prompt`) to determine the target language and output filenames. 

Key features include support for overwriting existing files via a `--force` flag, and customizable output paths using `-o` for runnable code and `-oe` for example code. The workflow involves parsing the input filename to extract the basename and language, determining the correct file extensions, and constructing output paths. Depending on whether the input is a prompt or a code file, ""pdd"" executes specific steps: generating code from prompts using a `code_generator` module, or creating example usage files using a `context_generator` module. The text includes examples of command-line usage and references helper scripts for tasks like extension lookup and path construction.",59a3a06a611946dd14657809e4e0ea6b7afb0930441ec5206186690340abd267
context/split/2/pdd.py,"The provided text outlines the implementation of a Python command-line tool named `pdd.py`. This utility is designed to compile prompt files into runnable code or generate example code from existing source files. It leverages the `rich` library for enhanced console output and relies on external modules like `code_generator`, `context_generator`, and `get_extension` for its core logic. The script uses `argparse` to manage command-line arguments, allowing users to specify input files, output paths for runnable code (`-o`), and output paths for example code (`-oe`). Key features include automatic file extension handling (defaulting to `.prompt`), language detection based on filenames, and a safety mechanism that prompts users before overwriting existing files unless the `--force` flag is used. The workflow involves reading an input file, determining the target language, generating the appropriate code or context, and saving the results to the designated locations.",1021df55a642878527ad41794379eb1ba835c739a1e6f13b0896f2a0605ff8c2
context/split/2/initial_pdd_python.prompt,"The provided text outlines the specifications for a Python command-line utility named ""pdd"". This tool is designed to compile prompt files into runnable code or generate example code from existing source files. It utilizes the Python `rich` library for formatted console output. The program accepts input files following a specific naming convention (`basename_language.prompt`) and supports flexible output configurations, allowing users to specify filenames, paths, or rely on defaults derived from the input basename. Key features include handling file overwrites via a `--force` flag, and specific flags (`-o`, `-oe`) to control the generation and location of runnable code and example files. The workflow involves parsing the input filename to determine the language and extension, resolving output paths based on user arguments, and then invoking internal generators (`code_generator` or `context_generator`) to produce the final files.",4337784a5517d6e80fac11df769749c916c003261b27b36d6d5e6c7b580af78c
context/split/5/modified_cli_python.prompt,"The provided text is a detailed prompt specification for an AI agent to generate a Python command-line interface (CLI) tool named ""pdd"". The tool is to be built using the `click` library and serves as a developer utility for managing code generation, testing, and prompt engineering workflows. 

The specification outlines the directory structure and requires the implementation of a main `cli` entry point. It details numerous sub-commands, including `generate` (code generation), `example` (context generation), `test` (unit test creation), `preprocess` (prompt processing), `fix` (error correction), `split` (prompt splitting), `change` (prompt modification), `update` (prompt updating), `detect` (change detection), `conflicts` (conflict resolution), `crash` (crash fixing), `trace` (execution tracing), and `bug` (bug-to-test conversion). 

The prompt includes placeholders for external context files (e.g., `<include>context/python_preamble.prompt</include>`) and specific usage examples for internal modules to ensure the generated code correctly utilizes existing functionality like `track_cost`, `construct_paths`, and various generator functions. It emphasizes avoiding naming conflicts between CLI commands and imported function names.",05b105bbb2992d5fd110c1fe13558c710ee84bb05d56e15595a832f654a39144
context/split/5/cli_python.prompt,"This file contains a detailed prompt for an LLM to generate a Python command-line interface (CLI) tool named ""pdd"". The tool is to be built using the Python Click library and serves as a wrapper for various internal modules related to code generation, testing, and prompt management. The prompt outlines the directory structure, specific import naming conventions to avoid conflicts with Click commands (e.g., importing 'preprocess' as 'preprocess_func'), and provides a list of required commands such as 'generate', 'example', 'test', 'preprocess', 'fix', 'split', 'change', 'update', 'detect', 'conflicts', 'crash', 'trace', and 'bug'. It also includes instructions for handling shell completion installation and tracking output costs via CSV files. The prompt references numerous external example files to guide the implementation of each specific command's logic.",f4ebb644bcad3cade98d856a268cb99231f7532f623bcfcfd4ec6d1f3cdb8213
context/split/5/track_cost_python.prompt,"The file outlines the requirements for creating a Python decorator named `track_cost` designed for the ""pdd"" command-line interface, which utilizes the Click library. The primary purpose of this decorator is to monitor and log the cost associated with executing specific commands without interrupting the main program flow. It functions by wrapping a command function to record the start and end times of execution. The decorator is responsible for accessing the Click context to retrieve command details, extracting cost and model information from the function's return values, and identifying input and output file paths from arguments. It then appends this data—including timestamp, model, command name, cost, and file paths—to a CSV file specified either directly or via an environment variable. The instructions emphasize robustness, requiring graceful exception handling using Rich's `rprint` to ensure logging errors do not crash the application, and the use of `functools.wraps` to preserve original function metadata.",d9191e985846507bdfa6b7ccf7f8b3de2fc59d251aab381b303f031d7d42f98f
context/split/5/split_track_cost.py,"The provided file content is extremely minimal, consisting solely of the Python decorator syntax `@track_cost`. This snippet likely represents a fragment of a larger codebase, specifically a decorator intended to be applied to a function or method. In a software development context, such a decorator is typically used to monitor, calculate, or log the computational cost, resource usage, or financial expense associated with executing the decorated function. For example, in cloud computing or LLM (Large Language Model) applications, a `@track_cost` decorator might intercept the function call to count tokens used or API calls made, subsequently updating a usage tracker. Without the accompanying function definition or the implementation of the decorator itself, the code is non-functional on its own. It serves as a marker or annotation indicating that the subsequent code block is subject to cost tracking logic defined elsewhere in the project's utility or monitoring modules.",92e4ef534572378e08209f913e43d248e18cd10e70de8f2fa15528cbc41200e1
context/split/5/cli.py,"This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` library. It serves as the main entry point for the application, orchestrating various AI-driven development tasks. The CLI includes a global configuration for model strength, temperature, verbosity, and cost tracking via a decorator named `track_cost`, which logs usage metrics to a CSV file.

The file implements numerous commands including `generate` (creating code from prompts), `test` (generating unit tests), `fix` (iteratively repairing code based on errors), `update` (synchronizing prompts with code changes), and `split` (breaking down complex prompts). Other utilities include `preprocess` for prompt formatting, `detect` for identifying necessary changes, `conflicts` for analyzing prompt discrepancies, and `trace` for mapping code lines back to prompt lines. It also features a `bug` command to generate reproduction tests from observed outputs and an installation helper for shell completion. Each command handles file path construction, invokes specific backend logic modules, and manages output file writing and user feedback via the `rich` library.",9eea12ec7e9e8f98c0075207b0d88e60c3e01bc1128e25e0c80362028ae1d217
context/change/20/generate_test.py,"This Python script defines a function named `generate_test` designed to automatically create unit tests for a given piece of code using Large Language Models (LLMs) and the Langchain framework. The function takes inputs such as the original prompt, the source code, the desired LLM strength, temperature, and the target programming language. It begins by validating these inputs and loading a specific prompt template from a file path defined in environment variables. The script then utilizes a helper function, `llm_selector`, to choose an appropriate model based on the specified strength and temperature. It constructs a Langchain processing chain to generate the test code, calculating and displaying estimated token usage and costs via the `rich` library. Crucially, the function includes logic to handle incomplete LLM outputs; it checks if the generation finished abruptly and, if so, triggers a continuation process. Finally, the output undergoes post-processing, and the function returns the generated unit test code, the total calculated cost, and the name of the model used, while handling potential file or value errors gracefully.",7088603cf835c09d0466679a8d08109bdaebfd64d5e69a3e3f0369f549b8f7b7
context/change/20/change.prompt,"The provided file outlines a specific refactoring task related to updating prompt templates. The core instruction is to modify input prompts because the methods `llm_invoke` and `load_prompt_template` have replaced previous implementations. The file provides a set of four concrete examples demonstrating this change. Each example, identified by an ID (1 through 4), contrasts a `before_example` file path with an `after_example` file path. These paths point to specific prompt files (e.g., `unfinished_prompt_python.prompt`, `context_generator_python.prompt`, `code_generator_python.prompt`, and `generate_test_python.prompt`) and their corresponding updated versions within a `context/change/` directory structure. Essentially, the file serves as a guide or a dataset for automating or understanding the migration of prompt files to a new format compatible with the updated invocation methods.",bf518ed2b6ee2ac5fef90b4f4f41180d0780e28ed6061096731936357c6be375
context/change/20/generate_test_python.prompt,"This file contains a detailed prompt specification for an expert Python Software Engineer agent. The objective is to create a Python function named `generate_test` that automatically generates unit tests for a given code file. The prompt outlines the function's inputs (including the original prompt, code, model strength, temperature, and language) and outputs (the generated unit test, total cost, and model name). It provides specific instructions on using internal modules for preprocessing prompts, selecting LLM models via `llm_selector`, and handling LangChain Expression Language (LCEL). The process involves a multi-step workflow: loading a specific prompt file, preprocessing inputs, invoking the model with token counting and cost estimation, pretty-printing results, detecting incomplete outputs to trigger continuation, and post-processing the final result. The prompt also references several external context files for examples and preambles.",f4154f38a0e4ee9fc9d090806c5864298d1b0f179d0e08f76418db8160a7906e
context/change/20/updated_generate_test_python.prompt,"This file provides a detailed specification for creating a Python function named `generate_test`. The function's primary purpose is to automatically generate unit tests for a given piece of code using a Large Language Model (LLM). The specification outlines the required inputs, which include the original prompt, the source code, model parameters (strength, temperature), the target language, and a verbosity flag. The expected output is a tuple containing the generated unit test code, the total cost of the operation, and the name of the model used.

The document details a specific six-step workflow: loading a prompt template (`generate_test_LLM`), preprocessing inputs, invoking the LLM, handling verbose logging (including token counts and costs), checking for and resolving incomplete generations, and post-processing the final result. It also includes references to internal helper modules for tasks like loading templates, invoking the LLM, and handling unfinished prompts. Finally, the specification emphasizes the need for graceful error handling throughout the process.",e3e6d61f17deaf125fcf4bc8c28ccc2e3d5f475b291b2455a563e41831e6a2ab
context/change/18/updated_unfinished_prompt_python.prompt,"This file outlines the specifications for creating a Python function named `unfinished_prompt`. The primary goal of this function is to analyze a given text string (`prompt_text`) and determine whether it represents a complete thought or requires continuation. The function accepts inputs for the prompt text, model strength, temperature, and a verbosity flag. It is expected to return a tuple containing a structured reasoning string, a boolean indicating completion status (`is_finished`), the total cost, and the model name. The implementation steps involve loading a specific prompt template (`unfinished_prompt_LLM`) and utilizing an internal `llm_invoke` module to process the text, parsing the resulting Pydantic output to extract the required return values. The file also references internal context examples for loading templates and invoking LLMs.",69a573fc153e4f4f86817317054b0854bb3d0fd93b2d1238a8d78c62e76357a3
context/change/18/unfinished_prompt_python.prompt,"This file outlines the specifications for an AI agent acting as an expert Python engineer to generate a function named 'unfinished_prompt'. The purpose of this function is to analyze a given text string ('prompt_text') and determine whether it represents a complete thought or requires continuation. The function takes inputs for the prompt text, model strength, and temperature. It is required to utilize the Langchain LCEL framework, load a specific prompt template from an environment path, and employ an internal 'llm_selector' module for model selection and cost calculation. The expected output includes a structured reasoning string, a boolean indicating completion status ('is_finished'), the total cost of the operation, and the model name used. The process involves specific steps for loading resources, invoking the LLM with JSON output parsing, and pretty-printing execution details like token counts and costs using the 'rich' library.",4fe9f4f136ebb461f65832e3675afe6d31a8e27e2ec324c0980fd4eeb6e3651e
context/change/18/change.prompt,"The provided file content outlines a required update to a prompt or code structure due to a change in methodology. Specifically, it notes that the previous method has been superseded by the functions `llm_invoke` and `load_prompt_template`. The text instructs the user to modify the prompt accordingly to reflect this replacement. To assist with this transition, the file references two examples: a ""before"" state located at `context/change/17/unfinished_prompt_python.prompt` and an ""after"" state located at `context/change/17/updated_unfinished_prompt_python.prompt`. These examples are intended to demonstrate the specific changes needed to align with the new implementation strategy.",fb1bc585a05b415040d556f2c0c63edecd7672dc6f150a251b6879427c138f51
context/change/18/code_generator_python.prompt,"The provided text outlines a specification for an expert Python engineer to create a function named ""code_generator."" This function is designed to compile a raw text prompt into a runnable code file. The specification details the required inputs (prompt string, target language, model strength, and temperature) and outputs (runnable code string, total cost, and model name). It includes references to external context files for a Python preamble, LCEL examples, and internal module usage examples (preprocessing, LLM selection, unfinished prompt detection, generation continuation, and postprocessing). The core logic is broken down into an eight-step process using LangChain: preprocessing the prompt, creating an LCEL template, selecting an LLM, executing the model with cost/token tracking, pretty-printing results, handling incomplete generations via a specific detection function, postprocessing the output, and finally returning the code, cost, and model name.",5b5b11fad7d22d99712ca4597b5a9a1e3a7d52eb402e2ab804458be11b6b6e56
context/change/18/updated_code_generator_python.prompt,"This file outlines the specifications for an AI agent, acting as an expert Python engineer, to create a Python function named `code_generator`. The primary goal of this function is to compile a raw text prompt into a runnable code file. The document details the required inputs (prompt, language, model strength, temperature, and verbosity) and expected outputs (runnable code, total cost, and model name). It provides specific instructions on utilizing internal modules for tasks such as loading prompt templates, invoking LLMs, preprocessing prompts, handling unfinished generations, and postprocessing results. The workflow is broken down into seven distinct steps using Langchain, covering preprocessing, template loading, LLM invocation with cost tracking, markdown rendering, completion detection with recursive generation if necessary, and final postprocessing. It emphasizes error handling and verbose logging options.",0bd90df38d9390dd23b7fa9f119314f15dbcaf365d16d28abfde6f5728c5d53f
context/change/18/code_generator.py,"This Python script defines a `code_generator` function designed to automate the creation of programming code using a Large Language Model (LLM). The process begins by preprocessing a raw prompt and selecting an appropriate LLM based on specified strength and temperature parameters via an `llm_selector` module. The script then constructs a LangChain prompt template, invokes the model, and tracks token usage and costs for both input and output. A key feature of the workflow is its ability to handle incomplete outputs; it checks the end of the generated text to determine if the response was cut off. If unfinished, it triggers a continuation process; otherwise, it runs a post-processing step to refine the code for the target language. Throughout execution, the script utilizes the `rich` library to print formatted logs, Markdown-rendered results, and cost estimates to the console, ultimately returning the final runnable code, the total calculated cost, and the name of the model used.",c2df2d5c6852d547b62ac13ba50e6cd63cfce7e86431ee0e73c678660f9ce9e7
context/change/9/initial_fix_error_loop_python.prompt,"This document outlines the requirements for creating a Python function named ""fix_error_loop"" designed to iteratively repair unit tests and their corresponding code files. The function takes inputs including file paths for the unit test, the code under test, and a verification program, along with LLM parameters like strength and temperature, and a maximum attempt limit. The process involves a loop that runs pytest, capturing output to an error log. If tests fail, the function analyzes the error log, creates backup files stamped with failure counts and iteration numbers, and invokes a helper function, ""fix_errors_from_unit_tests,"" to generate fixes. These fixes are applied and verified against a separate verification program; if verification fails, the backups are restored. The loop continues until the tests pass or the maximum attempts are reached. The function is required to use the ""rich"" library for pretty-printed console output, handle file I/O and subprocess errors gracefully, and return a status object containing success flags, final file contents, and the total attempt count.",dcb17e57a63d38312a12e92a7bf7f662adfb5939a145a942c7ce97ea65072656
context/change/9/initial_fix_error_loop.py,"This Python script defines a function named `fix_error_loop` designed to iteratively debug and repair code and unit tests. The function takes paths to a unit test file, a code file, and a verification program, along with parameters for an external error-fixing mechanism (strength, temperature, and max attempts). 

The process operates in a loop where it first runs the unit tests using `pytest` and captures the output. If the tests pass, the loop terminates successfully. If they fail, the script reads the error log, creates backups of the current files, and invokes an imported function `fix_errors_from_unit_tests` to generate potential fixes based on the error output. 

If fixes are proposed, the script updates the files and runs a separate verification program to ensure the changes are valid. If verification fails, the script reverts to the backup files. This cycle repeats until the tests pass or the maximum number of attempts is reached. Finally, the function performs one last test run and returns the success status, the final content of the files, and the total attempt count.",c09034f0ac25551111af9568b2a7d57ab640ee2b56497cfb771af0af7d7ea258
context/change/9/final_fix_error_loop_python.prompt,"The provided text outlines the specifications for a Python function named `fix_error_loop`, designed to iteratively repair errors in a unit test and its associated code file using an LLM. The function takes inputs including file paths for the unit test, code, and a verification program, along with LLM parameters like strength, temperature, maximum attempts, and a budget. The process involves a loop that runs `pytest`, captures errors, and utilizes a helper function `fix_errors_from_unit_tests` to generate fixes if tests fail. It manages file backups, tracks costs, and verifies that changes do not break the core functionality using the verification program. The loop continues until success, the attempt limit is reached, or the budget is exceeded. Finally, the function ensures the best-performing iteration is restored and returns the success status, final file contents, total attempts, and total cost, all while using the `rich` library for formatted console output.",e2a611a318013d20bb6910092141ae2fe67c6330cee00a0828c4b64cf5d51639
context/change/9/change.prompt,"The file outlines specific updates required for a prompt or function definition related to an automated code-fixing tool named `fix_errors_from_unit_tests`. It provides a Python code example demonstrating how to invoke this function with inputs such as unit test code, the code under test, an error message, and parameters for model strength and temperature. The example shows the function returning updated unit tests, updated code, and the total cost. Additionally, the instructions specify two new functional requirements: first, the system must track the total cost of all runs and halt execution if a specified 'budget' is exceeded. Second, upon completion, the function must identify and return the most successful iteration of the code and unit tests. This selection process prioritizes iterations with the lowest number of 'ERROR's first, followed by the lowest number of 'FAILED' tests, ensuring the final output represents the most stable version achieved during the process.",6634e986dc230c7880aeb4b976f545350d3a72c0dcc94078a2164e74a561ec6e
context/change/11/initial_split_python.prompt,"The file outlines the requirements for a Python function named ""split,"" designed to decompose a single input prompt into a ""sub_prompt"" and a ""modified_prompt"" without losing functionality. The function takes an input prompt, generated code, example usage code, and LLM parameters (strength and temperature) as inputs. It utilizes the Langchain LCEL framework and specific prompt templates loaded from file paths to process these inputs. The workflow involves preprocessing prompts, selecting an LLM via an `llm_selector` utility, and executing a two-step LLM invocation process: first to generate the split content and second to extract the specific prompts into a JSON format. The function is required to calculate and pretty-print token counts and estimated costs using the Python Rich library for all console output. Ultimately, it returns the two resulting prompt strings and the total financial cost of the operation, while handling potential edge cases and errors.",b023c7200fa40dd4e4629ce84264033380c8b5b3563b4a3636aec7d574e40ad3
context/change/11/initial_code_generator.py,"This Python script defines a `code_generator` function designed to automate the creation of programming code using Large Language Models (LLMs) via the LangChain framework. The core function takes a raw prompt, target language, model strength, and temperature as inputs. It orchestrates a multi-step pipeline: first, it preprocesses the raw prompt using a custom `preprocess` module (handling features like recursive expansion or double curly brackets). Next, it selects an appropriate LLM and cost parameters using an `llm_selector` based on the desired strength. The script then executes the prompt through a LangChain pipeline (template, model, and string parser), calculating and displaying token usage and estimated costs for both input and output. The raw model output is then passed to a `postprocess` function to extract or refine the runnable code. Finally, the function returns the clean, runnable code and the total calculated cost. The script utilizes the `rich` library for formatted console output, providing visual feedback at each stage (preprocessed prompt, model output, costs). An `if __name__ == ""__main__"":` block is included to demonstrate usage with a sample prompt for generating a Python factorial function.",415b0161c9a487eec0da91cfc582cb3eeae46bee8d5c163d07eba6cc3fb5721e
context/change/11/change.prompt,"The file outlines requirements for creating a Python function named ""change"". This function is designed to process a specific prompt file located at 'prompts/change_LLM.prompt'. The core logic involves running this prompt through a LangChain Expression Language (LCEL) pipeline, followed by post-processing steps similar to the current context. The primary goal of the ""change"" function is to output a modified version of an initial prompt. The file also details the expected inputs and outputs for the underlying 'change_LLM' prompt mechanism. Specifically, it requires three inputs: 'input_prompt' (the original prompt text), 'input_code' (code generated from that original prompt), and 'change_prompt' (instructions on how to alter the original prompt). The final output of this process is a 'modified_prompt', which represents the updated string derived from applying the change instructions to the original input.",55472b854c77c56f47ed8e83fcc4588f14a6eacf0b23b693f84357c276058978
context/change/11/initial_code_generator_python.prompt,"The provided text outlines a specification for an expert Python engineer to create a function named ""code_generator."" This function is designed to compile a raw text prompt into a runnable code file, utilizing the Langchain library and LCEL (LangChain Expression Language). The function accepts four inputs: the raw prompt string, the target programming language (e.g., Python, Bash), a strength parameter (float 0-1) for model selection, and a temperature setting for the LLM. The expected outputs are a string of runnable code and the total cost of the operation. The process involves a seven-step workflow: preprocessing the prompt, creating an LCEL template, selecting an appropriate LLM via a selector module, executing the model while tracking token usage and cost, pretty-printing the output using the ""rich"" library (including Markdown formatting), post-processing the result to extract code blocks, and finally returning the clean code and total cost. The specification also references several external context files for examples on preprocessing, LLM selection, and post-processing.",460661d899c5324a82e4172d05852aa94e00225c1f4dc9e6abd1ffdb581649ef
context/change/7/final_fix_errors_python.prompt,"This document outlines the requirements for creating a Python function named ""fix_error_loop"" designed to iteratively repair unit tests and their corresponding code files. The function takes inputs including file paths for the unit test, the code under test, and a verification program, along with LLM parameters like strength and temperature, and a maximum attempt limit. The process involves a loop that runs pytest, capturing output to an error log. If tests fail, the function analyzes the error log, creates backup files stamped with failure counts and iteration numbers, and invokes a helper function, ""fix_errors_from_unit_tests,"" to generate fixes. These fixes are applied and verified against a separate verification program; if verification fails, the backups are restored. The loop continues until the tests pass or the maximum attempts are reached. The function is required to use the ""rich"" library for pretty-printed console output, handle file I/O and subprocess errors gracefully, and return a status object containing success flags, final file contents, and the total attempt count.",dcb17e57a63d38312a12e92a7bf7f662adfb5939a145a942c7ce97ea65072656
context/change/7/initial_fix_errors_python.prompt,"The provided text outlines a specification for an expert Python software engineer to create a script named ""fix_errors.py"". This script is designed to automate the debugging process using a CLI tool called ""pdd"" and the ""rich"" library for pretty-printed console output. The script accepts arguments for a unit test file, a code file, a verification program, a strength parameter, and a retry limit. The workflow involves an iterative loop: first, it removes old logs and runs pytest, piping output to an error log. If tests fail, it analyzes the error count, backs up the current files with versioned names based on failure counts and iteration number, and then invokes ""pdd"" to attempt a fix on the source files. It subsequently verifies that the code still runs; if successful, the loop repeats, otherwise, it reverts changes and retries the fix. Finally, it runs pytest one last time to report the results.",647488c8bba7eeaabcf4f78c17a8bfbdf486fd26971d7f8f89533266a6fb5e3b
context/change/7/change.prompt,"This document outlines the specifications for a Python code module designed to replace a command-line interface (CLI) program named 'pdd'. The core functionality is encapsulated in a function initially identified as `fix_errors_from_unit_tests`, which aims to automatically resolve errors encountered during unit testing. The module takes four primary inputs: the failing `unit_test` code, the implementation `code` being tested, the specific `error` message generated, and a `strength` parameter (a float between 0 and 1) that influences the underlying Large Language Model's behavior. The function returns four outputs: boolean flags indicating if the unit test or code was updated (`updated_unit_test`, `updated_code`), and the actual corrected strings for both the unit test and the implementation code (`fixed_unit_test`, `fixed_code`). Additionally, the documentation notes a modification to the original design: the main entry point will be a Python function named `fix_error_loop`, which will accept an additional `temperature` parameter, further customizing the generation process. The provided example demonstrates how to call this module programmatically within a `__main__` block.",09f6d7298550a6ad36ef5c1175fa9e0386469eb5e9bc17476524033b9d067529
context/change/7/initial_fix_errors.py,"The provided text outlines the creation of a Python script named `fix_errors.py`, designed to automate the debugging and repair of code using unit tests and an external tool called PDD. The script operates in a loop, executing unit tests via `pytest` and logging the results to `error.log`. If failures or errors are detected, the script creates backup copies of the current code and test files, tagged with the iteration count and error metrics. It then invokes the PDD tool to attempt a fix based on the error log. Following the attempted fix, a verification program is run to ensure the code's integrity; if verification fails, the script reverts to the backup files. This process repeats until all tests pass or a maximum number of iterations is reached. The script requires command-line arguments for the test file, code file, verification program, fix strength, and iteration limit.",ba61c10f4ecce85b97dfdc751830d1d15a6d8b26bd3649928da0ec843867ff6e
context/change/16/fix_errors_from_unit_tests_example.py,"This Python script serves as a demonstration and entry point for utilizing the `fix_errors_from_unit_tests` function from the `pdd` package. The script defines a `main` function that sets up a hypothetical scenario involving a buggy unit test and a simple Python function. Specifically, it defines a `test_add` unit test containing an intentional assertion error (asserting that 0 + 0 equals 1) and a corresponding `add` function implementation. It also initializes parameters such as a prompt description, a simulated error message string, a log file path, and configuration settings for an LLM (strength and temperature). The core of the script invokes `fix_errors_from_unit_tests` with these inputs to attempt an automated repair of the code or the test. Finally, it uses the `rich` library to print formatted results to the console, displaying whether the unit test or code was updated, the fixed versions of the content, the total cost of the operation, and the name of the model used. The script is designed to be executed directly as a standalone module.",65ffaca611e08ebc0c56469a3be2fbb0abf8d17b6b91a2b263233c99743b5694
context/change/16/fix_code_module_errors_python.prompt,"This document outlines the specifications for a Python function named `fix_code_module_errors`, designed to rectify errors in a code module that caused a program crash. Acting as an expert Python Software Engineer, the function takes inputs such as the original program, the prompt that generated the code, the problematic code module, error logs, and LLM configuration parameters (strength and temperature). The process involves a multi-step workflow using LangChain Expression Language (LCEL). First, it loads specific prompt templates from a project directory. Then, it utilizes an `llm_selector` to choose an appropriate Large Language Model based on the provided strength. The workflow executes two main LLM calls: the first generates a fix based on the errors, and the second extracts structured data (booleans for updates and strings for the fixed code) from that fix using a JSON parser. The function is required to track and pretty-print token usage and costs at each step, ultimately returning the fixed program, fixed code, update flags, the total cost of the operation, and the model name used.",6db86946ec70cfdaaaa9e4b36790cf24a35b38c2ef7b32e33f3e2f2c3c47accc
context/change/16/fix_error_loop_python.prompt,"This document outlines the specifications for a Python function named ""fix_error_loop,"" designed to iteratively repair errors in a unit test and its associated code file. The function acts as an automated debugging loop, utilizing an LLM (Large Language Model) to generate fixes based on pytest outputs. Key inputs include file paths for the unit test, code, and a verification program, along with LLM parameters like strength, temperature, and budget constraints. The process involves initializing counters and removing old logs, followed by a loop that runs pytest, captures errors, and attempts fixes using a helper function ""fix_errors_from_unit_tests."" The loop manages file backups, tracks costs, and verifies code integrity after changes. If verification fails, it restores previous versions. The function tracks the ""best iteration"" based on the lowest number of failures and errors. Upon completion or budget exhaustion, it ensures the best version is restored and returns the success status, final file contents, total attempts, and accrued costs. The document also includes context placeholders for internal modules and example pytest outputs to guide the implementation.",e8305dac166fd8799877e1de5a9f577e70a241b564fd7d80ca13fa15f54bcf00
context/change/16/modified_fix_error_loop_python.prompt,"The provided text outlines the specifications for a Python function named `fix_code_loop`, designed to iteratively repair errors in a code module using an LLM. The function takes inputs such as file paths for the code and a verification program, the original prompt, model parameters (strength, temperature), and constraints like maximum attempts and budget. 

The process involves a loop that runs until success, the attempt limit is reached, or the budget is exceeded. In each iteration, the function executes the verification program and logs any errors. If errors occur, it backs up the current files and calls a helper function, `fix_code_module_errors`, to generate fixes based on the error log. The function tracks costs and attempts, updates the code files if fixes are proposed, and re-verifies the solution. If a fix fails verification, the system restores the previous working version. Finally, the function returns a boolean success status, the final code and program contents, the total attempts made, the accumulated cost, and the model name used.",b8d999aca35c02331fb444c7db8e89f1fc3561ca11e097205741d65c508db8c9
context/change/16/fix_code_module_errors_example.py,"This file contains a Python script that demonstrates the usage of the `fix_code_module_errors` function from the `pdd.fix_code_module_errors` module. The script defines a `main` function which sets up a scenario involving a buggy Python program. Specifically, it defines a `program_with_error` string where a function attempts to sum a string instead of a list of numbers, causing a `TypeError`. The script also defines the original prompt used to generate the code, the isolated code module containing the function definition, and the simulated error message string. It then calls `fix_code_module_errors` with these inputs, along with parameters for model strength and temperature. The function returns several values indicating whether updates are needed, the fixed versions of the program and code module, the cost of the operation, and the model name used. Finally, the script prints these results to the console to verify the fix. The code serves as an example or test case for an automated code repair tool.",e04c30498a2bb7de25d0df307457e6f7b156baaadfa28e0c9a11f33fb8f49594
context/change/16/change.prompt,"The file outlines specific modifications to a code generation or error-fixing workflow. It instructs the replacement of the 'fix_error_loop' prompt with content from 'context/change/16/fix_error_loop_python.prompt'. Furthermore, it directs a change in function calls: instead of invoking 'fix_errors_from_units_tests' (referenced with an example file), the system should now call 'fix_code_module_errors', providing an example implementation for this new function. Finally, it specifies the prompt associated with this new function, pointing to 'context/change/16/fix_code_module_errors_python.prompt'.",98a3af97a4e02ef8594decf9fda9e92e64a755e8ae1766198e92777c3a029731
context/change/16/fix_error_loop.py,"This Python module implements an automated, iterative workflow for fixing errors in source code and unit tests. The core functionality is driven by the `fix_error_loop` function, which manages a cycle of testing, analysis, and patching. 

The process begins by executing `pytest` on a provided unit test file. The script parses the output to quantify failures and errors using the `extract_test_results` helper. If issues are found, the system creates timestamped backups of the current files. It then invokes an external function, `fix_errors_from_unit_tests`, to generate potential fixes for the code or the test itself based on the error logs and a provided prompt.

A key feature is the inclusion of a secondary verification step: if the source code is modified, a verification program is run to prevent regressions. If this verification fails, changes are immediately reverted. The script tracks the performance of each attempt using the `IterationResult` dataclass. After reaching the maximum number of attempts or the budget limit, it performs a final assessment. If the final state is less stable than a previous attempt, the script automatically restores the files from the most successful iteration found during the process.",01872cfcc6c3f5095551acd109e06b35ece96c15ca75dcf40ac395c3ed867e40
context/change/6/initial_xml_tagger_python.prompt,"This document outlines the requirements for creating a Python function named ""xml_tagger"" designed to enhance LLM prompts by adding XML tags for better structure and readability. Acting as an expert Python engineer, the developer must implement this function using the Langchain library and LCEL (LangChain Expression Language). The function takes a raw prompt string, along with strength and temperature parameters for the LLM, as input. The process involves a multi-step workflow: first, loading specific prompt templates from a project path; second, using an initial LLM call to generate an XML-tagged analysis of the raw prompt; and third, using a secondary extraction prompt to isolate the final XML-tagged string from the analysis. The implementation requires the use of the `rich` library for pretty-printing console output, including token counts calculated via `tiktoken` and cost estimates based on the selected model. The final output is the cleaned, XML-tagged prompt string. The code must also include robust error handling for missing parameters or model response issues.",a0bf28aba89354b28131e03d81f319567db18c9dd8caa0051a4d7ba4393b0cec
context/change/6/final_xml_tagger_python.prompt,"This document outlines the requirements for creating a Python function named ""xml_tagger"" designed to enhance LLM prompts by adding structured XML tags. The function takes a raw prompt string, along with strength and temperature parameters, and returns the enhanced prompt and the total execution cost. The implementation must utilize the Langchain library's LCEL (LangChain Expression Language) and the Python ""rich"" library for pretty-printing console output. The process involves a multi-step workflow: first, loading specific prompt templates from a project directory defined by the $PDD_PATH environment variable; second, running an initial LCEL chain to generate an XML-tagged analysis of the raw prompt; and third, running a secondary LCEL chain to extract the final tagged content into a JSON format. The function must also integrate a helper utility for LLM selection and token counting to calculate and display costs at each step. Finally, the code is expected to handle errors gracefully and return the cleaned ""xml_tagged"" string alongside the total calculated cost.",9ead2043eef028452b68a623678b10ff6e54597642fa161bde840f44fb7f3a2c
context/change/6/initial_xml_tagger.py,"This Python script defines a function named `xml_tagger` designed to process raw text prompts by applying XML tagging using a Large Language Model (LLM) workflow orchestrated via LangChain. The script begins by setting up a SQLite cache for LLM responses and defining a Pydantic model, `XMLTaggedOutput`, to structure the final JSON output. The core function takes a raw prompt, a strength parameter, and a temperature setting as inputs. It retrieves prompt templates from file paths specified by an environment variable (`PDD_PATH`) and selects an appropriate LLM using a custom `llm_selector` utility. The workflow proceeds in two main stages: first, it converts the raw prompt into an XML-oriented analysis using an `xml_convertor` template; second, it extracts the specific XML-tagged content using an `extract_xml` template and a JSON output parser. Throughout the process, the script calculates and prints token counts and estimated costs using `tiktoken`, utilizing the `rich` library for formatted console output. Finally, it returns the processed XML-tagged string.",57efa1ce5b19060faa671ae23b6c64c631bb9fdea304d3fc38d7079719987b74
context/change/6/change.prompt,"The provided text outlines specific technical requirements for updating a software component, likely within a Large Language Model (LLM) application framework. The primary directive is to replace the current method of counting tokens in prompts; specifically, the system should switch from using the 'tiktoken' library to a custom 'token_counter' utility derived from the 'llm_selector' module. This change suggests a move towards a more standardized or internal mechanism for handling token metrics. Additionally, the text specifies an enhancement for the 'xml tagger' component. It requires that this tagger be updated to calculate and return the total financial cost associated with executing LangChain Expression Language (LCEL) chains. This implies a need for better observability regarding resource consumption and pricing within the application's execution flow.",0973aaea3cef9e6549d2ee870b42ca7ca13f2ff6bdc1ee151e3a303849f55cd2
context/change/17/updated_unfinished_prompt_python.prompt,"This file outlines the specifications for creating a Python function named `unfinished_prompt`. The primary goal of this function is to analyze a given text string (`prompt_text`) and determine whether it represents a complete thought or requires continuation. The function accepts inputs for the prompt text, model strength, temperature, and a verbosity flag. It is expected to return a tuple containing a structured reasoning string, a boolean indicating completion status (`is_finished`), the total cost, and the model name. The implementation steps involve loading a specific prompt template (`unfinished_prompt_LLM`) and utilizing an internal `llm_invoke` module to process the text, parsing the resulting Pydantic output to extract the required return values. The file also references internal context examples for loading templates and invoking LLMs.",69a573fc153e4f4f86817317054b0854bb3d0fd93b2d1238a8d78c62e76357a3
context/change/17/continue_generation.py,"This Python file defines a `continue_generation` function designed to iteratively extend and complete text generation using Large Language Models (LLMs). It leverages the LangChain library for prompt management and chaining, and Pydantic for structured output parsing. The process involves loading and preprocessing specific prompts from an environment-defined path, selecting appropriate LLMs based on desired strength and temperature, and then entering a loop. In this loop, the function generates text, checks if the generation is complete using a helper function (`unfinished_prompt`), and trims the results to ensure coherence. It calculates and tracks the cost of API usage throughout the process. The module also includes helper classes for structured output (`TrimResultsOutput`, `TrimResultsContinuedOutput`) and a utility function `extract_text_from_response` to handle JSON parsing of LLM responses. The final output is a completed code block or text, the total cost, and the model name used.",6d48786ab0ae7ac29cb5852dfca0de6af5363657967f82e99d009ae0a042d97a
context/change/17/updated_continue_generation_python.prompt,"This file outlines the specifications for a Python function named ""continue_generation,"" designed to manage and complete the output generation process of a Large Language Model (LLM). The function takes an initial formatted prompt, partial LLM output, and configuration parameters like strength, temperature, and verbosity as inputs. Its primary goal is to produce a complete ""final_llm_output"" string, along with the total cost and model name used.

The process involves several distinct steps: loading and preprocessing specific prompt templates ('continue_generation_LLM', 'trim_results_start_LLM', 'trim_results_LLM'), and initially trimming the starting output. The core logic resides in a loop where the function repeatedly invokes the LLM to continue generation. It uses a helper function, ""unfinished_prompt,"" to check the last 600 characters of the output to determine if the generation is complete. If incomplete, it appends the new content and continues; if complete, it performs a final trim using a specific prompt template before breaking the loop. The function also includes instructions for error handling and verbose logging, ensuring robust execution and cost tracking throughout the iterative generation process.",cc1e0b15c386c41512b665d7fb540d0384ccd0a17b3d6cad8423f984b0a26e78
context/change/17/unfinished_prompt_python.prompt,"This file outlines the specifications for an AI agent acting as an expert Python engineer to generate a function named 'unfinished_prompt'. The purpose of this function is to analyze a given text string ('prompt_text') and determine whether it represents a complete thought or requires continuation. The function takes inputs for the prompt text, model strength, and temperature. It is required to utilize the Langchain LCEL framework, load a specific prompt template from an environment path, and employ an internal 'llm_selector' module for model selection and cost calculation. The expected output includes a structured reasoning string, a boolean indicating completion status ('is_finished'), the total cost of the operation, and the model name used. The process involves specific steps for loading resources, invoking the LLM with JSON output parsing, and pretty-printing execution details like token counts and costs using the 'rich' library.",4fe9f4f136ebb461f65832e3675afe6d31a8e27e2ec324c0980fd4eeb6e3651e
context/change/17/change.prompt,"The provided file content outlines a required update to a prompt or code structure due to a change in methodology. Specifically, it notes that the previous method has been superseded by the functions `llm_invoke` and `load_prompt_template`. The text instructs the user to modify the prompt accordingly to reflect this replacement. To assist with this transition, the file references two examples: a ""before"" state located at `context/change/17/unfinished_prompt_python.prompt` and an ""after"" state located at `context/change/17/updated_unfinished_prompt_python.prompt`. These examples are intended to demonstrate the specific changes needed to align with the new implementation strategy.",fb1bc585a05b415040d556f2c0c63edecd7672dc6f150a251b6879427c138f51
context/change/17/continue_generation_python.prompt,"This document outlines the specifications for creating a Python function named ""continue_generation,"" designed to complete the generation of a prompt using a Large Language Model (LLM). The function takes inputs such as a formatted input prompt, existing LLM output, and model parameters like strength and temperature. It outputs the final completed string, the total cost of execution, and the model name used. The process involves several steps: loading and preprocessing specific prompt files from an environmental path, setting up Langchain LCEL templates, and selecting an appropriate LLM model. The core logic requires running a ""trim_results_start"" chain to extract initial code blocks, followed by a loop that uses a ""continue_generation"" chain to extend the output. This loop persists until an ""unfinished_prompt"" check determines the generation is complete. Finally, the results are trimmed and formatted, costs are calculated across all model invocations, and the final output is returned.",0bad36512ca7aa74cb49aa6b4a53c0d0c0764264897c1c336b9a488f31c12e66
context/change/1/initial_code_generator.py,"This file defines a Python function named `code_generator` designed to generate code based on a user prompt using a Large Language Model (LLM). The process involves several distinct steps orchestrated through a pipeline. First, the raw input prompt is preprocessed using a custom `preprocess` module. Next, a LangChain `PromptTemplate` is created, and an appropriate LLM is selected via an `llm_selector` function based on desired strength and temperature parameters. The system utilizes `tiktoken` to calculate token usage and estimates costs for both input and output, displaying these details via the `rich` library's console. It also implements caching using `SQLiteCache`. After invoking the model chain, the raw result is displayed as Markdown. Finally, the output undergoes post-processing to extract runnable code for the specified programming language, and the total operation cost is calculated and returned alongside the code. Error handling is included to catch and report exceptions during execution.",d9a74c307cfa97c080110b0867bb6cd0e653f0819fa843ba269f20f2b281e244
context/change/1/change.prompt,"The file contents describe a specific code refactoring or implementation directive. It instructs the developer or system to replace the usage of the 'tiktoken' library with a 'token_counter' utility derived from the 'llm_selector' module. The primary purpose of this change is to handle the counting of tokens within a prompt, suggesting a move towards a more abstracted or project-specific token counting mechanism rather than relying directly on the external 'tiktoken' dependency.",c020c75bbee108543aa8569af733d5aee35329ca7c4947d59aa3ed9f3f91c86f
context/change/1/final_code_generator_python.prompt,"The provided text outlines a specification for an expert Python engineer to create a function named ""code_generator."" This function is designed to compile a raw text prompt into a runnable code file, utilizing the Langchain library and LCEL (LangChain Expression Language). The function accepts four inputs: the raw prompt string, the target programming language (e.g., Python, Bash), a strength parameter (float 0-1) for model selection, and a temperature setting for the LLM. The expected outputs are a string of runnable code and the total cost of the operation. The process involves a seven-step workflow: preprocessing the prompt, creating an LCEL template, selecting an appropriate LLM via a selector module, executing the model while tracking token usage and cost, pretty-printing the output using the ""rich"" library (including Markdown formatting), post-processing the result to extract code blocks, and finally returning the clean code and total cost. The specification also references several external context files for examples on preprocessing, LLM selection, and post-processing.",460661d899c5324a82e4172d05852aa94e00225c1f4dc9e6abd1ffdb581649ef
context/change/1/initial_code_generator_python.prompt,"The provided text outlines a specification for an expert Python engineer to create a function named ""code_generator."" This function is designed to compile a raw text prompt into a runnable code file, utilizing the Langchain library and its LCEL (LangChain Expression Language) framework. The function accepts four inputs: the raw prompt string, the target programming language (e.g., Python, Bash), a strength parameter (0-1) to determine the LLM model's capability, and a temperature setting for the model. The expected outputs are a string of runnable code and the total cost of the operation. The process involves a seven-step workflow: preprocessing the prompt, creating an LCEL template, selecting the appropriate LLM based on the strength parameter, executing the model while calculating token usage and cost via tiktoken, pretty-printing the results using the ""rich"" library, post-processing the output to extract code blocks, and finally returning the clean code and total cost. The specification also references several external context files for examples on preprocessing, LLM selection, postprocessing, and token counting.",c29bf72464415b60b85b670a2a85d06139818ebe9c821cf72c07bdfb877d76a5
context/change/10/initial_fix_errors_from_unit_tests_python.prompt,"The file outlines a specification for an expert Python Software Engineer to create a function named ""fix_errors_from_unit_tests"". This function is designed to automatically resolve errors in code and unit tests using Langchain LCEL (LangChain Expression Language). The process involves a multi-step workflow: first, loading specific prompt files from a project directory; second, invoking an LLM (selected based on strength and temperature) to generate fixes for the provided unit test, code, and error logs; and third, using a second LLM call with a JSON parser to extract structured data—specifically boolean flags for updates and the fixed code strings. The function must also calculate and pretty-print the token usage and financial cost of the LLM operations using the ""rich"" library for console output. It requires handling inputs like the unit test code, source code, error messages, and model parameters, ultimately returning the fixed code, update status flags, and total cost.",e996814013526e08ecd4258f33dc84ba9c3594d3662496c081332567bce8e5a3
context/change/10/change.prompt,"The file outlines a specific modification to the `fix_errors_from_unit_tests` function. The primary change involves adding a new input parameter named `error_file`. This file is intended to store the output from the initial LCEL (LangChain Expression Language) execution, which is also printed to the console. To ensure clarity and readability, the instructions specify that a separator must be inserted between the existing content of the `error_file` and the newly appended LCEL output. This separation is crucial for distinguishing which parts of the log originate from specific function executions, thereby facilitating easier debugging and log analysis.",545412589a4de78e8428fb1e82973fa61366869013f81bf2ffde503cf66a1153
context/change/10/final_fix_errors_from_unit_tests_python.prompt,"This document outlines the specifications for a Python function named ""fix_errors_from_unit_tests,"" designed to automate the debugging process using Large Language Models (LLMs) via LangChain. The function takes inputs including unit test code, the code under test, error messages, a log file path, and LLM configuration parameters (strength and temperature). Its primary goal is to analyze errors and generate fixed versions of both the unit test and the source code. 

The process involves an 11-step workflow. First, it loads specific prompt templates from a project directory. It then executes a two-stage LangChain LCEL pipeline. The first stage uses the LLM to analyze the errors and propose fixes, logging the output and costs to a specified file while providing console feedback using the ""rich"" library. The second stage parses this analysis into structured JSON data to extract boolean flags for updates and the actual fixed code strings. Finally, the function calculates the total token cost for both LLM runs and returns the update status, fixed code, and total cost. The instructions emphasize robust error handling for file I/O and LLM interactions.",afdcebec86210c8c94977ec521b9f57743078ff5613e8050d960f449f804d7ad
context/change/10/initial_fix_errors_from_unit_tests.py,"This Python script defines a function named `fix_errors_from_unit_tests` designed to automatically resolve errors in unit tests and their corresponding code using Large Language Models (LLMs). The script leverages the `langchain` library for LLM orchestration, `rich` for terminal formatting, and a custom `llm_selector` module for model management. It initializes an SQLite cache to optimize performance and reduce costs. The core function operates in a two-step process. First, it loads prompt templates from an external directory specified by the `PDD_PATH` environment variable. It then invokes an LLM to analyze the provided unit test, source code, and error message to generate a fix. Second, it uses another LLM call to parse the previous output and extract the specific corrected code and unit test strings into a structured JSON format. Throughout execution, the script calculates and displays token usage and financial costs associated with the API calls. Finally, it returns a tuple containing flags indicating whether updates were made, the fixed code segments, and the total operation cost, while handling potential exceptions gracefully.",7a3eb969bcaa76dc85d59c4013a41a7da5bb7472daf97df2b2cad0475642d2c7
context/change/19/updated_unfinished_prompt_python.prompt,"This file outlines the specifications for creating a Python function named `unfinished_prompt`. The primary goal of this function is to analyze a given text string (`prompt_text`) and determine whether it represents a complete thought or requires continuation. The function accepts inputs for the prompt text, model strength, temperature, and a verbosity flag. It is expected to return a tuple containing a structured reasoning string, a boolean indicating completion status (`is_finished`), the total cost, and the model name. The implementation steps involve loading a specific prompt template (`unfinished_prompt_LLM`) and utilizing an internal `llm_invoke` module to process the text, parsing the resulting Pydantic output to extract the required return values. The file also references internal context examples for loading templates and invoking LLMs.",69a573fc153e4f4f86817317054b0854bb3d0fd93b2d1238a8d78c62e76357a3
context/change/19/updated_context_generator_python.prompt,"This file outlines the specifications for creating a Python function named ""context_generator."" The primary goal of this function is to generate a concise usage example for a provided code module. The function accepts several inputs, including the code module itself, the original prompt used to create it, the programming language (defaulting to Python), and parameters for the LLM model such as strength and temperature. It returns a tuple containing the generated example code, the total cost of the operation, and the name of the model used. The process involves four main steps: loading a specific prompt template ('example_generator_LLM'), invoking the LLM with the provided parameters, detecting and handling incomplete generations using internal helper functions, and finally post-processing the result before returning the output. The file also references several internal modules for tasks like loading templates, invoking the LLM, and handling unfinished prompts.",fa70f2c3b7026a9c4b6d13b1a54692de580c0e3783cf2417c84899a51f2f6176
context/change/19/unfinished_prompt_python.prompt,"This file outlines the specifications for an AI agent acting as an expert Python engineer to generate a function named 'unfinished_prompt'. The purpose of this function is to analyze a given text string ('prompt_text') and determine whether it represents a complete thought or requires continuation. The function takes inputs for the prompt text, model strength, and temperature. It is required to utilize the Langchain LCEL framework, load a specific prompt template from an environment path, and employ an internal 'llm_selector' module for model selection and cost calculation. The expected output includes a structured reasoning string, a boolean indicating completion status ('is_finished'), the total cost of the operation, and the model name used. The process involves specific steps for loading resources, invoking the LLM with JSON output parsing, and pretty-printing execution details like token counts and costs using the 'rich' library.",4fe9f4f136ebb461f65832e3675afe6d31a8e27e2ec324c0980fd4eeb6e3651e
context/change/19/change.prompt,"The file provides instructions for updating a prompt to reflect a change in methodology. Specifically, it mandates replacing the current method with `llm_invoke` and `load_prompt_template`. To illustrate the required modification, the file includes an example section contrasting a ""before"" state, which references an `unfinished_prompt_python.prompt` file, with an ""after"" state that points to an `updated_unfinished_prompt_python.prompt` file. This structure serves as a guide for implementing the necessary code or prompt refactoring.",0ff732468c287e399268934d058fac0d96a4fdbc42d6bb339bbd00ae52a07e2b
context/change/19/context_generator_python.prompt,"The provided file outlines the specifications for creating a Python function named ""context_generator."" This function is designed to generate concise usage examples for a given code module using a Large Language Model (LLM). The specification details the required inputs, which include the code module string, the original prompt used to create it, the programming language (defaulting to Python), and model parameters like strength and temperature. The expected outputs are the generated example code, the total cost of the operation, and the name of the model used.

The document provides a step-by-step implementation guide using LangChain Expression Language (LCEL). Key steps include loading a specific prompt file from an environment path, preprocessing prompts, selecting an LLM via an internal `llm_selector` module, and invoking the model. It also mandates handling incomplete generations using an `unfinished_prompt` function and a `continue_generation` function, as well as post-processing the final output. The file references several internal context examples for tasks like preprocessing, token counting, and post-processing, ensuring the engineer follows specific internal patterns and cost-tracking mechanisms.",1a59e3e1f7d3aa598a9a92552c6300b827c1d0791903757150d7fbbee41ff889
context/change/19/context_generator.py,"This Python file defines a function named `context_generator` designed to generate concise usage examples for a given code module using a Large Language Model (LLM). The process begins by validating the `PDD_PATH` environment variable and loading a specific prompt template (`example_generator_LLM.prompt`). It utilizes a custom `llm_selector` to choose an appropriate model based on desired strength and temperature. The function preprocesses both the template and the user prompt before invoking a LangChain LCEL pipeline to generate the initial output. It includes logic to handle incomplete generations by checking the output's completion status and continuing generation if necessary. Finally, the result undergoes post-processing to refine the code example. Throughout the execution, the script tracks token usage and calculates the total financial cost of the LLM operations, logging debug information and errors to the console via the `rich` library.",046c50404d58d06586c6acb481acbd2e20042de7155b5f7eaffab5ebebceda0a
context/change/8/initial_fix_errors_from_unit_tests_python.prompt,"This document outlines the requirements for a Python function named `fix_errors_from_unit_tests`, designed to automatically resolve unit test errors using a Large Language Model (LLM) via Langchain. The function takes the unit test code, the code under test, the error message, and a model strength parameter as inputs. It operates in a multi-step process: first, it loads specific prompt templates from a project directory defined by the `$PDD_PATH` environment variable. Then, it constructs a Langchain LCEL pipeline to analyze the errors and generate fixes, utilizing a temperature of 0 for deterministic output. The process involves calculating and pretty-printing token usage and costs using `tiktoken` and the `rich` library. A second LCEL step parses the LLM's textual analysis into structured JSON output, extracting boolean flags for updates (`update_unit_test`, `update_code`) and the corrected code strings (`fixed_unit_test`, `fixed_code`). Finally, the function returns these four values after displaying the total cost of the operation.",d7a9a11b7642bf867eeae01da392ae21070135994493a5ca37adb1ef2d426127
context/change/8/change.prompt,"The file outlines specific updates required for a codebase involving LLM operations. First, it mandates replacing the direct usage of the 'tiktoken' library with a 'token_counter' utility from the 'llm_selector' module for counting tokens in prompts. Second, the 'fix_errors_from_unit_tests' function needs to be updated to accept a 'temperature' parameter and utilize it within its logic. Finally, the system is required to calculate and output the total financial cost associated with the LangChain Expression Language (LCEL) execution runs.",d199bc8789406397849d0c5ab6389d9db19cff85a3724c2a0333ba7b8c455ba1
context/change/8/final_fix_errors_from_unit_tests_python.prompt,"This document outlines the requirements for creating a Python function named `fix_errors_from_unit_tests`, designed to resolve errors in code and unit tests using Langchain LCEL. The function takes inputs such as the unit test code, the code under test, error messages, and LLM configuration parameters (strength and temperature). It outputs boolean flags indicating updates, the fixed code and unit test strings, and the total operational cost. The process involves a multi-step workflow: loading specific prompt files from the project directory, selecting an LLM model, and executing two distinct LCEL chains. The first chain generates a fix based on the errors, while the second extracts structured JSON data (updated code/tests) from that fix. The function must also utilize the `rich` library for pretty-printing console output, track token usage and costs for both LLM runs, and handle potential errors gracefully.",9846fde278e0a34b15bcc506160c161df3fc3bdb53004615f524770c460bfae5
context/change/8/initial_fix_errors_from_unit_tests.py,"This Python script defines a function, `fix_errors_from_unit_tests`, designed to automatically correct coding errors identified during unit testing using Large Language Models (LLMs). The script leverages the LangChain framework to orchestrate a two-step process. First, it loads prompt templates from external files and uses an LLM (selected via a custom `llm_selector` based on a strength parameter) to analyze the provided unit test, source code, and error message. This step generates a textual explanation and proposed fix. Second, the script employs another LLM call to parse the initial analysis into a structured format using a Pydantic model named `FixResult`. This structured output separates the results into boolean flags indicating whether updates are needed and strings containing the actual fixed code and unit tests. The script includes functionality for caching LLM responses to SQLite to save costs, and it utilizes `tiktoken` to calculate and print the token usage and estimated costs for each API call. An example usage block at the end demonstrates how to invoke the function with sample inputs.",0093a6e6dba15c0e14d9df90de11940a49782e1bd0b4bdf22db995718900adb8
context/change/21/change.prompt,"The provided file content outlines instructions for creating a wrapper prompt, specifically designated as `*_main`, to simplify the integration of prompts into a main CLI program. The goal is to reduce complexity by enabling the CLI to execute the prompt with a single line of code. The instructions specify that only the `_main` wrapper prompt should be created, as the core input prompt resides in a separate file. The content includes an example structure demonstrating a `before_example` with a prompt and code, and an `after_example` showing the resulting `auto_deps_main_python.prompt`. Additionally, it references a CLI command README file for further details on command functionality.",df1f9c1accc768b7a9a42e859d92151926fcdc26230020e9ef480483c8864d2e
context/change/21/auto_deps_main_python.prompt,"The provided file is a prompt template designed for an AI model to generate a Python function named `auto_deps_main`. This function is intended to be a core component of a command-line interface (CLI) tool called `pdd`, specifically handling the logic for an `auto-deps` command. The prompt outlines the function's inputs, which include a Click context object, paths for input prompts and directories, a CSV path for dependency tracking, an output path, and a force scan flag. It specifies the expected output as a tuple containing the modified prompt string, the operation cost, and the model name. The template includes references to external context files, such as a Python preamble, Click library usage examples, and internal module examples for path construction and include insertion. It also references a README file to provide context on how the `auto-deps` command should function within the broader application.",7a2db061d10da159b9656f76988e86fe480df50db534f946fcc7b1b79eac4a91
context/change/21/auto_include_python.prompt,"This file outlines the specifications for a Python function named ""auto_include,"" designed to automatically identify and generate necessary dependencies based on an input prompt. The function takes several inputs, including an input prompt string, a directory path for dependencies, a CSV file string, and parameters for the LLM model such as strength, temperature, and verbosity. It returns a tuple containing the generated dependencies string, an updated CSV output, the total cost of the operation, and the name of the LLM model used.

The process involves a multi-step workflow: first, loading specific prompt templates ('auto_include_LLM' and 'extract_auto_include_LLM'); second, utilizing a 'summarize_directory' function to process the CSV file and generate a list of available includes (file paths and summaries); third, invoking an LLM with the input prompt and available includes to select relevant dependencies; and fourth, extracting the final string of includes using a secondary LLM prompt. The file also references internal module examples for loading templates, invoking LLMs, and summarizing directories to guide the implementation.",218dc5ec63e4f861af8ae1edcda9f958dd07f7700e6d46a5476f42d9eec41786
context/change/21/auto_include.py,"This Python script defines a function named `auto_include` designed to automatically identify and insert relevant file dependencies into a given prompt using Large Language Models (LLMs). The core functionality involves a multi-step process: first, it validates inputs and loads necessary prompt templates. It then calls `summarize_directory` to generate summaries of files within a specified path, potentially utilizing an existing CSV cache. These summaries are parsed and fed into an LLM via `llm_invoke` to determine which files are relevant to the user's `input_prompt`. A subsequent extraction step refines this output into a structured string of includes using a Pydantic model (`AutoIncludeOutput`). The script utilizes the `rich` library for console output and error handling. It returns a tuple containing the identified dependencies, the CSV summary data, the total cost of LLM usage, and the model name used. A `main` function is provided to demonstrate example usage with sample inputs.",ad932af885a1834d387f2789811b3f369e0d134de0271df965e3d7a065dd7f1c
context/change/simple_math/change.prompt,"The provided file content appears to be a brief instruction or context prompt intended for a software development task, specifically related to a regression test for a module or function named ""simple_math"". The text outlines a specific refactoring requirement: the addition of type hints to a function's signature and its return value. This suggests the code is likely written in a language that supports optional static typing, such as Python. The prompt serves as a directive for a developer or an automated coding assistant to modify existing code to improve type safety and readability without altering the underlying logic, ensuring that the ""simple_math"" functionality remains consistent during regression testing.",95fb98651b0fd99b0bb8144f7a6ef78d1f3558f74ceb980d7d44fc2713a539f4
context/change/4/initial_postprocess.py,"This Python script defines a `postprocess` function designed to extract code from raw LLM output, utilizing a secondary LLM call if necessary. The function takes the raw output, the target programming language, a strength parameter, and a temperature setting as inputs. If the `strength` is set to 0, it bypasses the LLM extraction and defaults to a simpler `postprocess_0` function. For non-zero strength values, the script loads a specific prompt template (`extract_code_LLM.prompt`) from a directory defined by the `PDD_PATH` environment variable. It then selects an appropriate LLM model using `llm_selector` based on the provided strength and temperature. The script constructs a LangChain LCEL chain combining the prompt, the selected model, and a JSON output parser. Before execution, it calculates and prints the estimated cost based on token counts using `tiktoken`. The chain is then invoked to process the input, and the resulting extracted code is retrieved from the JSON response. Finally, the script prints the extracted code and the total cost (input plus output) using the `rich` library for formatting, before returning both the code and the cost to the caller.",6f98012fc0b9ad94acba1c8ec2358bd2e5b695e79ef6f0f868e43d2705abe0e3
context/change/4/initial_postprocess_python.prompt,"The file outlines the requirements for creating a Python function named `postprocess` designed to extract and clean executable code from raw LLM output. The function accepts inputs including the raw `llm_output`, the target programming `language`, a `strength` parameter (defaulting to 0.9), and `temperature`. The process involves a conditional logic flow: if the strength is 0, it defaults to a zero-cost extraction method (`postprocess_0`). Otherwise, it utilizes a more complex workflow involving the loading of a specific prompt file (`extract_code_LLM.prompt`) from the project path, creating a Langchain LCEL template, and selecting an LLM via a helper function. The function must calculate and pretty-print token usage and costs using `tiktoken`, invoke the model to return a JSON object containing the cleaned code, and strip markdown backticks if present. The final output includes the `extracted_code` string and the `total_cost` of the operation, with requirements for graceful error handling throughout.",05495463cb8cde9e403585cea916616aeea9a41d3c928b38955c6710db56c5d2
context/change/4/change.prompt,"The file contents describe a specific code refactoring or implementation directive. It instructs the developer or system to replace the usage of the 'tiktoken' library with a 'token_counter' utility derived from the 'llm_selector' module. The primary purpose of this change is to handle the counting of tokens within a prompt, suggesting a move towards a more abstracted or project-specific token counting mechanism rather than relying directly on the external 'tiktoken' dependency.",c020c75bbee108543aa8569af733d5aee35329ca7c4947d59aa3ed9f3f91c86f
context/change/4/final_postprocess_python.prompt,"The file outlines the requirements for a Python function named `postprocess`, designed to extract and clean executable code from a Large Language Model's (LLM) mixed text and code output. The function accepts inputs including the raw `llm_output`, the target programming `language`, a `strength` parameter (defaulting to 0.9), and `temperature`. If the `strength` is 0, the function defaults to a zero-cost post-processing method. Otherwise, it utilizes a Langchain LCEL pipeline, loading a specific prompt template from an environment path (`$PDD_PATH`). The process involves selecting an LLM model, calculating token usage and costs, and invoking the model to return a JSON object containing the cleaned code. The function must handle stripping markdown backticks, pretty-printing status updates and the final code using the `rich` library, and returning both the `extracted_code` string and the `total_cost` of the operation. Error handling for missing parameters or model issues is required.",a01ccd6084923b345905f2a253f1dd8e40d26e68c5fb944341daf53c4a6b50ae
context/change/15/initial_cli.py,"This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` and `rich` libraries. It serves as the main entry point for the application, orchestrating various AI-driven development tasks. The CLI includes commands for generating code from prompts (`generate`), creating examples (`example`), generating unit tests (`test`), preprocessing prompts (`preprocess`), fixing code errors iteratively (`fix`), splitting complex prompts (`split`), modifying prompts based on changes (`change`), and updating prompts to reflect code modifications (`update`). It also includes utility functions for logging costs to a CSV file and installing shell tab completion. The tool integrates with LangChain, utilizing SQLite caching to optimize costs and performance. Each command handles file I/O, invokes specific backend logic (imported from other modules), and provides user feedback via a rich console interface.",b607c3e67702ad3bc5e61cc40ce85571e4ff09015de96016c8d7d56eb78b8c74
context/change/15/README.md,"This document outlines the features and usage of the PDD (Prompt-Driven Development) Command Line Interface, a tool designed to streamline software development using AI models. PDD enables developers to generate code, create examples, run unit tests, and manage prompt files across various programming languages like Python, JavaScript, and Java. The CLI supports a wide array of commands including `generate` for creating code, `fix` for debugging, `split` for managing large prompts, and `crash` for resolving runtime errors. It offers global options for controlling AI model strength and temperature, as well as cost tracking capabilities via CSV output. The tool emphasizes flexibility through environment variables for output paths and supports multi-command chaining for complex workflows. Additionally, the documentation covers security considerations, troubleshooting steps, and integration strategies for CI/CD pipelines, positioning PDD as a comprehensive solution for maintaining code quality and efficiency in prompt-driven engineering environments.",013091190282df4fc6944ba7f0aa93ee1785953e3815a2b2aee2c9245ff0894d
context/change/15/change.prompt,"The provided file content is a brief instruction related to updating documentation or prompts. It specifically notes that new commands have been introduced, referencing a README file located at 'context/change/15/README.md'. The core directive is to incorporate these newly added commands, such as 'detect', into the relevant prompt structure. The text serves as a changelog note or a developer task to ensure that the system's prompts are synchronized with the latest command updates documented in the specified path.",e84c7f0f10bcd76b0ef282e5d97d67532efa99bdb3e468c2fa02d2e0e5435a4b
context/change/15/initial_cli_python.prompt,"The provided text outlines the specifications for building a Python command-line interface (CLI) tool named ""pdd"". This tool is designed to be built using the `click` library for command handling and the `rich` library for pretty-printing console output. The project structure includes directories for source code, prompts, context, and data. The core functionality revolves around several commands: `generate` for creating runnable code from prompts; `example` for generating context examples from code; `test` for creating unit tests; `preprocess` for preparing prompts (with an XML sub-command); `fix` for resolving code errors based on unit test feedback (including a looping mechanism); `split` for dividing prompt files; `change` for modifying prompts; and `update` for refreshing prompt files. Additionally, the tool includes an `install_completion` command to manage shell auto-completion scripts. The instructions emphasize avoiding naming conflicts between CLI commands and internal functions, using relative imports, and leveraging specific helper modules for file loading and path construction.",f82bd3b41440348f58b8d2a98606b1edc4db6ff8eb20922551e7fd4f79c9003b
context/change/15/modified_initial_cli.prompt,"The provided text serves as a detailed specification prompt for an AI to generate a Python command-line interface (CLI) tool named ""pdd"". The tool is designed to be built using the `click` library for command handling and the `rich` library for formatted console output. The prompt outlines the project's directory structure and lists specific commands the CLI must support, including `generate`, `example`, `test`, `preprocess`, `fix`, `split`, `change`, `update`, `detect`, `conflicts`, `crash`, and `install_completion`. For each command, the prompt references specific example files (e.g., `<./context/code_generator_example.py>`) that demonstrate the intended functionality, such as generating code from prompts, creating unit tests, preprocessing files, and fixing errors based on test outputs. It also includes instructions on handling imports to avoid naming conflicts and managing file paths.",5415a463b9a2048fb889ced2d0c4f76a727cbe7b8e2d0d95ade7500ce2d35495
context/change/3/initial_test_generator_python.prompt,"The file outlines the requirements for a Python function named `test_generator`, designed to automatically create unit tests from a given code file using a Large Language Model (LLM). The function takes inputs such as the original prompt, the source code, model strength, temperature, and the target language. It returns the generated unit test code and the total cost of the operation. The process involves several steps: loading a specific prompt template from an environment path, preprocessing the prompt, creating a LangChain LCEL template, selecting an appropriate LLM based on the provided parameters, and invoking the model. The function is also required to utilize the `rich` library for pretty-printing console output, including token counts calculated via `tiktoken` and cost estimates. Finally, the output must be post-processed to extract runnable code from the model's response before returning the final unit test and total cost.",2cbcf0515c1fb80cecd158b4311b995eee532f589f056ded1da75cfecd26989f
context/change/3/intial_test_generator.py,"This Python script defines a function named `test_generator` designed to automate the creation of unit tests for a given piece of code. The function orchestrates a workflow involving Large Language Models (LLMs) using the LangChain library. It begins by loading a specific prompt template from a file path defined in environment variables. The script then preprocesses this template and initializes an LLM via a custom `llm_selector` module, which selects a model based on provided strength and temperature parameters. The core logic involves constructing a LangChain pipeline that feeds the original task prompt, the source code, and the programming language into the model to generate test cases. Throughout the process, the script calculates and displays estimated costs for input and output tokens using `tiktoken`. After generating the initial response, it utilizes a `postprocess` function to refine the output into usable unit test code. Finally, the function returns the generated unit test and the total calculated cost, while handling and logging any exceptions that occur during execution using the `rich` library for console output.",461bc71259c7cc21645e4109483b1e84b90338709bddc6904b9d336e8157a014
context/change/3/final_test_generator_python.prompt,"The file outlines the requirements for a Python function named `test_generator`, designed to automatically create unit tests from a given code file. Acting as an expert Python Software Engineer, the function takes inputs such as the original prompt, the source code, model strength, temperature, and the target language. It utilizes the Langchain library and LCEL (LangChain Expression Language) to process these inputs. The workflow involves loading a specific prompt template from an environment path, preprocessing it, selecting an appropriate LLM via `llm_selector`, and invoking the model while tracking token usage and cost. The output is then post-processed to extract runnable unit test code from the model's response, separating code from text. Finally, the function returns the generated unit test code and the total calculated cost, while utilizing the Python `rich` library for pretty-printing console output throughout the process.",9b81f2b3c56affef9963b6ef595d45329651b99f9f33bf60c5798b5114650f77
context/change/3/change.prompt,"The file contents describe a specific code refactoring or implementation directive. It instructs the developer or system to replace the usage of the 'tiktoken' library with a 'token_counter' utility derived from the 'llm_selector' module. The primary purpose of this change is to handle the counting of tokens within a prompt, suggesting a move towards a more abstracted or project-specific token counting mechanism rather than relying directly on the external 'tiktoken' dependency.",c020c75bbee108543aa8569af733d5aee35329ca7c4947d59aa3ed9f3f91c86f
context/change/12/final_unfinished_prompt_python.prompt,"This document outlines the requirements for creating a Python function named 'unfinished_prompt', designed to evaluate whether a given text prompt is complete or requires continuation. The function takes 'prompt_text', 'strength', and 'temperature' as inputs and returns a reasoning string, a boolean status ('is_finished'), and an optional total cost. The implementation process involves five steps: loading a specific prompt file using the '$PDD_PATH' environment variable, creating a Langchain LCEL template that outputs JSON, selecting an LLM model via a helper function, executing the analysis while calculating and printing token usage and costs, and finally returning the results. The instructions emphasize using relative imports for dependencies like 'llm_selector', handling errors gracefully, and utilizing the 'rich' library for pretty-printing status updates and results.",49363e12ed87470d174ca3735c6f0484e9acbced0ada19fc3dc965b9e808722e
context/change/12/initial_postprocess.py,"This Python file defines a `postprocess` function designed to extract and format code snippets from the raw text output of a Large Language Model (LLM). The function takes the LLM's raw output, the target programming language, and optional parameters for model strength and temperature as inputs. It first checks if the `strength` parameter is zero; if so, it defaults to a simpler `postprocess_0` function. Otherwise, it proceeds with a more complex workflow using LangChain components. This involves loading a specific prompt template from a file path defined by the `PDD_PATH` environment variable and initializing an LLM via an `llm_selector` utility. The function constructs a processing chain that feeds the raw output into the selected model to isolate the code. It calculates and prints token usage and estimated costs for both input and output using the `rich` library for console formatting. Finally, it cleans the extracted code by removing markdown code block delimiters (triple backticks) and returns a tuple containing the clean code string and the total estimated cost. Error handling is included to catch and report exceptions during the process.",845d5ada84654630518eda1f06e372bd2af87a230077ce8078efc543b9286714
context/change/12/initial_postprocess_python.prompt,"The file outlines the requirements for creating a Python function named `postprocess` designed to refine raw string output from a Large Language Model (LLM) into executable code. The function accepts the LLM's output, the target programming language, and parameters for model strength and temperature. Its primary goal is to extract code blocks while properly commenting out non-code text. The logic involves a conditional workflow: if the strength parameter is zero, it defaults to a zero-cost post-processing method (`postprocess_0`). Otherwise, it employs a more complex Langchain LCEL pipeline. This pipeline involves loading a specific prompt template from an environment path, selecting an LLM model via a helper function, and invoking the model to generate a JSON response containing the cleaned code. The function must also handle token counting and cost estimation, pretty-print status updates and the final code using the `rich` library, and strip markdown code fences (triple backticks) from the result. Error handling and relative imports for dependencies are also required.",5fe8c783e322fab102ae0ea87817e30a24571dfc745e235568a8b61216f88478
context/change/12/change.prompt,"The file contains a single, specific instruction intended for a Large Language Model (LLM). This instruction directs the model to generate a new prompt. The goal of this generated prompt is to create a function named 'unfinished_prompt'. This function is specifically designed to handle the execution of another prompt identified as 'unfinished_prompt_LLM'. The file includes an XML-like tag structure referencing an external file path ('prompts/unfinished_prompt_LLM.prompt'), suggesting a system where prompts are modular and can include other prompt files.",af205548a344020b63fb246f92c6b1ef95390f76693c4efee4e3d03e86423306
context/change/2/final_context_generator_python.prompt,"The provided text outlines a specification for an expert Python engineer to create a function named ""context_generator."" This function is designed to automatically generate concise usage examples for a given code module. The function takes several inputs: the code module itself, the original prompt used to create it, the programming language (defaulting to Python), and parameters for the LLM model such as strength and temperature. The expected outputs are the generated example code string and the total computational cost.

The implementation plan involves a multi-step process using LangChain LCEL (LangChain Expression Language). It requires loading a specific prompt template from a project directory defined by an environment variable ($PDD_PATH). The workflow includes selecting an appropriate LLM, preprocessing the input prompt, and invoking the model with specific parameters. Crucially, the function must calculate and display token usage and costs. Finally, the raw output from the LLM must be post-processed to extract runnable code, separating it from explanatory text, before returning the final example and the total cost.",385d6ba50da55b5e361831090410ad30050c67b09f87c78814698504bb5d9894
context/change/2/change.prompt,"The file contents describe a specific code refactoring or implementation directive. It instructs the developer or system to replace the usage of the 'tiktoken' library with a 'token_counter' utility derived from the 'llm_selector' module. The primary purpose of this change is to handle the counting of tokens within a prompt, suggesting a move towards a more abstracted or project-specific token counting mechanism rather than relying directly on the external 'tiktoken' dependency.",c020c75bbee108543aa8569af733d5aee35329ca7c4947d59aa3ed9f3f91c86f
context/change/2/initial_context_generator_python.prompt,"The provided file outlines a specification for an expert Python engineer to create a function named ""context_generator."" This function is designed to automatically generate concise usage examples for a given code module. It takes several inputs: the code module itself, the original prompt used to create it, the programming language (defaulting to Python), and parameters for the LLM model such as strength and temperature. The function is expected to return the generated example code and the total cost of the operation.

The implementation details require using LangChain's LCEL (LangChain Expression Language) framework. The process involves seven specific steps: loading a prompt template from a specific file path defined by an environment variable, creating an LCEL template, selecting an appropriate LLM model, preprocessing the input prompt, and invoking the model with specific parameters. Additionally, the function must calculate and pretty-print token usage and costs using tiktoken, post-process the model's output to extract runnable code, and finally return the clean example code along with the total financial cost.",9cd2c95487a87d5477be43fe8b75e9d7e18c1a665b1dc88bbe634a3c8c6a2501
context/change/2/initial_context_generator.py,"This Python script defines a function named `context_generator` designed to generate example code snippets using a Large Language Model (LLM). The function takes a code module, a user prompt, a target programming language, and model parameters (strength and temperature) as inputs. It orchestrates a multi-step process involving loading a specific prompt template from a file path defined by an environment variable (`PDD_PATH`), selecting an appropriate LLM via a custom `llm_selector`, and preprocessing the user input. The script utilizes LangChain's LCEL syntax to create a processing chain that pipes the formatted prompt through the selected model and parses the string output. It also incorporates `tiktoken` to calculate token usage and estimates the financial cost of the API calls, displaying these metrics to the console using the `rich` library. Finally, the raw output undergoes a post-processing step to refine the code before the function returns both the generated example code and the total calculated cost of the operation.",41caae210b6ef96935fdc2aa17ce7d2d6f9ac8af5c120fff001e27e0d2dd7709
context/change/13/modified_initial_split.prompt,"This document outlines the requirements for creating a Python function named ""continue_generation"" designed to ensure the completeness of Large Language Model (LLM) outputs. The function, intended for a Python package using relative imports and the Rich library for console output, takes a formatted input prompt and the current LLM output as inputs. Its primary goal is to return a fully completed string, ""final_llm_output,"" along with the total cost of generation. The process involves loading a specific prompt file, preprocessing it, and creating a Langchain LCEL template. The function must utilize an ""llm_selector"" for model selection and token counting. It executes a loop where it checks if the generation is incomplete using an ""unfinished_prompt"" helper; if so, it continues generating content until finished. Throughout the process, the function is required to pretty-print outputs, token counts, and estimated costs to the console, while also handling edge cases and errors robustly.",75c138c60dc6b19011ebf541a8f550551a25d47f7b7fd773aa6bb8f3011e3b5f
context/change/13/initial_split_python.prompt,"The file outlines the requirements for creating a Python function named ""split"" designed to decompose a single input prompt into a ""sub_prompt"" and a ""modified_prompt"" without losing functionality. Intended for a Python package using relative imports, the function must utilize the Python Rich library for pretty-printing console output. The process involves loading specific prompt files, preprocessing them (handling curly brackets appropriately), and employing Langchain LCEL templates to interact with an LLM. The function takes inputs including the original prompt, generated code, example usage code, and model parameters (strength, temperature). It executes a multi-step workflow: running the initial prompt through an LLM, calculating token usage and costs via an `llm_selector`, and then using a second extraction prompt to parse the LLM's output into JSON to retrieve the split prompts. Finally, it returns the two resulting prompts and the total operational cost, while ensuring robust error handling for edge cases.",6757971fa9db7569eae50474c720e7b049baeb7b804256a02441be3660ab5414
context/change/13/initial_split.py,"This Python script defines a function named `split` designed to process and split input code using a Large Language Model (LLM). The script leverages the `langchain` library for LLM interactions and prompt management, and the `rich` library for formatted console output. The `split` function takes an input prompt, code, example code, and model parameters (strength and temperature) as arguments. It loads specific prompt templates from files, preprocesses them, and uses a custom `llm_selector` to choose an appropriate model. The function executes a two-step LLM chain: first, it generates a split of the code based on the inputs, calculating token usage and estimated costs; second, it parses the LLM's output into a structured JSON format to extract a 'sub_prompt' and a 'modified_prompt'. The script also includes caching via SQLite to optimize performance and cost. An example usage block at the end demonstrates how to call the function and print the results.",61247194419b17b8cd986efdbbbb6782a34e89b3cfe6e895ce24419490b33578
context/change/13/change.prompt,"The file describes a Python function named 'continue_generation' designed to complete unfinished outputs from a Large Language Model (LLM). This function takes a formatted input prompt and the initial, potentially incomplete, LLM output as arguments. It utilizes a specific prompt template located at 'prompts/continue_generation_LLM.prompt' to generate the missing content. The process involves a loop that checks if the generation is incomplete using a helper function called 'unfinished_prompt'. If the output is detected as unfinished, the function triggers the continuation prompt and appends the new text to the existing LLM output, repeating this cycle until the generation is fully complete.",662ac3b1a52fdde909657242e63a887aaf0b17272d79301a184b51c27049479e
context/change/5/initial_split_python.prompt,"This document outlines the requirements for a Python function named ""split,"" designed to decompose a single input prompt into a ""sub_prompt"" and a ""modified_prompt"" without losing functionality. The function targets an expert Python Software Engineer and mandates the use of the Python Rich library for pretty-printing console output. Inputs include the original prompt, generated code, example usage code, and LLM parameters like strength and temperature. The process involves loading specific prompt files, preprocessing them, and utilizing Langchain LCEL templates to interact with an LLM selected via an `llm_selector`. The workflow requires a two-step LLM interaction: first to generate a split suggestion based on the inputs, and second to extract the specific sub-prompt and modified prompt as JSON. Throughout execution, the function must calculate and display token counts and estimated costs using `tiktoken`. Finally, it returns the two resulting prompts and the total cost, while ensuring robust error handling for edge cases.",6a653c48fcdcf33dcc269eeea3282660082a42c71c64bd1397ec10234fed7570
context/change/5/final_split_python.prompt,"The file outlines the requirements for a Python function named ""split,"" designed to decompose a single input prompt into a ""sub_prompt"" and a ""modified_prompt"" without losing functionality. The function leverages the Langchain LCEL framework and requires inputs such as the original prompt, generated code, example usage code, and LLM parameters like strength and temperature. The process involves loading specific prompt templates, preprocessing them, and executing a two-step LLM interaction: first to generate a split strategy and second to extract the specific prompt components in JSON format. The function must also utilize a custom ""llm_selector"" for model selection and cost estimation, pretty-print all console outputs using the Rich library (including token counts and costs), and handle edge cases robustly. The final output includes the two resulting prompt strings and the total calculated cost of the operation.",29740d1d81ac51d191ef18e748e8ab5a62e41df41a9623e12f4b3b620ba49e7c
context/change/5/initial_split.py,"This file implements a Python function named `split` designed to process prompts and code using Large Language Models (LLMs) via the Langchain library. The function orchestrates a multi-step workflow: first, it loads specific prompt templates from file paths defined by an environment variable (`PDD_PATH`). It then utilizes a custom `llm_selector` to choose an appropriate model based on desired strength and temperature. The core logic involves two main LLM invocations: one to process the initial input (prompt, code, and examples) and a second to extract structured JSON output containing a 'sub_prompt' and a 'modified_prompt'. Throughout the execution, the script uses the `tiktoken` library to calculate token usage and estimates the financial cost of the API calls. Finally, it employs the `rich` library to pretty-print status updates, costs, and the resulting prompts in Markdown format before returning the extracted prompts and the total calculated cost.",90fc7bff1e86307d7e11ef7f24eb8cd2dcb922a75d8a40083c3a22a2c9a47c28
context/change/5/change.prompt,"The file contents describe a specific code refactoring or implementation directive. It instructs the developer or system to replace the usage of the 'tiktoken' library with a 'token_counter' utility derived from the 'llm_selector' module. The primary purpose of this change is to handle the counting of tokens within a prompt, suggesting a move towards a more abstracted or project-specific token counting mechanism rather than relying directly on the external 'tiktoken' dependency.",c020c75bbee108543aa8569af733d5aee35329ca7c4947d59aa3ed9f3f91c86f
context/change/14/initial_change_python.prompt,"This document outlines the requirements for creating a Python function named ""change"" designed to modify an existing prompt based on specific instructions. The function takes inputs including an initial prompt, generated code, modification instructions, and LLM parameters (strength and temperature). It outputs a modified prompt string, the total operational cost, and the model name used. The implementation must utilize the Python Rich library for console output and relative imports for modules. The process involves several steps: loading specific prompt files, preprocessing them (handling curly brackets), creating Langchain LCEL templates, and selecting an LLM model with token counting capabilities. The function executes a two-step LLM process: first, generating a raw modified prompt based on the inputs, and second, extracting the final structured ""modified_prompt"" from that output using a secondary extraction prompt. Throughout execution, the function must calculate and display token counts and estimated costs, handle edge cases, and return the final modified prompt along with cost metrics.",5e4d1b24cfaf48eed80704c64cc75d9c888914189ba700487e3d77c3b6a97f3b
context/change/14/modified_initial_change.prompt,"The file outlines the requirements for a Python function named `detect_change`, designed to analyze a list of prompt files against a change description to determine necessary modifications. The function takes inputs including a list of prompt filenames, a description of the changes, and LLM parameters like strength and temperature. It outputs a list of JSON objects detailing required changes, the total operational cost, and the model name used. The implementation involves several steps: loading specific prompt templates (`detect_change_LLM.prompt` and `extract_detect_change_LLM.prompt`), preprocessing them, and utilizing Langchain LCEL for execution. It requires selecting an LLM model via `llm_selector` to handle token counting and cost estimation. The process includes two main LLM invocations: first to analyze the prompts and description, and second to extract a structured JSON list of changes from the initial analysis. All console output must be formatted using the Python Rich library, and the code should handle edge cases and errors robustly.",6855a0031175a7ed777a9ed3460fe01a7e98cae3057615d2ca3e14e5df1674fa
context/change/14/initial_change.py,"This Python script defines a function named `change` designed to automate the modification of prompts using Large Language Models (LLMs). Leveraging the `langchain` framework for orchestration and the `rich` library for formatted console output, the function executes a multi-step workflow. It begins by loading specific prompt templates from a directory defined by an environment variable and selecting an appropriate LLM configuration via a `llm_selector` helper based on provided strength and temperature parameters. The core logic involves two sequential LangChain operations: first, a generation chain that takes an input prompt, associated code, and a change description to produce a raw modification result; and second, an extraction chain that parses this output to isolate the final `modified_prompt` using a JSON parser. Throughout the process, the code calculates and displays token usage and estimated financial costs for each step. It includes error handling for file access and JSON parsing issues, ultimately returning the modified prompt, the total cost, and the model name used.",fec3fba35a4710e02375bdb017bcf33882c2a3b690733fc9d5c6021c1602785c
context/change/14/change.prompt,"The provided file contents outline a prompt designed to generate Python code for a function named 'detect_change'. This function's primary objective is to create a list of necessary modifications to existing prompts based on a user-provided change description. The function accepts a list of prompt filenames (e.g., [""prompt1_filename"", ""prompt2_filename""]) and a description of the desired change as inputs. To accomplish its task, the function utilizes two specific embedded LLM prompts: 'detect_change_LLM', which is included from 'prompts/detect_change_LLM.prompt', and 'extract_detect_change_LLM', included from 'prompts/extract_detect_change_LLM.prompt'. The process concludes by extracting a structured list identifying which prompts require updates and detailing the specific changes needed for each.",b781292c1505d291af2765278aade539d947b51d6ddae09d042f1dbe7a1a2d64
context/change/22/preprocess.py,"This Python script implements a text preprocessing utility, likely designed for preparing prompts for Large Language Models (LLMs) or template rendering systems. It utilizes the `rich` library to provide styled console feedback during execution. The core functionality is encapsulated in the `preprocess` function, which orchestrates a series of text transformations. The script handles dynamic content insertion through two primary mechanisms: resolving file paths embedded within triple backticks and processing custom XML-like tags. Specifically, it supports an `<include>` tag for inserting external file content and a `<shell>` tag for executing system commands and embedding their standard output. Both inclusion methods support recursive processing. Additionally, the script features a robust mechanism to escape curly braces by doubling them (converting `{` to `{{`), which is crucial for preventing syntax errors in Python string formatting. This escaping logic is context-aware, distinguishing between general text and code blocks, and allows users to specify a list of keys to exclude from escaping, ensuring intended variables remain valid placeholders.",eff9a44440459568c8508c99b180659c2bc81ada5580f793a075296aafd91c2d
context/change/22/preprocess_main_python.prompt,"The provided file is a prompt template designed for an expert Python engineer AI. Its primary objective is to generate a Python function named `preprocess_main`, which serves as a Command Line Interface (CLI) wrapper for preprocessing prompt files. The prompt specifies that the function should utilize the `click` library for CLI interactions, accepting inputs such as the prompt file path, an optional output path, and a boolean flag for XML delimiter insertion. The expected output of the generated function is a tuple containing the preprocessed prompt string, the operation cost, and the model name. The prompt template includes references to external context files, such as Python preambles, examples of `click` usage, and internal module examples (`construct_paths`, `preprocess`, `xml_tagger`). It also references a README file to ensure the generated code aligns with existing documentation and supports global options like verbosity.",5d1fc2fd413af4318ba2f5fc32b59dbd17e313e368bb8b6422ca4a2d17dc2091
context/change/22/change.prompt,"The provided file contains instructions and examples for creating a specific type of wrapper prompt, denoted as `*_main`. The primary goal outlined in the file is to simplify the main Command Line Interface (CLI) program by encapsulating complexity within these wrapper prompts, allowing the CLI to execute tasks with a single line of code. The instructions specify that only the `_main` wrapper prompt should be generated, as the core input prompt resides in a separate file. The content includes XML-like tags defining examples (`<examples>`) that demonstrate the transformation from a `before_example` state (containing a prompt and associated code) to an `after_example` state (the resulting `*_main` prompt). Additionally, it references a README file (`<cli_command_readme>`) which presumably contains further details on the CLI command's operation.",b3adb705bb6357568444a9c46928e8183382cbf8db0a313c7e6e907d15933d3c
context/change/22/preprocess_python.prompt,"The file outlines the requirements for a Python function named `preprocess_prompt`, designed to prepare prompt strings for Large Language Models (LLMs). The function takes a prompt string, along with optional boolean flags for recursion and curly bracket doubling, and an optional list of keys to exclude from doubling. Its primary task is to parse and process specific XML-like tags within the prompt: `<include>` tags replace the tag with the contents of a referenced file, `<pdd>` tags act as comments and are removed entirely, and `<shell>` tags execute shell commands and replace the tag with the command's output. The function must handle nested includes recursively if the `recursive` flag is set, supporting both XML-style includes and file paths enclosed in angle brackets within triple backticks. Additionally, if `double_curly_brackets` is True, the function must escape single curly brackets by doubling them, unless the enclosed key is in the `exclude_keys` list, while handling nested brackets via recursion. Finally, the function should resolve file paths relative to the current directory and return the cleaned, preprocessed prompt string with whitespace trimmed.",eb8f9a57c8e111424fb259290b44663eea87d8d6941113f9f578d9dc084ee313
context/fix_errors_from_unit_tests/1/conflicts_in_prompts.py,"This Python file defines functionality for detecting and analyzing conflicts between two text prompts using Large Language Models (LLMs). It utilizes the LangChain library to orchestrate the interaction with LLMs and Pydantic for structured data validation. The core component is the `conflicts_in_prompts` function, which takes two prompt strings as input along with optional parameters for model strength and temperature. 

The process involves a multi-step workflow: first, it loads specific prompt templates from a directory specified by the `PDD_PATH` environment variable. It then uses an `llm_selector` utility to choose an appropriate model based on the requested strength. The function executes a primary LLM chain to generate a raw analysis of potential conflicts between the two inputs. Subsequently, a second extraction chain parses this raw output into a structured JSON format defined by the `ConflictOutput` and `Conflict` Pydantic models. This structure includes descriptions, explanations, and specific suggestions for resolving the conflicts in each prompt. Finally, the function returns a list of identified conflicts, the total estimated cost of the API calls, and the name of the model used.",e1a0cd705e318d7f93d847b383a646ec764d0decc95526ddc68d616783fa8e21
context/fix_errors_from_unit_tests/1/conflicts_in_prompts_python.prompt,"This file provides a detailed specification for an expert Python engineer to create a function named ""conflicts_in_prompts"". The purpose of this function is to analyze two input prompts, identify conflicts between them, and suggest resolutions. The function takes two prompt strings, a strength parameter (default 0.5), and a temperature parameter (default 0) as inputs. It returns a list of conflict dictionaries (containing descriptions, explanations, and suggestions for both prompts), the total cost of the operation, and the model name used.

The implementation instructions require using the LangChain Expression Language (LCEL). The process involves six specific steps: loading prompt templates from environment paths, creating an LCEL template for conflict detection, selecting an LLM using an internal `llm_selector` module, running the prompts through the model, and then using a second LCEL template to extract the results into a structured JSON format. The specification also emphasizes calculating and pretty-printing token counts and costs at various stages using the `llm_selector`'s token counter. It includes references to external context files for Python preambles, LCEL examples, and internal module usage.",c02afe41a32b1ddb8b4c9c7c6ad6a74581a8969f88d787b16b9e79a8ffbc1276
context/fix_errors_from_unit_tests/1/test_conflicts_in_prompts.py,"This file contains a suite of unit tests for the `conflicts_in_prompts` function, which appears to be part of a module named `pdd`. The tests are written using the `pytest` framework and rely heavily on `unittest.mock` to isolate the function from external dependencies like file systems, environment variables, and Large Language Model (LLM) calls. 

The test suite includes several fixtures to mock the environment (`PDD_PATH`), prompt files, the LLM selector mechanism, and the JSON output parser. The tests cover various scenarios: successful execution where conflicts are correctly identified and parsed; error handling for missing environment variables (`PDD_PATH`); handling of `FileNotFoundError` when prompt templates are missing; and graceful handling of unexpected exceptions during execution. Additionally, there is a test case verifying that custom parameters for `strength` and `temperature` are correctly passed to the underlying LLM selector.",1c49a90ac93a88b00811fb98b28661b6dd788cb9086b7778f232f5e692ada024
context/fix_errors_from_unit_tests/4/test_detect_change_1_0_1.py,"This file contains a suite of unit tests for the `detect_change` function within the `pdd` module, utilizing the `pytest` framework. The tests are designed to verify the behavior of the function under various scenarios, including successful execution and error handling. 

The file defines several fixtures to mock external dependencies and environment variables, such as `mock_environment` for setting the `PDD_PATH`, `mock_file_contents` for simulating file reads, `mock_llm_selector` for simulating Large Language Model interactions, and `mock_preprocess` for bypassing preprocessing logic. 

The test cases cover four main scenarios: `test_detect_change_success` verifies that the function correctly processes a prompt file and returns the expected change instructions and costs; `test_detect_change_file_not_found` ensures the function handles missing files gracefully by returning an empty list; `test_detect_change_json_decode_error` checks the robustness against malformed JSON outputs from the LLM; and `test_detect_change_unexpected_error` confirms that general exceptions are caught without crashing the application. Overall, the file ensures the reliability of the change detection logic by isolating it from external file systems and API calls.",ce21ab0c26b1dc32f6d00ad1699a191804913e3cdabce68b68cfb968241e90e5
context/fix_errors_from_unit_tests/4/detect_change_1_0_1.py,"This file defines a Python function named `detect_change` designed to analyze a list of prompt files and determine necessary modifications based on a provided change description. The function leverages Large Language Models (LLMs) via LangChain to perform this analysis. It begins by loading specific prompt templates (`detect_change_LLM.prompt` and `extract_detect_change_LLM.prompt`) from a directory specified by an environment variable. The process involves preprocessing these templates and the change description, selecting an appropriate LLM based on strength and temperature parameters, and then invoking the model to generate an analysis of which prompts require updates. A secondary extraction step parses the LLM's raw output into a structured JSON format containing a list of changes. The function calculates and reports the token usage and estimated cost for these operations using the `rich` library for console output. Finally, it returns a list of required changes, the total cost of the operation, and the name of the model used, while handling potential errors such as missing files or invalid JSON.",8f6abd862b4b6bfb75367f9e0e891e3587830f6616bae02e3b0bbd13bf571ade
context/fix_errors_from_unit_tests/4/test_detect_change.py,"This file contains a suite of unit tests for the `detect_change` function within the `pdd.detect_change` module, utilizing the `pytest` framework. The tests rely heavily on mocking external dependencies and internal components to isolate the function's logic. Key components mocked include the `PromptTemplate`, `LLM` (Large Language Model), `OutputParser`, file system operations (`builtins.open`), and environment variables (`PDD_PATH`).

The test suite covers several scenarios:
1.  **Success Case (`test_detect_change_success`)**: Verifies that the function correctly processes prompt files, invokes the LLM chain, parses the output, and returns the expected list of changes, total cost, and model name.
2.  **File Not Found (`test_detect_change_file_not_found`)**: Ensures the function handles missing files gracefully by returning an empty result and zero cost.
3.  **JSON Decode Error (`test_detect_change_json_decode_error`)**: Checks the function's resilience against malformed JSON output from the LLM, ensuring it fails safely without crashing.
4.  **Unexpected Error (`test_detect_change_unexpected_error`)**: Tests the general exception handling mechanism when an unforeseen error occurs during execution.

Overall, the file ensures the robustness of the change detection logic by simulating various success and failure conditions.",8323dc0c5697c60fb4d86275525538e8500805a1a2b2f8e9875f96f462184111
context/fix_errors_from_unit_tests/4/detect_change_python.prompt,"This file provides a detailed specification for an AI agent acting as an expert Python Software Engineer. The agent's primary task is to generate a Python function named ""detect_change"". This function is designed to analyze a list of prompt files against a provided change description to determine which prompts require modification. The specification outlines the function's inputs (prompt files, change description, strength, temperature) and outputs (a list of changes, total cost, and model name). It includes references to context files for Python preambles and LangChain examples. The core of the instruction is a step-by-step procedure for the function logic: loading specific prompt templates, preprocessing inputs, utilizing a LangChain LCEL pipeline with an LLM selector for model management and cost calculation, and executing a two-step LLM process (detection followed by extraction). Finally, the function must pretty-print the results using Rich Markdown and return the structured change list along with cost metrics.",70c81d3b3c7354a1213c362cac334741c95ef71917442103444a514aad84cc46
context/fix_errors_from_unit_tests/4/detect_change.py,"This Python file defines a function named `detect_change` designed to analyze a list of prompt files against a change description to identify necessary modifications. The process involves several steps: loading specific prompt templates (`detect_change_LLM.prompt` and `extract_detect_change_LLM.prompt`) from a directory specified by an environment variable, preprocessing these templates, and selecting an appropriate Large Language Model (LLM) based on strength and temperature parameters using a custom `llm_selector`. The core logic reads the content of the provided prompt files, constructs a prompt list, and executes a LangChain-based processing chain to generate an analysis of required changes. Subsequently, a second LLM call extracts a structured JSON list of changes from the initial analysis. The function calculates and reports the token usage and estimated cost for each step using the `rich` library for console output. Finally, it returns a list of changes (containing prompt names and instructions), the total estimated cost, and the name of the model used, while handling potential file or JSON errors gracefully.",43ad9e25e79a3f7c6e0dd30ee31c2d28a96602ee14c0a93e00407c511664e30b
context/fix_errors_from_unit_tests/3/test_context_generator.py,"This file contains a suite of unit tests for the `context_generator` function within the `pdd.context_generator` module, utilizing the `pytest` framework. The tests employ several fixtures (`mock_environment`, `mock_file_content`, `mock_llm_selector`, `mock_chain_run`) to simulate external dependencies like environment variables, file I/O, and Large Language Model (LLM) interactions. The test suite covers various scenarios: a successful execution path where the generator produces output correctly; a scenario handling unfinished prompts that require continued generation; a failure case where the required `PDD_PATH` environment variable is missing, triggering a `ValueError`; a `FileNotFoundError` scenario which asserts a safe fallback return value; and a general exception handling test where an error during preprocessing results in an empty return tuple. Extensive mocking is used for internal calls such as `preprocess`, `llm_selector`, `LLMChain`, `unfinished_prompt`, and `postprocess` to isolate the unit logic.",653b103f8921a11155192345c234a1856a80bc190b9baca47c5f19e4b23a9b3f
context/fix_errors_from_unit_tests/3/context_generator_python.prompt,"The provided file outlines the specifications for creating a Python function named ""context_generator."" This function is designed to generate concise usage examples for a given code module using a Large Language Model (LLM). The specification details the required inputs, which include the code module string, the original prompt used to create it, the programming language (defaulting to Python), and model parameters like strength and temperature. The expected outputs are the generated example code, the total cost of the operation, and the name of the model used.

The document provides a step-by-step implementation guide using LangChain Expression Language (LCEL). Key steps include loading a specific prompt file from an environment path, preprocessing prompts, selecting an LLM via an internal `llm_selector` module, and invoking the model. It also mandates handling incomplete generations using an `unfinished_prompt` function and a `continue_generation` function, as well as post-processing the final output. The file references several internal context examples for tasks like preprocessing, token counting, and post-processing, ensuring the engineer follows specific internal patterns and cost-tracking mechanisms.",1a59e3e1f7d3aa598a9a92552c6300b827c1d0791903757150d7fbbee41ff889
context/fix_errors_from_unit_tests/3/context_generator.py,"This file defines a Python function named `context_generator` designed to generate example code using a Large Language Model (LLM). The function orchestrates a multi-step process that begins by validating the `PDD_PATH` environment variable and loading a specific prompt template (`example_generator_LLM.prompt`). It utilizes a custom `llm_selector` to choose an appropriate model based on strength and temperature parameters. The workflow involves preprocessing both the template and the user's input prompt, creating a LangChain `LLMChain`, and executing the model to generate code. Crucially, the function includes logic to handle incomplete outputs by detecting unfinished generation and invoking a `continue_generation` routine. It also performs post-processing on the final output. Throughout the execution, the code calculates and prints the estimated token usage and financial cost using a token counter, returning the generated code, total cost, and the name of the model used.",13e2be9a3c3f6fc3e1bce5184358a9d3aa2c4eb161d9f75022b4354c0fe3453d
context/fix_errors_from_unit_tests/2/test_code_generator.py,"This file contains a suite of unit tests for the `code_generator` function within the `pdd.code_generator` module, utilizing the `pytest` framework and `unittest.mock` library. The tests verify the function's behavior across three primary scenarios: successful code generation, handling of incomplete generation requiring continuation, and exception handling. 

The first test case, `test_code_generator_success`, mocks dependencies like `preprocess`, `llm_selector`, and `postprocess` to simulate a standard workflow where code is generated, processed, and returned correctly with accurate cost calculations. The second test, `test_code_generator_incomplete_generation`, simulates a scenario where the initial prompt response is unfinished, verifying that the `continue_generation` logic is triggered and functions correctly. Finally, `test_code_generator_exception_handling` ensures that the function gracefully handles errors by returning empty strings and zero costs when an exception (e.g., during preprocessing) occurs. Throughout the file, mock data is defined for prompts, language settings, and model parameters to ensure consistent testing conditions.",f17f51a3df266a3912ce071c83a62889d2b2d3d4281dc84f578e8c5fce032a4c
context/fix_errors_from_unit_tests/2/code_generator_python.prompt,"This file outlines a prompt for an expert Python engineer to create a function named ""code_generator."" The function's purpose is to compile a raw text prompt into a runnable code file. The prompt specifies the function's inputs (prompt string, target language, model strength, and temperature) and outputs (runnable code, total cost, and model name). It includes references to external context files for Python preambles, LangChain Expression Language (LCEL) examples, and internal module usage (preprocessing, LLM selection, unfinished prompt detection, continuation, and postprocessing). The prompt details an 8-step workflow using LangChain: preprocessing the prompt, creating an LCEL template, selecting an LLM, running the model with cost/token tracking, displaying results with Markdown, detecting and handling incomplete generations, postprocessing the output, and finally returning the code, cost, and model name.",d901b1f56706f97efddd1f30075456f5441419f5276191220e4bed7d09204a77
context/fix_errors_from_unit_tests/2/code_generator.py,"This file defines a `code_generator` function designed to generate programming code based on a user prompt using a Large Language Model (LLM). The process begins by preprocessing the raw prompt to handle recursive elements and formatting. It then selects an appropriate LLM based on a specified strength and temperature using an `llm_selector` module. The prompt is executed through a LangChain pipeline, and the initial result is displayed and cost-calculated based on token usage. The function includes logic to detect if the LLM's output is incomplete; if so, it triggers a continuation process. If the output is complete, it undergoes post-processing to refine the code for the target language. Throughout the execution, the script tracks and prints the estimated costs for input, output, and any additional processing steps, returning the final runnable code, the total cost, and the name of the model used. Error handling is included to manage value errors and unexpected exceptions.",112974499903ccf96fb523e49dea795f2928cceab035a1c6fe554073efec8a0e
context/fix_errors_from_unit_tests/5/error_log.txt,"The provided file content captures the output of a failed `pytest` session executed on a macOS system using Python 3.12.4. The test session, initiated in the `/Users/gregtanaka/Documents/PDD` directory, utilized `pytest` version 8.3.2 along with plugins for coverage (`cov-5.0.0`) and asynchronous I/O (`anyio-4.4.0`). The execution attempted to collect tests but ultimately found zero items. The process concluded almost instantly (0.00s) without running any tests. The primary reason for this outcome is explicitly stated as an error: the test runner could not locate a specific file or directory, identified as `/path/to/non/existent/file.py`. This indicates that the command was likely invoked with an incorrect file path argument, preventing the test suite from launching successfully.",f0eac5f3c31ac6f7f53f16ad78e018d2f05c4d7982d67d9f041f948b4f54a260
context/fix_errors_from_unit_tests/5/continue_generation.py,"This Python script defines a module for continuing and refining text generation using Large Language Models (LLMs) via the LangChain framework. The core functionality is encapsulated in the `continue_generation` function, which orchestrates a multi-step process to extend an initial LLM output. The process involves loading and preprocessing specific prompt templates from a directory specified by an environment variable. It utilizes an `llm_selector` to choose appropriate models based on desired strength and temperature settings. The script iteratively generates text, checking for completion using a helper function `unfinished_prompt`. It employs auxiliary chains to trim and format the results, ensuring the output is clean code blocks or text. Throughout the execution, the script tracks and prints token usage and associated costs using the `rich` library for console output. Pydantic models are defined to structure the expected JSON outputs from the LLMs for trimming tasks.",60870002c908d1af45df2bb36cccf3c0ffac431e52f8a52881982a97dea7b7e0
context/fix_errors_from_unit_tests/5/continue_generation_2_0_1.py,"This Python script defines a function named `continue_generation` designed to extend and complete code generation tasks using Large Language Models (LLMs) via the LangChain framework. The process begins by loading and preprocessing specific prompt templates from a directory specified by an environment variable. It utilizes Pydantic models (`TrimResultsOutput` and `TrimResultsContinuedOutput`) to structure the parsing of LLM outputs. The core logic involves an iterative loop where the script first extracts an initial code block from previous output, then repeatedly invokes an LLM to continue generating code based on the formatted input and the current code block. Inside the loop, the script checks if the generation is complete using a helper function `unfinished_prompt`. If unfinished, it appends the new content and continues; if finished, it trims the final output for cleanliness and breaks the loop. Throughout the execution, the script tracks token usage and calculates costs associated with different LLM calls (generation, trimming, and completion checking), displaying progress and costs via the `rich` library. Finally, it returns the fully generated code block, the total cost, and the model name used.",c1c92431563dde04a966bfc7d11408f82ac7a350c872b2615d9deacaf920db6e
context/fix_errors_from_unit_tests/5/test_continue_generation.py,"This file contains a suite of unit tests for the `continue_generation` function within the `pdd` module, utilizing the `pytest` framework. The tests verify the functionality of continuing text generation from a Large Language Model (LLM) output. Key components include fixtures for mocking the environment (`mock_environment`), prompt templates (`mock_prompts`), the LLM selection logic (`mock_llm_selector`), and the logic for determining if a prompt is unfinished (`mock_unfinished_prompt`). The test cases cover various scenarios: successful generation with mocked dependencies, error handling when the `PDD_PATH` environment variable is missing, handling of `FileNotFoundError` when prompt files are inaccessible, verifying behavior over multiple generation iterations, and ensuring accurate cost calculation. The tests heavily rely on `unittest.mock` to isolate the function from external file systems and actual LLM API calls.",08fc2afe93e17da1d224e31652ea38b3a383a1a4672d2b8f90bfbbdd4f17184f
context/fix_errors_from_unit_tests/5/continue_generation_python.prompt,"This document outlines the specifications for creating a Python function named ""continue_generation,"" designed to complete the generation of a prompt using a Large Language Model (LLM). The function takes inputs such as a formatted input prompt, existing LLM output, and model parameters like strength and temperature. It outputs the final completed string, the total cost of execution, and the model name used. The process involves several steps: loading and preprocessing specific prompt files from an environmental path, setting up Langchain LCEL templates, and selecting an appropriate LLM model. The core logic requires running a ""trim_results_start"" chain to extract initial code blocks, followed by a loop that uses a ""continue_generation"" chain to extend the output. This loop persists until an ""unfinished_prompt"" check determines the generation is complete. Finally, the results are trimmed and formatted, costs are calculated across all model invocations, and the final output is returned.",0bad36512ca7aa74cb49aa6b4a53c0d0c0764264897c1c336b9a488f31c12e66
context/fix_errors_from_unit_tests/5/test_continue_generation_2_0_1.py,"This file contains a suite of unit tests for the `continue_generation` function within the `pdd` module, utilizing the `pytest` framework. The tests verify the functionality of continuing text generation processes, likely involving Large Language Models (LLMs). Key components include fixtures for mocking environment variables (`PDD_PATH`), prompt templates, LLM selection logic, and unfinished prompt detection. The test cases cover various scenarios: successful generation completion, handling missing environment variables, managing file not found errors, and verifying behavior over multiple generation iterations. Additionally, there is a specific test to ensure the accurate calculation of total costs associated with the generation process. The tests make extensive use of `unittest.mock` to simulate external dependencies like file operations, LLM responses, and console output.",967abdff9f87433d54f94bcb50397587a8330cd3fc854da619f2bdcf616dd39a
context/core/cli_example.py,"This Python script serves as a comprehensive demonstration and test suite for the PDD (Prompt Driven Development) CLI module. It illustrates how to programmatically interact with the main CLI entry point (`cli`) and its custom Click Group class (`PDDCLI`). The script defines several functions to showcase various global options and features, including displaying help and version information, listing available contexts from configuration files, and invoking the CLI with parameters for AI model strength, temperature, and verbosity. It also demonstrates specific flags like `--quiet` for suppressing output, `--output-cost` for tracking usage costs in a CSV file, `--context` for overriding configuration contexts, and `--core-dump` for generating debug bundles. The `main` function orchestrates the execution of these examples, providing a structured overview of the CLI's capabilities, error handling, and configuration management.",48068766551cf90d9fdca612b4210b4d901441931e3578e23fbf72083e95e075
context/core/utils_example.py,"This Python script serves as a demonstration and documentation suite for the `pdd.core.utils` module, which provides helper functions for the PDD CLI tool. The file defines several example functions, each targeting a specific utility from the core module, such as `_first_pending_command` (for parsing Click context arguments), `_api_env_exists` (checking for API configuration files), `_completion_installed` (verifying shell tab completion), and `_project_has_local_configuration` (detecting local environment settings). Additionally, it illustrates the logic behind `_should_show_onboarding_reminder`, which determines if new users need setup guidance based on environment variables and existing configurations. The script includes a `main` entry point that executes these examples sequentially, printing the results to standard output to visualize how the utility functions behave under different scenarios (e.g., using `unittest.mock` to simulate CLI contexts).",efdc29724f5f2de035193da6f57f5354448c920c79182138a648369a75dd91c2
context/core/cloud_example.py,"This Python script serves as a comprehensive example and demonstration of the `CloudConfig` module within the PDD CLI project. It illustrates how to manage centralized cloud configuration settings, specifically focusing on retrieving endpoint URLs, handling authentication via JWT tokens, and managing environment variable overrides. The script includes several functions that simulate different usage scenarios: `example_url_configuration` shows how to switch between default production URLs and local emulator URLs; `example_cloud_enabled_check` verifies if cloud features are active based on API keys; `example_jwt_token_handling` demonstrates retrieving authentication tokens either from environment variables (for CI/CD) or via a mocked interactive device flow; and `example_cloud_timeout` shows how to configure request timeouts. Additionally, it includes error handling examples for missing configuration keys. The script uses `unittest.mock` to simulate environment variables and network responses, allowing it to run as a standalone demonstration without requiring actual cloud connectivity.",8a14ae9e69dbccfbbfffd9bde1ea11192334a10aaf9d5b5e6dba5ac23ba0dd90
context/core/errors_example.py,"This Python script serves as a comprehensive demonstration of the `pdd.core.errors` module, which provides centralized error handling for the PDD CLI. The script illustrates how to use the module's key components: a `Rich` console with custom themes for styled output, and functions for handling exceptions gracefully without crashing the application. 

The `main` function walks through several examples, including: printing messages with specific styles (info, warning, success, etc.); handling various exception types like `FileNotFoundError`, `ValueError`, and `IOError`; and demonstrating the difference between verbose and quiet error reporting modes. Additionally, the script shows how the module collects errors into a buffer for core dump analysis. It includes examples of retrieving these collected errors, writing them to a JSON file for debugging purposes, and clearing the error buffer. Finally, it confirms that the `handle_error` function captures exceptions without re-raising them, allowing the CLI to continue execution or exit cleanly.",b50dbf08aec028bd57a8fe74762bf92aee9a1fe953fb3d7db50cc686c72bfee3
context/core/dump_example.py,"This Python script serves as a comprehensive example and demonstration of the `pdd.core.dump` module, which handles core dump generation and debugging utilities for the PDD CLI tool. The script illustrates how to capture execution contexts, errors, and environment details to facilitate issue reproduction and diagnosis.

The file defines several example functions corresponding to key features of the dump module: `example_write_core_dump` mocks a Click context to simulate writing a JSON core dump file containing cost and model usage data; `example_github_config` checks for necessary environment variables (`PDD_GITHUB_TOKEN`, `PDD_GITHUB_REPO`) required for automated issue reporting; `example_write_replay_script` generates a shell script to reproduce a specific CLI command execution; and `example_build_issue_markdown` constructs a formatted GitHub issue body from a core dump payload. Finally, `example_post_issue_to_github` outlines the signature for posting issues via the GitHub API. The script includes a main execution block that runs these examples sequentially, outputting results to a local directory.",0416a935ac490b958ceedfc6e68c7fbd7e383966374ba13ccf7ebfeaf8ad0803
context/insert/1/prompt_to_update.prompt,"This file outlines the requirements for an expert Python Software Engineer to create a function named ""postprocess"". The primary goal of this function is to extract specific programming code from a raw string output generated by a Large Language Model (LLM), which typically contains a mix of conversational text and code blocks. The function accepts inputs such as the raw LLM output, the target programming language, and configuration parameters like model strength, temperature, and verbosity. It returns a tuple containing the cleaned code string, the total cost of the operation, and the model name used. The logic involves a conditional check where a strength of 0 triggers a simpler ""postprocess_0"" function. Otherwise, it utilizes an ""extract_code_LLM.prompt"" template via an ""llm_invoke"" call to parse the output. The process includes specific cleaning steps, such as stripping triple backticks and language identifiers from the code block boundaries. All console output is required to be formatted using the Python ""rich"" library.",098afefde3d09926ba42f4cd18881b7dd447003bb7084c9d8ad0d60fd0be8805
context/insert/1/updated_prompt.prompt,"This file outlines the requirements for a Python function named `postprocess`, designed to extract code blocks from a mixed-text string generated by a Large Language Model (LLM). Acting as an expert Python Software Engineer, the user is instructed to create a function that accepts inputs such as the raw LLM output, the target programming language, model strength, temperature, and a verbosity flag. The function returns a tuple containing the extracted code, the total cost of the operation, and the model name used. The process involves conditional logic: if the strength is 0, it delegates to a `postprocess_0` function. Otherwise, it loads a specific prompt template (`extract_code_LLM.prompt`) and utilizes an internal `llm_invoke` module to process the text. The instructions specify cleaning the output by removing markdown code fences (triple backticks) and language identifiers. The file also references internal modules for loading prompt templates and invoking LLMs, providing context via included example files.",1fcdbba25fca894689f6d83c6461a9b054b938e7c936b1d5035f5a2b0cea7756
context/insert/1/dependencies.prompt,"The provided file contents serve as a documentation snippet or usage guide for internal modules within a software project. It specifically outlines how to utilize two key functionalities: loading prompt templates and executing prompts using an `llm_invoke` function. The guide is structured using XML-like tags to delineate sections. Under the `<internal_modules>` tag, it provides two distinct examples. The first, `<load_prompt_template_example>`, demonstrates the method for importing or accessing prompt templates, pointing to a specific file path: `context/load_prompt_template_example.py`. The second, `<llm_invoke_example>`, illustrates how to run prompts using the `llm_invoke` mechanism, referencing the file `context/llm_invoke_example.py`. Essentially, this file acts as a quick reference for developers to locate the source code examples necessary for implementing prompt management and LLM invocation within the system.",ee4a1c42a3115dc65850843bc461fbe6488db13f9897eae96d4db7815902a92b
context/insert/2/prompt_to_update.prompt,"This file outlines the specifications for an AI agent acting as an expert Python engineer to generate a function named ""conflicts_in_prompts"". The purpose of this function is to analyze two input prompts, identify conflicts between them, and suggest resolutions. The function takes two prompt strings, a model strength float, and a temperature float as inputs. It is required to output a list of necessary changes in JSON format, the total cost of the operation, and the model name used. The implementation details specify using LangChain Expression Language (LCEL) and loading specific prompt templates from a project path defined by the $PDD_PATH environment variable. The process involves a multi-step workflow: first, running a conflict detection LLM to generate a markdown analysis; second, pretty-printing cost and token usage; and third, using a second extraction LLM to parse that analysis into a structured JSON list of changes. The file also references external context files for Python preambles and LCEL examples.",a738bbf964d351f4e4a728992c6dd3d1b7d8ce34ca50d52497c2c9178c2b8c74
context/insert/2/updated_prompt.prompt,"This file provides a detailed specification for creating a Python function named ""conflicts_in_prompts"". The function's purpose is to analyze two input prompts, identify conflicts between them, and suggest resolutions. It requires inputs for the two prompts, along with optional parameters for model strength and temperature. The expected output includes a list of necessary changes in JSON format, the total cost of the operation, and the name of the LLM model used.

The specification outlines a seven-step process using LangChain Expression Language (LCEL). Key steps include loading specific prompt templates from a project path defined by the $PDD_PATH environment variable, selecting an appropriate LLM using an internal `llm_selector` module, and calculating token usage and costs. The workflow involves two main LLM calls: first, to generate a markdown analysis of the conflicts, and second, to extract a structured JSON list of changes from that analysis. The file also references external context files for Python preambles, LCEL examples, and internal module usage examples to guide the implementation.",3be57216b5220b17d916ccac261216247bcd07ea280ea3dd92d171c48e2c21cf
context/insert/2/dependencies.prompt,"The provided file content serves as a documentation snippet or a template demonstrating how to include and reference internal code modules. Specifically, it showcases an example of how to use an internal module named `llm_selector`. The content includes an XML-like structure `<internal_example_modules>` which wraps a specific example case. This case illustrates selecting a LangChain Large Language Model (LLM) and counting tokens, referencing an external Python file located at `./context/llm_selector_example.py` via an `<include>` tag. The structure suggests this file is part of a larger system for managing or documenting code examples and internal tooling usage.",f75d15f0817d09a3aca235f8060ce3c16c82886609ec19b84ea72e71eaa0778f
context/detect_change/1/prompt_list.txt,"The provided input contains a collection of prompt definitions designed to instruct an LLM to generate specific Python functions for an automated coding workflow. The collection begins with a `python_preamble` that establishes coding standards, such as using relative imports, the Rich library for console output, and robust error handling.

The primary prompts define three distinct capabilities:
1. **Code Modification (`change_python`):** This prompt instructs the creation of a function that modifies existing code based on user instructions. It utilizes LangChain LCEL, calculates token costs, and employs specific sub-prompts to extract and apply changes.
2. **Automated Debugging (`fix_error_loop_python`):** This defines a complex iterative process to fix unit test errors. The function runs `pytest`, parses error logs, creates file backups, and repeatedly attempts to fix code within a specified budget and attempt limit, verifying fixes against a separate program.
3. **Code Generation (`code_generator_python`):** This prompt guides the creation of a function to compile raw prompts into runnable code files. It handles preprocessing, model selection, cost tracking, and logic to detect and complete unfinished model generations.

Collectively, these prompts outline a system for generating, modifying, and debugging Python code using LLMs and LangChain.",26ea374cd215a2eacd72f63eabca529bb0498a353de793434d5cc53943f5e199
context/detect_change/1/change_python.prompt,"This document outlines the requirements for creating a Python function named ""change"" designed to modify an existing prompt based on specific instructions. The function takes inputs including an initial prompt, generated code, modification instructions, and LLM parameters (strength and temperature). It outputs a modified prompt string, the total operational cost, and the model name used. The implementation must utilize the Python Rich library for console output and relative imports for modules. The process involves several steps: loading specific prompt files, preprocessing them (handling curly brackets), creating Langchain LCEL templates, and selecting an LLM model with token counting capabilities. The function executes a two-step LLM process: first, generating a raw modified prompt based on the inputs, and second, extracting the final structured ""modified_prompt"" from that output using a secondary extraction prompt. Throughout execution, the function must calculate and display token counts and estimated costs, handle edge cases, and return the final modified prompt along with cost metrics.",5e4d1b24cfaf48eed80704c64cc75d9c888914189ba700487e3d77c3b6a97f3b
context/detect_change/1/fix_error_loop_python.prompt,"This document outlines the specifications for a Python function named ""fix_error_loop,"" designed to iteratively repair errors in a unit test and its associated code file using an LLM. The function takes inputs including file paths for the unit test, code, and a verification program, as well as parameters for the LLM (strength, temperature), a maximum attempt limit, and a budget. 

The process involves a loop that runs until the tests pass, the maximum attempts are reached, or the budget is exceeded. In each iteration, the function executes the unit test using pytest, logging the output. If failures occur, it backs up the current files, analyzes the error logs, and calls a helper function, ""fix_errors_from_unit_tests,"" to generate fixes. The function tracks costs and validates code changes using the verification program. If verification fails, changes are reverted. The system also tracks the ""best iteration"" based on the lowest number of errors and failures. Upon completion, it ensures the best version of the code is restored if the final run wasn't the most successful, returning the final status, file contents, attempt count, and total cost. All console output is formatted using the Python ""rich"" library.",e5613bb6ce63aa396e54904fdbbeef99f854b20676c737a39f14256a765bc2ba
context/detect_change/1/code_generator_python.prompt,"This file outlines a prompt for an expert Python engineer to create a function named ""code_generator"". The primary purpose of this function is to compile a raw text prompt into a runnable code file in a specified language (e.g., Python, Bash). The function takes inputs including the raw prompt, target language, model strength, and temperature, and returns the runnable code, total cost, and the model name used.

The prompt details a specific workflow using LangChain Expression Language (LCEL). The process involves eight steps: preprocessing the raw prompt, creating an LCEL template, selecting an LLM using an internal `llm_selector` module, and executing the prompt while tracking token usage and cost. It includes logic to handle incomplete generations by detecting unfinished output and continuing generation if necessary. Finally, the output is post-processed, and the total cost is calculated. The file references several external context files and examples for internal modules like preprocessing, LLM selection, unfinished prompt detection, and post-processing.",2cb942ea670aa6742a3feff76ca34121806135bafcc7fe39bd7d5e64994e35cc
context/detect_change/1/python_preamble.prompt,"The provided text outlines specific requirements for developing a Python function within a package structure. It mandates the use of relative imports, specifically denoted by a single dot, to reference internal modules, ensuring the code integrates seamlessly into the larger package ecosystem. A key requirement is the implementation of the Python Rich library to handle all console output, guaranteeing that any information displayed to the user is aesthetically formatted and 'pretty printed.' Furthermore, the instructions emphasize robust error handling; the function must be designed to gracefully manage edge cases, such as missing input data or potential errors arising from model interactions. In these scenarios, the function is expected to provide clear, informative error messages to guide the user, rather than failing silently or crashing. Overall, the directive focuses on creating a modular, user-friendly, and visually polished component of a Python package.",afcfd4246bc7fa0387c45154a99e65c483e3e6513dad984b94e75ce36fff64b9
context/detect_change/1/detect_change_output.txt,"This document outlines a plan to standardize Python-related instructions across various prompt files by utilizing a shared `context/python_preamble.prompt`. The goal is to reduce redundancy and improve consistency by centralizing common requirements, such as using the Python Rich library for output and handling relative imports. The analysis considers three implementation strategies: direct inclusion, conditional inclusion, and reference-based inclusion. Plan C, reference-based inclusion, is selected as the optimal approach because it minimizes redundancy, facilitates easy updates, and aligns with existing system capabilities using XML include tags. The document details specific instructions for modifying two files: `prompts/change_python.prompt` and `prompts/fix_error_loop_python.prompt`. For each file, the plan prescribes adding an XML include tag for the preamble and removing specific redundant text that is now covered by the shared file. It notes that `context/python_preamble.prompt` and `prompts/code_generator_python.prompt` do not require changes.",2b81cf48819c551c13bcbf9483112c26e883ce9f466d132b2334992886b6b393
context/detect_change/2/change_python.prompt,"The file outlines the requirements for an expert Python Software Engineer to create a function named ""change"". This function is designed to modify an existing ""input_prompt"" based on a user-provided ""change_prompt"" and associated ""input_code"". The function must utilize the Python Rich library for pretty-printing console output and employ relative imports. It requires specific inputs including the prompts, code, and LLM parameters like strength and temperature. The process involves loading specific prompt files, preprocessing them, and executing a multi-step Langchain LCEL workflow. This workflow includes selecting an LLM model, counting tokens to estimate costs, and running two distinct LLM invocations: one to generate the modification logic and another to extract the final ""modified_prompt"" as JSON. The function must return the modified prompt string, the total cost of operations, and the model name used, while handling edge cases and errors robustly.",937a920d597a336286c7ae934dfd64482679bc3d20baeefd9cd02277a46c8ee9
context/detect_change/2/unfinished_prompt_python.prompt,"The provided text outlines the specifications for a Python function named `unfinished_prompt`, designed to determine whether a given text prompt is complete or requires continuation. The function takes `prompt_text`, `strength`, and `temperature` as inputs and returns a structured assessment including `reasoning`, a boolean `is_finished` status, and optional cost and model details. The implementation requires using the Langchain LCEL framework, loading a specific prompt template from an environment-variable-defined path, and utilizing a helper module `llm_selector` for model selection and token counting. The process involves five steps: loading the prompt, creating an LCEL template, selecting the LLM, executing the analysis while logging token usage and costs via the `rich` library, and finally returning the parsed results. Error handling and relative imports are emphasized requirements.",2fa8a55103f26a33a516d4cd2add38b10b2a4a6975e2eca3b14ddb297c8a1405
context/detect_change/2/change.prompt,"The provided file content appears to be a snippet from a prompt engineering template or configuration system. It instructs the user or system to utilize a specific file, `context/python_preamble.prompt`, to make prompts more compact, noting that some prompts may already include it. The content includes placeholders or directives for including external files, specifically referencing `<include>context/python_preamble.prompt</include>` within a `<preamble>` tag and `<include>prompts/code_generator_python.prompt</include>` within an `<example>` tag. Essentially, it serves as a meta-instruction for constructing prompts by reusing existing preamble and example components.",44c27e23c016ad4b255cbd1b844e482a87673fb9679f3c053ec2609377c34d9e
context/detect_change/2/preprocess_python.prompt,"The provided text outlines the requirements for a Python function named 'preprocess_prompt', designed to prepare prompt strings for Large Language Models (LLMs). The function takes a prompt string, a recursive boolean flag, a double_curly_brackets flag, and an optional list of keys to exclude from bracket doubling. Its primary goal is to process specific XML-like tags within the prompt using regular expressions. Key tags include 'include' for inserting file contents directly, 'pdd' for removing comments, and 'shell' for executing and capturing shell command output. The function must handle nested includes recursively if the recursive flag is set, supporting two inclusion methods: standard XML 'include' tags and file paths enclosed in angle brackets within triple backticks. Additionally, if enabled, the function doubles single curly brackets to escape them, unless the enclosed keys are in the exclusion list. File paths are resolved using a 'PDD_PATH' environment variable via a helper function 'get_file_path'. The implementation requires using the 'rich' library for pretty-printing console messages to track progress, and the final output is the cleaned, preprocessed prompt string with whitespace trimmed.",50f953ffe80221f6217e0a58f2a0a0ca2a3390402903b3b3a03e858a7e487997
context/detect_change/2/python_preamble.prompt,"The provided text outlines specific requirements for developing a Python function within a package structure. It mandates the use of relative imports, specifically denoted by a single dot, to reference internal modules, ensuring the code integrates seamlessly into the larger package ecosystem. A key requirement is the implementation of the Python Rich library to handle all console output, guaranteeing that any information displayed to the user is aesthetically formatted and 'pretty printed.' Furthermore, the instructions emphasize robust error handling; the function must be designed to gracefully manage edge cases, such as missing input data or potential errors arising from model interactions. In these scenarios, the function is expected to provide clear, informative error messages to guide the user, rather than failing silently or crashing. Overall, the directive focuses on creating a modular, user-friendly, and visually polished component of a Python package.",afcfd4246bc7fa0387c45154a99e65c483e3e6513dad984b94e75ce36fff64b9
context/detect_change/2/prompt_list.json,"The provided file contains a JSON array defining specifications for four Python functions designed to automate various aspects of LLM prompt engineering. 

1.  **change_python.prompt**: Describes a function named ""change"" that modifies an input prompt based on a specific ""change_prompt"" instruction. It utilizes Langchain LCEL, handles token counting, and calculates the cost of the operation.
2.  **preprocess_python.prompt**: Outlines a ""preprocess_prompt"" function responsible for parsing prompt strings. It handles specific XML-like tags (such as `include`, `pdd`, and `shell`), manages recursive file inclusions, and formats curly brackets for LLM compatibility.
3.  **unfinished_prompt_python.prompt**: Details a function called ""unfinished_prompt"" that analyzes text to determine if a prompt is complete. It returns a boolean status and reasoning, aiding in iterative prompt generation.
4.  **xml_tagger_python.prompt**: Specifies an ""xml_tagger"" function that enhances raw prompts by adding structural XML tags to improve clarity. It employs a multi-step LLM process to generate and extract the tagged content.

All functions are designed to integrate with a project environment (referencing `$PDD_PATH`), use the `rich` library for console output, and include mechanisms for error handling and cost estimation.",2126c50e4b33078edbfd15319b1f7e966afc6b6e495a8de3db68e2321348e266
context/detect_change/2/change_detect.csv,"The provided file content outlines a series of identical change instructions for three specific prompt files located within the `context/detect_change/2/` directory: `change_python.prompt`, `preprocess_python.prompt`, and `unfinished_prompt_python.prompt`. For each of these files, the instruction mandates the insertion of a standard preamble file, `./context/python_preamble.prompt`, immediately following the role and goal statements using XML `include` tags. Furthermore, the instructions require the removal of any redundant guidelines already covered by this new preamble, specifically mentioning tasks like pretty printing and edge case handling. The goal is to streamline the prompts by leveraging a shared context file while preserving the unique logic and flow specific to each individual prompt.",853a291e2b92085b3471b1dc9ee9d8487dc8a4262043256766f89031c1639fcd
context/detect_change/2/xml_tagger_python.prompt,"The provided file outlines the requirements for creating a Python function named ""xml_tagger,"" designed to enhance raw LLM prompts by adding structural XML tags. Acting as an expert Python engineer, the developer is tasked with implementing a multi-step process using LangChain Expression Language (LCEL). The function takes a raw prompt string, along with strength and temperature parameters, and returns the enhanced prompt, the total execution cost, and the model name used. The workflow involves loading specific prompt templates from a project path, utilizing an internal `llm_selector` module for model selection and token counting, and executing two distinct LCEL chains. The first chain generates an analysis with XML tags, while the second extracts the clean, tagged prompt from that analysis into a JSON format. The process requires pretty-printing status updates, token counts, and costs at each stage, culminating in a final output of the refined prompt and the aggregated cost of the operations.",e1872f733299b344649a9f6778c5aa3d82acde7d2c2690217483e51f8ee518c1
context/detect_change/2/detect_change_output.txt,"This document outlines a plan to standardize Python-related prompts by integrating a centralized preamble file (`context/python_preamble.prompt`). The goal is to reduce redundancy and improve maintainability by consolidating common instructions regarding relative imports, pretty printing with the Python Rich library, and edge case handling. The analysis evaluates three implementation strategies: direct inclusion, conditional inclusion, and a hybrid approach. Plan C, the hybrid approach, is selected as the optimal strategy because it offers the flexibility to include the preamble only where it adds value while retaining specific existing instructions. The document then applies this plan to specific prompt files. It identifies `change_python.prompt`, `preprocess_python.prompt`, and `unfinished_prompt_python.prompt` as requiring modification. For each of these files, specific instructions are provided to insert the preamble immediately after the role/goal statement using XML include tags and to remove any redundant text. The `xml_tagger_python.prompt` is noted as already compliant, requiring no changes. The overall outcome is a targeted refactoring of prompt files to ensure consistency and cleaner code generation instructions.",9af4d232b7d214e792627e60c15900857a58104cb9dbc0a31a1db5ad894d3ebf
context/update/1/modified_change.py,"This Python script defines a function named `change` designed to automate the modification of prompts using Large Language Models (LLMs). Leveraging the `langchain` framework for orchestration and the `rich` library for formatted console output, the function executes a multi-step workflow. It begins by loading specific prompt templates from a directory defined by an environment variable and selecting an appropriate LLM configuration via a `llm_selector` helper based on provided strength and temperature parameters. The core logic involves two sequential LangChain operations: first, a generation chain that takes an input prompt, associated code, and a change description to produce a raw modification result; and second, an extraction chain that parses this output to isolate the final `modified_prompt` using a JSON parser. Throughout the process, the code calculates and displays token usage and estimated financial costs for each step. It includes error handling for file access and JSON parsing issues, ultimately returning the modified prompt, the total cost, and the model name used.",fec3fba35a4710e02375bdb017bcf33882c2a3b690733fc9d5c6021c1602785c
context/update/1/initial_change_python.prompt,"This document outlines the requirements for a Python function named ""change,"" designed to modify an existing prompt based on specific instructions. Acting as an expert Python Software Engineer, the developer must implement this function to take inputs such as an initial prompt, generated code, modification instructions, and LLM parameters (strength and temperature). The function's primary goal is to produce a ""modified_prompt"" by processing these inputs through a Langchain LCEL pipeline. Key steps include loading specific prompt templates, preprocessing them, selecting an LLM model via a selector utility, and executing a two-step invocation process: first to generate a raw change output and second to extract the structured modified prompt as JSON. The implementation must utilize the Python Rich library for pretty-printing console output, handle relative imports, calculate and display token usage and costs, and manage edge cases robustly. The final output includes the modified prompt string, the total cost of operations, and the model name used.",e541f45a7505b59b512d1a42e03b8ab4066069874f074341ad5f781c77e8fa08
context/update/1/initial_change.py,"This Python file defines a function named `change` that orchestrates a multi-step process to modify prompts using a Large Language Model (LLM). The function takes an initial prompt, input code, a description of desired changes, and model parameters (strength and temperature) as inputs. It first loads specific prompt templates from external files (`change_LLM.prompt` and `extract_prompt_change_LLM.prompt`) and preprocesses them. Using a helper `llm_selector`, it configures an LLM and calculates token costs. The process involves two main LLM invocations: first, to generate a modification based on the inputs using a `StrOutputParser`, and second, to extract the final `modified_prompt` from the previous output using a `JsonOutputParser`. The script utilizes the `rich` library for formatted console output, displaying intermediate results, token counts, and estimated costs at each step. Finally, it returns the modified prompt, the total calculated cost, and the name of the model used, while including error handling for file operations and JSON parsing.",43fa387f9f1cb1f4a6d4b214e4da990fdbb39b71b0a8e6df52cbd9a6ce6a0f56
context/update/1/final_change_python.prompt,"This document outlines the requirements for creating a Python function named ""change"" designed to modify an existing prompt based on specific instructions. The function takes inputs including an initial prompt, generated code, modification instructions, and LLM parameters (strength and temperature). It outputs a modified prompt string, the total operational cost, and the model name used. The implementation must utilize the Python Rich library for console output and relative imports for modules. The process involves several steps: loading specific prompt files, preprocessing them (handling curly brackets), creating Langchain LCEL templates, and selecting an LLM model with token counting capabilities. The function executes a two-step LLM process: first, generating a raw modified prompt based on the inputs, and second, extracting the final structured ""modified_prompt"" from that output using a secondary extraction prompt. Throughout execution, the function must calculate and display token counts and estimated costs, handle edge cases, and return the final modified prompt along with cost metrics.",5e4d1b24cfaf48eed80704c64cc75d9c888914189ba700487e3d77c3b6a97f3b
context/update/2/initial_continue_generation.py,"This Python script defines a function named `continue_generation` designed to extend and refine text generation from a Large Language Model (LLM), specifically focusing on code blocks. The process begins by loading and preprocessing specific prompt templates from a directory specified by an environment variable. It utilizes LangChain components to create processing chains for continuing generation and trimming results, employing Pydantic models (`TrimResultsOutput` and `TrimResultsContinuedOutput`) to structure the output parsing.

The core logic involves an iterative loop where the script first extracts an initial code block from a provided LLM output. It then repeatedly invokes an LLM to continue generating content based on the original prompt and the current code block. Inside the loop, the script checks if the generation is complete using a helper function `unfinished_prompt`. If unfinished, the loop continues; if finished, the result is trimmed and appended to the final output. Throughout the execution, the script calculates and logs the cost of token usage for each step using an `llm_selector`. Finally, it pretty-prints the resulting code block to the console using Markdown formatting and returns the complete code, total cost, and model name.",e9cd8ad342ad7d0699595cab2854584162848e8c37719b0d3221a57b5fa03186
context/update/2/initial_continue_generation_python.prompt,"The provided text outlines the requirements for creating a Python function named `continue_generation`, designed to complete the output of a Large Language Model (LLM) prompt. Acting as part of a Python package, the function utilizes relative imports and the Rich library for console output. It takes a formatted input prompt, initial LLM output, strength, and temperature as inputs, and returns the final completed output, total cost, and model name. The process involves several steps: loading and preprocessing specific prompt files from an environmental path, setting up Langchain LCEL templates, and selecting an LLM model with cost tracking. The function then iteratively generates content by checking for incompleteness using a helper function (`unfinished_prompt`), appending new generations until the output is complete. It also employs specific prompts to trim results at the start and end of the process. Throughout execution, the function must calculate and print token costs for every model invocation.",03d2933ae91c1d02aa24778c8d1f3be290a8e234f62d7955c65ab65a0e0ee98e
context/update/2/modified_continue_generation.py,"This Python script defines a function named `continue_generation` designed to extend and complete code generation tasks using Large Language Models (LLMs) via the LangChain framework. The process begins by loading and preprocessing specific prompt templates from a directory specified by the `PDD_PATH` environment variable. It utilizes an `llm_selector` to choose appropriate models based on desired strength and temperature settings. The core logic involves an iterative loop where the script first extracts an initial code block from previous output, then repeatedly invokes an LLM to continue generating code based on the formatted input and the current code block. Inside the loop, it checks if the generation is complete using a helper function `unfinished_prompt`. If finished, it trims the final output for cleanliness; if not, it appends the new content and continues the loop. Throughout the execution, the script tracks and prints token usage and costs using the `rich` library for formatted console output. Finally, it returns the fully generated code block, the total cost incurred, and the name of the model used.",c5c930022897786dc668e13d807b29f7eef887ae731cdccee2a4415a1827260d
context/update/2/modified_initial_continue_generation.prompt,"The provided text outlines the requirements for creating a Python function named `continue_generation`, designed to complete the generation of a prompt using a Large Language Model (LLM). The function, intended for a Python package, must utilize relative imports and the Rich library for console output. It takes a formatted input prompt, existing LLM output, and model parameters (strength, temperature) as inputs, returning the final completed output, total cost, and model name. The implementation involves a multi-step process: loading and preprocessing specific prompt files from an environment path, setting up Langchain LCEL templates, and selecting an LLM model with cost tracking. The core logic requires trimming the initial output, running a continuation loop that checks for completeness using an `unfinished_prompt` helper, and iteratively appending generated text until finished. Finally, the function must trim the results, pretty-print the final output with token costs, and return the accumulated string and metrics.",5e4246af93242342aceea14974d072b9fb4216e286d6a27082f69c18ce7e51fe
context/output/math_lib.py,"The provided file content consists of a concise Python function definition named `factorial`. This function is designed to calculate the factorial of a given non-negative integer `n`. The implementation utilizes a recursive approach, which is a common algorithmic technique for this specific mathematical operation. The logic is expressed using a ternary conditional operator (often called a one-line if-else statement) for brevity and readability. Specifically, the function checks if the input `n` is less than or equal to 1. If this condition is true, it returns 1, serving as the base case for the recursion to prevent infinite loops. If the input is greater than 1, the function returns the product of `n` and a recursive call to `factorial` with the argument `n-1`. This structure effectively computes the product of all positive integers down to 1. The code is syntactically correct Python but lacks docstrings, type hinting, or error handling for negative inputs, representing a minimal, functional implementation of the mathematical concept.",7f4329fc240c49cf36459e57b938dfd9d0592b86d887e3643507746a943b0aa9
context/output/firebase-debug.log,"This file contains a series of debug and error logs from the Firebase CLI, recorded on December 20, 2025. The logs capture two distinct attempts to execute commands, occurring at approximately 07:36 UTC and 21:16 UTC. Both attempts exhibit identical patterns of warnings and failures.

Initially, the logs show repeated warnings regarding the `firebase.json` configuration file, specifically noting an ""unknown property"" related to `firestore-stripe-payments` within the `/extensions` object. Following these configuration checks, the system attempts to authorize the command using the signed-in user `glt@promptdriven.ai`. The CLI identifies that the required OAuth scopes (including email, openid, and cloud platform permissions) are needed.

However, the authentication process fails consistently. The logs indicate that the current tokens are invalid (expired), prompting the system to attempt a token refresh via the Google OAuth2 API. These refresh requests result in `400 Bad Request` errors. Consequently, the operations terminate with a fatal ""Authentication Error,"" explicitly stating that the credentials are no longer valid and instructing the user to re-authenticate using `firebase login --reauth` or generate a new CI token.",184f6b63fc24e4f4efb596727b52a1d6ffe36be0e65cd358a3fb6a6f0fbd2426
context/output/generated_math.py,"This file is a Python module designed for performing mathematical calculations, specifically focusing on the computation of factorials. It defines a single function named `calculate_factorial` which takes a non-negative integer `n` as input and returns its factorial (n!). The function includes robust error handling, raising a `TypeError` if the input is not an integer and a `ValueError` if the input is negative. The implementation uses an iterative approach with a `for` loop to calculate the product of numbers from 2 up to `n`, handling the base cases of 0 and 1 by returning 1 immediately. Additionally, the file includes a standard `if __name__ == ""__main__"":` block that serves as a simple usage example or test case. When the script is executed directly, it calculates the factorial of 5 and prints the result to the console, demonstrating the function's expected behavior.",259a387df1f02f8db75673d0ef8ff49701392bf64f99bae62e7c2443930d5ce2
context/output/factorial_spec.md,"The file contains a single, concise instruction related to a programming task. Specifically, it requests the creation of a function designed to calculate the factorial of a given number. This is a common introductory problem in computer science and algorithm design, often used to demonstrate concepts such as recursion or iterative loops. The prompt implies that the user is expected to write code, likely in a specific programming language not mentioned here, to perform this mathematical operation, which involves multiplying a sequence of descending natural numbers.",a37c783065288fff9bfdcb544dffaf7e194cec365f731adef625a218e753460f
context/output/calculator_prompt.md,"The provided file outlines the specifications for a basic Python calculator module. It instructs an expert Python engineer to develop a module that performs fundamental arithmetic operations. Specifically, the module must include two primary functions: `add(a, b)`, which calculates the sum of two numbers, and `subtract(a, b)`, which calculates the difference between two numbers. The requirements specify that these functions must accept numeric inputs, supporting both integers and floating-point numbers, and return results that correspond to the input types.",5acec3fa891fd98b02e17987ba8fb19b6cb1d3e6291053704b94effde0cfa48a
context/output/fix_attempt.log,"The file documents a debugging history involving two attempts to fix a Python script. In the first attempt, the verification process failed with a `TypeError` because the function `calculate_average` was passed a string (""123"") instead of the expected list of numbers. This caused a crash when the `sum()` function attempted to add an integer to a string character. The LLM analysis identified the root cause as a mismatch between the function's expected input (a list of numbers) and the actual input provided by the calling program. The proposed solution was to update the calling program to convert the string into a list of integers (`[1, 2, 3]`) before passing it to the function, rather than modifying the function itself. The decision was made to update the program but not the code module. The second attempt shows a successful verification with a return code of 0, producing the correct output ""Average is: 2.0"", indicating the fix was effective.",fd37e0478def242ef21e0ed6245eaa1dd057c74284c9a7849b11a5d37b709b59
context/output/verification_history.log,"The provided file content is extremely brief, consisting of only two lines of text that appear to be log entries or notes regarding an automated process involving a Large Language Model (LLM). The text documents two distinct attempts made by the LLM to modify code or text. In the first attempt, the model focused on correcting indentation issues. In the second attempt, the model shifted its strategy to renaming variables. There is no further context provided regarding the success or failure of these attempts, the specific code being modified, or the overall goal of the operation. The content serves as a minimal record of sequential actions taken by an AI system during a debugging or refactoring task.",dd6f9159a9296a3ed810011fd61b314a5d9198e730d529d174b639b13f660833
context/output/crash.log,"The provided file content captures a Python traceback error message originating from a script named 'run_factorial.py'. The traceback indicates that the execution failed at line 2 of the file. The specific error raised is a 'NotImplementedError', which typically signifies that a particular method or function has been defined but not yet implemented with working logic. This kind of error is often used as a placeholder during development to remind programmers to add the necessary code later. The brevity of the traceback suggests that the script is likely very short or that the error occurred almost immediately upon execution. No other context, variable states, or preceding log messages are provided in this snippet, limiting the diagnosis strictly to the fact that the code explicitly raised this exception rather than encountering a syntax or runtime logic error.",78c871483681d984405b4743ab75f7f109e0e13cee6d873e9ec5560237c164d6
context/output/test_calculator.py,"The provided file contains a Python unit test script designed to verify the functionality of a calculator module. It imports the `unittest` framework and specific functions—`add` and `subtract`—from a local module named `calculator`. The script defines a test class named `TestCalculator` which inherits from `unittest.TestCase`. Within this class, two test methods are implemented: `test_add` and `test_subtract`. The `test_add` method asserts that adding 2 and 3 correctly results in 5, while the `test_subtract` method asserts that subtracting 2 from 5 correctly results in 3. These tests ensure basic arithmetic operations are performing as expected within the application.",b811342a94f0ab315e398fbd3615d322290d961c386895cb808d57924302b05d
context/output/output.csv,"The file appears to be an empty CSV (Comma Separated Values) template or a header-only file intended for storing metadata about files. It contains a single line defining three column headers: ""full_path"", which likely stores the absolute or relative location of a file; ""file_summary"", intended for a textual description or abstract of that file's contents; and ""content_hash"", which is typically used for data integrity verification or change detection (e.g., MD5 or SHA checksums). There are no data rows present in the provided content, suggesting the file is either initialized and waiting for data entry or serves as a schema definition for a logging or indexing system.",5981f8e241d6613e6818d5165c8239dda258a3b8446231754c23df9ab14562f6
context/output/verify_code_1.py,"The provided file content is a short Python script designed to demonstrate an intentional error scenario when using a specific function. It begins by importing a function named `calculate_average` from a module called `module_to_test`. The script then attempts to invoke this function by passing a string value (""123"") as an argument instead of a numerical list or number. A comment within the code explicitly notes that this operation is expected to cause an error due to the incorrect data type being passed. Finally, the script attempts to print the result of this calculation using an f-string, though execution would likely fail before reaching this step if the function does not handle string inputs gracefully.",5b27cfe4730e1f2fb6eae0d8eacdb27dfb0649b35fedec13cba74c9a4c839bb0
context/output/verify_code.py,"The provided file content is a short Python script designed to demonstrate or test a specific function named `calculate_average`. It begins by importing this function from a module named `module_to_test`. The script then performs a data transformation step where the string ""123"" is converted into a list of integers `[1, 2, 3]` using a list comprehension. This list of integers is assigned to the variable `numbers`. Subsequently, the `calculate_average` function is called with `numbers` as the argument, and the returned value is stored in the variable `result`. Finally, the script prints the calculated average to the console using an f-string formatted as ""Average is: {result}"". The code serves as a simple usage example for the imported utility function.",ace7e24c314272d1b64ab2988cdb3c6226613e17d5e7311ad21743f3ff2c5218
context/output/run_factorial.py,"The provided file content is a very short Python script consisting of two lines of code. The first line imports a specific function named 'factorial' from a module or library called 'math_lib'. This suggests that there is an external dependency or a local file named 'math_lib.py' that contains mathematical utility functions. The second line of the script executes the imported 'factorial' function with the integer argument 5 and prints the result to the standard output. In a standard mathematical context, the factorial of 5 is calculated as 5 * 4 * 3 * 2 * 1, which equals 120. Therefore, when this script is run, it is expected to display the number 120. The code demonstrates a basic usage of modular programming in Python, separating function definitions from their execution logic.",358700b193421ad73e7b54d24f5db8bed2aae472ed115328abfdb80156434e09
context/output/module_to_test_1.py,"The provided file content is a brief Python code snippet that defines a single function named `calculate_average`. This function is designed to compute the arithmetic mean of a collection of numbers. It accepts one argument, `numbers`, which is expected to be an iterable sequence such as a list or tuple containing numerical values. Inside the function body, the built-in `sum()` function is utilized to calculate the total of all elements within the input sequence, and the `len()` function is used to determine the count of elements. The function then performs a division operation, dividing the sum by the count, and returns the resulting float value representing the average. The code is concise and relies on standard Python built-ins without any external dependencies or error handling for edge cases like empty lists.",7bfbc5dc080c8f3cc699cc3f6711b09d3fcf23a7ee2a69150972137800a79e86
context/output/module_to_test.py,"The provided file content is a brief Python code snippet that defines a single function named `calculate_average`. This function is designed to compute the arithmetic mean of a collection of numbers. It accepts one argument, `numbers`, which is expected to be an iterable sequence such as a list or tuple containing numerical values. Inside the function body, the built-in `sum()` function is utilized to calculate the total of all elements within the input sequence, and the `len()` function is used to determine the count of elements. The function then performs a division operation, dividing the sum by the count, and returns the resulting float value representing the average. The code is concise and relies on standard Python built-ins without any external dependencies or error handling for edge cases like empty lists.",7bfbc5dc080c8f3cc699cc3f6711b09d3fcf23a7ee2a69150972137800a79e86
context/output/calculator.py,"The provided file content consists of a short Python function definition named ""add"". This function takes two arguments, ""a"" and ""b"". However, despite the function name suggesting an addition operation, the implementation contains a logical error or bug. The return statement performs subtraction (""a - b"") instead of addition. This discrepancy is explicitly noted in a comment on the same line, which states ""Bug: subtraction instead of addition"". The code serves as a simple example of a semantic error where the code runs without syntax errors but produces incorrect results based on its intended purpose.",9fc16fc114d3dc01eb3126751237e7ccebc965df65471ae39bfcdf77c873dd04
context/output/spec.md,"The provided file content outlines a very brief specification for a calculator functionality. Specifically, it requests the creation of a single function named ""add"" which takes two parameters, ""a"" and ""b"". The purpose of this function is to perform an addition operation on these two inputs and return the result of ""a + b"". This appears to be a minimal requirement document or a coding challenge prompt focused on basic arithmetic operations.",d5cfe160eccf93924e57107cbd706844d14717c067c9df0f987900cda3a06830
context/output/batch_changes.csv,"The provided file content appears to be a comma-separated values (CSV) list defining a set of tasks for modifying specific prompt files within a software project. It contains two columns: 'prompt_name', which specifies the file path of the prompt to be edited, and 'change_instructions', which details the specific modifications required for that file. There are two entries listed. The first entry targets the file './output/src/batch/utils_python.prompt' and requests the addition of type hints as well as the improvement of docstrings by including examples. The second entry targets './output/src/batch/helpers_python.prompt' and instructs the implementer to add error handling mechanisms for invalid inputs. Essentially, this file serves as a manifest or a to-do list for refactoring Python-related prompt templates to enhance code quality, documentation, and robustness.",a8c2db16d1a65026be7a051510917478789dde12d5be5dd687abdc181ab35495
context/output/calculate_pi.py,"This Python script implements a Monte Carlo simulation to estimate the mathematical constant Pi. It defines a function, `estimate_pi`, which takes an integer representing the number of iterations as an argument. The function generates random points within a unit square using `random.uniform` and counts how many fall within a quarter circle defined by the equation x² + y² <= 1. The ratio of points inside the circle to the total number of points is then multiplied by 4 to approximate Pi. The script includes a main execution block that runs the estimation with 1,000 iterations, prints the estimated value, compares it to the actual value of Pi, and calculates the error margin.",19f4817c782276e163b4e7c87d77fab3779f563ef84f6e23060607e72fca6249
context/output/driver.py,"The provided file content is a brief Python script that serves as a basic unit test or sanity check for a calculator module. It begins by importing the `add` function from a module named `calculator`. Following the import, the script executes a conditional check to verify the correctness of the `add` function. Specifically, it calls `add(2, 2)` and compares the result to the integer 4. If the result does not equal 4, the script raises an Exception with the message 'Math failed'. This code snippet is likely part of a larger testing suite or a simple verification step to ensure that the fundamental addition logic within the `calculator` module is functioning as expected before proceeding with further execution.",dbc91cdff8a6be9c52f8a96cc91350248307066dd77cf06ef00d855c4f2da531
context/output/pi_result.txt,"The file contains the output of a computational estimation of the mathematical constant Pi. It displays the result of an approximation algorithm run for 1000 iterations, yielding a value of 3.22. This estimated value is then compared against the actual, high-precision value of Pi (3.141592653589793). Finally, the file reports the margin of error between the estimated and actual values, which is calculated to be approximately 0.078407. The content serves as a brief report on the accuracy of a specific Pi estimation method.",0c0197c628668b47ff221ee51e164f30dd40b934febe3b42e1efbb897cdf005c
context/output/examples/pipeline_example.py,"This file demonstrates the usage of the `DataPipeline` class, imported from the `data_pipeline` module. It defines a `main` function that initializes a new pipeline instance and chains several data processing operations. Specifically, the script loads data from a JSON file named ""input.json"", filters the dataset to retain only rows where the ""active"" field is true, transforms the ""name"" field to uppercase, and finally exports the processed data to ""output.json"". The script includes a standard entry point check to execute the `main` function when run directly.",8c9e91e7c48fc79bc29e29ef4ce68812216c23fa02a64466b699f8d087f7ea26
context/output/prompts/calculator_python.prompt,"The provided file outlines the specifications for creating a basic calculator module. It details the requirement to implement four fundamental arithmetic operations: addition, subtraction, multiplication, and division. Specifically, the module must include an 'add' function to sum two numbers, a 'subtract' function to deduct the second number from the first, a 'multiply' function to calculate the product of two numbers, and a 'divide' function to compute the quotient of the first number divided by the second. A critical constraint for the division function is the mandatory inclusion of error handling to raise a ValueError if the divisor is zero. Furthermore, the specification mandates that all functions must accept floating-point numbers as parameters and return floating-point results to ensure precision. Finally, the document requires that every function be properly documented with docstrings, ensuring code readability and maintainability.",1126584b8813ac598558a656fa6f7ed8603e392ead883f94f1f0cd13835c986f
context/output/prompts/data_pipeline_python.prompt,"This file outlines the specifications for a comprehensive Data Processing Pipeline Module designed to handle various stages of data manipulation. The module features robust data loading capabilities, supporting CSV, JSON, and Parquet formats, along with streaming for large files and automatic schema detection. It includes extensive data transformation tools allowing for row filtering, column mapping, aggregation via groupby operations, and dataset joining. To ensure data integrity, the pipeline incorporates a validation layer that handles schema verification, type checking, null value management, and numeric range validation. Finally, the module supports exporting processed data into multiple formats with options for compression and column-based partitioning. The overall architecture is intended to be modular, facilitating the chaining of these operations for flexible workflows.",3c17cf17c6ae64151868d820cd92425381eece3d4f29d1b6957ac5d3b9240c10
context/output/prompts/add_logging.prompt,"The provided file outlines a set of requirements for implementing comprehensive logging within a Python codebase. The primary directive is to integrate Python's built-in logging module across all functions to enhance observability and debugging capabilities. Specifically, the instructions mandate that every function must log its entry point, capturing and recording the parameters passed to it. Similarly, the exit point of each function must be logged, including the return values generated. Furthermore, the requirements specify that any exceptions encountered during execution must be caught and logged to ensure errors are properly tracked. Finally, the developer is instructed to utilize appropriate log levels—such as DEBUG for detailed flow information, INFO for general operational events, WARNING for potential issues, and ERROR for actual failures—to categorize the log messages effectively.",1350a442ccdaa6188bd926e14f529af335deb2da918e860b84b40aa1a5b54ebe
context/output/prompts/greeter_python.prompt,"The provided file outlines the specifications for a basic software component identified as the 'Greeter Module.' This module is designed to handle simple social interactions through string manipulation. It requires the implementation of two distinct functions. The first function, 'greet(name),' is tasked with accepting a name as an argument and returning an appropriate greeting message tailored to that individual. The second function, 'farewell(name),' mirrors this structure but instead generates and returns a farewell message for the specified name. Essentially, the file serves as a concise functional requirement or a stub for a utility module focused on generating personalized salutations and valedictions.",0d8fb32db1eeee9f932806e856a416a48112c30a22ac3d099b25cb5c90839ba6
context/output/modified/data_pipeline_modified_python.prompt,"The provided text outlines instructions for creating a data processing script using a `DataPipeline` class imported from a module named `data_pipeline_core`. The objective is to demonstrate the orchestration of a complete workflow by chaining methods provided by the pipeline implementation. Key requirements for the script include instantiating the pipeline object, loading a dataset (e.g., from JSON), and performing data manipulation tasks such as filtering active rows and transforming columns. Additionally, the script must validate the data against a schema, handle validation errors, and perform aggregation or join operations to enrich the dataset. Finally, the workflow should conclude by exporting the processed data to formats like JSON or Parquet, utilizing compression and partitioning features. The text emphasizes leveraging the modular nature of the pipeline to handle loading, transformation, validation, and export seamlessly.",f8eca37f927001e1db43813afc2b75b48d3019d7cc6b8e29e1d2964e2a03ec5d
context/output/modified/calculator_with_logging_python.prompt,"The provided file outlines the specifications for creating a Python module named ""calculator.py"". Acting as an expert Python Software Engineer, the task involves implementing a simple calculator with four core arithmetic functions: addition, subtraction, multiplication, and division. Each function must accept and return floating-point numbers and include descriptive docstrings. A critical requirement is the integration of comprehensive logging using Python's logging module. This includes configuring console output, logging function entry with parameters at the DEBUG level, recording successful exits with return values at the INFO level, and capturing exceptions—specifically division by zero—at the ERROR level. The module must also handle the setup of the logger to ensure it functions correctly when the script is executed.",414ae2404faba20e2555c7d3dd17cdc5f3c0c18706a1dade36e689b755295ea4
context/output/modified/greeter_updated_python.prompt,"The provided file outlines the specifications for creating a Python utility module named `greeter.py`, designed to enhance user interaction through personalized greeting and farewell messages. As an expert Python developer, the task involves implementing three core functions: `get_time_of_day`, `greet`, and `farewell`. The `get_time_of_day` helper function must determine the current time period (morning, afternoon, or evening) based on system time. The `greet` function requires logic to switch between formal, time-specific greetings (e.g., 'Good morning') and informal 'Hello' messages based on a boolean flag. Similarly, the `farewell` function must provide distinct formal and informal closing statements. The requirements emphasize code quality, mandating the use of Python type hints, Google-style docstrings, and f-strings for formatting. Additionally, the instructions specify using the standard `datetime` library for time logic and ensuring precise handling of hour boundaries to avoid logic errors.",4fab9537f1adfab29d6ed2bbc40651e707addc2802147641afa0e4620d9759da
context/output/modified/data_loading_python.prompt,"This document outlines the requirements for implementing a modular `DataPipeline` class in Python designed for efficient data processing. The class must support a fluent interface, allowing method chaining for loading, transforming, and exporting data. Key features include the ability to load data from CSV, JSON, and Parquet files, with a strong emphasis on handling large datasets via streaming or chunking rather than loading entire files into memory. The pipeline must automatically detect schema information upon initial loading. Transformation capabilities are extensive, requiring methods for filtering rows based on conditions, transforming individual columns, performing group-by aggregations, and joining datasets with support for inner and left joins. Validation utilities are also specified, including schema validation, null checks, and numeric range verification. Finally, the class must support exporting processed data back to CSV, JSON, and Parquet formats, with advanced options for compression (e.g., gzip, snappy) and partitioning data into subdirectories based on column values. Helper methods for retrieving data and clearing the pipeline state are also required.",c528c39ba57983785d2e71f0c8ffe043165979523a7f8d3e181ba21858ae08b1
context/output/src/data_pipeline.py,"The provided file implements a Python class named `DataPipeline` designed to facilitate data processing tasks. This class serves as a pipeline for handling data ingestion, manipulation, and export. It includes methods for loading data from sources such as JSON files (`load_json`) and CSV files (`load_csv`, though the implementation is currently a placeholder). The class supports method chaining for operations like filtering rows based on a callable condition (`filter`) and transforming specific column values using a provided function (`transform`). Additionally, it offers functionality to validate the data against a specified type schema (`validate_schema`) to ensure data integrity. Finally, the pipeline allows users to export the processed data back to a JSON file (`export_json`) or retrieve the current dataset directly as a list of dictionaries (`get_data`).",aa25c7097f39e861b031dfdb947db3720eebc3a3e17846dce3e6a4e794c9d452
context/output/src/greeter_modified.py,"This Python module provides enhanced greeting and farewell functionality with time-awareness. It imports the `datetime` class to determine the current time of day. The primary function, `greet`, accepts a name and an optional boolean flag for formality. If the formal flag is set to true, the function returns a time-specific greeting (Good morning, Good afternoon, or Good evening) based on the current hour; otherwise, it defaults to a simple ""Hello"". Similarly, the `farewell` function generates a parting message, offering a polite, extended phrase for formal contexts (""It was a pleasure..."") and a standard ""Goodbye"" for informal ones. Additionally, a utility function named `get_time_of_day` is included, which returns a string indicating whether it is currently morning, afternoon, or evening based on the system clock. The code includes docstrings for each function specifying arguments and return values.",9a4c4ac85c48b4a1f1f7425ccd764d0c7a10c04398964c3fc91cce6ea294abfa
context/output/src/calculator.py,"This file defines a basic Python module designed to perform fundamental arithmetic operations. It includes four primary functions: `add`, `subtract`, `multiply`, and `divide`. Each function accepts two floating-point numbers as arguments and returns the result of the corresponding mathematical operation. The `add` function returns the sum of the two inputs, while `subtract` returns the difference between the first and second arguments. The `multiply` function calculates the product of the two numbers. The `divide` function computes the quotient of the first argument divided by the second; notably, it includes error handling to raise a `ValueError` if an attempt is made to divide by zero, ensuring the module handles this mathematical impossibility gracefully. The module is documented with docstrings for each function, providing clarity on their purpose and usage.",6c3cd11b680e65dd3e72dad9ada510b392df891a18b25252c1c893ead0a7e40b
context/output/src/greeter_original.py,"This file contains a simple Python module designed for basic greeting interactions. It defines two primary functions: `greet` and `farewell`. The `greet` function accepts a string argument representing a name and returns a formatted greeting string, specifically ""Hello, {name}!"". Similarly, the `farewell` function takes a name as input and returns a formatted farewell message, ""Goodbye, {name}!"". Both functions include type hinting to indicate that the input and output are strings, and they are documented with docstrings explaining their purpose.",f17ff25b5235c5970cc73bbe0504c93e8deb45b3e94f6d77838ad4c516005f6b
context/output/src/batch/utils.py,"This file provides a collection of utility functions designed for basic string manipulation in Python. It includes two primary functions: `capitalize_words` and `reverse_string`. The `capitalize_words` function takes a string as input and returns a new string where the first letter of every word is capitalized, achieved by splitting the text into individual words, capitalizing each one, and then joining them back together. The `reverse_string` function accepts a string and returns it with the characters in reverse order, utilizing Python's slice notation for efficient processing. These utilities are simple, reusable components suitable for text processing tasks that require standard formatting or transformation operations.",808db9684a1cf7bc8a87179189a36a919e487b38c1418a8e817325a94f6fd88b
context/output/src/batch/helpers_python.prompt,"The provided file content outlines a requirement for creating a set of helper functions designed to perform specific operations on lists. It identifies two primary functions to be implemented. The first function, 'flatten_list', is intended to process nested lists—lists that contain other lists as elements—and convert them into a single, one-dimensional list structure. This operation is commonly used to simplify complex data structures for easier processing. The second function, 'unique_items', aims to filter a given list and return a new collection containing only the distinct elements, effectively removing any duplicate entries. This is useful for data cleaning and ensuring data integrity. Together, these helper functions suggest a focus on data manipulation and preprocessing tasks, likely within a programming context such as Python or JavaScript, where list handling is a frequent necessity.",607da04e3cce0ad4952a0f38dfe3e321d899ab451836d8c744d26f76da5bffb0
context/output/src/batch/helpers.py,"This file defines a set of utility functions for manipulating lists in Python. It includes two primary functions: `flatten_list` and `unique_items`. The `flatten_list` function is designed to recursively process nested lists, converting a multi-level list structure into a single, one-dimensional list containing all original elements. The `unique_items` function filters a given list to return only distinct elements while preserving their original order of appearance, utilizing a set to track seen items for efficiency. Both functions are type-hinted to accept lists containing any data type.",7e40273917fba20d73db0bba1c214d71cc28725acfa44748ffb2a51d51b3669d
context/output/src/batch/utils_python.prompt,"The provided file contents outline a requirement for creating a set of utility functions specifically designed for string manipulation. It lists two distinct functions to be implemented. The first function, 'capitalize_words', is intended to take a string input and transform it by capitalizing the first letter of every word within that string. The second function, 'reverse_string', requires logic to reverse the sequence of characters in a given string. These instructions serve as a brief specification for a developer to build a basic string processing library or module.",181589c88c94b3d4bdd41b984ae5335f3607455583ae45a261319facd1d14120
context/server/executor_example.py,"This Python script serves as a demonstration and test harness for a command execution module, specifically focusing on `ClickCommandExecutor` and `execute_pdd_command`. It begins by importing necessary libraries and attempting to import specific classes from a `pdd.server.executor` package, providing mock fallback classes (`CapturedOutput`, `ClickCommandExecutor`) if the import fails to ensure the script remains runnable in standalone contexts.

The script defines a sample Click command named `greet_command` which accepts arguments like `name`, `count`, and `loud`, and utilizes a context object to retrieve configuration values. It also defines a streaming callback function, `on_output`, to demonstrate real-time output handling.

Two main demonstration functions are provided: `demo_executor_class`, which instantiates the `ClickCommandExecutor` with a base context and callback to run the `greet_command`, and `demo_high_level_helper`, which shows how to use the `execute_pdd_command` helper function to run commands by name string. The script concludes with a main execution block that runs both demos, printing results such as exit codes, costs, and captured standard output/error to the console using the `rich` library for formatting.",fb5d8d40aeeb2bf401dc6a84a43bee6936ee1e734b2b00bd08e7fbc44219a9cf
context/server/security_example.py,"This Python script demonstrates the implementation of a secure FastAPI server using a custom security module (`pdd.server.security`). It initializes a FastAPI application and configures several security layers, including a `PathValidator` to prevent directory traversal attacks by restricting file access to a specific project root and blacklisting sensitive files like `.git` or `.env`. The script sets up middleware for security logging and configures CORS to allow requests only from specific local origins. Authentication is handled via a token dependency, requiring a bearer token for protected routes. The application defines three endpoints: a public status check, a secured data endpoint requiring authentication, and a file reading endpoint that combines authentication with path validation to safely handle user-provided file paths. Finally, it includes a main block to run the server using `uvicorn` on localhost, printing examples of valid and malicious access attempts to the console.",dc4f3dc06455ba6180f4536aa67ce4e9a5efd52e0e9cd7702c3f2ec6ef96aac9
context/server/token_counter_example.py,"This Python script serves as a demonstration and usage example for a custom module named `token_counter`. The script primarily illustrates how to utilize various functions within the `token_counter` module to analyze text for Large Language Model (LLM) usage. It begins by ensuring the module can be imported and defines a helper function, `create_dummy_pricing_csv`, to generate a temporary CSV file containing pricing data for models like GPT-4 and Claude 3 Opus. The `main` function orchestrates the workflow: it first sets up sample text and ensures the pricing data exists. It then proceeds to demonstrate four key capabilities: counting tokens in the sample text using `tiktoken`, retrieving context limits for specific model families (e.g., GPT-4 Turbo), estimating financial costs based on the generated pricing CSV, and generating comprehensive metrics that include usage percentages and costs for a specific model (Claude Sonnet). Finally, the script outputs these metrics in a JSON format to the console, providing a clear example of how to integrate token counting and cost estimation into a broader application.",08885532e1406cee06db0398232624d7d9a3db72a9843152fa81db38027cd8a3
context/server/models_example.py,"This Python script serves as a demonstration and usage guide for the Pydantic models defined in the `pdd.server.models` module. It defines a single function, `example_usage`, which systematically walks through creating instances of various data models including `FileMetadata`, `CommandRequest`, `JobHandle`, `WSMessage`, `StdoutMessage`, `WriteFileRequest`, and `ServerStatus`. The script highlights key features such as input validation (e.g., catching invalid file paths), serialization to JSON and dictionaries using Pydantic v2 methods like `model_dump_json`, and handling polymorphic WebSocket messages. It effectively acts as documentation by code, showing developers how to interact with the server's data structures for tasks like file handling, job management, and status reporting.",223a832718e72ca6640728e4bd700da58edb89667931085d7d949774d8fbc228
context/server/terminal_spawner_example.py,"This Python script serves as a demonstration and test utility for the `TerminalSpawner` class, which is part of a larger package (likely named `pdd`). The script begins by configuring the system path to ensure the `pdd.server.terminal_spawner` module can be imported correctly, handling potential import errors gracefully. The `main` function showcases three distinct use cases for spawning external terminal windows. First, it launches a simple terminal that prints a greeting and lists the current directory contents. Second, it demonstrates the ability to specify a working directory by spawning a terminal that opens in the user's home folder. Finally, it simulates a job execution scenario by passing a `job_id` and `server_port`, illustrating how the spawner constructs commands intended to callback to a server upon completion (though it notes the callback will fail without an active server). Throughout execution, the script prints status messages to the console indicating the success or failure of each spawn attempt, providing immediate feedback on the functionality of the `TerminalSpawner` across different configurations.",de89761e9228d058064a52ed3bf4d84f2ceba1991e5b8e0a45a4af8f0c477e19
context/server/__init___example.py,"This Python script serves as a comprehensive usage example and demonstration for the `pdd.server` package. It illustrates three core functionalities of the system: security validation, asynchronous job management, and API server interaction. First, the `demo_security_validation` function showcases the `PathValidator` component, proving its ability to accept valid file paths within a project root while correctly rejecting directory traversal attacks (e.g., accessing `../../etc/passwd`). Second, the `demo_job_manager` function utilizes `asyncio` to demonstrate the `JobManager` class, simulating the submission, execution, and status tracking of a background job using a mock executor. Finally, the `demo_server_interaction` function spins up the actual FastAPI server in a background thread using `run_server`. It then performs a live HTTP GET request to the server's status endpoint (`/api/v1/status`) to verify connectivity and retrieve version information. The script uses the `rich` library for formatted console output and includes fallback import logic to support both installed package usage and local development environments.",8817e0d29e93b3b67a7fe8fb08962ea8df850bbceda7390bbc12d42afd5070b8
context/server/jobs_example.py,"This Python script serves as a demonstration and usage example for a `JobManager` class within a project named `pdd`. It illustrates how to initialize the manager with concurrency limits and register lifecycle callbacks for job start and completion events. The script defines a `mock_worker` function to simulate asynchronous tasks with variable durations and potential failure states, replacing actual CLI command execution for testing purposes. In the `main` execution flow, the script submits a batch of five jobs with different parameters to demonstrate queuing behavior (where only two jobs run concurrently). It specifically showcases features such as job submission, real-time status logging via callbacks, cancelling a queued job, and handling simulated errors. Finally, the script waits for all tasks to complete, prints a summary table of job statuses and costs, performs a cleanup of old job records, and gracefully shuts down the manager.",7694bd486d012fd9d52543c5f79a289f61dac3ed0f8c94c2ea9fd25ed6333f12
context/server/app_example.py,"This Python script serves as a demonstration and testing utility for the `pdd` server application. It illustrates how to initialize, configure, and interact with the server programmatically. The script begins by setting up the necessary import paths to access the `pdd` package and defines a helper function, `start_server_in_thread`, to run the Uvicorn-based server in a separate daemon thread, allowing the main thread to act as a client simultaneously.

The `main` function orchestrates the workflow: it first creates a temporary dummy project directory containing a test file. After launching the server on localhost port 9999, the script performs a series of HTTP requests to verify functionality. These tests include checking the server's status endpoint (`/api/v1/status`) to retrieve version and uptime information, attempting a path traversal attack to confirm security measures correctly block unauthorized file access (returning a 403 Forbidden status), and verifying the accessibility of the OpenAPI documentation (`/docs`). Finally, the script cleans up by removing the temporary directory and exits, relying on the daemon thread to shut down the server automatically.",554bb0307f2f7a0dc79ef86f8a4fd42e5017225d0c56379ad99d2939f3c1087a
context/server/click_executor_example.py,"This Python script serves as a comprehensive usage example and test suite for a custom module named `ClickCommandExecutor`. It demonstrates how to programmatically execute `click`-based CLI commands within a Python application rather than invoking them from a shell. The script begins by defining sample `click` commands: a `greet` command that accepts arguments and options, and a `fail_command` designed to raise exceptions. It then implements four distinct demonstration functions: `demo_basic_execution`, which shows how to run a command and capture standard output; `demo_streaming_execution`, which illustrates real-time output handling via callbacks; `demo_error_handling`, which verifies the capture of standard error and exception details; and `demo_context_passing`, which proves the ability to inject shared state into the command context. The script includes fallback import logic to handle potential path issues and a main entry point that sequentially runs all demonstrations, printing the results to the console.",a61ee766f5b09bfa0de58533125b85d8855186f17f3109101887177a87d8d5c9
context/server/routes/session_example.py,"This Python module defines a FastAPI router for handling remote session status endpoints within the `pdd.server.routes.session` namespace. It establishes an API route prefixed with `/api/v1/session` and tagged as ""session"". The core component is the `RemoteSessionInfo` Pydantic model, which structures the response data for session queries, including fields for the session ID, cloud URL, registration boolean status, and a timestamp for when registration occurred. The module implements a single GET endpoint, `/info`, designed to return the current server's registration status with PDD Cloud. Although the implementation of the `get_session_info` function is elided with an ellipsis, the provided comments and example usage demonstrate how it is intended to interact with a session manager to retrieve active session details. If a session manager is active, it populates the response model with the relevant session ID and cloud URL; otherwise, it returns a status indicating the server is not registered.",7afec7e9141ead74f2ba74ac5a70a7f35793eb3409606539c81cf1e7b34ed1a9
context/server/routes/prompts_example.py,"This Python script serves as a standalone demonstration and test harness for a FastAPI router module, likely named `prompts_module.py`. It is designed to simulate the functionality of a prompt analysis API without requiring the full `pdd` package environment. The script begins by mocking several external dependencies using `unittest.mock` and `sys.modules`, specifically simulating the behavior of security validators (`PathValidator`), token counters (`get_token_metrics`), and preprocessing utilities (`preprocess`). 

Following the setup, the script defines a `create_example_app` function that initializes a FastAPI application, includes the target router, and configures a temporary file system with a dummy prompt file (`test_prompt.txt`) for testing purposes. It also injects a mocked path validator into the router. Finally, the `run_examples` function executes a series of integration tests using `TestClient`. These tests demonstrate three key scenarios: analyzing a file with preprocessing enabled, analyzing raw string content directly, and handling errors for non-existent files. The script concludes by cleaning up the temporary directory.",5209a59a02b915a352516a2539b4ebd240462b6ef704fa244dd15eee7eb7a48c
context/server/routes/config_example.py,"This Python module defines a FastAPI router dedicated to server configuration endpoints, specifically designed to help frontend applications synchronize with the backend environment. Located under the `/api/v1/config` prefix, the module establishes a route for retrieving the cloud functions URL. It utilizes Pydantic to define a `CloudUrlResponse` model, ensuring structured responses containing both the `cloud_url` and the current `environment`. The primary endpoint, `GET /cloud-url`, is intended to return this configuration data, preventing mismatches between staging and production environments by ensuring the frontend communicates with the correct cloud services. The file also includes commented example usage demonstrating how to register this router within a main FastAPI application and what a typical JSON response would look like.",58f2b27cdf85200a52fbe76324d02431dc62f339afd3896de37c0fe759f55cde
context/server/routes/websocket_example.py,"This Python script serves as a comprehensive demonstration and testing utility for the WebSocket routing module within the PDD Server architecture. It illustrates how to implement real-time communication features, specifically focusing on job output streaming, file system monitoring, and connection management. The script sets up a mock environment—including a mock FastAPI application, job manager, and WebSocket client—to simulate server behavior without requiring the full production stack. 

The code defines several key example functions: `example_job_streaming` shows how to broadcast job progress, standard output/error logs, and completion status to clients; `example_file_watching` simulates file system events (creation, modification, deletion) and broadcasts them; `example_connection_management` handles multiple client connections and subscriptions; and `example_error_handling` demonstrates robustness against invalid inputs or dropped connections. By running these asynchronous examples via `asyncio.run(main())`, developers can verify the WebSocket protocol's functionality, ensuring that messages are correctly formatted, routed, and handled for various server events.",a4c2313449f3174039edf4ab1f2030247b4da0eb31e6411b1008516be95eb3cf
context/server/routes/auth_example.py,"This file provides a standalone, executable example and test harness for an authentication module, likely named `auth_routes.py`. It is designed to demonstrate and verify the functionality of FastAPI routes related to GitHub Device Flow authentication without requiring the full production environment or external dependencies.

The script begins by mocking necessary environment variables (like `GITHUB_CLIENT_ID`) and internal dependencies. Specifically, it uses `unittest.mock.MagicMock` to simulate the behavior of the `pdd` and `pdd.get_jwt_token` modules, including mocking the `DeviceFlow` class to return predictable device codes and simulating a successful token polling process. It then attempts to import the `auth_router` from a local file.

A `FastAPI` application instance is created, including the imported authentication router, and wrapped in a `TestClient`. The core of the script is the `run_example_flow` function, which programmatically steps through the authentication lifecycle: checking the initial authentication status, initiating a login request (which returns verification codes), polling for the login status using a generated poll ID, logging out, and finally verifying the status is reset. This setup allows developers to test the API endpoints and logic flow in isolation.",aa1714b9b68296ad8138e652fa00452e43697313d4de624be49f73fcd46d301e
context/server/routes/files_example.py,"This Python script implements a standalone FastAPI application designed to simulate a file system management server. It defines a set of Pydantic models to represent file structures (FileTreeNode), content (FileContent), and operations (WriteFileRequest, WriteResult). The script includes a mock security layer with a `PathValidator` class to prevent directory traversal attacks by ensuring all file operations remain within a designated temporary project root. 

The core functionality is encapsulated in an `APIRouter` which exposes three main endpoints: `/tree` for retrieving a hierarchical view of the file system up to a specified depth, `/content` for reading file contents, and `/write` for creating or updating files. The script also includes helper functions for building file trees and detecting binary files. 

The `main` execution block sets up a temporary directory with dummy files (`hello.txt`, `src/main.py`) to serve as the sandbox environment, initializes the FastAPI app, registers the router, and launches the server using Uvicorn on localhost port 8000. It also handles cleanup by removing the temporary directory upon server shutdown.",9e5d8e77e1d0238394c331a872fcb31e4597100fc9427d8bba7ee3550df4323d
context/server/routes/commands_example.py,"This Python script demonstrates a self-contained FastAPI application that implements a job execution system for a hypothetical 'PDD' (Prompt Driven Development) tool. It mocks external dependencies (models and a job manager) to create a runnable example without needing the full project structure. The script defines Pydantic models for command requests and job results, and a `MockJobManager` that simulates asynchronous background task processing. It sets up an API router with endpoints to submit commands (`/execute`) and check job status (`/jobs/{job_id}`). Finally, it includes a `run_example` function that uses `TestClient` to simulate a client submitting a 'generate' command, polling for its completion, and testing error handling for invalid commands.",d567af5f8a7e828a23e1c652dd3c6a109e8dd2ca0a4384cd6ae1c684ebcf150c
context/server/routes/architecture_example.py,"This Python script serves as a demonstration and testing harness for an architecture validation module, likely part of a larger system named 'pdd'. It illustrates two primary methods of interacting with the validation logic: direct function calls and HTTP API requests via FastAPI. 

The script defines a `create_app` function to initialize a FastAPI application that includes an `architecture_router`. It then provides two main demonstration functions: `demonstrate_direct_validation` and `demonstrate_api_usage`. The direct validation demo tests the internal `_validate_architecture` function against three scenarios: a valid module structure, a structure with circular dependencies (causing errors), and a structure with missing dependencies and duplicates (triggering errors and warnings). 

The API usage demo utilizes `TestClient` to simulate a POST request to the `/api/v1/architecture/validate` endpoint with a sample payload, printing the resulting validation status. The script concludes with a `__main__` block that executes these demonstrations sequentially, handling potential import errors if the underlying architecture module is missing, and includes commented-out code for running the actual Uvicorn server.",6adddc3703c575ef7eaa12c58df0067e438c2fe328e5037fb69856595e862bf8
context/server/routes/__init___example.py,"This Python script serves as an illustrative example for utilizing the `pdd.server.routes` module within a FastAPI application. It demonstrates the architectural pattern of modularizing API routes by importing them from separate packages and integrating them into a central application instance. Although the script mocks the actual sub-modules (`files_router`, `commands_router`, and `websocket_router`) for demonstration purposes, it clearly outlines how these components would typically be structured in a production environment, such as in `pdd/server/main.py`. 

The code defines a `create_app` factory function that initializes a FastAPI application with specific metadata (title, description, version) and registers the mocked routers using `app.include_router`. It also includes a `main` entry point that instantiates the app, prints a list of registered routes (both HTTP and WebSocket) to the console for verification, and launches the server using `uvicorn` on localhost port 8000. This example effectively guides developers on setting up the PDD (Prompt Driven Development) Server's routing infrastructure.",a8bed7e09c6cd96bf68bff186993dc56d32f106fee86ca14f7324285d44f2267
context/xml/1/split_xml_LLM.prompt,"The provided file outlines a prompt engineering task designed to train an LLM to split a single complex prompt into two distinct components: a 'sub_prompt' and a 'modified_prompt'. The goal is to modularize the original prompt without losing any functionality. The file structure includes a section of examples, which reference external file paths for inputs (initial prompts and code) and expected outputs (split prompts and example usage code). It defines the context for the AI, establishing its role as an Expert LLM Prompt Engineer. The input definitions clarify that the model will receive an initial prompt, the code generated from it, and an example of how the split code should function. The output definitions specify the requirement to generate the two new prompts. Finally, the instructions guide the model to first analyze the difficulties of the split, propose solutions, and then generate the actual 'sub_prompt' and 'modified_prompt' based on the provided inputs.",0b203bd64cd6c92f8a1b47a6ed243f8061590a43ad627d8c695bdf0b24324f02
context/xml/1/split_LLM.prompt,"The provided file content is a prompt template designed for an expert LLM Prompt Engineer. Its primary objective is to instruct the LLM on how to refactor a single, large `input_prompt` into two distinct components: a `sub_prompt` and a `modified_prompt`, ensuring no loss of functionality. The template includes a section for few-shot learning, referencing four specific examples that demonstrate this splitting process using various input files (e.g., `initial_pdd_python.prompt`, `pdd.py`). It defines the expected inputs—the original prompt, the code generated from it, and an example of how the split code should function—and the required outputs. Finally, it outlines a step-by-step instruction set for the LLM to follow: identifying difficulties in the split, proposing solutions, and then generating the actual `sub_prompt` and `modified_prompt` text.",48ad8ab8f21f72b82b32484e54fc6fd3c7ea101fe3dec54cedfc5cf35f30c1b9
context/xml/2/xml_convertor_LLM.prompt,"This file contains a prompt designed for an expert Prompt Engineer whose task is to enhance the structure and readability of a raw input prompt by adding appropriate XML tags. The prompt instructs the model to analyze the input, identify components such as instructions, context, and examples, and then insert XML tags like `<instructions>`, `<context>`, `<examples>`, and `<formatting>` without altering the original content. It includes an example of how a raw prompt should be transformed and outlines a step-by-step process for the model to follow: analyzing the input, discussing appropriate tags, and finally generating the tagged prompt. The goal is to improve the organization of prompts, especially those containing placeholders or extensive text.",6b13729c7cb616406629f73a7b6f431659721b9ca0347d0fc62cbbabf3c9381a
context/xml/2/xml_convertor_xml_LLM.prompt,"The provided file outlines a prompt template designed for an AI agent acting as an expert Prompt Engineer. The primary objective of this agent is to take a raw input prompt (`input_raw_prompt`) and enhance its structure and readability by inserting appropriate XML tags. The instructions explicitly state that the agent should not add any new content, but rather organize existing text using tags such as `<instructions>`, `<context>`, `<examples>`, and `<formatting>`. The process involves a three-step workflow: analyzing the raw prompt to identify its components, determining suitable XML tags, and inserting those tags into the correct locations. The file includes examples of how raw prompts should be transformed into tagged versions and provides general guidelines, noting that XML tags are particularly useful for organizing large blocks of text or placeholders.",b5222a1465e870334d6e22d916dbe691527b1cc8ace21d7d681c85bfb5578fc7
context/generate/7/README.md,"PDD (Prompt-Driven Development) is a command-line interface tool designed to streamline software development by leveraging AI models to generate code, create unit tests, and manage prompt files. Installed via pip, it requires API keys from providers like OpenAI or Anthropic and supports various programming languages through a specific `.prompt` file naming convention.

The tool offers a comprehensive suite of commands, including `generate` for creating code, `test` for producing coverage-based unit tests, and `fix`, which utilizes an iterative loop to resolve code errors automatically. Advanced features include `update` for syncing prompts with modified code, `split` for managing complex prompts, and `crash` or `bug` for debugging runtime issues. PDD emphasizes workflow efficiency through multi-command chaining, detailed cost tracking via CSV outputs, and extensive configuration options using environment variables. It is designed for integration into CI/CD pipelines, offering features like auto-updates, dependency management, and security controls to facilitate a robust prompt-driven development lifecycle.",c6e6e0c2878f1753ad0669145ed552627a8bb7ddddcc89bab7338df1e92a75bd
context/generate/7/code_generator_main.py,"The provided code defines a main function, `code_generator_main`, designed to orchestrate the generation of code based on a prompt file. It utilizes the `click` library for command-line interface context and `rich` for formatted terminal output. The function takes a Click context, a path to a prompt file, and an optional output path as arguments. 

Internally, it first constructs necessary file paths and determines the target programming language using the helper function `construct_paths`. It then reads the content of the prompt file. The core logic involves calling `code_generator` with parameters such as strength and temperature (retrieved from the context), which returns the generated code, the cost of the operation, and the model name used. 

If an output path is specified, the function saves the generated code to that file. Finally, unless quiet mode is enabled, it prints a success message, the model name, the total cost, and the save location to the console. The function includes error handling that catches exceptions, prints an error message in red, and exits the program with a status code of 1 if a failure occurs.",319d4e3596f837183ff589638fe6ac87f180b95d7c0956bfa9f3c77e787ab0dd
context/generate/7/code_generator_main_python.prompt,"The provided file is a prompt template designed for an LLM to generate a Python function named `code_generator_main`. This function serves as a Command Line Interface (CLI) wrapper for a code generation tool. The prompt instructs the LLM to act as an expert Python engineer and defines the specific inputs and outputs for the target function. Inputs include a Click context object (`ctx`), a path to a prompt file, and an optional output path. The expected output is a tuple containing the generated code string, the operation cost, and the model name. The prompt template includes placeholders for various context files, such as a Python preamble, examples of using the `click` library, internal module usage examples (specifically `construct_paths` and `code_generator`), and a README file detailing the CLI command's behavior. Essentially, this file acts as a meta-prompt to guide an AI in writing the main logic for a code generation CLI tool.",7a42d9a6e9fe4ec1d066474a1f6d027a8888f93df743c35e174bb253d44e792f
context/generate/6/conflicts_main_python.prompt,"This file contains a prompt for an expert Python engineer to develop the `conflicts_main` function for the `pdd` command-line program. The function is designed to handle the core logic of the 'conflicts' command, which analyzes potential conflicts between two prompt files. The prompt specifies the function's inputs (Click context, file paths) and outputs (conflict results, cost, model name). It outlines a step-by-step implementation plan: constructing file paths using internal utilities, loading file contents, invoking the `conflicts_in_prompts` function for analysis, updating result data with actual file paths, saving the output to a CSV file, and providing user feedback via the Rich library. The prompt also includes references to context files for Python preambles, Click library usage, and internal module examples (`construct_paths`, `conflicts_in_prompts`) to guide the implementation.",b850d40459d986b3e5e8141fe3987896e9c98b14f6ca7cc3e012da011387fc71
context/generate/6/conflicts_main.py,"This file defines the `conflicts_main` function, which serves as the core logic for a command-line tool designed to analyze and resolve conflicts between two text prompts. The function takes two prompt file paths and an optional output path as arguments. It utilizes helper functions to construct file paths and read the content of the prompts. The core analysis is performed by calling `conflicts_in_prompts`, using parameters like strength and temperature derived from the Click context. The results, which identify conflicting instructions and suggest changes, are then mapped back to their original filenames and saved to a CSV file. The function also handles user feedback by printing the analysis status, the model used, the total cost, and a detailed list of detected conflicts to the console using the `rich` library. Error handling is included to catch exceptions and exit gracefully.",e7a05ba27d622574ff90a578304b063aa68970bdd270edf852c86f13d9470937
context/generate/1/fix_errors_from_unit_tests_python.prompt,"The file outlines a specification for an expert Python Software Engineer to create a function named ""fix_errors_from_unit_tests"". This function is designed to automatically resolve errors in code and unit tests using Langchain LCEL (LangChain Expression Language). The process involves a multi-step workflow: first, loading specific prompt files from a project directory; second, invoking an LLM (selected based on strength and temperature) to generate fixes for the provided unit test, code, and error logs; and third, using a second LLM call with a JSON parser to extract structured data—specifically boolean flags for updates and the fixed code strings. The function must also calculate and pretty-print the token usage and financial cost of the LLM operations using the ""rich"" library for console output. It requires handling inputs like the unit test code, source code, error messages, and model parameters, ultimately returning the fixed code, update status flags, and total cost.",e996814013526e08ecd4258f33dc84ba9c3594d3662496c081332567bce8e5a3
context/generate/1/fix_errors_from_unit_tests.py,"This Python script defines a function named `fix_errors_from_unit_tests` designed to automatically resolve errors in unit tests and their corresponding code using Large Language Models (LLMs). The script leverages the `langchain` library for LLM orchestration, `rich` for terminal formatting, and a custom `llm_selector` module for model management. It initializes an SQLite cache to optimize performance and reduce costs. The core function operates in a two-step process. First, it loads prompt templates from an external directory specified by the `PDD_PATH` environment variable. It then invokes an LLM to analyze the provided unit test, source code, and error message to generate a fix. Second, it uses another LLM call to parse the previous output and extract the specific corrected code and unit test strings into a structured JSON format. Throughout execution, the script calculates and displays token usage and financial costs associated with the API calls. Finally, it returns a tuple containing flags indicating whether updates were made, the fixed code segments, and the total operation cost, while handling potential exceptions gracefully.",7a3eb969bcaa76dc85d59c4013a41a7da5bb7472daf97df2b2cad0475642d2c7
context/generate/8/llm_invoke_python.prompt,"The provided text outlines the requirements for generating a Python function named `llm_invoke` within a file called `llm_invoke.py`. This function is designed to execute Large Language Model (LLM) prompts using the LangChain framework and its caching capabilities. The function accepts inputs such as a prompt string, input JSON, a `strength` parameter (0.0 to 1.0) for model selection, temperature, verbosity, and an optional Pydantic output schema. A core feature is the dynamic selection of the LLM model based on the `strength` value: values below 0.5 interpolate based on cost relative to a base model (defaulting to ""gpt-4o-mini""), while values above 0.5 interpolate based on ELO score up to the highest-rated model. The instructions specify handling different model providers (like Google and OpenAI) correctly regarding token limits and structured outputs, specifically mandating `.with_structured_output()` for Pydantic models. Additionally, the prompt details verbose logging requirements, including printing model costs, token usage, and results using `rich` printing where possible.",e938fc6ef160c8bf1b07147276a7c1dc9fd957cadfefb7c2d866f0f5b9caf45e
context/generate/8/llm_invoke.py,"The provided file, `llm_invoke.py`, serves as a flexible utility wrapper for invoking various Large Language Models (LLMs) using the LangChain framework. Its primary purpose is to abstract model selection and execution based on performance requirements and cost constraints. The script relies on an external CSV file (`llm_model.csv`) to load metadata for supported models, including providers (OpenAI, Anthropic, Google, etc.), pricing per million tokens, and ""Coding Arena ELO"" ratings.

A core feature is the `select_model` function, which dynamically chooses an LLM based on a `strength` parameter (0.0 to 1.0). A strength below 0.5 prioritizes cost savings by selecting models cheaper than a default base, while a strength above 0.5 prioritizes performance by selecting models with higher ELO ratings. The main function, `llm_invoke`, orchestrates the process by validating inputs, initializing the selected model with appropriate API keys, and constructing a processing chain. It supports both standard string outputs and structured data extraction via Pydantic models. Additionally, the script includes a custom callback handler to track token usage, allowing it to calculate and return the specific financial cost of each execution alongside the generation result.",3a0d5138cc2b1f6668a94dff40cf5d97376bde2581e83e2538c3ef012a94f1bf
context/generate/4/cli_python.prompt,"This file contains a detailed prompt for an AI to generate a Python command-line interface (CLI) tool named ""pdd"". The tool is designed to be built using the `click` library and serves as a developer utility for managing code generation, testing, and prompt engineering workflows. The prompt outlines the directory structure, import conventions to avoid naming conflicts, and specific instructions for implementing various commands. These commands include `generate` (creating code from prompts), `example` (generating examples from code), `test` (creating unit tests), `preprocess` (handling prompt files), `fix` (resolving code errors), `split` (dividing prompts), `change` (modifying prompts), `update` (refreshing prompts), `detect` (identifying necessary changes), `conflicts` (resolving prompt conflicts), `crash` (fixing crash errors), `trace` (execution tracing), and `bug` (converting bugs to tests). It provides specific internal module usage examples for each command.",e795460d8284f0a103eca1ffc797f2a0f834225ca513e520f2c5af19f7f704a4
context/generate/4/cli.py,"This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the Python `click` library. It serves as the main entry point for an application designed to assist developers in generating, testing, fixing, and managing code through AI-driven prompts. The CLI exposes a variety of commands including `generate` (creating code from prompts), `test` (creating unit tests), `fix` (iteratively repairing code based on errors), `update` (modifying prompts based on code changes), and `detect` (analyzing necessary changes). 

The script handles global configuration options such as AI model strength, temperature, verbosity, and cost tracking. It integrates with various backend modules to perform specific tasks like preprocessing prompts, splitting complex files, tracing code lineage, and generating bug reproduction tests. Additionally, it includes utility commands for installing shell completion and managing file paths. The application emphasizes developer workflow automation by linking natural language prompts directly to executable code and test suites, while providing feedback on API costs and model usage.",aeaaceab84a98e9cb777d691f1a9de4aa3538569ce0fa0b732228c8dfbae51cf
context/generate/3/cli_python_preprocessed.prompt,"The provided file describes ""pdd"" (Prompt-Driven Development), a Python command-line interface (CLI) tool designed to streamline software development using AI models. PDD facilitates a workflow where developers define functionality in prompt files, which the tool then uses to generate code, unit tests, and usage examples. Key features include support for multiple programming languages (Python, JavaScript, C++, etc.), cost tracking for AI usage, and configurable model parameters like strength and temperature.

The CLI offers a comprehensive suite of commands: `generate` creates code from prompts; `test` produces unit tests; `example` generates usage examples; `fix` iteratively repairs code based on errors; and `preprocess` handles prompt formatting. Advanced commands like `split`, `change`, `update`, `detect`, `conflicts`, and `crash` allow for managing complex prompts, detecting necessary changes, resolving conflicts between prompts, and fixing runtime crashes. The tool supports multi-command chaining for complex workflows and integrates with environment variables for flexible output management. It emphasizes security and efficiency, aiming to automate repetitive coding tasks while maintaining high code quality through iterative testing and refinement.",19d93d524e9b050886a9ab599dab0d3ba2d637d9b31a0333e8285ec2a84c12a3
context/generate/3/cli.py,"This file implements the main entry point for a Prompt-Driven Development (PDD) Command Line Interface (CLI) tool, built using the Python `click` library. It orchestrates various AI-assisted software development tasks by importing logic from several internal modules. The script defines a global CLI group that accepts configuration parameters such as AI model `strength`, `temperature`, verbosity, and cost tracking options.

A key component is the `track_cost` decorator, which wraps commands to log execution time, API costs, and model usage to a CSV file. The CLI exposes a wide range of subcommands including `generate` (creating code from prompts), `test` (generating unit tests), and `fix` (iteratively resolving errors via a feedback loop). It also includes commands for prompt engineering workflows, such as `preprocess`, `split`, `change`, `update`, `detect`, and `conflicts`. Additionally, the tool features a `crash` command for fixing module errors and a utility to install shell auto-completion. The script utilizes the `rich` library for formatted console output and handles file path management for inputs and outputs.",43a3aaf18afc0af2bc214890f87e561f5f61e885452c4dc69daff01818e1eb4c
context/generate/2/cli_python.prompt,"The provided text outlines the specifications for building a Python command-line interface (CLI) tool named ""pdd"". This tool is designed to be built using the `click` library for command handling and the `rich` library for pretty-printing console output. The project structure includes directories for source code, prompts, context, and data. The core functionality revolves around several commands: `generate` for creating runnable code from prompts; `example` for generating context examples from code; `test` for creating unit tests; `preprocess` for preparing prompts (with an XML sub-command); `fix` for resolving code errors based on unit test feedback (including a looping mechanism); `split` for dividing prompt files; `change` for modifying prompts; and `update` for refreshing prompt files. Additionally, the tool includes an `install_completion` command to manage shell auto-completion scripts. The instructions emphasize avoiding naming conflicts between CLI commands and internal functions, using relative imports, and leveraging specific helper modules for file loading and path construction.",f82bd3b41440348f58b8d2a98606b1edc4db6ff8eb20922551e7fd4f79c9003b
context/generate/2/cli.py,"This file defines the command-line interface (CLI) for a Prompt-Driven Development (PDD) tool using the `click` and `rich` libraries. It serves as the main entry point for the application, orchestrating various AI-driven development tasks. The CLI includes commands for generating code from prompts (`generate`), creating examples (`example`), generating unit tests (`test`), preprocessing prompts (`preprocess`), fixing code errors iteratively (`fix`), splitting complex prompts (`split`), modifying prompts based on changes (`change`), and updating prompts to reflect code modifications (`update`). It also includes utility functions for logging costs to a CSV file and installing shell tab completion. The tool integrates with LangChain, utilizing SQLite caching to optimize costs and performance. Each command handles file I/O, invokes specific backend logic (imported from other modules), and provides user feedback via a rich console interface.",b607c3e67702ad3bc5e61cc40ce85571e4ff09015de96016c8d7d56eb78b8c74
context/generate/5/generate_output_paths_python.prompt,"This document outlines the requirements for implementing a Python function named `generate_output_paths` for the Prompt-Driven Development (PDD) CLI tool. The function is responsible for constructing appropriate output filenames based on the specific command executed (e.g., `generate`, `fix`, `split`, `test`), user-provided options, and environment variables. The document includes a comprehensive description of the PDD tool, detailing its version (0.1.0), supported languages, and prompt file naming conventions. It elaborates on global options, AI model configuration, and cost tracking features. Crucially, it lists specific commands and their default output naming behaviors, such as `generate` defaulting to `<basename>.<ext>` and `fix` producing multiple outputs like fixed code and test files. The text also specifies environment variables that override default paths (e.g., `PDD_GENERATE_OUTPUT_PATH`). The implementation task requires a multi-step approach: analyzing output construction methods, listing default conventions and environment variables, identifying error cases, and finally writing the code to handle these logic flows robustly, ensuring correct handling of file extensions and directory paths.",78d3b23bb07b5a646a123abe3fa266dd4ee6b98953c84e01ae93f598f5a9dcae
context/generate/5/generate_output_paths.py,"This file defines a Python utility function named `generate_output_paths` designed to determine file paths for various outputs based on a specified command. The function takes inputs such as the command type (e.g., 'generate', 'test', 'fix', 'split'), a dictionary of explicit output locations, a base filename, a language, and a file extension. It employs helper logic to construct default filenames tailored to each command—for instance, appending '_example' for example generation or '_fixed' for code repair tasks. Additionally, the function checks for environment variables (prefixed with `PDD_`) to override default paths if explicit locations are not provided. It handles logic for both single-file outputs and multi-file outputs (as seen in 'fix' and 'split' commands), ensuring that if a directory path is provided, the appropriate filename is appended to it. The final output is a dictionary mapping output keys to their resolved string file paths.",78a4d97bd4c80d164c19e59efdf0153d6d7eafb75faf4f077ed630e15602ad17
context/commands/generate_example.py,"This Python script serves as a comprehensive usage example and documentation for the `pdd.commands.generate` module within the PDD CLI tool. It demonstrates how to programmatically invoke and utilize three primary Click commands: `generate`, `example`, and `test`. The script defines helper functions to set up an output directory and create sample prompt and code files (specifically a simple calculator module) to simulate a real workflow.

The `main` function orchestrates five specific examples: utilizing the `generate` command to create code from prompts with various options like incremental generation and environment variables; using the `example` command to produce usage examples for generated code; employing the `test` command to generate or enhance unit tests, including coverage-based improvements; invoking commands programmatically via Click context management; and using templates with variable substitution. Additionally, the file highlights that all these commands are decorated with `@track_cost` to monitor LLM API usage expenses. The script is designed to be executable, printing detailed explanations of command signatures, arguments, and expected context objects to the console.",8edf3edbed63b4ee85d1ec2b7638d37498ae85538e5967213aaa66fcb3cf5385
context/commands/connect_example.py,"This Python script serves as a usage example and test suite for the `pdd connect` command module within a Click-based CLI application. It demonstrates four key scenarios: integrating the command into a main CLI group, simulating server startup using mocked dependencies (such as `uvicorn` and `webbrowser`) to verify argument parsing without blocking execution, testing security warnings when binding to remote interfaces without authentication tokens, and overriding the default browser target with a custom frontend URL. The script utilizes `unittest.mock` to isolate the command logic from external side effects and `click.testing.CliRunner` to programmatically invoke and verify the command's behavior and output.",d2bcda4ae1e9de22d1b657c28045a0b122dc1ef22a77e4d670f41229779a6083
context/commands/analysis_example.py,"This Python script serves as a demonstration and test harness for programmatically invoking various analysis commands from the `pdd` (Prompt Driven Development) library. It specifically showcases how to use the `detect_change`, `conflicts`, `bug`, `crash`, and `trace` commands by calling their Click command callbacks directly within a Python script rather than via the command line.

The script sets up a Click `Context` object to simulate the runtime environment, injecting configuration parameters such as verbosity, model strength, and temperature. It defines a helper function, `clean_output`, to manage file cleanup and prevent overwrite prompts. The `main` function iterates through five distinct scenarios: detecting necessary prompt changes based on a description, identifying conflicts between two prompt files, generating a unit test to reproduce a bug, analyzing and fixing a crash caused by division by zero, and tracing execution flow back to a prompt. For each scenario, the script generates dummy input files (prompts, code, logs) in an output directory, executes the corresponding analysis callback, and prints the results.",01ff82910c50ec8f8cfd22f8e3b7c011f286a4b9f90e7dd5b2eb685c1f339e56
context/commands/fix_example.py,"This Python script serves as a comprehensive demonstration and documentation for the `pdd fix` command within the `pdd` package. The script illustrates how to automatically repair errors in code and unit tests using AI-driven analysis of error logs and original prompts. It begins by setting up a simulated environment, creating sample files including a calculator module with an intentional bug (missing division-by-zero handling), a corresponding unit test, and an error log.

The script defines a CLI wrapper using `click` and provides six distinct example functions showcasing different usage patterns: basic command invocation, iterative fixing with loop mode and verification programs, agentic fallback for complex errors using broader context, automatic submission of successful fixes to the PDD Cloud, cost tracking via CSV output, and programmatic usage within Python code. Each example demonstrates specific configuration options such as budget constraints, maximum attempts, model strength, and file output paths. The `main` function orchestrates the setup but leaves the actual execution of examples commented out to prevent accidental API costs, serving as a template for users to understand and implement automated code repair workflows.",b5230334f121f2257f6ee42f1ae038b3c9c441e513b082e9856576aa0eefdf83
context/commands/utility_example.py,"This Python script serves as a demonstration and documentation tool for the utility commands within the `pdd` package, specifically focusing on the `install_completion_cmd` and `verify` Click commands. It illustrates how to programmatically interact with these tools without executing them fully, thereby avoiding changes to the user's shell configuration or incurring API costs. The script sets up a local environment by creating a directory of dummy files—including a prompt, a buggy Python module (`calculator.py`), and a verification script—to simulate a real-world workflow for the `verify` command. It then prints detailed information to the console using the `rich` library, explaining the inputs, outputs, and command-line arguments for both utilities. The `install_completion_cmd` section explains how shell completion is installed, while the `verify` section details the iterative code fixing process using LLMs, including parameters like budget and max attempts. Additionally, the script documents the specific return value structure of the `verify` command, describing the tuple format containing results, costs, and model names. This file acts as both an executable example and a reference guide for developers integrating or using `pdd` utility commands.",fb7389c0ec837b8b6ee481e2eda04800f565fe247643f5bb9922dd1dfdab984a
context/commands/modify_example.py,"This Python script serves as a comprehensive demonstration and test suite for the `modify` module within the `pdd` CLI tool. It specifically showcases three core commands—`split`, `change`, and `update`—which are designed to manage and evolve LLM prompts and code. The script begins by establishing a local output directory structure and generating necessary sample artifacts, including prompt files (`.prompt`), source code (`.py`), and change instructions. It then executes four distinct scenarios using `Click`'s testing runner: `split`, which demonstrates breaking a large prompt into modular sub-prompts; `change` (manual mode), which modifies an existing prompt based on specific textual instructions (e.g., adding logging); `change` (batch mode), which illustrates processing multiple prompt modifications simultaneously using a CSV file; and `update`, which performs a reverse-synchronization where a prompt file is updated to reflect manual edits made to the corresponding source code. The script is self-contained, handling file creation, command execution, and output verification.",35b98245915a4e4fe341affaa67da6b9e4a61a5f1c7a9f0d1fba042a3c280dc1
context/commands/auth_example.py,"This Python script serves as an integration example for the 'auth' command group within a CLI tool named PDD. It sets up the execution environment by adding the project root to `sys.path` to ensure internal modules can be imported correctly. The script attempts to import `auth_group` from `pdd.commands.auth` and registers it under a main Click group named `cli`. 

Instead of executing a specific command logic directly, the script is designed to demonstrate the structure and availability of authentication commands. When run as the main program, it prints an introductory message and simulates running `pdd auth --help` to display the help output for the registered authentication commands. It concludes by printing several usage examples to the console, showing users how to perform actions such as interactive login, headless login for CI environments, checking authentication status, retrieving tokens, and logging out. The primary purpose is to verify that the authentication subcommands are correctly wired into the main CLI structure.",d1e77e2ab5a5b6f3c400a8a68a73bb633f6d45e39dd4a90306dcc18c729d9000
context/commands/maintenance_example.py,"This Python script serves as a comprehensive example demonstrating the usage of maintenance commands within the `pdd` package's CLI framework. It showcases three primary Click commands: `sync` (for synchronizing prompts with code and tests), `auto-deps` (for analyzing and injecting dependencies into prompt files), and `setup` (for running an interactive setup utility). The script includes helper functions to create a mock project environment with sample prompt and code files. It then defines separate functions (`example_sync_command`, `example_auto_deps_command`, `example_setup_command`) that utilize `unittest.mock` to simulate the execution of these commands without making actual LLM calls or system modifications. Additionally, it includes an example of how to invoke these commands programmatically, bypassing the CLI parsing while maintaining the Click context.",f347251582821c4347664f674fdd6d5549bb14ba1d0e8bc428cffb9abedbfed3
context/commands/templates_example.py,"This Python script serves as a demonstration and documentation for the `pdd.commands.templates` module, specifically showcasing the CLI commands used to manage PDD templates. It utilizes the `click.testing.CliRunner` to programmatically invoke and display the output of three primary subcommands: `list`, `show`, and `copy`. The script defines separate functions for each operation: `example_list_templates` illustrates how to list available templates with options for JSON formatting and tag filtering; `example_show_template` demonstrates how to retrieve detailed metadata, variable definitions, and usage examples for a specific template; and `example_copy_template` shows how to duplicate a template file to a local destination directory. The `main` function orchestrates these examples, providing context on the purpose of the templates command group, which is designed to facilitate the discovery, inspection, and reuse of prompt templates within the PDD framework.",017d7a9cbdec60dd67cee662f3b21a68638137cd086fb335ef92889e1d91b09a
context/commands/misc_example.py,"This Python script serves as a comprehensive example and test suite for the `pdd.commands.misc.preprocess` module, specifically demonstrating the functionality of the `preprocess` Click command. The script illustrates how to prepare prompt files for Large Language Models (LLMs) by handling directives such as file includes, comments, shell commands, and web scraping locally without API calls. It defines a setup function to generate sample prompt and text files, followed by five distinct example functions that simulate command-line execution using `CliRunner`. These examples cover basic preprocessing, XML delimiter insertion, curly bracket doubling (with exclusion logic for template variables), recursive file inclusion, and a combination of these options. The script concludes with a main execution block that runs all examples sequentially, printing the results and exit codes to the console, thereby verifying the preprocessor's capabilities in handling various formatting and content integration tasks.",7d7985d21dd7b962257856cb92d13e26c016bc09ce14aaac4c68baaead97dc4b
context/commands/__init___example.py,"This Python script serves as a demonstration and usage example for the `register_commands` module within a project named PDD (Python Development Driver). The file illustrates how to set up a Click-based Command Line Interface (CLI) by importing and utilizing the `register_commands` function to attach various subcommands to a main `click.Group`. It defines a primary CLI entry point (`cli`) and a secondary custom example (`custom_cli`), showing that the registration process works on any Click group. The script documents the specific categories of commands available, including Generation, Fixing, Modification, Maintenance, Analysis, Miscellaneous, Utility, and Templates. When executed directly, the script prints a formatted list of these registered commands organized by category, displaying their help text summaries, and provides usage examples for invoking the CLI tools. Essentially, it acts as both a functional test of the command registration logic and a documentation reference for the available CLI capabilities.",cc6e0de7d94b1dddda35d9c8ecb800710263d928191f1d4cb2400bae36c31cb4
context/commands/sessions_example.py,"This file contains a Python test/demonstration script designed to verify the functionality of a CLI module, specifically the `pdd.commands.sessions` component. The script primarily focuses on mocking external dependencies to allow standalone execution without a real backend connection. It mocks several modules within the `pdd` namespace, including `core.cloud`, `remote_session`, and `utils`, replacing them with `unittest.mock.MagicMock` objects. Key mocked components include `CloudConfig` for handling JWT tokens and `RemoteSessionManager` for retrieving session data. The script defines a `DummySession` class and populates a fake dataset containing sample session information (IDs, project names, URLs, statuses). It implements asynchronous mock functions (`mock_list_sessions` and `mock_get_session`) to simulate API responses. The `run_example` function utilizes `click.testing.CliRunner` to programmatically invoke CLI commands (`list` and `info`) and print the results to the console using the `rich` library. This setup allows developers to visualize the output of commands like `sessions list` (in both table and JSON formats) and `sessions info <id>`, as well as handle error cases like missing sessions, ensuring the CLI interface behaves correctly under controlled conditions.",b4d9f8c45f3bf02f9e531bd411937872dfc77705a225a03f56df86c197da13bf
context/commands/output/change_request.txt,"The provided file content is extremely brief, consisting of a single directive: ""Update all functions to include type hints."" This instruction serves as a coding standard or refactoring task, likely intended for a software development context using a statically typed language or a dynamically typed language that supports optional typing, such as Python (with type hints introduced in PEP 484) or TypeScript. The goal of this directive is to improve code readability, maintainability, and error checking by explicitly declaring the expected data types for function arguments and return values. By implementing type hints, developers can leverage static analysis tools and IDE features to catch type-related bugs early in the development process, rather than at runtime. This task implies a review of an existing codebase where functions currently lack these annotations, requiring a systematic update to modernize the code and align it with best practices for type safety.",61b4a2bac1dc072dc710a4f5dec21e2ab76a153b70bd72e5e5dea884fd77965f
context/commands/output/logic.prompt,"The provided file content is an extremely brief pseudocode snippet consisting of two sequential instructions. The first instruction initializes a variable named 'x' by assigning it the integer value of 10. The second instruction outputs or displays the current value of that variable 'x' to the console or standard output. Essentially, this script demonstrates a fundamental programming concept of variable assignment followed by data retrieval and display. It represents the simplest form of a program, often used as a 'Hello World' equivalent for variable manipulation, verifying that the environment can store data in memory and then access that memory to show the result to the user.",ace8acbbfc091c05d9ba92195da1bda7e432f97d0e30b5a3e49cc2b0181804d2
context/commands/output/div.py,"The provided file content consists of a single, concise Python function definition named `div`. This function is designed to perform a basic arithmetic division operation. It accepts two parameters, `a` and `b`, which represent the dividend and the divisor, respectively. The function body contains a single return statement that executes the division of `a` by `b` using the standard division operator (`/`) and returns the resulting quotient. The code is written in a compact, one-line format, which is syntactically valid in Python for simple functions. There is no error handling included within this snippet, such as checks for division by zero, nor are there any type hints or docstrings provided to further explain the function's usage or constraints. Essentially, it is a minimal implementation of a division utility.",105a3a82396b72c9107bd28320c807d6b35fd586fb309894b05836eabc825209
context/commands/output/run_div_fixed.py,"The provided file content is a brief Python script designed to test the functionality of a custom division function. It begins by importing a function named `div` from a module also named `div`. The core purpose of the script is demonstrated in the subsequent lines, where it attempts to execute a division operation using `div(1, 0)`. This specific test case, involving a denominator of zero, is explicitly commented as a check to ensure that the program handles division by zero gracefully without crashing. The script concludes by printing the result of this function call to the standard output, allowing the user to verify the return value or error handling behavior of the imported `div` function.",01059e68c10a893269323bf042e8f2978e0d51f0fb4ccd910eb4b1f35f454127
context/commands/output/trace.log,"The provided file content appears to be a brief log or metadata snippet related to an interaction with an AI model. It contains three specific data points: the prompt line number, which is recorded as 2; the total cost of the operation, amounting to $0.001754; and the specific model identifier used for the task, listed as 'vertex_ai/gemini-3-flash-preview'. This snippet likely serves as a usage record for tracking expenses and model performance metrics within a larger system.",88bbf5715e39ce496a6246745da46405f99a60cc938cc496fef2f2a20ae092ef
context/commands/output/theme_light.txt,"The provided file content is extremely brief, consisting of a single directive: ""Use light theme default."" This statement appears to be a configuration setting or a user preference instruction, likely intended for a software application, website, or operating system interface. It explicitly specifies that the visual theme should be set to ""light"" mode by default, rather than a dark mode or a system-adaptive setting. In the context of UI/UX design or software development, this line would typically be found in a settings file (such as a JSON, YAML, or INI configuration), a CSS variable definition, or a documentation snippet guiding users or developers on how to establish the initial visual appearance of the interface. The instruction implies a binary choice between light and dark themes, prioritizing the former as the standard starting point for the user experience.",9b33e7cdd8868f0132464fbb58887101dd2ff07cfe5fa62ba59cfa30e3ac5bc9
context/commands/output/div.prompt,"The provided file content is extremely brief, consisting solely of the phrase ""Function to divide numbers."" This suggests that the file likely contains source code, documentation, or a placeholder for a mathematical operation intended to perform division. In a programming context, this would typically involve a function taking two arguments—a dividend and a divisor—and returning their quotient, potentially handling edge cases such as division by zero. However, without the actual code implementation, the specific language, syntax, or logic cannot be determined. The text serves primarily as a high-level description or comment indicating the intended purpose of the associated code block.",16b4531b811525ca4446b4ef06a459933d82e0462a67b45b300f0ee903e49afa
context/commands/output/div_fixed.py,"The provided file content defines a single Python function named `div` designed to perform division between two floating-point numbers. The function accepts two arguments: `a`, representing the numerator, and `b`, representing the denominator. A key feature of this function is its built-in error handling for division by zero. Before attempting the division operation, the code explicitly checks if the denominator `b` is equal to zero. If this condition is met, the function safely returns `0.0` instead of raising a runtime exception, ensuring the program continues to execute smoothly. Otherwise, it returns the result of `a` divided by `b`. The function is also documented with a docstring that details its purpose, the expected types for its arguments, and the nature of its return value, clarifying that it returns a float representing the quotient or a default value of 0.0 in the case of a zero denominator.",2942eecdac531d8106906eb2e0b44da0c5e3c7755ca06a668ecb0bb128882430
context/commands/output/detect_results.csv,"The provided file contains a set of instructions for updating specific prompt files, formatted as a CSV dataset with columns for the prompt name and the corresponding change instructions. The file lists two entries: 'math_ops.prompt' and 'string_ops.prompt'. Both entries share a nearly identical set of modification requests aimed at standardizing the output of an LLM. Specifically, the instructions require updating the persona to that of an 'expert Python engineer' and inserting a new section titled 'Code Style Requirements' before the function description. Within this new section, a specific rule must be added mandating that all functions be fully type-hinted, covering both parameters and return types. Furthermore, the goal statements in both prompts need to be modified to explicitly request a 'fully type-hinted Python function.' While the core logic of the prompts differs—one focuses on adding numbers and the other on concatenating strings—the primary objective of these changes is to enforce strict Python type-hinting standards across the generated code.",5f25d97cc5a6a09ee4f99655b102db3080a27b64cbcbe76c7cb3621eb75f4e47
context/commands/output/expected.txt,"The provided file content is extremely minimal, consisting solely of the single integer digit ""6"". There is no surrounding context, metadata, or additional text to indicate the significance of this number. It could represent a variety of data points such as a count, an index, a score, a version number, or a specific value in a larger dataset. Without further information or a larger file structure, it is impossible to determine the specific purpose or meaning of this datum. Essentially, the file contains a single numeric character with no other attributes or descriptive elements.",e7f6c011776e8db7cd330b54174fd76f7d0216b612387a5ffcfb81e6f0919683
context/commands/output/conflicts.csv,"The provided file contents outline specific change instructions for two prompt configuration files related to UI theming: 'theme_dark.txt' and 'theme_light.txt'. The instructions aim to resolve conflicts between static default settings for Dark and Light themes by introducing dynamic or conditional logic. For the Dark Theme prompt, the recommendation is to move away from a static default and instead implement logic based on the user's system preference, specific time ranges (18:00 to 06:00), or environmental context (low-light). Similarly, the Light Theme prompt instructions suggest resolving conflicts by deferring to system preferences (or lack thereof), using time-based logic (06:01 to 17:59), or establishing the light theme as a standard baseline. Both sets of instructions focus on creating a more adaptive and conflict-free user experience by removing rigid defaults.",43500019d4c7e73e7e3f3a6ea3ac790dc561de52537a6dd8050e451892b3a8be
context/commands/output/run_calc.py,"The provided file content is a very brief Python script that demonstrates the usage of a specific function imported from an external module. The script begins by importing a function named 'mul' from a module named 'calc'. This suggests the existence of a separate file or package named 'calc' which likely contains mathematical operations, specifically multiplication in this context. Following the import statement, the script executes a print function call. Inside this print statement, the imported 'mul' function is invoked with two integer arguments, 2 and 3. The purpose of this line is to calculate the product of these two numbers and display the result to the standard output. Essentially, this snippet serves as a simple driver code or a test case to verify that the multiplication function within the 'calc' module is working correctly, expecting an output of 6 when executed.",9ca9d810b8ecb23e3a9e6d19519cdf4d6f4f3faa0d0adfd702eb6a174f2b1dc3
context/commands/output/crash.log,"The provided file content consists of a single line representing a standard Python exception message: ""ZeroDivisionError: division by zero"". This error typically occurs during the execution of a Python script when a mathematical operation attempts to divide a number by zero, which is mathematically undefined and handled as a runtime exception in Python. The presence of this specific string suggests that the file is likely a log file, a traceback dump, or a console output capture from a failed program execution. It indicates a bug or an unhandled edge case in the code where the denominator in a division operation evaluated to zero. To resolve this, the underlying code would need to be inspected to ensure that variables used as divisors are checked for zero values before the division takes place, or the operation should be wrapped in a try-except block to handle the error gracefully.",b51373d22f5130e4b096f5e6e16a2f7e9164c3a1dac32ff9b24ce598de4952bb
context/commands/output/logic.py,"The provided file content is a very brief Python script consisting of two lines of code. The first line initializes a variable named 'x' and assigns it the integer value of 10. The second line utilizes the built-in 'print' function to output the value of the variable 'x' to the console. Essentially, this script demonstrates basic variable assignment and output operations in Python. When executed, the program will simply display the number '10'. The code is syntactically correct for Python 3 and serves as a fundamental example of storing data in a variable and retrieving it for display.",0c9d7d4b0fe53a09a6ca894c42916bd73a1ba21e039cc25ae91bf074795a48bc
context/commands/output/run_div.py,"The provided file content consists of a single line of Python code that attempts to execute a function call and print the result. Specifically, it calls a function named `div` with two integer arguments, 1 and 0, and wraps this call within a `print` statement. This code snippet appears to be a test case or a demonstration of error handling, as dividing by zero is mathematically undefined and typically raises a `ZeroDivisionError` in Python. The code implies the existence of a `div` function defined elsewhere, as it is not defined within the provided snippet. If the `div` function performs standard division without internal exception handling, this script will crash. Alternatively, the `div` function might be designed to handle such edge cases gracefully. The brevity of the code suggests it is likely part of a larger debugging session, a unit test for division logic, or a minimal reproduction example for a bug report concerning division by zero errors.",f2a81ccd3d52f7531a31c0f4c3e3092c5208cf73b32de1942b4d04994cafe00f
context/commands/output/calc.prompt,"The provided file content is extremely brief and describes a single, specific purpose: a function designed to perform multiplication on numbers. It does not contain actual code, implementation details, parameter definitions, or return type specifications. Instead, it serves as a high-level descriptor or a placeholder comment indicating that the associated code block or file is intended to handle the arithmetic operation of multiplying numerical values.",7adc9fd359d54a2dc71c6e56ea144316150763d2d5e4813c6c195d211f782565
context/commands/output/test_calc_bug.py,"This file contains a suite of unit tests for a function named `mul`, which is imported from a module named `calc`. The tests are implemented using the `pytest` framework and are designed to verify the correctness of a multiplication operation. The first test, `test_mul_basic_multiplication`, specifically targets a known discrepancy where the function might be incorrectly performing addition (returning 5 instead of 6 for inputs 2 and 3). Subsequent tests cover fundamental mathematical properties: `test_mul_zero_property` checks the absorbing property of zero, `test_mul_identity_property` verifies the identity property of one, and `test_mul_negative_numbers` ensures correct handling of negative integers. Finally, `test_mul_various_inputs` utilizes `pytest.mark.parametrize` to efficiently test a broader range of scenarios, including floating-point numbers and larger integers, ensuring the function is robust across different numeric types.",bc0273e41fca3a47ee7c3653293afb0aaef73145e8b3ac9d6ed8e42cb482eadb
context/commands/output/calc.py,"The provided file content consists of a single line of Python code defining a function named 'mul'. This function takes two arguments, 'a' and 'b'. However, the implementation contains a logical error or bug, as indicated by the inline comment '# Bug'. Instead of performing multiplication as the function name suggests, the code executes an addition operation, returning the sum of 'a' and 'b' ('a + b') rather than their product ('a * b'). This snippet serves as a simple example of a semantic error where the code runs successfully but produces incorrect results based on the intended functionality implied by the identifier name.",cc5ff03432397d68c660d212e29e3aac82238b88db26099669fdb09f68434f23
context/commands/output/math_ops.prompt,"The provided file content is extremely brief, consisting of a single imperative sentence: ""Create a function to add numbers."" This statement serves as a high-level instruction or a prompt for a programming task. It implies a requirement to write code, likely in a specific programming language (though none is specified), that defines a reusable block of code capable of accepting numerical inputs and returning their sum. The instruction is fundamental to computer science and software development, representing one of the simplest forms of arithmetic operations implemented as a function. Due to the lack of context, specific details such as the number of arguments, the data types (integers, floats), or the return format are left open to interpretation. Essentially, this is a foundational coding exercise or a placeholder comment often found in educational materials or initial project scaffolding.",d14653a7306d57b9a97a804ba24b625b7890f5ab004976e5184dd70bbda40951
context/commands/output/string_ops.prompt,"The provided file content is extremely brief and consists of a single instruction or prompt. It requests the creation of a programming function designed to concatenate strings. String concatenation is a fundamental operation in computer science where two or more character strings are joined end-to-end to form a new, single string. For example, joining 'Hello' and 'World' would result in 'HelloWorld'. While the specific programming language is not defined in the prompt, this is a common task in languages such as Python, JavaScript, Java, and C++. A typical implementation would involve taking two or more string arguments as input and returning a combined string as the output. The simplicity of the request suggests it could be a coding exercise, a placeholder for a larger project, or a test case for an AI code generation tool.",13fd51e92b7069d0e5bef1d9794b46d777b0c4bf28593c647cdef98d7dbb3654
context/commands/output/actual.txt,"The provided file content is extremely minimal, consisting solely of the single integer digit ""5"". There is no surrounding context, metadata, or additional text to indicate the significance of this number. It could represent a count, a score, an index, a configuration value, or simply a placeholder. Without further information regarding the file's origin, purpose, or associated data structures, it is impossible to derive a deeper meaning. The content is purely numerical and lacks any narrative, code structure, or descriptive elements. In essence, the file is a plain text representation of the number five.",ef2d127de37b942baad06145e54b0c619a1f22327b2ebbcfbec78f5564afe39d
context/commands/output/theme_dark.txt,"The provided file content is extremely brief, consisting of a single directive: ""Use dark theme default."" This statement likely serves as a configuration setting or a user preference indicator within a larger software application or system interface. It explicitly instructs the system to apply a ""dark theme"" as the standard visual mode, overriding any light or system-based theme settings that might otherwise be the default. In the context of UI/UX design, this suggests a preference for a color scheme that uses light-colored text, icons, and graphical user interface elements on a dark background. This is often chosen to reduce eye strain in low-light conditions or simply for aesthetic preference. While the file lacks further context, technical specifications, or code implementation details, its intent is unambiguous: the visual presentation should default to a dark mode.",e758776396d5f9a904e2ec2f56d4d5979636db17923bf4f3ac81db3f555e1080
