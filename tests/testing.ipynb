{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Applications/anaconda3/envs/pdd/lib/python312.zip', '/Applications/anaconda3/envs/pdd/lib/python3.12', '/Applications/anaconda3/envs/pdd/lib/python3.12/lib-dynload', '', '/Applications/anaconda3/envs/pdd/lib/python3.12/site-packages', '/Users/gltanaka/Documents/pdd/staging/pdd']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Define the project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../pdd'))\n",
    "\n",
    "# Add the project root directory to the PYTHONPATH\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Verify the project root is added\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_generator import context_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote the example to ../context/context_generator_example.py.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_generator(\"../pdd/context_generator.py\", \"../context/context_generator_example.py\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote the example to ../context/postprocess_example.py.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_generator(\"../pdd/postprocess.py\", \"../context/postprocess_example.py\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote the example to ../context/preprocess_example.py.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_generator(\"../pdd/preprocess.py\", \"../context/preprocess_example.py\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocess import postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_example = '''To implement the `context_generator` function as described, we will follow the steps outlined in your request. Below is the complete implementation of the function, which includes reading a Python file, preprocessing it, generating a prompt for the model, invoking the model, and writing the output to a specified file.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from preprocess import preprocess\n",
    "\n",
    "def context_generator(python_filename: str, output_filename: str, force: bool = False) -> bool:\n",
    "    # Step 1: Read the Python file\n",
    "    try:\n",
    "        with open(python_filename, 'r') as file:\n",
    "            python_code = file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {python_filename} does not exist.\")\n",
    "        return False\n",
    "\n",
    "    # Step 2: Preprocess the file\n",
    "    processed_content = preprocess(python_filename)\n",
    "\n",
    "    # Step 3: Generate a prompt for GPT-4\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert Python engineer. Based on the following Python code, generate a concise example of how to use the module properly.\n",
    "\n",
    "    Python Code:\n",
    "    ```python\n",
    "    {processed_content}\n",
    "    ```\n",
    "\n",
    "    Please provide the example in a Python code block.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 4: Create the LCEL template\n",
    "    prompt_template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "    # Step 5: Initialize the model\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Step 6: Combine with a model and parser\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    # Step 7: Run the template\n",
    "    result = chain.invoke({\"python_code\": processed_content, \"python_filename\": python_filename})\n",
    "\n",
    "    # Step 8: Process the output to comment out non-Python code\n",
    "    output_lines = result.splitlines()\n",
    "    commented_output = []\n",
    "    in_code_block = False\n",
    "\n",
    "    for line in output_lines:\n",
    "        if line.strip() == '```':\n",
    "            in_code_block = not in_code_block\n",
    "            commented_output.append(line)  # Keep the triple backticks\n",
    "        elif in_code_block:\n",
    "            commented_output.append(line)  # Keep the code lines\n",
    "        else:\n",
    "            commented_output.append(f\"# {line}\")  # Comment out non-code lines\n",
    "\n",
    "    final_output = \"\\n\".join(commented_output)\n",
    "\n",
    "    # Step 9: Write the output to the specified file\n",
    "    if os.path.exists(output_filename) and not force:\n",
    "        overwrite = input(f\"The file {output_filename} already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(\"Operation cancelled. The output file was not overwritten.\")\n",
    "            return False\n",
    "\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        output_file.write(final_output)\n",
    "\n",
    "    print(f\"Successfully wrote the example to {output_filename}.\")\n",
    "    return True\n",
    "```\n",
    "\n",
    "### Explanation of the Code:\n",
    "1. **File Reading**: The function attempts to read the specified Python file. If the file does not exist, it prints an error message and returns `False`.\n",
    "2. **Preprocessing**: It calls the `preprocess` function to process the content of the Python file.\n",
    "3. **Prompt Generation**: A prompt is created that instructs the model to generate a concise example based on the provided Python code.\n",
    "4. **Model Initialization**: The `ChatOpenAI` model is initialized with the specified parameters.\n",
    "5. **Chain Creation**: The prompt template is combined with the model and output parser.\n",
    "6. **Invocation**: The chain is invoked with the processed content and filename.\n",
    "7. **Output Processing**: The output is processed to comment out non-Python lines, ensuring that only valid Python code remains executable.\n",
    "8. **File Writing**: The function checks if the output file exists and prompts the user if they want to overwrite it unless `force` is `True`. It then writes the final output to the specified file.\n",
    "\n",
    "### Usage:\n",
    "You can call this function by providing the necessary arguments, like so:\n",
    "\n",
    "```python\n",
    "context_generator('your_python_file.py', 'output_example.py', force=False)\n",
    "```\n",
    "\n",
    "Make sure to replace `'your_python_file.py'` and `'output_example.py'` with the actual file names you want to use.\n",
    "To implement the context_generator function as described, we will follow the steps outlined in   \n",
    "your request. Below is the complete implementation of the function, which includes reading a     \n",
    "Python file, preprocessing it, generating a prompt for the model, invoking the model, and writing\n",
    "the output to a specified file.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# To implement the `context_generator` function as described, we will follow the steps outlined in your request. Below is the complete implementation of the function, which includes reading a Python file, preprocessing it, generating a prompt for the model, invoking the model, and writing the output to a specified file.\n",
      "\n",
      "# ```python\n",
      "import os\n",
      "import json\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_core.output_parsers import StrOutputParser\n",
      "from preprocess import preprocess\n",
      "\n",
      "def context_generator(python_filename: str, output_filename: str, force: bool = False) -> bool:\n",
      "    # Step 1: Read the Python file\n",
      "    try:\n",
      "        with open(python_filename, 'r') as file:\n",
      "            python_code = file.read()\n",
      "    except FileNotFoundError:\n",
      "        print(f\"Error: The file {python_filename} does not exist.\")\n",
      "        return False\n",
      "\n",
      "    # Step 2: Preprocess the file\n",
      "    processed_content = preprocess(python_filename)\n",
      "\n",
      "    # Step 3: Generate a prompt for GPT-4\n",
      "    prompt = f\"\"\"\n",
      "    You are an expert Python engineer. Based on the following Python code, generate a concise example of how to use the module properly.\n",
      "\n",
      "    Python Code:\n",
      "    ```python\n",
      "    {processed_content}\n",
      "    ```\n",
      "\n",
      "    Please provide the example in a Python code block.\n",
      "    \"\"\"\n",
      "\n",
      "    # Step 4: Create the LCEL template\n",
      "    prompt_template = PromptTemplate.from_template(prompt)\n",
      "\n",
      "    # Step 5: Initialize the model\n",
      "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
      "\n",
      "    # Step 6: Combine with a model and parser\n",
      "    chain = prompt_template | llm | StrOutputParser()\n",
      "\n",
      "    # Step 7: Run the template\n",
      "    result = chain.invoke({\"python_code\": processed_content, \"python_filename\": python_filename})\n",
      "\n",
      "    # Step 8: Process the output to comment out non-Python code\n",
      "    output_lines = result.splitlines()\n",
      "    commented_output = []\n",
      "    in_code_block = False\n",
      "\n",
      "    for line in output_lines:\n",
      "        if line.strip() == '```':\n",
      "            in_code_block = not in_code_block\n",
      "            commented_output.append(line)  # Keep the triple backticks\n",
      "        elif in_code_block:\n",
      "            commented_output.append(line)  # Keep the code lines\n",
      "        else:\n",
      "            commented_output.append(f\"# {line}\")  # Comment out non-code lines\n",
      "\n",
      "    final_output = \"\n",
      "\".join(commented_output)\n",
      "\n",
      "    # Step 9: Write the output to the specified file\n",
      "    if os.path.exists(output_filename) and not force:\n",
      "        overwrite = input(f\"The file {output_filename} already exists. Do you want to overwrite it? (y/n): \")\n",
      "        if overwrite.lower() != 'y':\n",
      "            print(\"Operation cancelled. The output file was not overwritten.\")\n",
      "            return False\n",
      "\n",
      "    with open(output_filename, 'w') as output_file:\n",
      "        output_file.write(final_output)\n",
      "\n",
      "    print(f\"Successfully wrote the example to {output_filename}.\")\n",
      "    return True\n",
      "# ```\n",
      "\n",
      "# ### Explanation of the Code:\n",
      "# 1. **File Reading**: The function attempts to read the specified Python file. If the file does not exist, it prints an error message and returns `False`.\n",
      "# 2. **Preprocessing**: It calls the `preprocess` function to process the content of the Python file.\n",
      "# 3. **Prompt Generation**: A prompt is created that instructs the model to generate a concise example based on the provided Python code.\n",
      "# 4. **Model Initialization**: The `ChatOpenAI` model is initialized with the specified parameters.\n",
      "# 5. **Chain Creation**: The prompt template is combined with the model and output parser.\n",
      "# 6. **Invocation**: The chain is invoked with the processed content and filename.\n",
      "# 7. **Output Processing**: The output is processed to comment out non-Python lines, ensuring that only valid Python code remains executable.\n",
      "# 8. **File Writing**: The function checks if the output file exists and prompts the user if they want to overwrite it unless `force` is `True`. It then writes the final output to the specified file.\n",
      "\n",
      "# ### Usage:\n",
      "# You can call this function by providing the necessary arguments, like so:\n",
      "\n",
      "# ```python\n",
      "# context_generator('your_python_file.py', 'output_example.py', force=False)\n",
      "# ```\n",
      "\n",
      "# Make sure to replace `'your_python_file.py'` and `'output_example.py'` with the actual file names you want to use.\n",
      "# To implement the context_generator function as described, we will follow the steps outlined in   \n",
      "# your request. Below is the complete implementation of the function, which includes reading a     \n",
      "# Python file, preprocessing it, generating a prompt for the model, invoking the model, and writing\n",
      "# the output to a specified file.\n"
     ]
    }
   ],
   "source": [
    "print(postprocess(output_example,\"python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_non_python(input_string):\n",
    "    lines = input_string.split('\\n')\n",
    "    output_lines = []\n",
    "    in_python_block = False\n",
    "    python_block_delimiter = '```'\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        # Check for Python code block delimiters\n",
    "        if stripped_line.startswith(python_block_delimiter):\n",
    "            in_python_block = not in_python_block\n",
    "            output_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        # If we're in a Python block, add the line as is\n",
    "        if in_python_block:\n",
    "            output_lines.append(line)\n",
    "        else:\n",
    "            # If we're not in a Python block, comment out the line\n",
    "            output_lines.append(f\"# {line}\")\n",
    "    \n",
    "    return '\\n'.join(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comment_non_python(output_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docu_care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
